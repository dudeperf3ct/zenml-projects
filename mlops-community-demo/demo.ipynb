{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ZenML Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Local Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "client = Client()\n",
    "client.activate_stack(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline, step\n",
    "\n",
    "\n",
    "@step\n",
    "def my_step():\n",
    "    print(\"Hello world!\")\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline():\n",
    "    my_step()\n",
    "\n",
    "\n",
    "my_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def some_other_step():\n",
    "    print(\"This is another step!\")\n",
    "\n",
    "\n",
    "some_other_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "client = Client()\n",
    "client.activate_stack(\"az_k8s_pigeon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline, step\n",
    "from settings import docker_settings, kubernetes_settings\n",
    "\n",
    "@step(settings={\"orchestrator.kubernetes\": kubernetes_settings})\n",
    "def k8s_step():\n",
    "    print(\"Training model...\")\n",
    "\n",
    "\n",
    "@pipeline(settings={\"docker\": docker_settings})\n",
    "def k8s_pipeline():\n",
    "    k8s_step()\n",
    "\n",
    "\n",
    "k8s_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import docker_settings, kubernetes_settings\n",
    "from zenml import step\n",
    "\n",
    "\n",
    "@step(\n",
    "    settings={\"orchestrator\": kubernetes_settings, \"docker\": docker_settings}\n",
    ")\n",
    "def k8s_step():\n",
    "    print(\"Training model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k8s_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's get my cat ready for GenAI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing process completed.\n"
     ]
    }
   ],
   "source": [
    "# resize the images so that they're maximum 1000 pixels on the longest side\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "data_dir = \"/home/strickvl/coding/zenml-projects/mlops-community-demo/data/cats_mixed\"\n",
    "\n",
    "target_dir = \"/home/strickvl/coding/zenml-projects/mlops-community-demo/data/cats_resized\"\n",
    "\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith((\".jpeg\", \".jpg\", \".png\")):\n",
    "        image_path = os.path.join(data_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Calculate the scaling factor\n",
    "        scale = min(1000 / width, 1000 / height)\n",
    "        \n",
    "        # Calculate new dimensions while maintaining aspect ratio\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        \n",
    "        # Resize the image\n",
    "        resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(target_dir, filename), resized_image)\n",
    "        # print(f\"Resized image: {filename}\")\n",
    "\n",
    "print(\"Resizing process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name      Size\n",
      "blupus-08.jpeg  206.30 KB\n",
      "aria-06.jpeg    110.77 KB\n",
      "blupus-01.jpeg  231.38 KB\n",
      "aria-01.jpeg    220.62 KB\n",
      "blupus-11.jpeg  251.65 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "resized_dir = \"/home/strickvl/coding/zenml-projects/mlops-community-demo/data/cats_resized\"\n",
    "\n",
    "image_data = []\n",
    "\n",
    "for filename in os.listdir(resized_dir):\n",
    "    if filename.endswith((\".jpeg\", \".jpg\", \".png\")):\n",
    "        image_path = os.path.join(resized_dir, filename)\n",
    "        file_size = os.path.getsize(image_path)  # Size in bytes\n",
    "        file_size_kb = file_size / 1024  # Convert bytes to kilobytes\n",
    "        image_data.append([filename, f\"{file_size_kb:.2f} KB\"])\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(image_data[:5], headers=[\"Image Name\", \"Size\"], tablefmt=\"plain\", colalign=(\"left\", \"left\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Local) Data Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation with Albumentations\n",
    "\n",
    "Using the Albumentations library to make a variety of transformations for our\n",
    "images to augment our dataset a little. We'll avoid things that crop the image,\n",
    "but we'll do things like change the brightness, contrast, and saturation. We'll\n",
    "also do things like add noise, blur, and sharpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RandomGamma(p=0.2),\n",
    "    A.RandomShadow(p=0.2),\n",
    "    A.RandomRain(p=0.2),\n",
    "    A.RandomSnow(p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved augmented image: augmented_blupus-08.jpeg\n",
      "Saved augmented image: augmented_aria-06.jpeg\n",
      "Saved augmented image: augmented_blupus-01.jpeg\n",
      "Saved augmented image: augmented_aria-01.jpeg\n",
      "Saved augmented image: augmented_blupus-11.jpeg\n",
      "Saved augmented image: augmented_blupus-14.jpeg\n",
      "Saved augmented image: augmented_aria-12.jpeg\n",
      "Saved augmented image: augmented_aria-14.jpeg\n",
      "Saved augmented image: augmented_blupus-16.jpeg\n",
      "Saved augmented image: augmented_blupus-03.jpeg\n",
      "Saved augmented image: augmented_blupus-17.jpeg\n",
      "Saved augmented image: augmented_aria-07.jpeg\n",
      "Saved augmented image: augmented_aria-10.jpeg\n",
      "Saved augmented image: augmented_blupus-13.jpeg\n",
      "Saved augmented image: augmented_aria-02.jpeg\n",
      "Saved augmented image: augmented_blupus-04.jpeg\n",
      "Saved augmented image: augmented_aria-05.jpeg\n",
      "Saved augmented image: augmented_blupus-07.jpeg\n",
      "Saved augmented image: augmented_blupus-09.jpeg\n",
      "Saved augmented image: augmented_blupus-06.jpeg\n",
      "Saved augmented image: augmented_blupus-10.jpeg\n",
      "Saved augmented image: augmented_aria-03.jpeg\n",
      "Saved augmented image: augmented_blupus-12.jpeg\n",
      "Saved augmented image: augmented_aria-04.jpeg\n",
      "Saved augmented image: augmented_aria-15.jpeg\n",
      "Saved augmented image: augmented_blupus-05.jpeg\n",
      "Saved augmented image: augmented_aria-08.jpeg\n",
      "Saved augmented image: augmented_aria-13.jpeg\n",
      "Saved augmented image: augmented_aria-11.jpeg\n",
      "Saved augmented image: augmented_blupus-15.jpeg\n",
      "Saved augmented image: augmented_aria-09.jpeg\n",
      "Saved augmented image: augmented_blupus-02.jpeg\n",
      "Augmentation process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_dir = \"/home/strickvl/coding/zenml-projects/mlops-community-demo/data/cats_mixed\"\n",
    "target_dir = \"/home/strickvl/coding/zenml-projects/mlops-community-demo/data/blupus\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith((\".jpeg\", \".jpg\", \".png\")):\n",
    "        image_path = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            # Apply the transformation\n",
    "            transformed = transform(image=image_np)\n",
    "            transformed_image = transformed[\"image\"]\n",
    "            \n",
    "            # Convert back to PIL Image\n",
    "            transformed_pil = Image.fromarray(transformed_image)\n",
    "            \n",
    "            # Save the augmented image\n",
    "            augmented_filename = f\"augmented_{filename}\"\n",
    "            augmented_path = os.path.join(target_dir, augmented_filename)\n",
    "            transformed_pil.save(augmented_path)\n",
    "            \n",
    "            print(f\"Saved augmented image: {augmented_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "print(\"Augmentation process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a Model: Dreambooth Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time: Make me a Blupus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-community",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
