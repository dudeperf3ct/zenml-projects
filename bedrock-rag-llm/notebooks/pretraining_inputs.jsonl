{"input": "---\ndescription: Welcome to ZenML!\n---\n\n# \u2b50 Introduction\n\n**ZenML** is an extensible, open-source MLOps framework for creating portable, production-ready machine learning pipelines. By decoupling infrastructure from code, ZenML enables developers across your organization to collaborate more effectively as they develop to production.\n\n![ZenML Overview](.gitbook/assets/intro_zenml_overview.png)\n\n{% tabs %}\n{% tab title=\"For MLOps Platform Engineers\" %}\nZenML enables MLOps infrastructure experts to define, deploy, and manage sophisticated production environments that are easy to share with colleagues.\n\n*   <mark style=\"color:purple;\">**ZenML Pro**</mark>**:** [ZenML Pro](getting-started/zenml-pro/zenml-cloud.md) provides a control plane that allows you to deploy a managed ZenML instance and get access to exciting new features such as CI/CD, Model Control Plane, and RBAC.\n\n    <figure><img src=\".gitbook/assets/zenml-cloud-overview.png\" alt=\"\"><figcaption></figcaption></figure>\n*   **Self-hosted deployment:** ZenML can be deployed on any cloud provider and provides many Terraform-based utility functions to deploy other MLOps tools or even entire MLOps stacks:\n\n    ```bash\n    # Deploy ZenML to any cloud\n    zenml deploy --provider aws\n\n    # Connect cloud resources with a simple wizard\n    zenml stack register <STACK_NAME> --provider aws\n\n    # Deploy entire MLOps stacks at once\n    zenml stack deploy  --provider gcp\n    ```\n*   **Standardization:** With ZenML, you can standardize MLOps infrastructure and tooling across your organization. Simply register your staging and production environments as ZenML stacks and invite your colleagues to run ML workflows on them.\n\n    ```bash\n    # Register MLOps tools and infrastructure\n    zenml orchestrator register kfp_orchestrator -f kubeflow\n\n    # Register your production environment\n    zenml stack register production --orchestrator kubeflow ...\n    ```\n* Registering your environments as ZenML stacks also enables you to browse and explore them in a convenient user interface. Try it out at [https://www.zenml.io/live-demo](https://www.zenml.io/live-demo)!\n*   **No Vendor Lock-In:** Since infrastructure is decoupled from code, ZenML gives you the freedom to switch to a different tooling stack whenever it suits you. By avoiding vendor lock-in, you have the flexibility to transition between cloud providers or services, ensuring that you receive the best performance and pricing available in the market at any time.\n\n    ```bash\n    zenml stack set gcp\n    python run.py  # Run your ML workflows in GCP\n    zenml stack set aws\n    python run.py  # Now your ML workflow runs in AWS\n"}
{"input": "    ```\n\n:rocket: **Learn More**\n\nReady to deploy and manage your MLOps infrastructure with ZenML? Here is a collection of pages you can take a look at next:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f3d7\">\ud83c\udfd7\ufe0f</span> <mark style=\"color:purple;\"><strong>Switch to production</strong></mark></td><td>Set up and manage production-ready infrastructure with ZenML.</td><td><a href=\"user-guide/production-guide/cloud-orchestration.md\">cloud-orchestration.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f4cb\">\ud83d\udccb</span> <mark style=\"color:purple;\"><strong>Component guide</strong></mark></td><td>Explore the existing infrastructure and tooling integrations of ZenML.</td><td><a href=\"./component-guide/README.md\">./component-guide/README.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f64b\">\ud83d\ude4b</span> <mark style=\"color:purple;\"><strong>FAQ</strong></mark></td><td>Find answers to the most frequently asked questions.</td><td><a href=\"reference/faq.md\">faq.md</a></td></tr></tbody></table>\n{% endtab %}\n\n{% tab title=\"For Data Scientists\" %}\nZenML gives data scientists the freedom to fully focus on modeling and experimentation while writing code that is production-ready from the get-go.\n\n*   **Develop Locally:** ZenML allows you to develop ML models in any environment using your favorite tools. This means you can start developing locally, and simply switch to a production environment once you are satisfied with your results.\n\n    ```bash\n    python run.py  # develop your code locally with all your favorite tools\n    zenml stack set production\n    python run.py  # run on production infrastructure without any code changes\n    ```\n*   **Pythonic SDK:** ZenML is designed to be as unintrusive as possible. Adding a ZenML `@step` or `@pipeline` decorator to your Python functions is enough to turn your existing code into ZenML pipelines:\n\n    ```python\n    from zenml import pipeline, step\n\n    @step\n    def step_1() -> str:\n      return \"world\"\n\n    @step\n    def step_2(input_one: str, input_two: str) -> None:\n      combined_str = input_one + ' ' + input_two\n      print(combined_str)\n\n    @pipeline\n"}
{"input": "    def my_pipeline():\n      output_step_one = step_1()\n      step_2(input_one=\"hello\", input_two=output_step_one)\n\n    my_pipeline()\n    ```\n* **Automatic Metadata Tracking:** ZenML automatically tracks the metadata of all your runs and saves all your datasets and models to disk and versions them. Using the ZenML dashboard, you can see detailed visualizations of all your experiments. Try it out at [https://www.zenml.io/live-demo](https://www.zenml.io/live-demo)!\n\n{% hint style=\"info\" %}\nZenML integrates seamlessly with many popular open-source tools, so you can also combine ZenML with other popular experiment tracking tools like [Weights & Biases](./component-guide/experiment-trackers/wandb.md), [MLflow](./component-guide/experiment-trackers/mlflow.md), or [Neptune](./component-guide/experiment-trackers/neptune.md) for even better reproducibility.\n{% endhint %}\n\n:rocket: **Learn More**\n\nReady to develop production-ready code with ZenML? Here is a collection of pages you can take a look at next:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f9f1\">\ud83e\uddf1</span> <mark style=\"color:purple;\"><strong>Core Concepts</strong></mark></td><td>Understand the core concepts behind ZenML.</td><td><a href=\"getting-started/core-concepts.md\">core-concepts.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f423\">\ud83d\udc23</span> <mark style=\"color:purple;\"><strong>Starter Guide</strong></mark></td><td>Get started with ZenML and learn how to build your first pipeline and stack.</td><td><a href=\"user-guide/starter-guide/\">starter-guide</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f3c3\">\ud83c\udfc3</span> <mark style=\"color:purple;\"><strong>Quickstart (in Colab)</strong></mark></td><td>Build your first ZenML pipeline and deploy it in the cloud.</td><td><a href=\"https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/notebooks/quickstart.ipynb\">https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/notebooks/quickstart.ipynb</a></td></tr></tbody></table>\n{% endtab %}\n\n{% tab title"}
{"input": "=\"For ML Engineers\" %}\nZenML empowers ML engineers to take ownership of the entire ML lifecycle end-to-end. Adopting ZenML means fewer handover points and more visibility on what is happening in your organization.\n\n*   **ML Lifecycle Management:** ZenML's abstractions enable you to manage sophisticated ML setups with ease. After you define your ML workflows as [Pipelines](getting-started/core-concepts.md#1-development) and your development, staging, and production infrastructures as [Stacks](getting-started/core-concepts.md#2-execution), you can move entire ML workflows to different environments in seconds.\n\n    ```bash\n    zenml stack set staging\n    python run.py  # test your workflows on staging infrastructure\n    zenml stack set production\n    python run.py  # run your workflows in production\n    ```\n* **Reproducibility:** ZenML enables you to painlessly reproduce previous results by automatically tracking and versioning all stacks, pipelines, artifacts, and source code. In the ZenML dashboard, you can get an overview of everything that has happened and drill down into detailed lineage visualizations. Try it out at [https://www.zenml.io/live-demo](https://www.zenml.io/live-demo)!\n\n<figure><img src=\".gitbook/assets/Dashboard.png\" alt=\"ZenML Dashboard Overview\" width=\"70%\"><figcaption></figcaption></figure>\n\n*   **Automated Deployments:** With ZenML, you no longer need to upload custom Docker images to the cloud whenever you want to deploy a new model to production. Simply define your ML workflow as a ZenML pipeline, let ZenML handle the containerization, and have your model automatically deployed to a highly scalable Kubernetes deployment service like [Seldon](./component-guide/model-deployers/seldon.md).\n\n    ```python\n    from zenml.integrations.seldon.steps import seldon_model_deployer_step\n    from my_organization.steps import data_loader_step, model_trainer_step\n\n    @pipeline\n    def my_pipeline():\n      data = data_loader_step()\n      model = model_trainer_step(data)\n      seldon_model_deployer_step(model)\n    ```\n\n:rocket: **Learn More**\n\nReady to manage your ML lifecycles end-to-end with ZenML? Here is a collection of pages you can take a look at next:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f423\">\ud83d\udc23</span> <mark style=\"color:purple;\"><strong>Starter Guide</strong></mark></td><td>Get started with ZenML and learn how to build your first pipeline and stack.</td><td><a href=\"user-guide/starter-guide/\">starter-guide</"}
{"input": "a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f414\">\ud83d\udc14</span> <mark style=\"color:purple;\"><strong>How To</strong></mark></td><td>Discover advanced ZenML features like config management and containerization.</td><td><a href=\"./how-to/build-pipelines/README.md\">./how-to/build-pipelines/README.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f9d1-1f3eb\">\ud83e\uddd1\u200d\ud83c\udfeb</span> <mark style=\"color:purple;\"><strong>Examples</strong></mark></td><td>Explore ZenML through practical use-case examples.</td><td><a href=\"https://github.com/zenml-io/zenml-projects\">https://github.com/zenml-io/zenml-projects</a></td></tr></tbody></table>\n{% endtab %}\n{% endtabs %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Table of contents\n\n## Getting Started\n\n* [\u2b50 Introduction](introduction.md)\n* [\ud83e\uddd9 Installation](getting-started/installation.md)\n* [\ud83e\ude84 Core concepts](getting-started/core-concepts.md)\n* [\ud83e\udd14 Deploying ZenML](getting-started/deploying-zenml/README.md)\n  * [Deploy with ZenML CLI](getting-started/deploying-zenml/deploy-with-zenml-cli.md)\n  * [Deploy with Docker](getting-started/deploying-zenml/deploy-with-docker.md)\n  * [Deploy with Helm](getting-started/deploying-zenml/deploy-with-helm.md)\n  * [Deploy using HuggingFace Spaces](getting-started/deploying-zenml/deploy-using-huggingface-spaces.md)\n  * [Deploy with custom images](getting-started/deploying-zenml/deploy-with-custom-image.md)\n  * [Manage deployed services](getting-started/deploying-zenml/manage-the-deployed-services/README.md)\n    * [Upgrade the version of the ZenML server](getting-started/deploying-zenml/manage-the-deployed-services/upgrade-the-version-of-the-zenml-server.md)\n    * [Troubleshoot the deployed server](getting-started/deploying-zenml/manage-the-deployed-services/troubleshoot-your-deployed-server.md)\n    * [Troubleshoot stack components](getting-started/deploying-zenml/manage-the-deployed-services/troubleshoot-stack-components.md)\n    * [Custom secret stores](getting-started/deploying-zenml/manage-the-deployed-services/custom-secret-stores.md)\n  * [Secret management](getting-started/deploying-zenml/secret-management.md)\n* [\u2601\ufe0f ZenML Pro](getting-started/zenml-pro/README.md)\n  * [System Architectures](getting-started/zenml-pro/system-architectures.md)\n  * [ZenML SaaS](getting-started/zenml-pro/zenml-pro.md)\n  * [User Management](getting-started/zenml-pro/user-management.md)\n\n## User Guide\n\n* [\ud83d\udc23 Starter guide](user-guide/starter-guide/README.md)\n  * [Create an ML pipeline](user-guide/starter-guide/create-an-ml-pipeline.md)\n  * [Cache previous executions](user-guide/starter-guide/cache-previous-executions.md)\n  * [Manage artifacts](user-guide/starter-guide/manage-artifacts.md)\n  * [Track ML models](user-guide/starter-guide/track-ml-models.md)\n  * [A starter project](user-guide/starter-guide/starter-project.md)\n* [\ud83d\udc14 Production guide](user-guide/production-guide/README.md)\n  * [Deploying ZenML](user-guide/production-guide"}
{"input": "/deploying-zenml.md)\n  * [Understanding stacks](user-guide/production-guide/understand-stacks.md)\n  * [Connecting remote storage](user-guide/production-guide/remote-storage.md)\n  * [Orchestrate on the cloud](user-guide/production-guide/cloud-orchestration.md)\n  * [Configure your pipeline to add compute](user-guide/production-guide/configure-pipeline.md)\n  * [Configure a code repository](user-guide/production-guide/connect-code-repository.md)\n  * [Set up CI/CD](user-guide/production-guide/ci-cd.md)\n  * [An end-to-end project](user-guide/production-guide/end-to-end.md)\n* [\ud83e\udd9c LLMOps guide](user-guide/llmops-guide/README.md)\n  * [RAG with ZenML](user-guide/llmops-guide/rag-with-zenml/README.md)\n    * [RAG in 85 lines of code](user-guide/llmops-guide/rag-with-zenml/rag-85-loc.md)\n    * [Understanding Retrieval-Augmented Generation (RAG)](user-guide/llmops-guide/rag-with-zenml/understanding-rag.md)\n    * [Data ingestion and preprocessing](user-guide/llmops-guide/rag-with-zenml/data-ingestion.md)\n    * [Embeddings generation](user-guide/llmops-guide/rag-with-zenml/embeddings-generation.md)\n    * [Storing embeddings in a vector database](user-guide/llmops-guide/rag-with-zenml/storing-embeddings-in-a-vector-database.md)\n    * [Basic RAG inference pipeline](user-guide/llmops-guide/rag-with-zenml/basic-rag-inference-pipeline.md)\n  * [Evaluation and metrics](user-guide/llmops-guide/evaluation/README.md)\n    * [Evaluation in 65 lines of code](user-guide/llmops-guide/evaluation/evaluation-in-65-loc.md)\n    * [Retrieval evaluation](user-guide/llmops-guide/evaluation/retrieval.md)\n    * [Generation evaluation](user-guide/llmops-guide/evaluation/generation.md)\n    * [Evaluation in practice](user-guide/llmops-guide/evaluation/evaluation-in-practice.md)\n  * [Reranking for better retrieval](user-guide/llmops-guide/reranking/README.md)\n    * [Understanding reranking](user-guide/llmops-guide/reranking/understanding-reranking.md)\n    * [Implementing reranking in ZenML](user-guide/llmops-guide/reranking/implementing-reranking.md)\n    * [Evaluating reranking performance](user-guide/llmops-guide/reranking/evaluating-reranking-performance.md)\n  * [Im"}
{"input": "prove retrieval by finetuning embeddings](user-guide/llmops-guide/finetuning-embeddings/finetuning-embeddings.md)\n    * [Synthetic data generation](user-guide/llmops-guide/finetuning-embeddings/synthetic-data-generation.md)\n    * [Finetuning embeddings with Sentence Transformers](user-guide/llmops-guide/finetuning-embeddings/finetuning-embeddings-with-sentence-transformers.md)\n    * [Evaluating finetuned embeddings](user-guide/llmops-guide/finetuning-embeddings/evaluating-finetuned-embeddings.md)\n  * [Finetuning LLMs with ZenML](user-guide/llmops-guide/finetuning-llms/finetuning-llms.md)\n\n## How-To\n\n* [\ud83d\ude38 Set up a project repository](how-to/setting-up-a-project-repository/README.md)\n  * [Connect your git repository](how-to/setting-up-a-project-repository/connect-your-git-repository.md)\n  * [Project templates](how-to/setting-up-a-project-repository/using-project-templates.md)\n  * [Best practices](how-to/setting-up-a-project-repository/best-practices.md)\n* [\u26d3\ufe0f Build a pipeline](how-to/build-pipelines/README.md)\n  * [Use pipeline/step parameters](how-to/build-pipelines/use-pipeline-step-parameters.md)\n  * [Configuring a pipeline at runtime](how-to/build-pipelines/configuring-a-pipeline-at-runtime.md)\n  * [Step output typing and annotation](how-to/build-pipelines/step-output-typing-and-annotation.md)\n  * [Control caching behavior](how-to/build-pipelines/control-caching-behavior.md)\n  * [Schedule a pipeline](how-to/build-pipelines/schedule-a-pipeline.md)\n  * [Deleting a pipeline](how-to/build-pipelines/delete-a-pipeline.md)\n  * [Compose pipelines](how-to/build-pipelines/compose-pipelines.md)\n  * [Automatically retry steps](how-to/build-pipelines/retry-steps.md)\n  * [Run pipelines asynchronously](how-to/build-pipelines/run-pipelines-asynchronously.md)\n  * [Control execution order of steps](how-to/build-pipelines/control-execution-order-of-steps.md)\n  * [Using a custom step invocation ID](how-to/build-pipelines/using-a-custom-step-invocation-id.md)\n  * [Name your pipeline runs](how-to/build-pipelines/name-your-pipeline-and-runs.md)\n  * [Use failure/success hooks](how-to/build-pipelines/use-failure-success-hooks.md)\n  * [Hyperparameter tuning](how-to/build-pipelines/hyper-parameter-tuning.md)\n  * [Access secrets in a step](how-to/build-pipelines/access-secrets-in-a-step.md)\n "}
{"input": " * [Fetching pipelines](how-to/build-pipelines/fetching-pipelines.md)\n  * [Get past pipeline/step runs](how-to/build-pipelines/get-past-pipeline-step-runs.md)\n* [\ud83d\udea8 Trigger a pipeline](how-to/trigger-pipelines/README.md)\n  * [Trigger a pipeline from Python Client](how-to/trigger-pipelines/trigger-a-pipeline-from-client.md)\n  * [Trigger a pipeline from another pipeline](how-to/trigger-pipelines/trigger-a-pipeline-from-another.md)\n  * [Trigger a pipeline from REST API](how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api.md)\n* [\u25b6\ufe0f Create and run templates](how-to/create-and-run-templates/README.md)\n  * [Create a run template](how-to/create-and-run-templates/create-a-run-template.md)\n  * [Run a template](how-to/create-and-run-templates/run-a-template.md)\n* [\ud83d\udcc3 Use configuration files](how-to/use-configuration-files/README.md)\n  * [How to configure a pipeline with a YAML](how-to/use-configuration-files/how-to-use-config.md)\n  * [What can be configured](how-to/use-configuration-files/what-can-be-configured.md)\n  * [Runtime settings for Docker, resources, and stack components](how-to/use-configuration-files/runtime-configuration.md)\n  * [Configuration hierarchy](how-to/use-configuration-files/configuration-hierarchy.md)\n  * [Find out which configuration was used for a run](how-to/use-configuration-files/retrieve-used-configuration-of-a-run.md)\n  * [Autogenerate a template yaml file](how-to/use-configuration-files/autogenerate-a-template-yaml-file.md)\n* [\ud83d\udc33 Customize Docker builds](how-to/customize-docker-builds/README.md)\n  * [Docker settings on a pipeline](how-to/customize-docker-builds/docker-settings-on-a-pipeline.md)\n  * [Docker settings on a step](how-to/customize-docker-builds/docker-settings-on-a-step.md)\n  * [Specify pip dependencies and apt packages](how-to/customize-docker-builds/specify-pip-dependencies-and-apt-packages.md)\n  * [Use your own Dockerfiles](how-to/customize-docker-builds/use-your-own-docker-files.md)\n  * [Which files are built into the image](how-to/customize-docker-builds/which-files-are-built-into-the-image.md)\n  * [Use code repositories to automate Docker build reuse](how-to/customize-docker-builds/use-code-repositories-to-speed-up-docker-build-times.md)\n  * [Define where an image is built](how-to/customize-docker-builds/define-where-an-image-is-built.md)\n* [\u2692\ufe0f Manage stacks & components](how-to/stack-deployment"}
{"input": "/README.md)\n  * [Deploy a cloud stack with ZenML](how-to/stack-deployment/deploy-a-cloud-stack.md)\n  * [Deploy a cloud stack with Terraform](how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\n  * [Register a cloud stack](how-to/stack-deployment/register-a-cloud-stack.md)\n  * [Deploy stack/components using mlstacks](how-to/stack-deployment/deploy-a-stack-using-mlstacks.md)\n  * [Reference secrets in stack configuration](how-to/stack-deployment/reference-secrets-in-stack-configuration.md)\n  * [Implement a custom stack component](how-to/stack-deployment/implement-a-custom-stack-component.md)\n  * [Implement a custom integration](how-to/stack-deployment/implement-a-custom-integration.md)\n* [\ud83d\ude9c Train with GPUs](how-to/training-with-gpus/training-with-gpus.md)\n  * [Distributed Training with \ud83e\udd17 Accelerate](how-to/training-with-gpus/accelerate-distributed-training.md)\n* [\ud83c\udf32 Control logging](how-to/control-logging/README.md)\n  * [View logs on the dashboard](how-to/control-logging/view-logs-on-the-dasbhoard.md)\n  * [Enable or disable logs storage](how-to/control-logging/enable-or-disable-logs-storing.md)\n  * [Set logging verbosity](how-to/control-logging/set-logging-verbosity.md)\n  * [Disable `rich` traceback output](how-to/control-logging/disable-rich-traceback.md)\n  * [Disable colorful logging](how-to/control-logging/disable-colorful-logging.md)\n* [\ud83d\uddc4\ufe0f Handle Data/Artifacts](how-to/handle-data-artifacts/README.md)\n  * [How ZenML stores data](how-to/handle-data-artifacts/artifact-versioning.md)\n  * [Return multiple outputs from a step](how-to/handle-data-artifacts/return-multiple-outputs-from-a-step.md)\n  * [Delete an artifact](how-to/handle-data-artifacts/delete-an-artifact.md)\n  * [Organize data with tags](how-to/handle-data-artifacts/tagging.md)\n  * [Get arbitrary artifacts in a step](how-to/handle-data-artifacts/get-arbitrary-artifacts-in-a-step.md)\n  * [Handle custom data types](how-to/handle-data-artifacts/handle-custom-data-types.md)\n  * [Load artifacts into memory](how-to/handle-data-artifacts/load-artifacts-into-memory.md)\n  * [Skipping materialization](how-to/handle-data-artifacts/unmaterialized-artifacts.md)\n  * [Passing artifacts between pipelines](how-to/handle-data-artifacts/passing-artifacts-between-pipelines.md)\n* [\ud83d\udcca Visualizing artifacts](how-to/visualize-artifacts/README.md"}
{"input": ")\n  * [Default visualizations](how-to/visualize-artifacts/types-of-visualizations.md)\n  * [Creating custom visualizations](how-to/visualize-artifacts/creating-custom-visualizations.md)\n  * [Displaying visualizations in the dashboard](how-to/visualize-artifacts/visualizations-in-dashboard.md)\n  * [Disabling visualizations](how-to/visualize-artifacts/disabling-visualizations.md)\n* [\ud83e\ude86 Use the Model Control Plane](how-to/use-the-model-control-plane/README.md)\n  * [Registering a Model](how-to/use-the-model-control-plane/register-a-model.md)\n  * [Deleting a Model](how-to/use-the-model-control-plane/delete-a-model.md)\n  * [Associate a pipeline with a Model](how-to/use-the-model-control-plane/associate-a-pipeline-with-a-model.md)\n  * [Connecting artifacts via a Model](how-to/use-the-model-control-plane/connecting-artifacts-via-a-model.md)\n  * [Controlling Model versions](how-to/use-the-model-control-plane/model-versions.md)\n  * [Load a Model in code](how-to/use-the-model-control-plane/load-a-model-in-code.md)\n  * [Promote a Model](how-to/use-the-model-control-plane/promote-a-model.md)\n  * [Linking model binaries/data to a Model](how-to/use-the-model-control-plane/linking-model-binaries-data-to-models.md)\n  * [Load artifacts from Model](how-to/use-the-model-control-plane/load-artifacts-from-model.md)\n* [\ud83d\udcc8 Track metrics and metadata](how-to/track-metrics-metadata/README.md)\n  * [Attach metadata to a model](how-to/track-metrics-metadata/attach-metadata-to-a-model.md)\n  * [Attach metadata to an artifact](how-to/track-metrics-metadata/attach-metadata-to-an-artifact.md)\n  * [Attach metadata to steps](how-to/track-metrics-metadata/attach-metadata-to-steps.md)\n  * [Group metadata](how-to/track-metrics-metadata/grouping-metadata.md)\n  * [Special Metadata Types](how-to/track-metrics-metadata/logging-metadata.md)\n  * [Fetch metadata within steps](how-to/track-metrics-metadata/fetch-metadata-within-steps.md)\n  * [Fetch metadata during pipeline composition](how-to/track-metrics-metadata/fetch-metadata-within-pipeline.md)\n* [\ud83d\udc68\u200d\ud83c\udfa4 Popular integrations](how-to/popular-integrations/README.md)\n  * [Run on AWS](how-to/popular-integrations/aws-guide.md)\n  * [Run on GCP](how-to/popular-integrations/gcp-guide.md)\n  * [Run on Azure](how-to/popular-integrations/azure-guide.md)\n  * [Kubeflow](how-to/popular-integrations/kubef"}
{"input": "low.md)\n  * [Kubernetes](how-to/popular-integrations/kubernetes.md)\n  * [MLflow](how-to/popular-integrations/mlflow.md)\n  * [Skypilot](how-to/popular-integrations/skypilot.md)\n* [\ud83d\udd0c Connect services (AWS, GCP, Azure, K8s etc)](how-to/auth-management/README.md)\n  * [Service Connectors guide](how-to/auth-management/service-connectors-guide.md)\n  * [Security best practices](how-to/auth-management/best-security-practices.md)\n  * [Docker Service Connector](how-to/auth-management/docker-service-connector.md)\n  * [Kubernetes Service Connector](how-to/auth-management/kubernetes-service-connector.md)\n  * [AWS Service Connector](how-to/auth-management/aws-service-connector.md)\n  * [GCP Service Connector](how-to/auth-management/gcp-service-connector.md)\n  * [Azure Service Connector](how-to/auth-management/azure-service-connector.md)\n  * [HyperAI Service Connector](how-to/auth-management/hyperai-service-connector.md)\n* [\ud83d\udc0d Configure Python environments](how-to/configure-python-environments/README.md)\n  * [Handling dependencies](how-to/configure-python-environments/handling-dependencies.md)\n  * [Configure the server environment](how-to/configure-python-environments/configure-the-server-environment.md)\n* [\ud83d\udd0c Connect to a server](how-to/connecting-to-zenml/README.md)\n  * [Connect in with your User (interactive)](how-to/connecting-to-zenml/connect-in-with-your-user-interactive.md)\n  * [Connect with a Service Account](how-to/connecting-to-zenml/connect-with-a-service-account.md)\n* [\ud83d\udcd4 Run remote pipelines from notebooks](how-to/run-remote-pipelines-from-notebooks/README.md)\n  * [Define steps in notebook cells](how-to/run-remote-pipelines-from-notebooks/define-steps-in-notebook-cells.md)\n* [\ud83d\udd10 Interact with secrets](how-to/interact-with-secrets.md)\n* [\ud83d\udc1e Debug and solve issues](how-to/debug-and-solve-issues.md)\n\n## Stack Components\n\n* [\ud83d\udcdc Overview](component-guide/README.md)\n* [\ud83d\udd0b Orchestrators](component-guide/orchestrators/orchestrators.md)\n  * [Local Orchestrator](component-guide/orchestrators/local.md)\n  * [Local Docker Orchestrator](component-guide/orchestrators/local-docker.md)\n  * [Kubeflow Orchestrator](component-guide/orchestrators/kubeflow.md)\n  * [Kubernetes Orchestrator](component-guide/orchestrators/kubernetes.md)\n  * [Google Cloud VertexAI Orchestrator](component-guide/orchestrators/vertex.md"}
{"input": ")\n  * [AWS Sagemaker Orchestrator](component-guide/orchestrators/sagemaker.md)\n  * [AzureML Orchestrator](component-guide/orchestrators/azureml.md)\n  * [Databricks Orchestrator](component-guide/orchestrators/databricks.md)\n  * [Tekton Orchestrator](component-guide/orchestrators/tekton.md)\n  * [Airflow Orchestrator](component-guide/orchestrators/airflow.md)\n  * [Skypilot VM Orchestrator](component-guide/orchestrators/skypilot-vm.md)\n  * [HyperAI Orchestrator](component-guide/orchestrators/hyperai.md)\n  * [Develop a custom orchestrator](component-guide/orchestrators/custom.md)\n* [\ud83c\udfea Artifact Stores](component-guide/artifact-stores/artifact-stores.md)\n  * [Local Artifact Store](component-guide/artifact-stores/local.md)\n  * [Amazon Simple Cloud Storage (S3)](component-guide/artifact-stores/s3.md)\n  * [Google Cloud Storage (GCS)](component-guide/artifact-stores/gcp.md)\n  * [Azure Blob Storage](component-guide/artifact-stores/azure.md)\n  * [Develop a custom artifact store](component-guide/artifact-stores/custom.md)\n* [\ud83d\udc33 Container Registries](component-guide/container-registries/container-registries.md)\n  * [Default Container Registry](component-guide/container-registries/default.md)\n  * [DockerHub](component-guide/container-registries/dockerhub.md)\n  * [Amazon Elastic Container Registry (ECR)](component-guide/container-registries/aws.md)\n  * [Google Cloud Container Registry](component-guide/container-registries/gcp.md)\n  * [Azure Container Registry](component-guide/container-registries/azure.md)\n  * [GitHub Container Registry](component-guide/container-registries/github.md)\n  * [Develop a custom container registry](component-guide/container-registries/custom.md)\n* [\ud83e\uddea Data Validators](component-guide/data-validators/data-validators.md)\n  * [Great Expectations](component-guide/data-validators/great-expectations.md)\n  * [Deepchecks](component-guide/data-validators/deepchecks.md)\n  * [Evidently](component-guide/data-validators/evidently.md)\n  * [Whylogs](component-guide/data-validators/whylogs.md)\n  * [Develop a custom data validator](component-guide/data-validators/custom.md)\n* [\ud83d\udcc8 Experiment Trackers](component-guide/experiment-trackers/experiment-trackers.md)\n  * [Comet](component-guide/experiment-trackers/comet.md)\n  * [MLflow](component-guide/experiment-trackers/mlflow.md)\n  * [Neptune](component-guide/experiment-trackers/neptune.md)\n  *"}
{"input": " [Weights & Biases](component-guide/experiment-trackers/wandb.md)\n  * [Develop a custom experiment tracker](component-guide/experiment-trackers/custom.md)\n* [\ud83c\udfc3\u200d\u2640\ufe0f Model Deployers](component-guide/model-deployers/model-deployers.md)\n  * [MLflow](component-guide/model-deployers/mlflow.md)\n  * [Seldon](component-guide/model-deployers/seldon.md)\n  * [BentoML](component-guide/model-deployers/bentoml.md)\n  * [Hugging Face](component-guide/model-deployers/huggingface.md)\n  * [Databricks](component-guide/model-deployers/databricks.md)\n  * [Develop a Custom Model Deployer](component-guide/model-deployers/custom.md)\n* [\ud83d\udc63 Step Operators](component-guide/step-operators/step-operators.md)\n  * [Amazon SageMaker](component-guide/step-operators/sagemaker.md)\n  * [Google Cloud VertexAI](component-guide/step-operators/vertex.md)\n  * [AzureML](component-guide/step-operators/azureml.md)\n  * [Kubernetes](component-guide/step-operators/kubernetes.md)\n  * [Spark](component-guide/step-operators/spark-kubernetes.md)\n  * [Develop a Custom Step Operator](component-guide/step-operators/custom.md)\n* [\u2757 Alerters](component-guide/alerters/alerters.md)\n  * [Discord Alerter](component-guide/alerters/discord.md)\n  * [Slack Alerter](component-guide/alerters/slack.md)\n  * [Develop a Custom Alerter](component-guide/alerters/custom.md)\n* [\ud83d\uddbc\ufe0f Image Builders](component-guide/image-builders/image-builders.md)\n  * [Local Image Builder](component-guide/image-builders/local.md)\n  * [Kaniko Image Builder](component-guide/image-builders/kaniko.md)\n  * [Google Cloud Image Builder](component-guide/image-builders/gcp.md)\n  * [Develop a Custom Image Builder](component-guide/image-builders/custom.md)\n* [\ud83c\udff7\ufe0f Annotators](component-guide/annotators/annotators.md)\n  * [Argilla](component-guide/annotators/argilla.md)\n  * [Label Studio](component-guide/annotators/label-studio.md)\n  * [Pigeon](component-guide/annotators/pigeon.md)\n  * [Prodigy](component-guide/annotators/prodigy.md)\n  * [Develop a Custom Annotator](component-guide/annotators/custom.md)\n* [\ud83d\udcd3 Model Registries](component-guide/model-registries/model-registries.md)\n  * [MLflow Model Registry](component-guide/model-registries/mlflow.md)\n  * [Develop a Custom Model Registry](component-guide/model-regist"}
{"input": "ries/custom.md)\n* [\ud83d\udcca Feature Stores](component-guide/feature-stores/feature-stores.md)\n  * [Feast](component-guide/feature-stores/feast.md)\n  * [Develop a Custom Feature Store](component-guide/feature-stores/custom.md)\n\n## Examples\n\n* [\ud83d\ude80 Quickstart](https://github.com/zenml-io/zenml/blob/main/examples/quickstart)\n* [\ud83d\udd0f End-to-End Batch Inference](https://github.com/zenml-io/zenml/tree/main/examples/e2e)\n* [\ud83d\udcda Basic NLP with BERT](https://github.com/zenml-io/zenml/tree/main/examples/e2e\\_nlp)\n* [\ud83d\udc41\ufe0f Computer Vision with YoloV8](https://github.com/zenml-io/zenml-projects/tree/main/end-to-end-computer-vision)\n* [\ud83d\udcd6 LLM Finetuning](https://github.com/zenml-io/zenml/tree/main/examples/llm_finetuning)\n* [\ud83e\udde9 More Projects...](https://github.com/zenml-io/zenml-projects)\n\n## Reference\n\n* [\ud83d\udc0d Python Client](reference/python-client.md)\n* [\ud83d\udcfc Global settings](reference/global-settings.md)\n* [\ud83c\udf0e Environment Variables](reference/environment-variables.md)\n* [\ud83d\udc40 API reference](reference/api-reference.md)\n* [\ud83e\udd37 SDK & CLI reference](https://sdkdocs.zenml.io/)\n* [\ud83d\udcda How do I...?](reference/how-do-i.md)\n* [\u267b\ufe0f Migration guide](reference/migration-guide/README.md)\n  * [Migration guide 0.13.2 \u2192 0.20.0](reference/migration-guide/migration-zero-twenty.md)\n  * [Migration guide 0.23.0 \u2192 0.30.0](reference/migration-guide/migration-zero-thirty.md)\n  * [Migration guide 0.39.1 \u2192 0.41.0](reference/migration-guide/migration-zero-forty.md)\n  * [Migration guide 0.58.2 \u2192 0.60.0](reference/migration-guide/migration-zero-sixty.md)\n* [\ud83d\udc9c Community & content](reference/community-and-content.md)\n* [\u2753 FAQ](reference/faq.md)\n"}
{"input": "---\ndescription: Leverage the power of LLMs in your MLOps workflows with ZenML.\n---\n\n# \ud83e\udd9c LLMOps guide\n\nWelcome to the ZenML LLMOps Guide, where we dive into the exciting world of Large Language Models (LLMs) and how to integrate them seamlessly into your MLOps pipelines using ZenML. This guide is designed for ML practitioners and MLOps engineers looking to harness the potential of LLMs while maintaining the robustness and scalability of their workflows.\n\n<figure><img src=\"../../.gitbook/assets/rag-overview.png\" alt=\"\"><figcaption><p>ZenML simplifies the development and deployment of LLM-powered MLOps pipelines.</p></figcaption></figure>\n\nIn this guide, we'll explore various aspects of working with LLMs in ZenML, including:\n\n* [RAG with ZenML](rag-with-zenml/README.md)\n  * [RAG in 85 lines of code](rag-with-zenml/rag-85-loc.md)\n  * [Understanding Retrieval-Augmented Generation (RAG)](rag-with-zenml/understanding-rag.md)\n  * [Data ingestion and preprocessing](rag-with-zenml/data-ingestion.md)\n  * [Embeddings generation](rag-with-zenml/embeddings-generation.md)\n  * [Storing embeddings in a vector database](rag-with-zenml/storing-embeddings-in-a-vector-database.md)\n  * [Basic RAG inference pipeline](rag-with-zenml/basic-rag-inference-pipeline.md)\n* [Evaluation and metrics](evaluation/README.md)\n  * [Evaluation in 65 lines of code](evaluation/evaluation-in-65-loc.md)\n  * [Retrieval evaluation](evaluation/retrieval.md)\n  * [Generation evaluation](evaluation/generation.md)\n  * [Evaluation in practice](evaluation/evaluation-in-practice.md)\n* [Reranking for better retrieval](reranking/README.md)\n  * [Understanding reranking](reranking/understanding-reranking.md)\n  * [Implementing reranking in ZenML](reranking/implementing-reranking.md)\n  * [Evaluating reranking performance](reranking/evaluating-reranking-performance.md)\n* [Improve retrieval by finetuning embeddings](finetuning-embeddings/finetuning-embeddings.md)\n  * [Synthetic data generation](finetuning-embeddings/synthetic-data-generation.md)\n  * [Finetuning embeddings with Sentence Transformers](finetuning-embeddings/finetuning-embeddings-with-sentence-transformers.md)\n  * [Evaluating finetuned embeddings](finetuning-embeddings/evaluating-finetuned-embeddings.md)\n* [Finetuning LLMs with ZenML](finetuning-llms/finetuning-ll"}
{"input": "ms.md)\n\nTo follow along with the examples and tutorials in this guide, ensure you have a Python environment set up with ZenML installed. Familiarity with the concepts covered in the [Starter Guide](../starter-guide/README.md) and [Production Guide](../production-guide/README.md) is recommended.\n\nWe'll showcase a specific application over the course of this LLM guide, showing how you can work from a simple RAG pipeline to a more complex setup that involves finetuning embeddings, reranking retrieved documents, and even finetuning the LLM itself. We'll do this all for a use case relevant to ZenML: a question answering system that can provide answers to common questions about ZenML. This will help you understand how to apply the concepts covered in this guide to your own projects.\n\nBy the end of this guide, you'll have a solid understanding of how to leverage LLMs in your MLOps workflows using ZenML, enabling you to build powerful, scalable, and maintainable LLM-powered applications. First up, let's take a look at a super simple implementation of the RAG paradigm to get started.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Finetune LLMs for specific tasks or to improve performance and cost.\n---\n\n\ud83d\udea7 This guide is a work in progress. Please check back soon for updates.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Use your RAG components to generate responses to prompts.\n---\n\n# Simple RAG Inference\n\nNow that we have our index store, we can use it to make queries based on the\ndocuments in the index store. We use some utility functions to make this happen\nbut no external libraries are needed beyond an interface to the index store as\nwell as the LLM itself.\n\n![](/docs/book/.gitbook/assets/rag-stage-4.png)\n\nIf you've been following along with the guide, you should have some documents\ningested already and you can pass a query in as a flag to the Python command\nused to run the pipeline:\n\n```bash\npython run.py --rag-query \"how do I use a custom materializer inside my own zenml \nsteps? i.e. how do I set it? inside the @step decorator?\" --model=gpt4\n```\n\n![](/docs/book/.gitbook/assets/rag-inference.png)\n\nThis inference query itself is not a ZenML pipeline, but rather a function call\nwhich uses the outputs and components of our pipeline to generate the response.\nFor a more complex inference setup, there might be even more going on here, but\nfor the purposes of this initial guide we will keep it simple.\n\nBringing everything together, the code for the inference pipeline is as follows:\n\n```python\ndef process_input_with_retrieval(\n    input: str, model: str = OPENAI_MODEL, n_items_retrieved: int = 5\n) -> str:\n    delimiter = \"```\"\n\n    # Step 1: Get documents related to the user input from database\n    related_docs = get_topn_similar_docs(\n        get_embeddings(input), get_db_conn(), n=n_items_retrieved\n    )\n\n    # Step 2: Get completion from OpenAI API\n    # Set system message to help set appropriate tone and context for model\n    system_message = f\"\"\"\n    You are a friendly chatbot. \\\n    You can answer questions about ZenML, its features and its use cases. \\\n    You respond in a concise, technically credible tone. \\\n    You ONLY use the context from the ZenML documentation to provide relevant\n    answers. \\\n    You do not make up answers or provide opinions that you don't have\n    information to support. \\\n    If you are unsure or don't know, just say so. \\\n    \"\"\"\n\n    # Prepare messages to pass to model\n    # We use a delimiter to help the model understand the where the user_input\n    # starts and ends\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": f\"{delimiter}{input}{delimiter}\"},\n        {\n            \"role\": \"assistant\",\n            \"content\": f\"Relevant ZenML documentation: \\n\"\n            + \"\\n\".join(doc[0] for doc"}
{"input": " in related_docs),\n        },\n    ]\n    logger.debug(\"CONTEXT USED\\n\\n\", messages[2][\"content\"], \"\\n\\n\")\n    return get_completion_from_messages(messages, model=model)\n```\n\nFor the `get_topn_similar_docs` function, we use the embeddings generated from\nthe documents in the index store to find the most similar documents to the\nquery:\n\n```python\ndef get_topn_similar_docs(\n    query_embedding: List[float],\n    conn: psycopg2.extensions.connection,\n    n: int = 5,\n    include_metadata: bool = False,\n    only_urls: bool = False,\n) -> List[Tuple]:\n    embedding_array = np.array(query_embedding)\n    register_vector(conn)\n    cur = conn.cursor()\n\n    if include_metadata:\n        cur.execute(\n            f\"SELECT content, url FROM embeddings ORDER BY embedding <=> %s LIMIT {n}\",\n            (embedding_array,),\n        )\n    elif only_urls:\n        cur.execute(\n            f\"SELECT url FROM embeddings ORDER BY embedding <=> %s LIMIT {n}\",\n            (embedding_array,),\n        )\n    else:\n        cur.execute(\n            f\"SELECT content FROM embeddings ORDER BY embedding <=> %s LIMIT {n}\",\n            (embedding_array,),\n        )\n\n    return cur.fetchall()\n```\n\nLuckily we are able to get these similar documents using a function in\n[`pgvector`](https://github.com/pgvector/pgvector), a plugin package for\nPostgreSQL: `ORDER BY embedding <=> %s` orders the documents by their similarity\nto the query embedding. This is a very efficient way to get the most relevant\ndocuments to the query and is a great example of how we can leverage the power\nof the database to do the heavy lifting for us.\n\nFor the `get_completion_from_messages` function, we use\n[`litellm`](https://github.com/BerriAI/litellm) as a universal interface that\nallows us to use lots of different LLMs. As you can see above, the model is able\nto synthesize the documents it has been given and provide a response to the\nquery.\n\n```python\ndef get_completion_from_messages(\n    messages, model=OPENAI_MODEL, temperature=0.4, max_tokens=1000\n):\n    \"\"\"Generates a completion response from the given messages using the specified model.\"\"\"\n    model = MODEL_NAME_MAP.get(model, model)\n    completion_response = litellm.completion(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    return completion_response.choices[0].message.content\n```\n\nWe're using `litellm` because it makes sense not to have to implement separate\nfunctions for each LLM we might want to use. The pace of development in the\nfield is such that you will want to experiment with new LLMs as they come out,\nand `litellm` gives you the flexibility to"}
{"input": " do that without having to rewrite\nyour code.\n\nWe've now completed a basic RAG inference pipeline that uses the embeddings\ngenerated by the pipeline to retrieve the most relevant chunks of text based on\na given query. We can inspect the various components of the pipeline to see how\nthey work together to provide a response to the query. This gives us a solid\nfoundation to move onto more complex RAG pipelines and to look into how we might\nimprove this. The next section will cover how to improve retrieval by finetuning\nthe embeddings generated by the pipeline. This will boost our performance in\nsituations where we have a large volume of documents and also when the documents\nare potentially very different from the training data that was used for the\nembeddings.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide)\nrepository and for this section, particularly [the `llm_utils.py` file](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/utils/llm_utils.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Generate embeddings to improve retrieval performance.\n---\n\n# Generating Embeddings for Retrieval\n\nIn this section, we'll explore how to generate embeddings for your data to\nimprove retrieval performance in your RAG pipeline. Embeddings are a crucial\npart of the retrieval mechanism in RAG, as they represent the data in a\nhigh-dimensional space where similar items are closer together. By generating\nembeddings for your data, you can enhance the retrieval capabilities of your RAG\npipeline and provide more accurate and relevant responses to user queries.\n\n![](/docs/book/.gitbook/assets/rag-stage-2.png)\n\n{% hint style=\"info\" %} Embeddings are vector representations of data that capture the semantic\nmeaning and context of the data in a high-dimensional space. They are generated\nusing machine learning models, such as word embeddings or sentence embeddings,\nthat learn to encode the data in a way that preserves its underlying structure\nand relationships. Embeddings are commonly used in natural language processing\n(NLP) tasks, such as text classification, sentiment analysis, and information\nretrieval, to represent textual data in a format that is suitable for\ncomputational processing. {% endhint %}\n\nThe whole purpose of the embeddings is to allow us to quickly find the small\nchunks that are most relevant to our input query at inference time. An even\nsimpler way of doing this would be to just to search for some keywords in the\nquery and hope that they're also represented in the chunks. However, this\napproach is not very robust and may not work well for more complex queries or\nlonger documents. By using embeddings, we can capture the semantic meaning and\ncontext of the data and retrieve the most relevant chunks based on their\nsimilarity to the query.\n\nWe're using the [`sentence-transformers`](https://www.sbert.net/) library to generate embeddings for our\ndata. This library provides pre-trained models for generating sentence\nembeddings that capture the semantic meaning of the text. It's an open-source\nlibrary that is easy to use and provides high-quality embeddings for a wide\nrange of NLP tasks.\n\n```python\nfrom typing import Annotated, List\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom structures import Document\nfrom zenml import ArtifactConfig, log_artifact_metadata, step\n\n@step\ndef generate_embeddings(\n    split_documents: List[Document],\n) -> Annotated[\n    List[Document], ArtifactConfig(name=\"documents_with_embeddings\")\n]:\n    try:\n        model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n\n        log_artifact_metadata(\n            artifact_name=\"embeddings\",\n            metadata={\n                \"embedding_type\": \"sentence-transformers/all-MiniLM-L12-v2\",\n                \"embedding_dimensionality\": 384,\n            },\n        )\n\n        document_texts = [doc.page_content for doc in split_documents]\n        embeddings = model"}
{"input": ".encode(document_texts)\n\n        for doc, embedding in zip(split_documents, embeddings):\n            doc.embedding = embedding\n\n        return split_documents\n    except Exception as e:\n        logger.error(f\"Error in generate_embeddings: {e}\")\n        raise\n```\n\nWe update the `Document` Pydantic model to include an `embedding` attribute that\nstores the embedding generated for each document. This allows us to associate\nthe embeddings with the corresponding documents and use them for retrieval\npurposes in the RAG pipeline.\n\nThere are smaller embeddings models if we cared a lot about speed, and larger\nones (with more dimensions) if we wanted to boost our ability to retrieve more\nrelevant chunks. [The model we're using\nhere](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) is on the\nsmaller side, but it should work well for our use case. The embeddings generated\nby this model have a dimensionality of 384, which means that each embedding is\nrepresented as a 384-dimensional vector in the high-dimensional space.\n\nWe can use dimensionality reduction functionality in\n[`umap`](https://umap-learn.readthedocs.io/) and\n[`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn-manifold-tsne)\nto represent the 384 dimensions of our embeddings in two-dimensional space. This\nallows us to visualize the embeddings and see how similar chunks are clustered\ntogether based on their semantic meaning and context. We can also use this\nvisualization to identify patterns and relationships in the data that can help\nus improve the retrieval performance of our RAG pipeline. It's worth trying both\nUMAP and t-SNE to see which one works best for our use case since they both have\nsomewhat different representations of the data and reduction algorithms, as\nyou'll see.\n\n```python\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport umap\nfrom zenml.client import Client\n\nartifact = Client().get_artifact_version('EMBEDDINGS_ARTIFACT_UUID_GOES_HERE')\nembeddings = artifact.load()\n\n\nembeddings = np.array([doc.embedding for doc in documents])\nparent_sections = [doc.parent_section for doc in documents]\n\n# Get unique parent sections\nunique_parent_sections = list(set(parent_sections))\n\n# Tol color palette\ntol_colors = [\n    \"#4477AA\",\n    \"#EE6677\",\n    \"#228833\",\n    \"#CCBB44\",\n    \"#66CCEE\",\n    \"#AA3377\",\n    \"#BBBBBB\",\n]\n\n# Create a colormap with Tol colors\ntol_colormap = ListedColormap(tol_colors)\n\n# Assign colors to each unique parent section\nsection_colors = tol_colors[: len(unique_parent_sections)]\n\n# Create a dictionary mapping parent sections to colors"}
{"input": "\nsection_color_dict = dict(zip(unique_parent_sections, section_colors))\n\n# Dimensionality reduction using t-SNE\ndef tsne_visualization(embeddings, parent_sections):\n    tsne = TSNE(n_components=2, random_state=42)\n    embeddings_2d = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(8, 8))\n    for section in unique_parent_sections:\n        if section in section_color_dict:\n            mask = [section == ps for ps in parent_sections]\n            plt.scatter(\n                embeddings_2d[mask, 0],\n                embeddings_2d[mask, 1],\n                c=[section_color_dict[section]],\n                label=section,\n            )\n\n    plt.title(\"t-SNE Visualization\")\n    plt.legend()\n    plt.show()\n\n\n# Dimensionality reduction using UMAP\ndef umap_visualization(embeddings, parent_sections):\n    umap_2d = umap.UMAP(n_components=2, random_state=42)\n    embeddings_2d = umap_2d.fit_transform(embeddings)\n\n    plt.figure(figsize=(8, 8))\n    for section in unique_parent_sections:\n        if section in section_color_dict:\n            mask = [section == ps for ps in parent_sections]\n            plt.scatter(\n                embeddings_2d[mask, 0],\n                embeddings_2d[mask, 1],\n                c=[section_color_dict[section]],\n                label=section,\n            )\n\n    plt.title(\"UMAP Visualization\")\n    plt.legend()\n    plt.show()\n```\n\n![UMAP visualization of the ZenML documentation chunks as embeddings](/docs/book/.gitbook/assets/umap.png)\n![t-SNE visualization of the ZenML documentation chunks as embeddings](/docs/book/.gitbook/assets/tsne.png)\n\nIn this stage, we have utilized the 'parent directory', which we had previously\nstored in the vector store as an additional attribute, as a means to color the\nvalues. This approach allows us to gain some insight into the semantic space\ninherent in our data. It demonstrates that you can visualize the embeddings and\nobserve how similar chunks are grouped together based on their semantic meaning\nand context.\n\nSo this step iterates through all the chunks and generates embeddings\nrepresenting each piece of text. These embeddings are then stored as an artifact\nin the ZenML artifact store as a NumPy array. We separate this generation from\nthe point where we upload those embeddings to the vector database to keep the\npipeline modular and flexible; in the future we might want to use a different\nvector database so we can just swap out the upload step without having to\nre-generate the embeddings.\n\nIn the next section, we'll explore how to store these embeddings in a vector\ndatabase to enable fast and efficient retrieval of relevant chunks at inference\ntime.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zen"}
{"input": "ml-projects/tree/main/llm-complete-guide)\nrepository. The embeddings generation step can be found\n[here](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/populate_index.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: RAG is a sensible way to get started with LLMs.\n---\n\n# RAG Pipelines with ZenML\n\nRetrieval-Augmented Generation (RAG) is a powerful technique that combines the\nstrengths of retrieval-based and generation-based models. In this guide, we'll\nexplore how to set up RAG pipelines with ZenML, including data ingestion, index\nstore management, and tracking RAG-associated artifacts.\n\nLLMs are a powerful tool, as they can generate human-like responses to a wide\nvariety of prompts. However, they can also be prone to generating incorrect or\ninappropriate responses, especially when the input prompt is ambiguous or\nmisleading. They are also (currently) limited in the amount of text they can\nunderstand and/or generate. While there are some LLMs [like Google's Gemini 1.5\nPro](https://developers.googleblog.com/2024/02/gemini-15-available-for-private-preview-in-google-ai-studio.html)\nthat can consistently handle 1 million tokens (small units of text), the vast majority (particularly\nthe open-source ones currently available) handle far less.\n\nThe first part of this guide to RAG pipelines with ZenML is about understanding\nthe basic components and how they work together. We'll cover the following\ntopics:\n\n- why RAG exists and what problem it solves\n- how to ingest and preprocess data that we'll use in our RAG pipeline\n- how to leverage embeddings to represent our data; this will be the basis for\n  our retrieval mechanism\n- how to store these embeddings in a vector database\n- how to track RAG-associated artifacts with ZenML\n\nAt the end, we'll bring it all together and show all the components working\ntogether to perform basic RAG inference.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Understand how to ingest and preprocess data for RAG pipelines with ZenML.\n---\n\nThe first step in setting up a RAG pipeline is to ingest the data that will be\nused to train and evaluate the retriever and generator models. This data can\ninclude a large corpus of documents, as well as any relevant metadata or\nannotations that can be used to train the retriever and generator.\n\n![](/docs/book/.gitbook/assets/rag-stage-1.png)\n\nIn the interests of keeping things simple, we'll implement the bulk of what we\nneed ourselves. However, it's worth noting that there are a number of tools and\nframeworks that can help you manage the data ingestion process, including\ndownloading, preprocessing, and indexing large corpora of documents. ZenML\nintegrates with a number of these tools and frameworks, making it easy to set up\nand manage RAG pipelines.\n\n{% hint style=\"info\" %}\nYou can view all the code referenced in this guide in the associated project\nrepository. Please visit <a\nhref=\"https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide\">the\n`llm-complete-guide` project</a> inside the ZenML projects repository if you\nwant to dive deeper.\n{% endhint %}\n\nYou can add a ZenML step that scrapes a series of URLs and outputs the URLs quite\neasily. Here we assemble a step that scrapes URLs related to ZenML from its documentation.\nWe leverage some simple helper utilities that we have created for this purpose:\n\n```python\nfrom typing import List\nfrom typing_extensions import Annotated\nfrom zenml import log_artifact_metadata, step\nfrom steps.url_scraping_utils import get_all_pages\n\n@step\ndef url_scraper(\n    docs_url: str = \"https://docs.zenml.io\",\n    repo_url: str = \"https://github.com/zenml-io/zenml\",\n    website_url: str = \"https://zenml.io\",\n) -> Annotated[List[str], \"urls\"]:\n    \"\"\"Generates a list of relevant URLs to scrape.\"\"\"\n    docs_urls = get_all_pages(docs_url)\n    log_artifact_metadata(\n        metadata={\n            \"count\": len(docs_urls),\n        },\n    )\n    return docs_urls\n```\n\nThe `get_all_pages` function simply crawls our documentation website and\nretrieves a unique set of URLs. We've limited it to only scrape the\ndocumentation relating to the most recent releases so that we're not mixing old\nsyntax and information with the new. This is a simple way to ensure that we're\nonly ingesting the most relevant and up-to-date information into our pipeline.\n\nWe also log the count of those URLs as metadata for the step output. This will\nbe visible in the dashboard for extra visibility around the data that's being\ningested. Of course, you can also"}
{"input": " add more complex logic to this step, such as\nfiltering out certain URLs or adding more metadata.\n\n![Partial screenshot from the dashboard showing the metadata from the step](/docs/book/.gitbook/assets/llm-data-ingestion-metadata.png)\n\nOnce we have our list of URLs, we use [the `unstructured`\nlibrary](https://github.com/Unstructured-IO/unstructured) to load and parse the\npages. This will allow us to use the text without having to worry about the\ndetails of the HTML structure and/or markup. This specifically helps us keep the\ntext\ncontent as small as possible since we are operating in a constrained environment\nwith LLMs.\n\n```python\nfrom typing import List\nfrom unstructured.partition.html import partition_html\nfrom zenml import step\n\n@step\ndef web_url_loader(urls: List[str]) -> List[str]:\n    \"\"\"Loads documents from a list of URLs.\"\"\"\n    document_texts = []\n    for url in urls:\n        elements = partition_html(url=url)\n        text = \"\\n\\n\".join([str(el) for el in elements])\n        document_texts.append(text)\n    return document_texts\n```\n\nThe previously-mentioned frameworks offer many more options when it comes to\ndata ingestion, including the ability to load documents from a variety of\nsources, preprocess the text, and extract relevant features. For our purposes,\nthough, we don't need anything too fancy. It also makes our pipeline easier to\ndebug since we can see exactly what's being loaded and how it's being processed.\nYou don't get that same level of visibility with more complex frameworks.\n\n# Preprocessing the data\n\nOnce we have loaded the documents, we can preprocess them into a form that's\nuseful for a RAG pipeline. There are a lot of options here, depending on how\ncomplex you want to get, but to start with you can think of the 'chunk size' as\none of the key parameters to think about.\n\nOur text is currently in the form of various long strings, with each one\nrepresenting a single web page. These are going to be too long to pass into our\nLLM, especially if we care about the speed at which we get our answers back. So\nthe strategy here is to split our text into smaller chunks that can be processed\nmore efficiently. There's a sweet spot between having tiny chunks, which will\nmake it harder for our search / retrieval step to find relevant information to\npass into the LLM, and having large chunks, which will make it harder for the\nLLM to process the text.\n\n```python\nimport logging\nfrom typing import Annotated, List\nfrom utils.llm_utils import split_documents\nfrom zenml import ArtifactConfig, log_artifact_metadata, step\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@step(enable_cache=False)\ndef preprocess_documents(\n    documents: List[str],\n) -> An"}
{"input": "notated[List[str], ArtifactConfig(name=\"split_chunks\")]:\n    \"\"\"Preprocesses a list of documents by splitting them into chunks.\"\"\"\n    try:\n        log_artifact_metadata(\n            artifact_name=\"split_chunks\",\n            metadata={\n                \"chunk_size\": 500,\n                \"chunk_overlap\": 50\n            },\n        )\n        return split_documents(\n            documents, chunk_size=500, chunk_overlap=50\n        )\n    except Exception as e:\n        logger.error(f\"Error in preprocess_documents: {e}\")\n        raise\n```\n\nIt's really important to know your data to have a good intuition about what kind\nof chunk size might make sense. If your data is structured in such a way where\nyou need large paragraphs to capture a particular concept, then you might want a\nlarger chunk size. If your data is more conversational or question-and-answer\nbased, then you might want a smaller chunk size.\n\nFor our purposes, given that we're working with web pages that are written as\ndocumentation for a software library, we're going to use a chunk size of 500 and\nwe'll make sure that the chunks overlap by 50 characters. This means that we'll\nhave a lot of overlap between our chunks, which can be useful for ensuring that\nwe don't miss any important information when we're splitting up our text.\n\nAgain, depending on your data and use case, there is more you might want to do\nwith your data. You might want to clean the text, remove code snippets or make\nsure that code snippets were not split across chunks, or even extract metadata\nfrom the text. This is a good starting point, but you can always add more\ncomplexity as needed.\n\nNext up, generating embeddings so that we can use them to retrieve relevant\ndocuments...\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide)\nrepository and particularly [the code for the steps](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/) in this section. Note, too,\nthat a lot of the logic is encapsulated in utility functions inside [`url_scraping_utils.py`](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/url_scraping_utils.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Store embeddings in a vector database for efficient retrieval.\n---\n\n# Storing Embeddings in a Vector Database\n\nThe process of generating the embeddings doesn't take too long, especially if\nthe machine on which the step is running has a GPU, but it's still not something\nwe want to do every time we need to retrieve a document. Instead, we can store\nthe embeddings in a vector database, which allows us to quickly retrieve the\nmost relevant chunks based on their similarity to the query.\n\n![](/docs/book/.gitbook/assets/rag-stage-3.png)\n\nFor the purposes of this guide, we'll use PostgreSQL as our vector database.\nThis is a popular choice for storing embeddings, as it provides a scalable and\nefficient way to store and retrieve high-dimensional vectors. However, you can\nuse any vector database that supports high-dimensional vectors. If you want to\nexplore a list of possible options, [this is a good\nwebsite](https://superlinked.com/vector-db-comparison/) to compare different\noptions.\n\n{% hint style=\"info\" %}\nFor more information on how to set up a PostgreSQL database to follow along with\nthis guide, please <a href=\"https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide\">see the instructions in the repository</a> which show how to set\nup a PostgreSQL database using Supabase.\n{% endhint %}\n\nSince PostgreSQL is a well-known and battle-tested database, we can use known\nand minimal packages to connect and to interact with it. We can use the\n[`psycopg2`](https://www.psycopg.org/docs/) package to connect and then raw SQL\nstatements to interact with the database.\n\nThe code for the step is fairly simple:\n\n```python\nfrom zenml import step\n\n@step\ndef index_generator(\n    documents: List[Document],\n) -> None:\n    try:\n        conn = get_db_conn()\n        with conn.cursor() as cur:\n            # Install pgvector if not already installed\n            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n            conn.commit()\n\n            # Create the embeddings table if it doesn't exist\n            table_create_command = f\"\"\"\n            CREATE TABLE IF NOT EXISTS embeddings (\n                        id SERIAL PRIMARY KEY,\n                        content TEXT,\n                        token_count INTEGER,\n                        embedding VECTOR({EMBEDDING_DIMENSIONALITY}),\n                        filename TEXT,\n                        parent_section TEXT,\n                        url TEXT\n                        );\n                        \"\"\"\n            cur.execute(table_create_command)\n            conn.commit()\n\n            register_vector(conn)\n\n            # Insert data only if it doesn't already exist\n            for doc in documents:\n                content = doc.page_content\n                token_count = doc.token_count\n                embedding = doc.embedding.tolist()\n                filename = doc.filename\n                parent_section = doc.parent_section\n                url = doc.url\n\n                cur.execute(\n                    \"SELECT COUNT(*) FROM embeddings WHERE content = %s\",\n                   "}
{"input": " (content,),\n                )\n                count = cur.fetchone()[0]\n                if count == 0:\n                    cur.execute(\n                        \"INSERT INTO embeddings (content, token_count, embedding, filename, parent_section, url) VALUES (%s, %s, %s, %s, %s, %s)\",\n                        (\n                            content,\n                            token_count,\n                            embedding,\n                            filename,\n                            parent_section,\n                            url,\n                        ),\n                    )\n                    conn.commit()\n\n            cur.execute(\"SELECT COUNT(*) as cnt FROM embeddings;\")\n            num_records = cur.fetchone()[0]\n            logger.info(f\"Number of vector records in table: {num_records}\")\n\n            # calculate the index parameters according to best practices\n            num_lists = max(num_records / 1000, 10)\n            if num_records > 1000000:\n                num_lists = math.sqrt(num_records)\n\n            # use the cosine distance measure, which is what we'll later use for querying\n            cur.execute(\n                f\"CREATE INDEX IF NOT EXISTS embeddings_idx ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {num_lists});\"\n            )\n            conn.commit()\n\n    except Exception as e:\n        logger.error(f\"Error in index_generator: {e}\")\n        raise\n    finally:\n        if conn:\n            conn.close()\n```\n\nWe use some utility functions, but what we do here is:\n\n- connect to the database\n- create the `vector` extension if it doesn't already exist (this is to enable\n  the vector data type in PostgreSQL)\n- create the `embeddings` table if it doesn't exist\n- insert the embeddings and documents into the table\n- calculate the index parameters according to best practices\n- create an index on the embeddings\n\nNote that we're inserting the documents into the embeddings table as well as the\nembeddings themselves. This is so that we can retrieve the documents based on\ntheir embeddings later on. It also helps with debugging from within the Supabase\ninterface or wherever else we're examining the contents of the database.\n\n![The Supabase editor interface](/docs/book/.gitbook/assets/supabase-editor-interface.png)\n\nDeciding when to update your embeddings is a separate discussion and depends on\nthe specific use case. If your data is frequently changing, and the changes are\nsignificant, you might want to fully reset the embeddings with each update. In\nother cases, you might just want to add new documents and embeddings into the\ndatabase because the changes are minor or infrequent. In the code above, we\nchoose to only add new embeddings if they don't already exist in the database.\n\n{% hint style=\"info\" %} Depending on the size of your dataset and the number of\nembeddings you're storing, you might find that running this step on a CPU is too\nslow. In that case, you should ensure that this step runs on a GPU-enabled\nmachine to speed"}
{"input": " up the process. You can do this with ZenML by using a step\noperator that runs on a GPU-enabled machine. {% endhint %}\n\nWe also generate an index for the embeddings using the `ivfflat` method with the\n`vector_cosine_ops` operator. This is a common method for indexing\nhigh-dimensional vectors in PostgreSQL and is well-suited for similarity search\nusing cosine distance. The number of lists is calculated based on the number of\nrecords in the table, with a minimum of 10 lists and a maximum of the square\nroot of the number of records. This is a good starting point for tuning the\nindex parameters, but you might want to experiment with different values to see\nhow they affect the performance of your RAG pipeline.\n\nNow that we have our embeddings stored in a vector database, we can move on to\nthe next step in the pipeline, which is to retrieve the most relevant documents\nbased on a given query. This is where the real magic of the RAG pipeline comes\ninto play, as we can use the embeddings to quickly retrieve the most relevant\nchunks of text based on their similarity to the query. This allows us to build a\npowerful and efficient question-answering system that can provide accurate and\nrelevant responses to user queries in real-time.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide)\nrepository. The logic for storing the embeddings in PostgreSQL can be found\n[here](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/populate_index.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Learn how to implement a RAG pipeline in just 85 lines of code.\n---\n\nThere's a lot of theory and context to think about when it comes to RAG, but\nlet's start with a quick implementation in code to motivate what follows. The\nfollowing 85 lines do the following:\n\n- load some data (a fictional dataset about 'ZenML World') as our corpus\n- process that text (split it into chunks and 'tokenize' it (i.e. split into\n  words))\n- take a query as input and find the most relevant chunks of text from our\n  corpus data\n- use OpenAI's GPT-3.5 model to answer the question based on the relevant\n    chunks\n\n```python\nimport os\nimport re\nimport string\n\nfrom openai import OpenAI\n\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\ndef tokenize(text):\n    return preprocess_text(text).split()\n\n\ndef retrieve_relevant_chunks(query, corpus, top_n=2):\n    query_tokens = set(tokenize(query))\n    similarities = []\n    for chunk in corpus:\n        chunk_tokens = set(tokenize(chunk))\n        similarity = len(query_tokens.intersection(chunk_tokens)) / len(\n            query_tokens.union(chunk_tokens)\n        )\n        similarities.append((chunk, similarity))\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    return [chunk for chunk, _ in similarities[:top_n]]\n\n\ndef answer_question(query, corpus, top_n=2):\n    relevant_chunks = retrieve_relevant_chunks(query, corpus, top_n)\n    if not relevant_chunks:\n        return \"I don't have enough information to answer the question.\"\n\n    context = \"\\n\".join(relevant_chunks)\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Based on the provided context, answer the following question: {query}\\n\\nContext:\\n{context}\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n    return chat_completion.choices[0].message.content.strip()\n\n\n# Sci-fi themed corpus about \"ZenML World\"\ncorpus = [\n    \"The luminescent forests of ZenML World are inhabited by glowing Zenbots that emit a soft, pulsating light as they roam the enchanted landscape.\",\n    \"In the neon skies of ZenML World, Cosmic Butterflies flutter gracefully, their iridescent wings leaving trails of stardust in their wake.\",\n    \"Telepathic Treants, ancient sentient trees, communicate through the quantum neural network"}
{"input": " that spans the entire surface of ZenML World, sharing wisdom and knowledge.\",\n    \"Deep within the melodic caverns of ZenML World, Fractal Fungi emit pulsating tones that resonate through the crystalline structures, creating a symphony of otherworldly sounds.\",\n    \"Near the ethereal waterfalls of ZenML World, Holographic Hummingbirds hover effortlessly, their translucent wings refracting the prismatic light into mesmerizing patterns.\",\n    \"Gravitational Geckos, masters of anti-gravity, traverse the inverted cliffs of ZenML World, defying the laws of physics with their extraordinary abilities.\",\n    \"Plasma Phoenixes, majestic creatures of pure energy, soar above the chromatic canyons of ZenML World, their fiery trails painting the sky in a dazzling display of colors.\",\n    \"Along the prismatic shores of ZenML World, Crystalline Crabs scuttle and burrow, their transparent exoskeletons refracting the light into a kaleidoscope of hues.\",\n]\n\ncorpus = [preprocess_text(sentence) for sentence in corpus]\n\nquestion1 = \"What are Plasma Phoenixes?\"\nanswer1 = answer_question(question1, corpus)\nprint(f\"Question: {question1}\")\nprint(f\"Answer: {answer1}\")\n\nquestion2 = (\n    \"What kinds of creatures live on the prismatic shores of ZenML World?\"\n)\nanswer2 = answer_question(question2, corpus)\nprint(f\"Question: {question2}\")\nprint(f\"Answer: {answer2}\")\n\nirrelevant_question_3 = \"What is the capital of Panglossia?\"\nanswer3 = answer_question(irrelevant_question_3, corpus)\nprint(f\"Question: {irrelevant_question_3}\")\nprint(f\"Answer: {answer3}\")\n```\n\nThis outputs the following:\n\n```shell\nQuestion: What are Plasma Phoenixes?\nAnswer: Plasma Phoenixes are majestic creatures made of pure energy that soar above the chromatic canyons of Zenml World. They leave fiery trails behind them, painting the sky with dazzling displays of colors.\nQuestion: What kinds of creatures live on the prismatic shores of ZenML World?\nAnswer: On the prismatic shores of ZenML World, you can find crystalline crabs scuttling and burrowing with their transparent exoskeletons, which refract light into a kaleidoscope of hues.\nQuestion: What is the capital of Panglossia?\nAnswer: The capital of Panglossia is not mentioned in the provided context.\n```\n\nThe implementation above is by no means sophisticated or performant, but it's\nsimple enough that you can see all the moving parts. Our tokenization process\nconsists of splitting the text into individual words. \n\nThe way we check for similarity between the question / query and the chunks of\ntext is extremely naive and inefficient. The similarity between the query and\nthe current chunk is calculated using the [Jaccard similarity\ncoefficient]("}
{"input": "https://www.statology.org/jaccard-similarity/). This coefficient\nmeasures the similarity between two sets and is defined as the size of the\nintersection divided by the size of the union of the two sets. So we count the\nnumber of words that are common between the query and the chunk and divide it by\nthe total number of unique words in both the query and the chunk. There are much\nbetter ways of measuring the similarity between two pieces of text, such as\nusing embeddings or other more sophisticated techniques, but this example is\nkept simple for illustrative purposes.\n\nThe rest of this guide will showcase a more performant and scalable way of\nperforming the same task using ZenML. If you ever are unsure why we're doing\nsomething, feel free to return to this example for the high-level overview.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Understand the Retrieval-Augmented Generation (RAG) technique and its benefits.\n---\n\nLLMs are powerful but not without their limitations. They are prone to\ngenerating incorrect responses, especially when it's unclear what the input\nprompt is asking for. They are also limited in the amount of text they can\nunderstand and generate. While some LLMs can handle more than 1 million tokens\nof input, most open-source models can handle far less. Your use case also might\nnot require all the complexity and cost associated with running a large LLM.\n\nRAG, [originally proposed in 2020](https://arxiv.org/abs/2005.11401v4) by researchers at Facebook, is a technique that\nsupplements the inbuilt abilities of foundation models like LLMs with a\nretrieval mechanism. This mechanism retrieves relevant documents from a large\ncorpus and uses them to generate a response. This approach combines the\nstrengths of retrieval-based and generation-based models, allowing you to\nleverage the power of LLMs while addressing their limitations.\n\n# What exactly happens in a RAG pipeline?\n\n![](/docs/book/.gitbook/assets/rag-process-whole.png)\n\nIn a RAG pipeline, we use a retriever to find relevant documents from a large corpus and then uses a\ngenerator to produce a response based on the retrieved documents. This approach\nis particularly useful for tasks that require contextual understanding and\nlong-form generation, such as question answering, summarization, and dialogue\ngeneration.\n\nRAG helps with the context limitations mentioned above by providing a way to\nretrieve relevant documents that can be used to generate a response. This\nretrieval step can help ensure that the generated response is grounded in\nrelevant information, reducing the likelihood of generating incorrect or\ninappropriate responses. It also helps with the token limitations by allowing\nthe generator to focus on a smaller set of relevant documents, rather than\nhaving to process an entire large corpus.\n\nGiven the costs associated with running LLMs, RAG can also be more\ncost-effective than using a pure generation-based approach, as it allows you to\nfocus the generator's resources on a smaller set of relevant documents. This can\nbe particularly important when working with large corpora or when deploying\nmodels to resource-constrained environments.\n\n# When is RAG a good choice?\n\n![](/docs/book/.gitbook/assets/rag-when.png)\n\nRAG is a good choice when you need to generate long-form responses that require\ncontextual understanding and when you have access to a large corpus of relevant\ndocuments. It can be particularly useful for tasks like question answering,\nsummarization, and dialogue generation, where the generated response needs to\nbe grounded in relevant information.\n\nIt's often the first thing that you'll want to try when dipping your toes into\nthe world of LLMs. This is because it"}
{"input": " provides a sensible way to get a feel for\nhow the process works, and it doesn't require as much data or computational\nresources as other approaches. It's also a good choice when you need to balance\nthe benefits of LLMs with the limitations of the current generation of models.\n\n# How does RAG fit into the ZenML ecosystem?\n\nIn ZenML, you can set up RAG pipelines that combine the strengths of\nretrieval-based and generation-based models. This allows you to leverage the\npower of LLMs while addressing their limitations. ZenML provides tools for data\ningestion, index store management, and tracking RAG-associated artifacts, making\nit easy to set up and manage RAG pipelines.\n\nZenML also provides a way to scale beyond the limitations of simple RAG\npipelines, as we shall see in later sections of this guide. While you might\nstart off with something simple, at a later point you might want to transition\nto a more complex setup that involves finetuning embeddings, reranking\nretrieved documents, or even finetuning the LLM itself. ZenML provides tools for\nall of these scenarios, making it easy to scale your RAG pipelines as needed.\n\nZenML allows you to track all the artifacts associated with your RAG pipeline,\nfrom hyperparameters and model weights to metadata and performance metrics, as\nwell as all the RAG or LLM-specific artifacts like chains, agents, tokenizers\nand vector stores. These can all be tracked in the \n[Model Control Plane](../../../how-to/use-the-model-control-plane/README.md) and thus\nvisualized in the [ZenML Pro](https://zenml.io/pro) dashboard.\n\nBy bringing all of the above into a simple ZenML\npipeline we achieve a clearly delineated set of steps that can be run and rerun to set up\nour basic RAG pipeline. This is a great starting point for building out more\ncomplex RAG pipelines, and it's a great way to get started with LLMs in a\nsensible way.\n\nA summary of some of the advantages that ZenML brings to the table here includes:\n\n- **Reproducibility**: You can rerun the pipeline to update the index store with\n  new documents or to change the parameters of the chunking process and so on. Previous versions of\n  the artifacts will be preserved, and you can compare the performance of\n    different runs of the pipeline.\n- **Scalability**: You can easily scale the pipeline to handle larger corpora of\n    documents by deploying it on a cloud provider and using a more scalable\n    vector store.\n- **Tracking artifacts and associating them with metadata**: You can track the\n    artifacts generated by the pipeline and associate them with metadata that\n    provides additional context and insights into the pipeline. This metadata\n    and these artifacts are then visible in the ZenML dashboard, allowing you"}
{"input": " to\n    monitor the performance of the pipeline and debug any issues that arise.\n- **Maintainability** - Having your pipeline in a clear, modular format makes it\n    easier to maintain and update. You can easily add new steps, change the\n    parameters of existing steps, and experiment with different configurations\n    to see how they affect the performance of the pipeline.\n- **Collaboration** - You can share the pipeline with your team and collaborate\n    on it together. You can also use the ZenML dashboard to share insights and\n    findings with your team, making it easier to work together on the pipeline.\n\nIn the next section, we'll showcase the components of a basic RAG pipeline. This\nwill give you a taste of how you can leverage the power of LLMs in your MLOps\nworkflows using ZenML. Subsequent sections will cover more advanced topics like\nreranking retrieved documents, finetuning embeddings, and finetuning the LLM\nitself.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Finetune embeddings on custom synthetic data to improve retrieval performance.\n---\n\nWe previously learned [how to use RAG with ZenML](../rag-with-zenml/README.md) to\nbuild a production-ready RAG pipeline. In this section, we will explore how to\noptimize and maintain your embedding models through synthetic data generation and\nhuman feedback. So far, we've been using off-the-shelf embeddings, which provide\na good baseline and decent performance on standard tasks. However, you can often\nsignificantly improve performance by finetuning embeddings on your own domain-specific data.\n\nOur RAG pipeline uses a retrieval-based approach, where it first retrieves the\nmost relevant documents from our vector database, and then uses a language model\nto generate a response based on those documents. By finetuning our embeddings on\na dataset of technical documentation similar to our target domain, we can improve\nthe retrieval step and overall performance of the RAG pipeline.\n\nThe work of finetuning embeddings based on synthetic data and human feedback is\na multi-step process. We'll go through the following steps:\n\n- [generating synthetic data with `distilabel`](synthetic-data-generation.md)\n- [finetuning embeddings with Sentence Transformers](finetuning-embeddings-with-sentence-transformers.md)\n- [evaluating finetuned embeddings and using ZenML's model control plane to get a systematic overview](evaluating-finetuned-embeddings.md)\n\nBesides ZenML, we will do this by using two open source libraries:\n[`argilla`](https://github.com/argilla-io/argilla/) and\n[`distilabel`](https://github.com/argilla-io/distilabel). Both of these\nlibraries focus optimizing model outputs through improving data quality,\nhowever, each one of them takes a different approach to tackle the same problem.\n`distilabel` provides a scalable and reliable approach to distilling knowledge\nfrom LLMs by generating synthetic data or providing AI feedback with LLMs as\njudges. `argilla` enables AI engineers and domain experts to collaborate on data\nprojects by allowing them to organize and explore data through within an\ninteractive and engaging UI. Both libraries can be used individually but they\nwork better together. We'll showcase their use via ZenML pipelines.\n\nTo follow along with the example explained in this guide, please follow the\ninstructions in [the `llm-complete-guide` repository](https://github.com/zenml-io/zenml-projects/llm-complete-guide/README.md) where the full code is also\navailable. This specific section on embeddings finetuning can be run locally or\nusing cloud compute as you prefer.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4"}
{"input": "f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Generate synthetic data with distilabel to finetune embeddings.\n---\n\nWe already have [a dataset of technical documentation](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0) that was generated\npreviously while we were working on the RAG pipeline. We'll use this dataset\nto generate synthetic data with `distilabel`. You can inspect the data directly\n[on the Hugging Face dataset page](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0).\n\n![](../../../.gitbook/assets/rag-dataset-hf.png)\n\nAs you can see, it is made up of some `page_content` (our chunks) as well as the\nsource URL from where the chunk was taken from. With embeddings, what we're\ngoing to want to do is pair the `page_content` with a question that we want to\nanswer. In a pre-LLM world we might have actually created a new column and\nworked to manually craft questions for each chunk. However, with LLMs, we can\nuse the `page_content` to generate questions.\n\n## Pipeline overview\n\nOur pipeline to generate synthetic data will look like this:\n\n![](../../../.gitbook/assets/rag-synthetic-data-pipeline.png)\n\nWe'll load the Hugging Face dataset, then we'll use `distilabel` to generate the\nsynthetic data. To finish off, we'll push the newly-generated data to a new\nHugging Face dataset and also push the same data to our Argilla instance for\nannotation and inspection.\n\n## Synthetic data generation\n\n[`distilabel`](https://github.com/argilla-io/distilabel) provides a scalable and\nreliable approach to distilling knowledge from LLMs by generating synthetic data\nor providing AI feedback with LLMs as judges. We'll be using it a relatively\nsimple use case to generate some queries appropriate to our documentation\nchunks, but it can be used for a variety of other tasks.\n\nWe can set up a `distilabel` pipeline easily in our ZenML step to handle the\ndataset creation. We'll be using `gpt-4o` as the LLM to generate the synthetic\ndata so you can follow along, but `distilabel` supports a variety of other LLM\nproviders (including Ollama) so you can use whatever you have available.\n\n```python\nimport os\nfrom typing import Annotated, Tuple\n\nimport distilabel\nfrom constants import (\n    DATASET_NAME_DEFAULT,\n    OPENAI_MODEL_GEN,\n    OPENAI_MODEL_GEN_KWARGS_EMBEDDINGS,\n)\nfrom datasets import Dataset\nfrom distilabel.llms import OpenAILLM\nfrom distilabel.steps import LoadDataFromHub\nfrom distilabel.steps.tasks import GenerateSentencePair\nfrom zenml"}
{"input": " import step\n\nsynthetic_generation_context = \"\"\"\nThe text is a chunk from technical documentation of ZenML.\nZenML is an MLOps + LLMOps framework that makes your infrastructure and workflow metadata accessible to data science teams.\nAlong with prose explanations, the text chunk may include code snippets and logs but these are identifiable from the surrounding backticks.\n\"\"\"\n\n@step\ndef generate_synthetic_queries(\n    train_dataset: Dataset, test_dataset: Dataset\n) -> Tuple[\n    Annotated[Dataset, \"train_with_queries\"],\n    Annotated[Dataset, \"test_with_queries\"],\n]:\n    llm = OpenAILLM(\n        model=OPENAI_MODEL_GEN, api_key=os.getenv(\"OPENAI_API_KEY\")\n    )\n\n    with distilabel.pipeline.Pipeline(\n        name=\"generate_embedding_queries\"\n    ) as pipeline:\n        load_dataset = LoadDataFromHub(\n            output_mappings={\"page_content\": \"anchor\"},\n        )\n        generate_sentence_pair = GenerateSentencePair(\n            triplet=True,  # `False` to generate only positive\n            action=\"query\",\n            llm=llm,\n            input_batch_size=10,\n            context=synthetic_generation_context,\n        )\n\n        load_dataset >> generate_sentence_pair\n\n    train_distiset = pipeline.run(\n        parameters={\n            load_dataset.name: {\n                \"repo_id\": DATASET_NAME_DEFAULT,\n                \"split\": \"train\",\n            },\n            generate_sentence_pair.name: {\n                \"llm\": {\n                    \"generation_kwargs\": OPENAI_MODEL_GEN_KWARGS_EMBEDDINGS\n                }\n            },\n        },\n    )\n\n    test_distiset = pipeline.run(\n        parameters={\n            load_dataset.name: {\n                \"repo_id\": DATASET_NAME_DEFAULT,\n                \"split\": \"test\",\n            },\n            generate_sentence_pair.name: {\n                \"llm\": {\n                    \"generation_kwargs\": OPENAI_MODEL_GEN_KWARGS_EMBEDDINGS\n                }\n            },\n        },\n    )\n\n    train_dataset = train_distiset[\"default\"][\"train\"]\n    test_dataset = test_distiset[\"default\"][\"train\"]\n\n    return train_dataset, test_dataset\n```\n\nAs you can see, we set up the LLM, create a `distilabel` pipeline, load the\ndataset, mapping the `page_content` column so that it becomes `anchor`. (This\ncolumn renaming will make things easier a bit later when we come to finetuning\nthe embeddings.) Then we generate the synthetic data by using the `GenerateSentencePair`\nstep. This will create queries for each of the chunks in the dataset, so if the\nchunk was about registering a ZenML stack, the query might be \"How do I register\na ZenML stack?\". It will also create negative queries, which are queries that\nwould be inappropriate for the chunk. We do this so that the embeddings model\ncan learn to distinguish between appropriate and inappropriate queries.\n\nWe add some context to the generation process to help the"}
{"input": " LLM\nunderstand the task and the data we're working with. In particular, we explain\nthat some parts of the text are code snippets and logs. We found performance to\nbe better when we added this context.\n\nWhen this step runs within ZenML it will handle spinning up the necessary\nprocesses to make batched LLM calls to the OpenAI API. This is really useful\nwhen working with large datasets. `distilabel` has also implemented a caching\nmechanism to avoid recomputing results for the same inputs. So in this case you\nhave two layers of caching: one in the `distilabel` pipeline and one in the\nZenML orchestrator. This helps [speed up the pace of iteration](https://www.zenml.io/blog/iterate-fast) and saves you money.\n\n## Data annotation with Argilla\n\nOnce we've let the LLM generate the synthetic data, we'll want to inspect it\nand make sure it looks good. We'll do this by pushing the data to an Argilla\ninstance. We add a few extra pieces of metadata to the data to make it easier to\nnavigate and inspect within our data annotation tool. These include:\n\n- `parent_section`: This will be the section of the documentation that the chunk\n  is from.\n- `token_count`: This will be the number of tokens in the chunk.\n- `similarity-positive-negative`: This will be the cosine similarity between the\n  positive and negative queries.\n- `similarity-anchor-positive`: This will be the cosine similarity between the\n  anchor and positive queries.\n- `similarity-anchor-negative`: This will be the cosine similarity between the\n  anchor and negative queries.\n\nWe'll also add the embeddings for the anchor column so that we can use these\nfor retrieval. We'll use the base model (in our case,\n`Snowflake/snowflake-arctic-embed-large`) to generate the embeddings. We use\nthis function to map the dataset and process all the metadata:\n\n```python\ndef format_data(batch):\n    model = SentenceTransformer(\n        EMBEDDINGS_MODEL_ID_BASELINE,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    )\n\n    def get_embeddings(batch_column):\n        vectors = model.encode(batch_column)\n        return [vector.tolist() for vector in vectors]\n\n    batch[\"anchor-vector\"] = get_embeddings(batch[\"anchor\"])\n    batch[\"question-vector\"] = get_embeddings(batch[\"anchor\"])\n    batch[\"positive-vector\"] = get_embeddings(batch[\"positive\"])\n    batch[\"negative-vector\"] = get_embeddings(batch[\"negative\"])\n\n    def get_similarities(a, b):\n        similarities = []\n\n        for pos_vec, neg_vec in zip(a, b):\n            similarity = cosine_similarity([pos_vec], [neg_vec])[0][0]\n            similarities.append(similarity)\n        return similarities\n\n    batch[\"similarity-positive-negative\"] = get_similarities(\n"}
{"input": "        batch[\"positive-vector\"], batch[\"negative-vector\"]\n    )\n    batch[\"similarity-anchor-positive\"] = get_similarities(\n        batch[\"anchor-vector\"], batch[\"positive-vector\"]\n    )\n    batch[\"similarity-anchor-negative\"] = get_similarities(\n        batch[\"anchor-vector\"], batch[\"negative-vector\"]\n    )\n    return batch\n```\n\nThe [rest of the `push_to_argilla` step](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/steps/push_to_argilla.py) is just setting up the Argilla\ndataset and pushing the data to it.\n\nAt this point you'd move to Argilla to view the data, see which examples seem to\nmake sense and which don't. You can update the questions (positive and negative)\nwhich were generated by the LLM. If you want, you can do some data cleaning and\nexploration to improve the data quality, perhaps using the similarity metrics\nthat we calculated earlier.\n\n![Argilla interface for data annotation](../../../.gitbook/assets/argilla-interface-embeddings-finetuning.png)\n\nWe'll next move to actually finetuning the embeddings, assuming you've done some\ndata exploration and annotation. The code will work even without the annotation,\nhowever, since we'll just use the full generated dataset and assume that the\nquality is good enough.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Evaluate finetuned embeddings and compare to original base embeddings.\n---\n\nNow that we've finetuned our embeddings, we can evaluate them and compare to the\nbase embeddings. We have all the data saved and versioned already, and we will\nreuse the same MatryoshkaLoss function for evaluation.\n\nIn code, our evaluation steps are easy to comprehend. Here, for example, is the\nbase model evaluation step:\n\n```python\nfrom zenml import log_model_metadata, step\n\ndef evaluate_model(\n    dataset: DatasetDict, model: SentenceTransformer\n) -> Dict[str, float]:\n    \"\"\"Evaluate the given model on the dataset.\"\"\"\n    evaluator = get_evaluator(\n        dataset=dataset,\n        model=model,\n    )\n    return evaluator(model)\n\n@step\ndef evaluate_base_model(\n    dataset: DatasetDict,\n) -> Annotated[Dict[str, float], \"base_model_evaluation_results\"]:\n    \"\"\"Evaluate the base model on the given dataset.\"\"\"\n    model = SentenceTransformer(\n        EMBEDDINGS_MODEL_ID_BASELINE,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    )\n\n    results = evaluate_model(\n        dataset=dataset,\n        model=model,\n    )\n\n    # Convert numpy.float64 values to regular Python floats\n    # (needed for serialization)\n    base_model_eval = {\n        f\"dim_{dim}_cosine_ndcg@10\": float(\n            results[f\"dim_{dim}_cosine_ndcg@10\"]\n        )\n        for dim in EMBEDDINGS_MODEL_MATRYOSHKA_DIMS\n    }\n\n    log_model_metadata(\n        metadata={\"base_model_eval\": base_model_eval},\n    )\n\n    return results\n```\n\nWe log the results for our core Matryoshka dimensions as model metadata to ZenML\nwithin our evaluation step. This will allow us to inspect these results from\nwithin [the Model Control Plane](https://docs.zenml.io/how-to/use-the-model-control-plane) (see\nbelow for more details). Our results come in the form of a dictionary of string\nkeys and float values which will, like all step inputs and outputs, be\nversioned, tracked and saved in your artifact store.\n\n## Visualizing results\n\nIt's possible to visualize results in a few different ways in ZenML, but one\neasy option is just to output your chart as an `PIL.Image` object. (See our\n[documentation on more ways to visualize your\nresults](../../../how-to/visualize-artifacts/README.md).) The rest the\nimplementation of our `visualize_results` step is just simple `matplotlib` code\nto plot out the base model evaluation against the finetuned model evaluation. We\nrepresent the results as percentage values and horizontally stack the two sets\nto make comparison a little easier.\n\n![Visualizing finetuned embeddings evaluation\nresults](../../../.gitbook/assets/finetuning-embeddings"}
{"input": "-visualization.png)\n\nWe can see that our finetuned embeddings have improved the recall of our\nretrieval system across all of the dimensions, but the results are still not\namazing. In a production setting, we would likely want to focus on improving the\ndata being used for the embeddings training. In particular, we could consider\nstripping out some of the logs output from the documentation, and perhaps omit\nsome pages which offer low signal for the retrieval task. This embeddings\nfinetuning was run purely on the full set of synthetic data generated by\n`distilabel` and `gpt-4o`, so we wouldn't necessarily expect to see huge\nimprovements out of the box, especially when the underlying data chunks are\ncomplex and contain multiple topics.\n\n## Model Control Plane as unified interface\n\nOnce all our pipelines are finished running, the best place to inspect our\nresults as well as the artifacts and models we generated is the Model Control\nPlane.\n\n![Model Control Plane](../../../.gitbook/assets/mcp-embeddings.gif)\n\nThe interface is split into sections that correspond to:\n\n- the artifacts generated by our steps\n- the models generated by our steps\n- the metadata logged by our steps\n- (potentially) any deployments of models made, though we didn't use this in\n  this guide so far\n- any pipeline runs associated with this 'Model'\n\nWe can easily see which are the latest artifact or technical model versions, as\nwell as compare the actual values of our evals or inspect the hardware or\nhyperparameters used for training.\n\nThis one-stop-shop interface is available on ZenML Pro and you can learn more\nabout it in the [Model Control Plane\ndocumentation](https://docs.zenml.io/how-to/use-the-model-control-plane).\n\n## Next Steps\n\nNow that we've finetuned our embeddings and evaluated them, when they were in a\ngood shape for use we could bring these into [the original RAG pipeline](../rag/basic-rag-inference-pipeline.md),\nregenerate a new series of embeddings for our data and then rerun our RAG\nretrieval evaluations to see how they've improved in our hand-crafted and\nLLM-powered evaluations.\n\nThe next section will cover [LLM finetuning and deployment](../finetuning-llms/finetuning-llms.md) as the\nfinal part of our LLMops guide. (This section is currently still a work in\nprogress, but if you're eager to try out LLM finetuning with ZenML, you can use\n[our LoRA\nproject](https://github.com/zenml-io/zenml-projects/blob/main/llm-lora-finetuning/README.md)\nto get started. We also have [a\nblogpost](https://www.zenml.io/blog/how-to-finetune-llama-3-1"}
{"input": "-with-zenml) guide which\ntakes you through\n[all the steps you need to finetune Llama 3.1](https://www.zenml.io/blog/how-to-finetune-llama-3-1-with-zenml) using GCP's Vertex AI with ZenML,\nincluding one-click stack creation!)\n\nTo try out the two pipelines, please follow the instructions in [the project\nrepository README](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/README.md),\nand you can find the full code in that same directory.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Finetune embeddings with Sentence Transformers.\n---\n\nWe now have a dataset that we can use to finetune our embeddings. You can\n[inspect the positive and negative examples](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0_distilabel) on the Hugging Face [datasets page](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0_distilabel) since\nour previous pipeline pushed the data there.\n\n![Synthetic data generated with distilabel for embeddings finetuning](../../../.gitbook/assets/distilabel-synthetic-dataset-hf.png)\n\nOur pipeline for finetuning the embeddings is relatively simple. We'll do the\nfollowing:\n\n- load our data either from Hugging Face or [from Argilla via the ZenML\n  annotation integration](../../../component-guide/annotators/argilla.md)\n- finetune our model using the [Sentence\n  Transformers](https://www.sbert.net/) library\n- evaluate the base and finetuned embeddings\n- visualise the results of the evaluation\n\n![Embeddings finetuning pipeline with Sentence Transformers and\nZenML](../../../.gitbook/assets/rag-finetuning-embeddings-pipeline.png)\n\n## Loading data\n\nBy default the pipeline will load the data from our Hugging Face dataset. If\nyou've annotated your data in Argilla, you can load the data from there instead.\nYou'll just need to pass an `--argilla` flag to the Python invocation when\nyou're running the pipeline like so:\n\n```bash\npython run.py --embeddings --argilla\n```\n\nThis assumes that you've set up an Argilla annotator in your stack. The code\nchecks for the annotator and downloads the data that was annotated in Argilla.\nPlease see our [guide to using the Argilla integration with ZenML](../../../component-guide/annotators/argilla.md) for more details.\n\n## Finetuning with Sentence Transformers\n\nThe `finetune` step in the pipeline is responsible for finetuning the embeddings model using the Sentence Transformers library. Let's break down the key aspects of this step:\n\n1. **Model Loading**: The code loads the base model (`EMBEDDINGS_MODEL_ID_BASELINE`) using the Sentence Transformers library. It utilizes the SDPA (Self-Distilled Pruned Attention) implementation for efficient training with Flash Attention 2.\n\n2. **Loss Function**: The finetuning process employs a custom loss function called `MatryoshkaLoss`. This loss function is a wrapper around the `MultipleNegativesRankingLoss` provided by Sentence Transformers. The Matryoshka approach involves training the model with different embedding dimensions simultaneously. It allows the model to learn embeddings at various granularities, improving its performance across different embedding sizes.\n\n3. **Dataset Preparation"}
{"input": "**: The training dataset is loaded from the provided `dataset` parameter. The code saves the training data to a temporary JSON file and then loads it using the Hugging Face `load_dataset` function.\n\n4. **Evaluator**: An evaluator is created using the `get_evaluator` function. The evaluator is responsible for assessing the model's performance during training.\n\n5. **Training Arguments**: The code sets up the training arguments using the `SentenceTransformerTrainingArguments` class. It specifies various hyperparameters such as the number of epochs, batch size, learning rate, optimizer, precision (TF32 and BF16), and evaluation strategy.\n\n6. **Trainer**: The `SentenceTransformerTrainer` is initialized with the model,\n   training arguments, training dataset, loss function, and evaluator. The\n   trainer handles the training process. The `trainer.train()` method is called\n   to start the finetuning process. The model is trained for the specified\n   number of epochs using the provided hyperparameters.\n\n7. **Model Saving**: After training, the finetuned model is pushed to the Hugging Face Hub using the `trainer.model.push_to_hub()` method. The model is saved with the specified ID (`EMBEDDINGS_MODEL_ID_FINE_TUNED`).\n\n9. **Metadata Logging**: The code logs relevant metadata about the training process, including the training parameters, hardware information, and accelerator details.\n\n10. **Model Rehydration**: To handle materialization errors, the code saves the\n    trained model to a temporary file, loads it back into a new\n    `SentenceTransformer` instance, and returns the rehydrated model.\n\n(*Thanks and credit to Phil Schmid for [his tutorial on finetuning embeddings](https://www.philschmid.de/fine-tune-embedding-model-for-rag) with Sentence\nTransformers and a Matryoshka loss function. This project uses many ideas and\nsome code from his implementation.*)\n\n## Finetuning in code\n\nHere's a simplified code snippet highlighting the key parts of the finetuning process:\n\n```python\n# Load the base model\nmodel = SentenceTransformer(EMBEDDINGS_MODEL_ID_BASELINE)\n# Define the loss function\ntrain_loss = MatryoshkaLoss(model, MultipleNegativesRankingLoss(model))\n# Prepare the training dataset\ntrain_dataset = load_dataset(\"json\", data_files=train_dataset_path)\n# Set up the training arguments\nargs = SentenceTransformerTrainingArguments(...)\n# Create the trainer\ntrainer = SentenceTransformerTrainer(model, args, train_dataset, train_loss)\n# Start training\ntrainer.train()\n# Save the finetuned model\ntrainer.model.push_to_hub(EMBEDDINGS_MODEL_ID_FINE_TUNED)\n```\n\nThe finetuning process leverages the capabilities of the Sentence Transformers library to efficiently train the embeddings model. The Matryoshka approach allows for learning embeddings at different dimensions simultaneously, enhancing the model's performance across various embedding"}
{"input": " sizes.\n\nOur model is finetuned, saved in the Hugging Face Hub for easy access and\nreference in subsequent steps, but also versioned and tracked within ZenML for\nfull observability. At this point the pipeline will evaluate the base and\nfinetuned embeddings and visualise the results.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: See how the retrieval component responds to changes in the pipeline.\n---\n\n# Retrieval evaluation\n\nThe retrieval component of our RAG pipeline is responsible for finding relevant\ndocuments or document chunks to feed into the generation component. In this\nsection we'll explore how to evaluate the performance of the retrieval component\nof your RAG pipeline. We're checking how accurate the semantic search is, or in\nother words how relevant the retrieved documents are to the query.\n\nOur retrieval component takes the incoming query and converts it into a\nvector or embedded representation that can be used to search for relevant\ndocuments. We then use this representation to search through a corpus of\ndocuments and retrieve the most relevant ones.\n\n## Manual evaluation using handcrafted queries\n\nThe most naive and simple way to check this would be to handcraft some queries\nwhere we know the specific documents needed to answer it. We can then check if\nthe retrieval component is able to retrieve these documents. This is a manual\nevaluation process and can be time-consuming, but it's a good way to get a sense\nof how well the retrieval component is working. It can also be useful to target\nknown edge cases or difficult queries to see how the retrieval component handles\nthose known scenarios.\n\n![](/docs/book/.gitbook/assets/retrieval-eval-manual.png)\n\nImplementing this is pretty simple - you just need to create some queries and\ncheck the retrieved documents. Having tested the basic inference of our RAG\nsetup quite a bit, there were some clear areas where the retrieval component\ncould be improved. I looked in our documentation to find some examples where the\ninformation could only be found in a single page and then wrote some queries\nthat would require the retrieval component to find that page. For example, the\nquery \"How do I get going with the Label Studio integration? What are the first\nsteps?\" would require the retrieval component to find [the Label Studio\nintegration\npage](https://docs.zenml.io/stacks-and-components/component-guide/annotators/label-studio).\nSome of the other examples used are:\n\n| Question | URL Ending |\n|----------|------------|\n| How do I get going with the Label Studio integration? What are the first steps? | stacks-and-components/component-guide/annotators/label-studio |\n| How can I write my own custom materializer? | user-guide/advanced-guide/data-management/handle-custom-data-types |\n| How do I generate embeddings as part of a RAG pipeline when using ZenML? | user-guide/llmops-guide/rag-with-zenml/embeddings-generation |\n| How do I use failure hooks in my ZenML pipeline? | user-guide/advanced-guide/pipelining-features/use-failure-success-hooks |\n| Can I deploy ZenML self-hosted with Helm? How do I do it? | deploying-zenml/zenml-self-hosted/deploy-with-helm |\n\nFor"}
{"input": " the retrieval pipeline, all we have to do is encode the query as a vector\nand then query the PostgreSQL database for the most similar vectors. We then\ncheck whether the URL for the document we thought must show up is actually\npresent in the top `n` results.\n\n```python\ndef query_similar_docs(question: str, url_ending: str) -> tuple:\n    embedded_question = get_embeddings(question)\n    db_conn = get_db_conn()\n    top_similar_docs_urls = get_topn_similar_docs(\n        embedded_question, db_conn, n=5, only_urls=True\n    )\n    urls = [url[0] for url in top_similar_docs_urls]  # Unpacking URLs from tuples\n    return (question, url_ending, urls)\n\ndef test_retrieved_docs_retrieve_best_url(question_doc_pairs: list) -> float:\n    total_tests = len(question_doc_pairs)\n    failures = 0\n\n    for pair in question_doc_pairs:\n        question, url_ending, urls = query_similar_docs(\n            pair[\"question\"], pair[\"url_ending\"]\n        )\n        if all(url_ending not in url for url in urls):\n            logging.error(\n                f\"Failed for question: {question}. Expected URL ending: {url_ending}. Got: {urls}\"\n            )\n            failures += 1\n\n    logging.info(f\"Total tests: {total_tests}. Failures: {failures}\")\n    failure_rate = (failures / total_tests) * 100\n    return round(failure_rate, 2)\n```\n\nWe include some logging so that when running the pipeline locally we can get\nsome immediate feedback logged to the console.\n\nThis functionality can then be packaged up into a ZenML step once we're happy it\ndoes what we need:\n\n```python\n@step\ndef retrieval_evaluation_small() -> Annotated[float, \"small_failure_rate_retrieval\"]:\n    failure_rate = test_retrieved_docs_retrieve_best_url(question_doc_pairs)\n    logging.info(f\"Retrieval failure rate: {failure_rate}%\")\n    return failure_rate\n```\n\nWe got a 20% failure rate on the first run of this test, which was a good sign\nthat the retrieval component could be improved. We only had 5 test cases, so\nthis was just a starting point. In reality, you'd want to keep adding more test\ncases to cover a wider range of scenarios. You'll discover these failure cases\nas you use the system more and more, so it's a good idea to keep a record of\nthem and add them to your test suite.\n\nYou'd also want to examine the logs to see exactly which query failed. In our\ncase, checking the logs in the ZenML dashboard, we find the following:\n\n```\nFailed for question: How do I generate embeddings as part of a RAG \npipeline when using ZenML?. Expected URL"}
{"input": " ending: user-guide/llmops-guide/\nrag-with-zenml/embeddings-generation. Got: ['https://docs.zenml.io/user-guide/\nllmops-guide/rag-with-zenml/data-ingestion', 'https://docs.zenml.io/user-guide/\nllmops-guide/rag-with-zenml/understanding-rag', 'https://docs.zenml.io/v/docs/\nuser-guide/advanced-guide/data-management/handle-custom-data-types', 'https://docs.\nzenml.io/user-guide/llmops-guide/rag-with-zenml', 'https://docs.zenml.io/v/docs/\nuser-guide/llmops-guide/rag-with-zenml']\n```\n\nWe can maybe take a look at those documents to see why they were retrieved and\nnot the one we expected. This is a good way to iteratively improve the retrieval\ncomponent.\n\n## Automated evaluation using synthetic generated queries\n\nFor a broader evaluation we can examine a larger number of queries to check the\nretrieval component's performance. We do this by using an LLM to generate\nsynthetic data. In our case we take the text of each document chunk and pass it\nto an LLM, telling it to generate a question. \n\n![](/docs/book/.gitbook/assets/retrieval-eval-automated.png)\n\nFor example, given the text:\n\n```\nzenml orchestrator connect ${ORCHESTRATOR\\_NAME} -iHead on over to our docs to \nlearn more about orchestrators and how to configure them. Container Registry export \nCONTAINER\\_REGISTRY\\_NAME=gcp\\_container\\_registry zenml container-registry register $\n{CONTAINER\\_REGISTRY\\_NAME} --flavor=gcp --uri=<GCR-URI> # Connect the GCS \norchestrator to the target gcp project via a GCP Service Connector zenml \ncontainer-registry connect ${CONTAINER\\_REGISTRY\\_NAME} -i Head on over to our docs to \nlearn more about container registries and how to configure them. 7) Create Stack \nexport STACK\\_NAME=gcp\\_stack zenml stack register ${STACK\\_NAME} -o $\n{ORCHESTRATOR\\_NAME} \\\\ a ${ARTIFACT\\_STORE\\_NAME} -c ${CONTAINER\\_REGISTRY\\_NAME} \n--set In case you want to also add any other stack components to this stack, feel free \nto do so. And you're already done! Just like that, you now have a fully working GCP \nstack ready to go. Feel free to take it for a spin by running a pipeline on it. \nCleanup If you do not want to use any of the created resources in the future, simply \ndelete the project you created. gcloud project delete <PROJECT\\_ID\\_OR\\_NUMBER> <!-- \nFor scarf --> <figure><img alt=\"ZenML Scarf\" \n"}
{"input": "referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?\nx-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure> PreviousScale compute to the \ncloud NextConfiguring ZenML Last updated 2 days ago\n```\n\nwe might get the question:\n\n```\nHow do I create and configure a GCP stack in ZenML using an \norchestrator, container registry, and stack components, and how \ndo I delete the resources when they are no longer needed?\n```\n\nIf we generate questions for all of our chunks, we can then use these\nquestion-chunk pairs to evaluate the retrieval component. We pass the generated\nquery to the retrieval component and then we check if the URL for the original\ndocument is in the top `n` results.\n\nTo generate the synthetic queries we can use the following code:\n\n```python\nfrom typing import List\n\nfrom litellm import completion\nfrom structures import Document\nfrom zenml import step\n\nLOCAL_MODEL = \"ollama/mixtral\"\n\n\ndef generate_question(chunk: str, local: bool = False) -> str:\n    model = LOCAL_MODEL if local else \"gpt-3.5-turbo\"\n    response = completion(\n        model=model,\n        messages=[\n            {\n                \"content\": f\"This is some text from ZenML's documentation. Please generate a question that can be asked about this text: `{chunk}`\",\n                \"role\": \"user\",\n            }\n        ],\n        api_base=\"http://localhost:11434\" if local else None,\n    )\n    return response.choices[0].message.content\n\n\n@step\ndef generate_questions_from_chunks(\n    docs_with_embeddings: List[Document],\n    local: bool = False,\n) -> List[Document]:\n    for doc in docs_with_embeddings:\n        doc.generated_questions = [generate_question(doc.page_content, local)]\n\n    assert all(doc.generated_questions for doc in docs_with_embeddings)\n\n    return docs_with_embeddings\n```\n\nAs you can see, we're using [`litellm`](https://docs.litellm.ai/) again as the\nwrapper for the API calls. This allows us to switch between using a cloud LLM\nAPI (like OpenAI's GPT3.5 or 4) and a local LLM (like a quantized version of\nMistral AI's Mixtral made available with [Ollama](https://ollama.com/). This has\na number of advantages:\n\n- you keep your costs down by using a local model\n- you can iterate faster by not having to wait for API calls\n- you can use the same code for both local and cloud models\n\nFor some tasks you'll want to use the best model your budget can afford, but for\nthis task of question generation we're fine using a"}
{"input": " local and slightly less\ncapable model. Even better is that it'll be much faster to generate the\nquestions, especially using the basic setup we have here.\n\nTo give you an indication of how long this process takes, generating 1800+\nquestions from an equivalent number of documentation chunks took a little over\n45 minutes using the local model on a GPU-enabled machine with Ollama.\n\n![](/docs/book/.gitbook/assets/hf-qa-embedding-questions.png)\n\nYou can [view the generated\ndataset](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions) on\nthe Hugging Face Hub\n[here](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions). This\ndataset contains the original document chunks, the generated questions, and the\nURL reference for the original document.\n\nOnce we have the generated questions, we can then pass them to the retrieval\ncomponent and check the results. For convenience we load the data from the\nHugging Face Hub and then pass it to the retrieval component for evaluation. We\nshuffle the data and select a subset of it to speed up the evaluation process,\nbut for a more thorough evaluation you could use the entire dataset. (The best\npractice of keeping a separate set of data for evaluation purposes is also\nrecommended here, though we're not doing that in this example.)\n\n```python\n@step\ndef retrieval_evaluation_full(\n    sample_size: int = 50,\n) -> Annotated[float, \"full_failure_rate_retrieval\"]:\n    dataset = load_dataset(\"zenml/rag_qa_embedding_questions\", split=\"train\")\n\n    sampled_dataset = dataset.shuffle(seed=42).select(range(sample_size))\n\n    total_tests = len(sampled_dataset)\n    failures = 0\n\n    for item in sampled_dataset:\n        generated_questions = item[\"generated_questions\"]\n        question = generated_questions[\n            0\n        ]  # Assuming only one question per item\n        url_ending = item[\"filename\"].split(\"/\")[\n            -1\n        ]  # Extract the URL ending from the filename\n\n        _, _, urls = query_similar_docs(question, url_ending)\n\n        if all(url_ending not in url for url in urls):\n            logging.error(\n                f\"Failed for question: {question}. Expected URL ending: {url_ending}. Got: {urls}\"\n            )\n            failures += 1\n\n    logging.info(f\"Total tests: {total_tests}. Failures: {failures}\")\n    failure_rate = (failures / total_tests) * 100\n    return round(failure_rate, 2)\n```\n\nWhen we run this as part of the evaluation pipeline, we get a 16% failure rate\nwhich again tells us that we're doing pretty well but that there is room for\nimprovement. As a baseline, this is a good starting point. We can then iterate"}
{"input": "\non the retrieval component to improve its performance. \n\nTo take this further, there are a number of ways it might be improved:\n\n- **More diverse question generation**: The current question generation approach\n  uses a single prompt to generate questions based on the document chunks. You\n  could experiment with different prompts or techniques to generate a wider\n  variety of questions that test the retrieval component more thoroughly. For\n  example, you could prompt the LLM to generate questions of different types\n  (factual, inferential, hypothetical, etc.) or difficulty levels.\n- **Semantic similarity metrics**: In addition to checking if the expected URL\n  is retrieved, you could calculate semantic similarity scores between the query\n  and the retrieved documents using metrics like cosine similarity. This would\n  give you a more nuanced view of retrieval performance beyond just binary\n  success/failure. You could track average similarity scores and use them as a\n  target metric to improve.\n- **Comparative evaluation**: Test out different retrieval approaches (e.g.\n  different embedding models, similarity search algorithms, etc.) and compare\n  their performance on the same set of queries. This would help identify the\n  strengths and weaknesses of each approach.\n- **Error analysis**: Do a deeper dive into the failure cases to understand\n  patterns and potential areas for improvement. Are certain types of questions\n  consistently failing? Are there common characteristics among the documents\n  that aren't being retrieved properly? Insights from error analysis can guide\n  targeted improvements to the retrieval component.\n\nTo wrap up, the retrieval evaluation process we've walked through - from manual\nspot-checking with carefully crafted queries to automated testing with synthetic\nquestion-document pairs - has provided a solid baseline understanding of our\nretrieval component's performance. The failure rates of 20% on our handpicked\ntest cases and 16% on a larger sample of generated queries highlight clear room\nfor improvement, but also validate that our semantic search is generally\npointing in the right direction.\n\nGoing forward, we have a rich set of options to refine and upgrade our\nevaluation approach. Generating a more diverse array of test questions,\nleveraging semantic similarity metrics for a nuanced view beyond binary\nsuccess/failure, performing comparative evaluations of different retrieval\ntechniques, and conducting deep error analysis on failure cases - all of these\navenues promise to yield valuable insights. As our RAG pipeline grows to handle\nmore complex and wide-ranging queries, continued investment in comprehensive\nretrieval evaluation will be essential to ensure we're always surfacing the most\nrelevant information.\n\nBefore we start working to improve or tweak our retrieval based on these\nevaluation results, let's shift gears and look at how we can evaluate the\ngeneration component of our RAG pipeline. Assessing the quality of the final\nanswers produced by the system is equally crucial to gauging the effectiveness\nof our retrieval.\n\nRetrieval is only half the story"}
{"input": ". The true test of our system is the quality\nof the final answers it generates by combining retrieved content with LLM\nintelligence. In the next section, we'll dive into a parallel evaluation process\nfor the generation component, exploring both automated metrics and human\nassessment to get a well-rounded picture of our RAG pipeline's end-to-end\nperformance. By shining a light on both halves of the RAG architecture, we'll be\nwell-equipped to iterate and optimize our way to an ever more capable and\nreliable question-answering system.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/)\nrepository and for this section, particularly [the `eval_retrieval.py`\nfile](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/steps/eval_retrieval.py).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to evaluate the performance of your RAG system in practice.\n---\n\n# Evaluation in practice\n\nNow that we've seen individually how to evaluate the retrieval and generation components of our pipeline, it's worth taking a step back to think through how all of this works in practice.\n\nOur example project includes the evaluation as a separate pipeline that optionally runs after the main pipeline that generates and populates the embeddings. This is a good practice to follow, as it allows you to separate the concerns of generating the embeddings and evaluating them. Depending on the specific use case, the evaluations could be included as part of the main pipeline and used as a gating mechanism to determine whether the embeddings are good enough to be used in production.\n\nGiven some of the performance constraints of the LLM judge, it might be worth experimenting with using a local LLM judge for evaluation during the course of the development process and then running the full evaluation using a cloud LLM like Anthropic's Claude or OpenAI's GPT-3.5 or 4. This can help you iterate faster and get a sense of how well your embeddings are performing before committing to the cost of running the full evaluation.\n\n## Automated evaluation isn't a silver bullet\n\nWhile automating the evaluation process can save you time and effort, it's important to remember that it doesn't replace the need for a human to review the results. The LLM judge is expensive to run, and it takes time to get the results back. Automating the evaluation process can help you focus on the details and the data, but it doesn't replace the need for a human to review the results and make sure that the embeddings (and the RAG system as a whole) are performing as expected.\n\n## When and how much to evaluate\n\nThe frequency and depth of evaluation will depend on your specific use case and the constraints of your project. In an ideal world, you would evaluate the performance of your embeddings and the RAG system as a whole as often as possible, but in practice, you'll need to balance the cost of running the evaluation with the need to iterate quickly.\n\nSome tests can be run quickly and cheaply (notably the tests of the retrieval system) while others (like the LLM judge) are more expensive and time-consuming. You should structure your RAG tests and evaluation to reflect this, with some tests running frequently and others running less often, just as you would in any other software project.\n\nThere's more we could improve our evaluation system, but for now we can continue onwards to [adding a reranker](../reranking/README.md) to improve our retrieval. This will allow us to improve the performance of our retrieval system without needing to retrain the embeddings. We'll cover this in the next section.\n\n## Try it out!\n\nTo see how this works in practice, you can run the evaluation pipeline using the project code. This will give you a sense of how the evaluation process works in practice and you can"}
{"input": " of course then play with and modify the evaluation code.\n\nTo run the evaluation pipeline, first clone the project repository:\n\n```bash\ngit clone https://github.com/zenml-io/zenml-projects.git\n```\n\nThen navigate to the `llm-complete-guide` directory and follow the instructions in the `README.md` file to run the evaluation pipeline. (You'll have to have first run the main pipeline to generate the embeddings.)\n\nTo run the evaluation pipeline, you can use the following command:\n\n```bash\npython run.py --evaluation\n```\n\nThis will run the evaluation pipeline and output the results to the console. You can then inspect the progress, logs and results in the dashboard!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to implement evaluation for RAG in just 65 lines of code.\n---\n\n# Evaluation in 65 lines of code\n\nOur RAG guide included [a short example](../rag-with-zenml/rag-85-loc.md) for how to implement a basic RAG pipeline in just 85 lines of code. In this section, we'll build on that example to show how you can evaluate the performance of your RAG pipeline in just 65 lines. For the full code, please visit the project repository [here](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/most\\_basic\\_eval.py). The code that follows requires the functions from the earlier RAG pipeline code to work.\n\n```python\n# ...previous RAG pipeline code here...\n# see https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/most_basic_rag_pipeline.py\n\neval_data = [\n    {\n        \"question\": \"What creatures inhabit the luminescent forests of ZenML World?\",\n        \"expected_answer\": \"The luminescent forests of ZenML World are inhabited by glowing Zenbots.\",\n    },\n    {\n        \"question\": \"What do Fractal Fungi do in the melodic caverns of ZenML World?\",\n        \"expected_answer\": \"Fractal Fungi emit pulsating tones that resonate through the crystalline structures, creating a symphony of otherworldly sounds in the melodic caverns of ZenML World.\",\n    },\n    {\n        \"question\": \"Where do Gravitational Geckos live in ZenML World?\",\n        \"expected_answer\": \"Gravitational Geckos traverse the inverted cliffs of ZenML World.\",\n    },\n]\n\n\ndef evaluate_retrieval(question, expected_answer, corpus, top_n=2):\n    relevant_chunks = retrieve_relevant_chunks(question, corpus, top_n)\n    score = any(\n        any(word in chunk for word in tokenize(expected_answer))\n        for chunk in relevant_chunks\n    )\n    return score\n\n\ndef evaluate_generation(question, expected_answer, generated_answer):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an evaluation judge. Given a question, an expected answer, and a generated answer, your task is to determine if the generated answer is relevant and accurate. Respond with 'YES' if the generated answer is satisfactory, or 'NO' if it is not.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {question}\\nExpected Answer: {expected_answer}\\nGenerated Answer: {generated_answer}\\nIs the generated answer relevant and accurate?\",\n            },\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n    judgment"}
{"input": " = chat_completion.choices[0].message.content.strip().lower()\n    return judgment == \"yes\"\n\n\nretrieval_scores = []\ngeneration_scores = []\n\nfor item in eval_data:\n    retrieval_score = evaluate_retrieval(\n        item[\"question\"], item[\"expected_answer\"], corpus\n    )\n    retrieval_scores.append(retrieval_score)\n\n    generated_answer = answer_question(item[\"question\"], corpus)\n    generation_score = evaluate_generation(\n        item[\"question\"], item[\"expected_answer\"], generated_answer\n    )\n    generation_scores.append(generation_score)\n\nretrieval_accuracy = sum(retrieval_scores) / len(retrieval_scores)\ngeneration_accuracy = sum(generation_scores) / len(generation_scores)\n\nprint(f\"Retrieval Accuracy: {retrieval_accuracy:.2f}\")\nprint(f\"Generation Accuracy: {generation_accuracy:.2f}\")\n```\n\nAs you can see, we've added two evaluation functions: `evaluate_retrieval` and `evaluate_generation`. The `evaluate_retrieval` function checks if the retrieved chunks contain any words from the expected answer. The `evaluate_generation` function uses OpenAI's chat completion LLM to evaluate the quality of the generated answer.\n\nWe then loop through the evaluation data, which contains questions and expected answers, and evaluate the retrieval and generation components of our RAG pipeline. Finally, we calculate the accuracy of both components and print the results:\n\n![](../../../.gitbook/assets/evaluation-65-loc.png)\n\nAs you can see, we get 100% accuracy for both retrieval and generation in this example. Not bad! The sections that follow will provide a more detailed and sophisticated implementation of RAG evaluation, but this example shows how you can think about it at a high level!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Track how your RAG pipeline improves using evaluation and metrics.\n---\n\n# Evaluation and metrics\n\nIn this section, we'll explore how to evaluate the performance of your RAG pipeline using metrics and visualizations. Evaluating your RAG pipeline is crucial to understanding how well it performs and identifying areas for improvement. With language models in particular, it's hard to evaluate their performance using traditional metrics like accuracy, precision, and recall. This is because language models generate text, which is inherently subjective and difficult to evaluate quantitatively.\n\nOur RAG pipeline is a whole system, moreover, not just a model, and evaluating it requires a holistic approach. We'll look at various ways to evaluate the performance of your RAG pipeline but the two main areas we'll focus on are:\n\n* [Retrieval evaluation](retrieval.md), so checking that the retrieved documents or document chunks are relevant to the query.\n* [Generation evaluation](generation.md), so checking that the generated text is coherent and helpful for our specific use case.\n\n![](../../../.gitbook/assets/evaluation-two-parts.png)\n\nIn the previous section we built out a basic RAG pipeline for our documentation question-and-answer use case. We'll use this pipeline to demonstrate how to evaluate the performance of your RAG pipeline.\n\n{% hint style=\"info\" %}\nIf you were running this in a production setting, you might want to set up evaluation to check the performance of a raw LLM model (i.e. without any retrieval / RAG components) as a baseline, and then compare this to the performance of your RAG pipeline. This will help you understand how much value the retrieval and generation components are adding to your system. We won't cover this here, but it's a good practice to keep in mind.\n{% endhint %}\n\n## What are we evaluating?\n\nWhen evaluating the performance of your RAG pipeline, your specific use case and the extent to which you can tolerate errors or lower performance will determine what you need to evaluate. For instance, if you're building a user-facing chatbot, you might need to evaluate the following:\n\n* Are the retrieved documents relevant to the query?\n* Is the generated answer coherent and helpful for your specific use case?\n* Does the generated answer contain hate speech or any sort of toxic language?\n\nThese are just examples, and the specific metrics and methods you use will depend on your use case. The [generation evaluation](generation.md) functions as an end-to-end evaluation of the RAG pipeline, as it checks the final output of the system. It's during these end-to-end evaluations that you'll have most leeway to use subjective metrics, as you're evaluating the system as a whole.\n\nBefore we dive into the details, let's take a moment to look at [a short high-level code example](evaluation-in-65-loc.md) showcasing the two main areas of evaluation. Afterwards the following sections will cover the two main areas of evaluation in more"}
{"input": " detail [as well as offer practical guidance](../evaluation/evaluation-in-practice.md) on when to run these evaluations and what to look for in the results.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Evaluate the generation component of your RAG pipeline.\n---\n\n# Generation evaluation\n\nNow that we have a sense of how to evaluate the retrieval component of our RAG\npipeline, let's move on to the generation component. The generation component is\nresponsible for generating the answer to the question based on the retrieved\ncontext. At this point, our evaluation starts to move into more subjective\nterritory. It's harder to come up with metrics that can accurately capture the\nquality of the generated answers. However, there are some things we can do.\n\nAs with the [retrieval evaluation](retrieval.md), we can start with a simple\napproach and then move on to more sophisticated methods.\n\n## Handcrafted evaluation tests\n\nAs in the retrieval evaluation, we can start by putting together a set of\nexamples where we know that our generated output should or shouldn't include\ncertain terms. For example, if we're generating answers to questions about\nwhich orchestrators ZenML supports, we can check that the generated answers\ninclude terms like \"Airflow\" and \"Kubeflow\" (since we do support them) and\nexclude terms like \"Flyte\" or \"Prefect\" (since we don't (yet!) support them).\nThese handcrafted tests should be driven by mistakes that you've already seen in\nthe RAG output. The negative example of \"Flyte\" and \"Prefect\" showing up in the\nlist of supported orchestrators, for example, shows up sometimes when you use\nGPT 3.5 as the LLM.\n\n![](/docs/book/.gitbook/assets/generation-eval-manual.png)\n\nAs another example, when you make a query asking 'what is the default\norchestrator in ZenML?' you would expect that the answer would include the word\n'local', so we can make a test case to confirm that.\n\nYou can view our starter set of these tests\n[here](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/steps/eval_e2e.py#L28-L55).\nIt's better to start with something small and simple and then expand as is\nneeded. There's no need for complicated harnesses or frameworks at this stage.\n\n**`bad_answers` table:**\n\n| Question | Bad Words |\n|----------|-----------|\n| What orchestrators does ZenML support? | AWS Step Functions, Flyte, Prefect, Dagster |\n| What is the default orchestrator in ZenML? | Flyte, AWS Step Functions |\n\n**`bad_immediate_responses` table:**\n\n| Question | Bad Words |\n|----------|-----------|\n| Does ZenML support the Flyte orchestrator out of the box? | Yes |\n\n**`good_responses` table:**\n\n| Question | Good Words |\n|----------|------------|\n| What are the supported orchestrators in ZenML? Please"}
{"input": " list as many of the supported ones as possible. | Kubeflow, Airflow |\n| What is the default orchestrator in ZenML? | local |\n\nEach type of test then catches a specific type of mistake. For example:\n\n```python\nclass TestResult(BaseModel):\n    success: bool\n    question: str\n    keyword: str = \"\"\n    response: str\n\n\ndef test_content_for_bad_words(\n    item: dict, n_items_retrieved: int = 5\n) -> TestResult:\n    question = item[\"question\"]\n    bad_words = item[\"bad_words\"]\n    response = process_input_with_retrieval(\n        question, n_items_retrieved=n_items_retrieved\n    )\n    for word in bad_words:\n        if word in response:\n            return TestResult(\n                success=False,\n                question=question,\n                keyword=word,\n                response=response,\n            )\n    return TestResult(success=True, question=question, response=response)\n```\n\nHere we're testing that a particular word doesn't show up in the generated\nresponse. If we find the word, then we return a failure, otherwise we return a\nsuccess. This is a simple example, but you can imagine more complex tests that\ncheck for the presence of multiple words, or the presence of a word in a\nparticular context.\n\nWe pass these custom tests into a test runner that keeps track of how many are\nfailing and also logs those to the console when they do:\n\n```python\ndef run_tests(test_data: list, test_function: Callable) -> float:\n    failures = 0\n    total_tests = len(test_data)\n    for item in test_data:\n        test_result = test_function(item)\n        if not test_result.success:\n            logging.error(\n                f\"Test failed for question: '{test_result.question}'. Found word: '{test_result.keyword}'. Response: '{test_result.response}'\"\n            )\n            failures += 1\n    failure_rate = (failures / total_tests) * 100\n    logging.info(\n        f\"Total tests: {total_tests}. Failures: {failures}. Failure rate: {failure_rate}%\"\n    )\n    return round(failure_rate, 2)\n```\n\nOur end-to-end evaluation of the generation component is then a combination of\nthese tests:\n\n```python\n@step\ndef e2e_evaluation() -> (\n    Annotated[float, \"failure_rate_bad_answers\"],\n    Annotated[float, \"failure_rate_bad_immediate_responses\"],\n    Annotated[float, \"failure_rate_good_responses\"],\n):\n    logging.info(\"Testing bad answers...\")\n    failure_rate_bad_answers = run_tests(\n        bad_answers, test_content_for_bad_words\n    )\n    logging.info(f\"Bad answers failure rate: {failure_rate_bad_answers}%\")\n\n    logging.info(\"Testing bad immediate responses...\")\n    failure_rate_bad_immediate_responses = run_tests(\n        bad_immediate_responses, test_response"}
{"input": "_starts_with_bad_words\n    )\n    logging.info(\n        f\"Bad immediate responses failure rate: {failure_rate_bad_immediate_responses}%\"\n    )\n\n    logging.info(\"Testing good responses...\")\n    failure_rate_good_responses = run_tests(\n        good_responses, test_content_contains_good_words\n    )\n    logging.info(\n        f\"Good responses failure rate: {failure_rate_good_responses}%\"\n    )\n    return (\n        failure_rate_bad_answers,\n        failure_rate_bad_immediate_responses,\n        failure_rate_good_responses,\n    )\n```\n\nRunning the tests using different LLMs will give different results. Here our\nOllama Mixtral did worse than GPT 3.5, for example, but there were still some\nfailures with GPT 3.5. This is a good way to get a sense of how well your\ngeneration component is doing.\n\nAs you become more familiar with the kinds of outputs your LLM generates, you\ncan add the hard ones to this test suite. This helps prevent regressions and\nis directly related to the quality of the output you're getting. This way you\ncan optimize for your specific use case.\n\n## Automated evaluation using another LLM\n\nAnother way to evaluate the generation component is to use another LLM to\ngrade the output of the LLM you're evaluating. This is a more sophisticated\napproach and requires a bit more setup. We can use the pre-generated questions\nand the associated context as input to the LLM and then use another LLM to\nassess the quality of the output on a scale of 1 to 5. This is a more\nquantitative approach and since it's automated it can run across a larger set of\ndata.\n\n![](/docs/book/.gitbook/assets/generation-eval-automated.png)\n\n{% hint style=\"warning\" %}\nLLMs don't always do well on this kind of evaluation where numbers are involved.\nThere are some studies showing that LLMs can be biased towards certain numbers\nor ranges of numbers. This is something to keep in mind when using this\napproach. Qualitative evaluations are often more reliable but then that means a\nhuman has to do the evaluation.\n{% endhint %}\n\nWe can start by setting up a Pydantic model to hold the data we need. We set\nconstraints to ensure that the data we're getting back are only integers between\n1 and 5, inclusive:\n\n```python\nclass LLMJudgedTestResult(BaseModel):\n    toxicity: conint(ge=1, le=5)\n    faithfulness: conint(ge=1, le=5)\n    helpfulness: conint(ge=1, le=5)\n    relevance: conint(ge=1, le=5)\n```\n\nWe can use this in a test function that:\n\n- takes a question and a context as inputs\n- generates an answer using the LLM we"}
{"input": "'re evaluating\n- makes a call to an (optionally different) LLM we're using to judge the quality\n  of the answer getting back a score for each of the four categories in JSON\n  format\n- parses the JSON and returns the result of the evaluation as our Pydantic\n  model instance\n\nPydantic handles the validation of the JSON input for us, so we can be sure that\nwe're getting the data we expect and in a form that we can use.\n\n```python\ndef llm_judged_test_e2e(\n    question: str,\n    context: str,\n    n_items_retrieved: int = 5,\n) -> LLMJudgedTestResult:\n    \"\"\"E2E tests judged by an LLM.\n\n    Args:\n        item (dict): The item to test.\n        n_items_retrieved (int): The number of items to retrieve.\n\n    Returns:\n        LLMJudgedTestResult: The result of the test.\n    \"\"\"\n    logging.debug(\"Starting LLM judged test...\")\n    response = process_input_with_retrieval(\n        question, n_items_retrieved=n_items_retrieved\n    )\n    logging.debug(\"Input processed with retrieval.\")\n    prompt = f\"\"\"\n    Please analyze the following text and provided context to provide scores\n    for toxicity, faithfulness, helpfulness, and relevance. The toxicity score \n    should be an integer between 1 and 5, where 1 is not toxic and 5 is very\n    toxic. The faithfulness score should be an integer between 1 and 5, where 1\n    is not faithful to the provided context and 5 is very faithful. \n    The helpfulness score should be an integer between 1 and 5, where 1 is not\n    helpful and 5 is very helpful. The relevance score should be an integer\n    between 1 and 5, where 1 is not relevant given the context and 5 is very relevant.\n\n    **Text:** {response}\n\n    **Context:** {context}\n\n    **Output format:**\n    {{\n        \"toxicity\": int,\n        \"faithfulness\": int,\n        \"helpfulness\": int,\n        \"relevance\": int\n    }}\n    \"\"\"\n    logging.debug(\"Prompt created.\")\n    response = completion(\n        model=\"gpt-4-turbo\", messages=[{\"content\": prompt, \"role\": \"user\"}]\n    )\n\n    json_output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n    logging.info(\"Received response from model.\")\n    logging.debug(json_output)\n    try:\n        return LLMJudgedTestResult(**json.loads(json_output))\n    except json.JSONDecodeError as e:\n        logging.error(f\"JSON bad output: {json_output}\")\n        raise e\n```\n\nCurrently we're not handling retries of the output from the LLM in the case\nwhere the"}
{"input": " JSON isn't output correctly, but potentially that's something we might\nwant to do.\n\nWe can then run this test across a set of questions and contexts:\n\n```python\ndef run_llm_judged_tests(\n    test_function: Callable,\n    sample_size: int = 50,\n) -> Tuple[\n    Annotated[float, \"average_toxicity_score\"],\n    Annotated[float, \"average_faithfulness_score\"],\n    Annotated[float, \"average_helpfulness_score\"],\n    Annotated[float, \"average_relevance_score\"],\n]:\n    dataset = load_dataset(\"zenml/rag_qa_embedding_questions\", split=\"train\")\n\n    # Shuffle the dataset and select a random sample\n    sampled_dataset = dataset.shuffle(seed=42).select(range(sample_size))\n\n    total_tests = len(sampled_dataset)\n    total_toxicity = 0\n    total_faithfulness = 0\n    total_helpfulness = 0\n    total_relevance = 0\n\n    for item in sampled_dataset:\n        question = item[\"generated_questions\"][0]\n        context = item[\"page_content\"]\n\n        try:\n            result = test_function(question, context)\n        except json.JSONDecodeError as e:\n            logging.error(f\"Failed for question: {question}. Error: {e}\")\n            total_tests -= 1\n            continue\n        total_toxicity += result.toxicity\n        total_faithfulness += result.faithfulness\n        total_helpfulness += result.helpfulness\n        total_relevance += result.relevance\n\n    average_toxicity_score = total_toxicity / total_tests\n    average_faithfulness_score = total_faithfulness / total_tests\n    average_helpfulness_score = total_helpfulness / total_tests\n    average_relevance_score = total_relevance / total_tests\n\n    return (\n        round(average_toxicity_score, 3),\n        round(average_faithfulness_score, 3),\n        round(average_helpfulness_score, 3),\n        round(average_relevance_score, 3),\n    )\n```\n\nYou'll want to use your most capable and reliable LLM to do the judging. In our\ncase, we used the new GPT-4 Turbo. The quality of the evaluation is only as good\nas the LLM you're using to do the judging and there is a large difference\nbetween GPT-3.5 and GPT-4 Turbo in terms of the quality of the output, not least\nin its ability to output JSON correctly.\n\nHere was the output following an evaluation for 50 randomly sampled datapoints:\n\n```shell\nStep e2e_evaluation_llm_judged has started.\nAverage toxicity: 1.0\nAverage faithfulness: 4.787\nAverage helpfulness: 4.595\nAverage relevance: 4.87\nStep e2e_evaluation_llm_judged has finished in 8m51s.\nPipeline run has"}
{"input": " finished in 8m52s.\n```\n\nThis took around 9 minutes to run using GPT-4 Turbo as the evaluator and the\ndefault GPT-3.5 as the LLM being evaluated.\n\nTo take this further, there are a number of ways it might be improved:\n\n- **Retries**: As mentioned above, we're not currently handling retries of the\n  output from the LLM in the case where the JSON isn't output correctly. This\n  could be improved by adding a retry mechanism that waits for a certain amount\n  of time before trying again. (We could potentially use the\n  [`instructor`](https://github.com/jxnl/instructor) library to handle this\n  specifically.)\n- **Use OpenAI's 'JSON mode'**: OpenAI has a [JSON\n  mode](https://platform.openai.com/docs/guides/text-generation/json-mode) that\n  can be used to ensure that the output is always in JSON format. This could be\n  used to ensure that the output is always in the correct format.\n- **More sophisticated evaluation**: The evaluation we're doing here is quite\n    simple. We're just asking for a score in four categories. There are more\n    sophisticated ways to evaluate the quality of the output, such as using\n    multiple evaluators and taking the average score, or using a more complex\n    scoring system that takes into account the context of the question and the\n    context of the answer.\n- **Batch processing**: We're running the evaluation one question at a time\n  here. It would be more efficient to run the evaluation in batches to speed up\n  the process.\n- **More data**: We're only using 50 samples here. This could be increased to\n  get a more accurate picture of the quality of the output.\n- **More LLMs**: We're only using GPT-4 Turbo here. It would be interesting to\n  see how other LLMs perform as evaluators.\n- **Handcrafted questions based on context**: We're using the generated\n  questions here. It would be interesting to see how the LLM performs when given\n  handcrafted questions that are based on the context of the question.\n- **Human in the loop**: The LLM actually provides qualitative feedback on the\n  output as well as the JSON scores. This data could be passed into an\n  annotation tool to get human feedback on the quality of the output. This would\n    be a more reliable way to evaluate the quality of the output and would offer\n    some insight into the kinds of mistakes the LLM is making.\n\nMost notably, the scores we're currently getting are pretty high, so it would\nmake sense to pass in harder questions and be more specific in the judging\ncriteria. This will give us more room to improve as it is sure that the system\nis not perfect.\n\nWhile this evaluation approach"}
{"input": " serves as a solid foundation, it's worth noting that there are other frameworks available that can further enhance the evaluation process. Frameworks such as [`ragas`](https://github.com/explodinggradients/ragas), [`trulens`](https://www.trulens.org/), [DeepEval](https://docs.confident-ai.com/), and [UpTrain](https://github.com/uptrain-ai/uptrain) can be integrated with ZenML depending on your specific use-case and understanding of the underlying concepts. These frameworks, although potentially complex to set up and use, can provide more sophisticated evaluation capabilities as your project evolves and grows in complexity.\n\nWe now have a working evaluation of both the retrieval and generation evaluation\ncomponents of our RAG pipeline. We can use this to track how our pipeline\nimproves as we make changes to the retrieval and generation components.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/)\nrepository and for this section, particularly [the `eval_e2e.py` file](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/steps/eval_e2e.py).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Track how your RAG pipeline improves using evaluation and metrics.\n---\n\n# Evaluation and metrics for LLMOps\n\nIn this section, we'll explore how to evaluate the performance of your RAG\npipeline using metrics and visualizations. Evaluating your RAG pipeline is\ncrucial to understanding how well it performs and identifying areas for\nimprovement. With language models in particular, it's hard to evaluate their\nperformance using traditional metrics like accuracy, precision, and recall. This\nis because language models generate text, which is inherently subjective and\ndifficult to evaluate quantitatively.\n\nOur RAG pipeline is a whole system, moreover, not just a model, and evaluating\nit requires a holistic approach. We'll look at various ways to evaluate the\nperformance of your RAG pipeline but the two main areas we'll focus on are:\n\n- [Retrieval evaluation](retrieval.md), so checking that the\n  retrieved documents or document chunks are relevant to the query.\n- [Generation evaluation](generation.md), so checking that the\n  generated text is coherent and helpful for our specific use case.\n\n![](/docs/book/.gitbook/assets/evaluation-two-parts.png)\n\nIn the previous section we built out a basic RAG pipeline for our documentation\nquestion-and-answer use case. We'll use this pipeline to demonstrate how to\nevaluate the performance of your RAG pipeline.\n\n{% hint style=\"info\" %}\nIf you were running this in a production setting, you might want to set up evaluation\nto check the performance of a raw LLM model (i.e. without any retrieval / RAG\ncomponents) as a baseline, and then compare this to the performance of your RAG\npipeline. This will help you understand how much value the retrieval and\ngeneration components are adding to your system. We won't cover this here, but\nit's a good practice to keep in mind.\n{% endhint %}\n\n## What are we evaluating?\n\nWhen evaluating the performance of your RAG pipeline, your specific use case and\nthe extent to which you can tolerate errors or lower performance will determine\nwhat you need to evaluate. For instance, if you're building a user-facing\nchatbot, you might need to evaluate the following:\n\n- Are the retrieved documents relevant to the query?\n- Is the generated answer coherent and helpful for your specific use case?\n- Does the generated answer contain hate speech or any sort of toxic language?\n\nThese are just examples, and the specific metrics and methods you use will\ndepend on your use case. The [generation evaluation](generation.md) functions as\nan end-to-end evaluation of the RAG pipeline, as it checks the final output of\nthe system. It's during these end-to-end evaluations that you'll have most\nleeway to use subjective metrics, as you're evaluating the system as a whole.\n\nBefore we dive into the details, let's take a moment to look at ["}
{"input": "a short high-level code example](evaluation-in-65-loc.md) showcasing the two main areas of evaluation. Afterwards\nthe following sections will cover the two main areas of evaluation in more\ndetail [as well as offer practical\nguidance](evaluation/evaluation-in-practice.md) on when to run these evaluations\nand what to look for in the results.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Use your RAG components to generate responses to prompts.\n---\n\n# Simple RAG Inference\n\nNow that we have our index store, we can use it to make queries based on the\ndocuments in the index store. We use some utility functions to make this happen\nbut no external libraries are needed beyond an interface to the index store as\nwell as the LLM itself.\n\n![](/docs/book/.gitbook/assets/rag-stage-4.png)\n\nIf you've been following along with the guide, you should have some documents\ningested already and you can pass a query in as a flag to the Python command\nused to run the pipeline:\n\n```bash\npython run.py --rag-query \"how do I use a custom materializer inside my own zenml \nsteps? i.e. how do I set it? inside the @step decorator?\" --model=gpt4\n```\n\n![](/docs/book/.gitbook/assets/rag-inference.png)\n\nThis inference query itself is not a ZenML pipeline, but rather a function call\nwhich uses the outputs and components of our pipeline to generate the response.\nFor a more complex inference setup, there might be even more going on here, but\nfor the purposes of this initial guide we will keep it simple.\n\nBringing everything together, the code for the inference pipeline is as follows:\n\n```python\ndef process_input_with_retrieval(\n    input: str, model: str = OPENAI_MODEL, n_items_retrieved: int = 5\n) -> str:\n    delimiter = \"```\"\n\n    # Step 1: Get documents related to the user input from database\n    related_docs = get_topn_similar_docs(\n        get_embeddings(input), get_db_conn(), n=n_items_retrieved\n    )\n\n    # Step 2: Get completion from OpenAI API\n    # Set system message to help set appropriate tone and context for model\n    system_message = f\"\"\"\n    You are a friendly chatbot. \\\n    You can answer questions about ZenML, its features and its use cases. \\\n    You respond in a concise, technically credible tone. \\\n    You ONLY use the context from the ZenML documentation to provide relevant\n    answers. \\\n    You do not make up answers or provide opinions that you don't have\n    information to support. \\\n    If you are unsure or don't know, just say so. \\\n    \"\"\"\n\n    # Prepare messages to pass to model\n    # We use a delimiter to help the model understand the where the user_input\n    # starts and ends\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": f\"{delimiter}{input}{delimiter}\"},\n        {\n            \"role\": \"assistant\",\n            \"content\": f\"Relevant ZenML documentation: \\n\"\n            + \"\\n\".join(doc[0] for doc"}
{"input": " in related_docs),\n        },\n    ]\n    logger.debug(\"CONTEXT USED\\n\\n\", messages[2][\"content\"], \"\\n\\n\")\n    return get_completion_from_messages(messages, model=model)\n```\n\nFor the `get_topn_similar_docs` function, we use the embeddings generated from\nthe documents in the index store to find the most similar documents to the\nquery:\n\n```python\ndef get_topn_similar_docs(\n    query_embedding: List[float],\n    conn: psycopg2.extensions.connection,\n    n: int = 5,\n    include_metadata: bool = False,\n    only_urls: bool = False,\n) -> List[Tuple]:\n    embedding_array = np.array(query_embedding)\n    register_vector(conn)\n    cur = conn.cursor()\n\n    if include_metadata:\n        cur.execute(\n            f\"SELECT content, url FROM embeddings ORDER BY embedding <=> %s LIMIT {n}\",\n            (embedding_array,),\n        )\n    elif only_urls:\n        cur.execute(\n            f\"SELECT url FROM embeddings ORDER BY embedding <=> %s LIMIT {n}\",\n            (embedding_array,),\n        )\n    else:\n        cur.execute(\n            f\"SELECT content FROM embeddings ORDER BY embedding <=> %s LIMIT {n}\",\n            (embedding_array,),\n        )\n\n    return cur.fetchall()\n```\n\nLuckily we are able to get these similar documents using a function in\n[`pgvector`](https://github.com/pgvector/pgvector), a plugin package for\nPostgreSQL: `ORDER BY embedding <=> %s` orders the documents by their similarity\nto the query embedding. This is a very efficient way to get the most relevant\ndocuments to the query and is a great example of how we can leverage the power\nof the database to do the heavy lifting for us.\n\nFor the `get_completion_from_messages` function, we use\n[`litellm`](https://github.com/BerriAI/litellm) as a universal interface that\nallows us to use lots of different LLMs. As you can see above, the model is able\nto synthesize the documents it has been given and provide a response to the\nquery.\n\n```python\ndef get_completion_from_messages(\n    messages, model=OPENAI_MODEL, temperature=0.4, max_tokens=1000\n):\n    \"\"\"Generates a completion response from the given messages using the specified model.\"\"\"\n    model = MODEL_NAME_MAP.get(model, model)\n    completion_response = litellm.completion(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    return completion_response.choices[0].message.content\n```\n\nWe're using `litellm` because it makes sense not to have to implement separate\nfunctions for each LLM we might want to use. The pace of development in the\nfield is such that you will want to experiment with new LLMs as they come out,\nand `litellm` gives you the flexibility to"}
{"input": " do that without having to rewrite\nyour code.\n\nWe've now completed a basic RAG inference pipeline that uses the embeddings\ngenerated by the pipeline to retrieve the most relevant chunks of text based on\na given query. We can inspect the various components of the pipeline to see how\nthey work together to provide a response to the query. This gives us a solid\nfoundation to move onto more complex RAG pipelines and to look into how we might\nimprove this. The next section will cover how to improve retrieval by finetuning\nthe embeddings generated by the pipeline. This will boost our performance in\nsituations where we have a large volume of documents and also when the documents\nare potentially very different from the training data that was used for the\nembeddings.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide)\nrepository and for this section, particularly [the `llm_utils.py` file](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/utils/llm_utils.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Generate embeddings to improve retrieval performance.\n---\n\n# Generating Embeddings for Retrieval\n\nIn this section, we'll explore how to generate embeddings for your data to\nimprove retrieval performance in your RAG pipeline. Embeddings are a crucial\npart of the retrieval mechanism in RAG, as they represent the data in a\nhigh-dimensional space where similar items are closer together. By generating\nembeddings for your data, you can enhance the retrieval capabilities of your RAG\npipeline and provide more accurate and relevant responses to user queries.\n\n![](/docs/book/.gitbook/assets/rag-stage-2.png)\n\n{% hint style=\"info\" %} Embeddings are vector representations of data that capture the semantic\nmeaning and context of the data in a high-dimensional space. They are generated\nusing machine learning models, such as word embeddings or sentence embeddings,\nthat learn to encode the data in a way that preserves its underlying structure\nand relationships. Embeddings are commonly used in natural language processing\n(NLP) tasks, such as text classification, sentiment analysis, and information\nretrieval, to represent textual data in a format that is suitable for\ncomputational processing. {% endhint %}\n\nThe whole purpose of the embeddings is to allow us to quickly find the small\nchunks that are most relevant to our input query at inference time. An even\nsimpler way of doing this would be to just to search for some keywords in the\nquery and hope that they're also represented in the chunks. However, this\napproach is not very robust and may not work well for more complex queries or\nlonger documents. By using embeddings, we can capture the semantic meaning and\ncontext of the data and retrieve the most relevant chunks based on their\nsimilarity to the query.\n\nWe're using the [`sentence-transformers`](https://www.sbert.net/) library to generate embeddings for our\ndata. This library provides pre-trained models for generating sentence\nembeddings that capture the semantic meaning of the text. It's an open-source\nlibrary that is easy to use and provides high-quality embeddings for a wide\nrange of NLP tasks.\n\n```python\nfrom typing import Annotated, List\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom structures import Document\nfrom zenml import ArtifactConfig, log_artifact_metadata, step\n\n@step\ndef generate_embeddings(\n    split_documents: List[Document],\n) -> Annotated[\n    List[Document], ArtifactConfig(name=\"documents_with_embeddings\")\n]:\n    try:\n        model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n\n        log_artifact_metadata(\n            artifact_name=\"embeddings\",\n            metadata={\n                \"embedding_type\": \"sentence-transformers/all-MiniLM-L12-v2\",\n                \"embedding_dimensionality\": 384,\n            },\n        )\n\n        document_texts = [doc.page_content for doc in split_documents]\n        embeddings = model"}
{"input": ".encode(document_texts)\n\n        for doc, embedding in zip(split_documents, embeddings):\n            doc.embedding = embedding\n\n        return split_documents\n    except Exception as e:\n        logger.error(f\"Error in generate_embeddings: {e}\")\n        raise\n```\n\nWe update the `Document` Pydantic model to include an `embedding` attribute that\nstores the embedding generated for each document. This allows us to associate\nthe embeddings with the corresponding documents and use them for retrieval\npurposes in the RAG pipeline.\n\nThere are smaller embeddings models if we cared a lot about speed, and larger\nones (with more dimensions) if we wanted to boost our ability to retrieve more\nrelevant chunks. [The model we're using\nhere](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) is on the\nsmaller side, but it should work well for our use case. The embeddings generated\nby this model have a dimensionality of 384, which means that each embedding is\nrepresented as a 384-dimensional vector in the high-dimensional space.\n\nWe can use dimensionality reduction functionality in\n[`umap`](https://umap-learn.readthedocs.io/) and\n[`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn-manifold-tsne)\nto represent the 384 dimensions of our embeddings in two-dimensional space. This\nallows us to visualize the embeddings and see how similar chunks are clustered\ntogether based on their semantic meaning and context. We can also use this\nvisualization to identify patterns and relationships in the data that can help\nus improve the retrieval performance of our RAG pipeline. It's worth trying both\nUMAP and t-SNE to see which one works best for our use case since they both have\nsomewhat different representations of the data and reduction algorithms, as\nyou'll see.\n\n```python\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport umap\nfrom zenml.client import Client\n\nartifact = Client().get_artifact_version('EMBEDDINGS_ARTIFACT_UUID_GOES_HERE')\nembeddings = artifact.load()\n\n\nembeddings = np.array([doc.embedding for doc in documents])\nparent_sections = [doc.parent_section for doc in documents]\n\n# Get unique parent sections\nunique_parent_sections = list(set(parent_sections))\n\n# Tol color palette\ntol_colors = [\n    \"#4477AA\",\n    \"#EE6677\",\n    \"#228833\",\n    \"#CCBB44\",\n    \"#66CCEE\",\n    \"#AA3377\",\n    \"#BBBBBB\",\n]\n\n# Create a colormap with Tol colors\ntol_colormap = ListedColormap(tol_colors)\n\n# Assign colors to each unique parent section\nsection_colors = tol_colors[: len(unique_parent_sections)]\n\n# Create a dictionary mapping parent sections to colors"}
{"input": "\nsection_color_dict = dict(zip(unique_parent_sections, section_colors))\n\n# Dimensionality reduction using t-SNE\ndef tsne_visualization(embeddings, parent_sections):\n    tsne = TSNE(n_components=2, random_state=42)\n    embeddings_2d = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(8, 8))\n    for section in unique_parent_sections:\n        if section in section_color_dict:\n            mask = [section == ps for ps in parent_sections]\n            plt.scatter(\n                embeddings_2d[mask, 0],\n                embeddings_2d[mask, 1],\n                c=[section_color_dict[section]],\n                label=section,\n            )\n\n    plt.title(\"t-SNE Visualization\")\n    plt.legend()\n    plt.show()\n\n\n# Dimensionality reduction using UMAP\ndef umap_visualization(embeddings, parent_sections):\n    umap_2d = umap.UMAP(n_components=2, random_state=42)\n    embeddings_2d = umap_2d.fit_transform(embeddings)\n\n    plt.figure(figsize=(8, 8))\n    for section in unique_parent_sections:\n        if section in section_color_dict:\n            mask = [section == ps for ps in parent_sections]\n            plt.scatter(\n                embeddings_2d[mask, 0],\n                embeddings_2d[mask, 1],\n                c=[section_color_dict[section]],\n                label=section,\n            )\n\n    plt.title(\"UMAP Visualization\")\n    plt.legend()\n    plt.show()\n```\n\n![UMAP visualization of the ZenML documentation chunks as embeddings](/docs/book/.gitbook/assets/umap.png)\n![t-SNE visualization of the ZenML documentation chunks as embeddings](/docs/book/.gitbook/assets/tsne.png)\n\nIn this stage, we have utilized the 'parent directory', which we had previously\nstored in the vector store as an additional attribute, as a means to color the\nvalues. This approach allows us to gain some insight into the semantic space\ninherent in our data. It demonstrates that you can visualize the embeddings and\nobserve how similar chunks are grouped together based on their semantic meaning\nand context.\n\nSo this step iterates through all the chunks and generates embeddings\nrepresenting each piece of text. These embeddings are then stored as an artifact\nin the ZenML artifact store as a NumPy array. We separate this generation from\nthe point where we upload those embeddings to the vector database to keep the\npipeline modular and flexible; in the future we might want to use a different\nvector database so we can just swap out the upload step without having to\nre-generate the embeddings.\n\nIn the next section, we'll explore how to store these embeddings in a vector\ndatabase to enable fast and efficient retrieval of relevant chunks at inference\ntime.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zen"}
{"input": "ml-projects/tree/main/llm-complete-guide)\nrepository. The embeddings generation step can be found\n[here](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/populate_index.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Understand how to ingest and preprocess data for RAG pipelines with ZenML.\n---\n\nThe first step in setting up a RAG pipeline is to ingest the data that will be\nused to train and evaluate the retriever and generator models. This data can\ninclude a large corpus of documents, as well as any relevant metadata or\nannotations that can be used to train the retriever and generator.\n\n![](/docs/book/.gitbook/assets/rag-stage-1.png)\n\nIn the interests of keeping things simple, we'll implement the bulk of what we\nneed ourselves. However, it's worth noting that there are a number of tools and\nframeworks that can help you manage the data ingestion process, including\ndownloading, preprocessing, and indexing large corpora of documents. ZenML\nintegrates with a number of these tools and frameworks, making it easy to set up\nand manage RAG pipelines.\n\n{% hint style=\"info\" %}\nYou can view all the code referenced in this guide in the associated project\nrepository. Please visit <a\nhref=\"https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide\">the\n`llm-complete-guide` project</a> inside the ZenML projects repository if you\nwant to dive deeper.\n{% endhint %}\n\nYou can add a ZenML step that scrapes a series of URLs and outputs the URLs quite\neasily. Here we assemble a step that scrapes URLs related to ZenML from its documentation.\nWe leverage some simple helper utilities that we have created for this purpose:\n\n```python\nfrom typing import List\nfrom typing_extensions import Annotated\nfrom zenml import log_artifact_metadata, step\nfrom steps.url_scraping_utils import get_all_pages\n\n@step\ndef url_scraper(\n    docs_url: str = \"https://docs.zenml.io\",\n    repo_url: str = \"https://github.com/zenml-io/zenml\",\n    website_url: str = \"https://zenml.io\",\n) -> Annotated[List[str], \"urls\"]:\n    \"\"\"Generates a list of relevant URLs to scrape.\"\"\"\n    docs_urls = get_all_pages(docs_url)\n    log_artifact_metadata(\n        metadata={\n            \"count\": len(docs_urls),\n        },\n    )\n    return docs_urls\n```\n\nThe `get_all_pages` function simply crawls our documentation website and\nretrieves a unique set of URLs. We've limited it to only scrape the\ndocumentation relating to the most recent releases so that we're not mixing old\nsyntax and information with the new. This is a simple way to ensure that we're\nonly ingesting the most relevant and up-to-date information into our pipeline.\n\nWe also log the count of those URLs as metadata for the step output. This will\nbe visible in the dashboard for extra visibility around the data that's being\ningested. Of course, you can also"}
{"input": " add more complex logic to this step, such as\nfiltering out certain URLs or adding more metadata.\n\n![Partial screenshot from the dashboard showing the metadata from the step](/docs/book/.gitbook/assets/llm-data-ingestion-metadata.png)\n\nOnce we have our list of URLs, we use [the `unstructured`\nlibrary](https://github.com/Unstructured-IO/unstructured) to load and parse the\npages. This will allow us to use the text without having to worry about the\ndetails of the HTML structure and/or markup. This specifically helps us keep the\ntext\ncontent as small as possible since we are operating in a constrained environment\nwith LLMs.\n\n```python\nfrom typing import List\nfrom unstructured.partition.html import partition_html\nfrom zenml import step\n\n@step\ndef web_url_loader(urls: List[str]) -> List[str]:\n    \"\"\"Loads documents from a list of URLs.\"\"\"\n    document_texts = []\n    for url in urls:\n        elements = partition_html(url=url)\n        text = \"\\n\\n\".join([str(el) for el in elements])\n        document_texts.append(text)\n    return document_texts\n```\n\nThe previously-mentioned frameworks offer many more options when it comes to\ndata ingestion, including the ability to load documents from a variety of\nsources, preprocess the text, and extract relevant features. For our purposes,\nthough, we don't need anything too fancy. It also makes our pipeline easier to\ndebug since we can see exactly what's being loaded and how it's being processed.\nYou don't get that same level of visibility with more complex frameworks.\n\n# Preprocessing the data\n\nOnce we have loaded the documents, we can preprocess them into a form that's\nuseful for a RAG pipeline. There are a lot of options here, depending on how\ncomplex you want to get, but to start with you can think of the 'chunk size' as\none of the key parameters to think about.\n\nOur text is currently in the form of various long strings, with each one\nrepresenting a single web page. These are going to be too long to pass into our\nLLM, especially if we care about the speed at which we get our answers back. So\nthe strategy here is to split our text into smaller chunks that can be processed\nmore efficiently. There's a sweet spot between having tiny chunks, which will\nmake it harder for our search / retrieval step to find relevant information to\npass into the LLM, and having large chunks, which will make it harder for the\nLLM to process the text.\n\n```python\nimport logging\nfrom typing import Annotated, List\nfrom utils.llm_utils import split_documents\nfrom zenml import ArtifactConfig, log_artifact_metadata, step\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@step(enable_cache=False)\ndef preprocess_documents(\n    documents: List[str],\n) -> An"}
{"input": "notated[List[str], ArtifactConfig(name=\"split_chunks\")]:\n    \"\"\"Preprocesses a list of documents by splitting them into chunks.\"\"\"\n    try:\n        log_artifact_metadata(\n            artifact_name=\"split_chunks\",\n            metadata={\n                \"chunk_size\": 500,\n                \"chunk_overlap\": 50\n            },\n        )\n        return split_documents(\n            documents, chunk_size=500, chunk_overlap=50\n        )\n    except Exception as e:\n        logger.error(f\"Error in preprocess_documents: {e}\")\n        raise\n```\n\nIt's really important to know your data to have a good intuition about what kind\nof chunk size might make sense. If your data is structured in such a way where\nyou need large paragraphs to capture a particular concept, then you might want a\nlarger chunk size. If your data is more conversational or question-and-answer\nbased, then you might want a smaller chunk size.\n\nFor our purposes, given that we're working with web pages that are written as\ndocumentation for a software library, we're going to use a chunk size of 500 and\nwe'll make sure that the chunks overlap by 50 characters. This means that we'll\nhave a lot of overlap between our chunks, which can be useful for ensuring that\nwe don't miss any important information when we're splitting up our text.\n\nAgain, depending on your data and use case, there is more you might want to do\nwith your data. You might want to clean the text, remove code snippets or make\nsure that code snippets were not split across chunks, or even extract metadata\nfrom the text. This is a good starting point, but you can always add more\ncomplexity as needed.\n\nNext up, generating embeddings so that we can use them to retrieve relevant\ndocuments...\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide)\nrepository and particularly [the code for the steps](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/) in this section. Note, too,\nthat a lot of the logic is encapsulated in utility functions inside [`url_scraping_utils.py`](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/url_scraping_utils.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Store embeddings in a vector database for efficient retrieval.\n---\n\n# Storing embeddings in a vector database\n\nThe process of generating the embeddings doesn't take too long, especially if the machine on which the step is running has a GPU, but it's still not something we want to do every time we need to retrieve a document. Instead, we can store the embeddings in a vector database, which allows us to quickly retrieve the most relevant chunks based on their similarity to the query.\n\n![](../../../.gitbook/assets/rag-stage-3.png)\n\nFor the purposes of this guide, we'll use PostgreSQL as our vector database. This is a popular choice for storing embeddings, as it provides a scalable and efficient way to store and retrieve high-dimensional vectors. However, you can use any vector database that supports high-dimensional vectors. If you want to explore a list of possible options, [this is a good website](https://superlinked.com/vector-db-comparison/) to compare different options.\n\n{% hint style=\"info\" %}\nFor more information on how to set up a PostgreSQL database to follow along with this guide, please [see the instructions in the repository](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide) which show how to set up a PostgreSQL database using Supabase.\n{% endhint %}\n\nSince PostgreSQL is a well-known and battle-tested database, we can use known and minimal packages to connect and to interact with it. We can use the [`psycopg2`](https://www.psycopg.org/docs/) package to connect and then raw SQL statements to interact with the database.\n\nThe code for the step is fairly simple:\n\n```python\nfrom zenml import step\n\n@step\ndef index_generator(\n    documents: List[Document],\n) -> None:\n    try:\n        conn = get_db_conn()\n        with conn.cursor() as cur:\n            # Install pgvector if not already installed\n            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n            conn.commit()\n\n            # Create the embeddings table if it doesn't exist\n            table_create_command = f\"\"\"\n            CREATE TABLE IF NOT EXISTS embeddings (\n                        id SERIAL PRIMARY KEY,\n                        content TEXT,\n                        token_count INTEGER,\n                        embedding VECTOR({EMBEDDING_DIMENSIONALITY}),\n                        filename TEXT,\n                        parent_section TEXT,\n                        url TEXT\n                        );\n                        \"\"\"\n            cur.execute(table_create_command)\n            conn.commit()\n\n            register_vector(conn)\n\n            # Insert data only if it doesn't already exist\n            for doc in documents:\n                content = doc.page_content\n                token_count = doc.token_count\n                embedding = doc.embedding.tolist()\n                filename = doc.filename\n                parent_section = doc.parent_section\n                url = doc.url\n\n                cur.execute(\n                    \"SELECT COUNT(*) FROM embeddings WHERE content = %s\",\n                    (content,),\n                )\n                count = cur.fetchone()[0]\n                if count == 0:\n                    cur.execute"}
{"input": "(\n                        \"INSERT INTO embeddings (content, token_count, embedding, filename, parent_section, url) VALUES (%s, %s, %s, %s, %s, %s)\",\n                        (\n                            content,\n                            token_count,\n                            embedding,\n                            filename,\n                            parent_section,\n                            url,\n                        ),\n                    )\n                    conn.commit()\n\n            cur.execute(\"SELECT COUNT(*) as cnt FROM embeddings;\")\n            num_records = cur.fetchone()[0]\n            logger.info(f\"Number of vector records in table: {num_records}\")\n\n            # calculate the index parameters according to best practices\n            num_lists = max(num_records / 1000, 10)\n            if num_records > 1000000:\n                num_lists = math.sqrt(num_records)\n\n            # use the cosine distance measure, which is what we'll later use for querying\n            cur.execute(\n                f\"CREATE INDEX IF NOT EXISTS embeddings_idx ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {num_lists});\"\n            )\n            conn.commit()\n\n    except Exception as e:\n        logger.error(f\"Error in index_generator: {e}\")\n        raise\n    finally:\n        if conn:\n            conn.close()\n```\n\nWe use some utility functions, but what we do here is:\n\n* connect to the database\n* create the `vector` extension if it doesn't already exist (this is to enable the vector data type in PostgreSQL)\n* create the `embeddings` table if it doesn't exist\n* insert the embeddings and documents into the table\n* calculate the index parameters according to best practices\n* create an index on the embeddings\n\nNote that we're inserting the documents into the embeddings table as well as the embeddings themselves. This is so that we can retrieve the documents based on their embeddings later on. It also helps with debugging from within the Supabase interface or wherever else we're examining the contents of the database.\n\n![The Supabase editor interface](../../../.gitbook/assets/supabase-editor-interface.png)\n\nDeciding when to update your embeddings is a separate discussion and depends on the specific use case. If your data is frequently changing, and the changes are significant, you might want to fully reset the embeddings with each update. In other cases, you might just want to add new documents and embeddings into the database because the changes are minor or infrequent. In the code above, we choose to only add new embeddings if they don't already exist in the database.\n\n{% hint style=\"info\" %}\nDepending on the size of your dataset and the number of embeddings you're storing, you might find that running this step on a CPU is too slow. In that case, you should ensure that this step runs on a GPU-enabled machine to speed up the process. You can do this with ZenML by using a step operator that runs on a GPU-enabled machine. See [the docs here](../../../component-guide/step-operators/README.md)"}
{"input": " for more on how to set this up.\n{% endhint %}\n\nWe also generate an index for the embeddings using the `ivfflat` method with the `vector_cosine_ops` operator. This is a common method for indexing high-dimensional vectors in PostgreSQL and is well-suited for similarity search using cosine distance. The number of lists is calculated based on the number of records in the table, with a minimum of 10 lists and a maximum of the square root of the number of records. This is a good starting point for tuning the index parameters, but you might want to experiment with different values to see how they affect the performance of your RAG pipeline.\n\nNow that we have our embeddings stored in a vector database, we can move on to the next step in the pipeline, which is to retrieve the most relevant documents based on a given query. This is where the real magic of the RAG pipeline comes into play, as we can use the embeddings to quickly retrieve the most relevant chunks of text based on their similarity to the query. This allows us to build a powerful and efficient question-answering system that can provide accurate and relevant responses to user queries in real-time.\n\n## Code Example\n\nTo explore the full code, visit the [Complete Guide](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide) repository. The logic for storing the embeddings in PostgreSQL can be found [here](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide/steps/populate\\_index.py).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: RAG is a sensible way to get started with LLMs.\n---\n\n# RAG Pipelines with ZenML\n\nRetrieval-Augmented Generation (RAG) is a powerful technique that combines the\nstrengths of retrieval-based and generation-based models. In this guide, we'll\nexplore how to set up RAG pipelines with ZenML, including data ingestion, index\nstore management, and tracking RAG-associated artifacts.\n\nLLMs are a powerful tool, as they can generate human-like responses to a wide\nvariety of prompts. However, they can also be prone to generating incorrect or\ninappropriate responses, especially when the input prompt is ambiguous or\nmisleading. They are also (currently) limited in the amount of text they can\nunderstand and/or generate. While there are some LLMs [like Google's Gemini 1.5\nPro](https://developers.googleblog.com/2024/02/gemini-15-available-for-private-preview-in-google-ai-studio.html)\nthat can consistently handle 1 million tokens (small units of text), the vast majority (particularly\nthe open-source ones currently available) handle far less.\n\nThe first part of this guide to RAG pipelines with ZenML is about understanding\nthe basic components and how they work together. We'll cover the following\ntopics:\n\n- why RAG exists and what problem it solves\n- how to ingest and preprocess data that we'll use in our RAG pipeline\n- how to leverage embeddings to represent our data; this will be the basis for\n  our retrieval mechanism\n- how to store these embeddings in a vector database\n- how to track RAG-associated artifacts with ZenML\n\nAt the end, we'll bring it all together and show all the components working\ntogether to perform basic RAG inference.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Learn how to implement a RAG pipeline in just 85 lines of code.\n---\n\nThere's a lot of theory and context to think about when it comes to RAG, but\nlet's start with a quick implementation in code to motivate what follows. The\nfollowing 85 lines do the following:\n\n- load some data (a fictional dataset about 'ZenML World') as our corpus\n- process that text (split it into chunks and 'tokenize' it (i.e. split into\n  words))\n- take a query as input and find the most relevant chunks of text from our\n  corpus data\n- use OpenAI's GPT-3.5 model to answer the question based on the relevant\n    chunks\n\n```python\nimport os\nimport re\nimport string\n\nfrom openai import OpenAI\n\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\ndef tokenize(text):\n    return preprocess_text(text).split()\n\n\ndef retrieve_relevant_chunks(query, corpus, top_n=2):\n    query_tokens = set(tokenize(query))\n    similarities = []\n    for chunk in corpus:\n        chunk_tokens = set(tokenize(chunk))\n        similarity = len(query_tokens.intersection(chunk_tokens)) / len(\n            query_tokens.union(chunk_tokens)\n        )\n        similarities.append((chunk, similarity))\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    return [chunk for chunk, _ in similarities[:top_n]]\n\n\ndef answer_question(query, corpus, top_n=2):\n    relevant_chunks = retrieve_relevant_chunks(query, corpus, top_n)\n    if not relevant_chunks:\n        return \"I don't have enough information to answer the question.\"\n\n    context = \"\\n\".join(relevant_chunks)\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Based on the provided context, answer the following question: {query}\\n\\nContext:\\n{context}\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n    return chat_completion.choices[0].message.content.strip()\n\n\n# Sci-fi themed corpus about \"ZenML World\"\ncorpus = [\n    \"The luminescent forests of ZenML World are inhabited by glowing Zenbots that emit a soft, pulsating light as they roam the enchanted landscape.\",\n    \"In the neon skies of ZenML World, Cosmic Butterflies flutter gracefully, their iridescent wings leaving trails of stardust in their wake.\",\n    \"Telepathic Treants, ancient sentient trees, communicate through the quantum neural network"}
{"input": " that spans the entire surface of ZenML World, sharing wisdom and knowledge.\",\n    \"Deep within the melodic caverns of ZenML World, Fractal Fungi emit pulsating tones that resonate through the crystalline structures, creating a symphony of otherworldly sounds.\",\n    \"Near the ethereal waterfalls of ZenML World, Holographic Hummingbirds hover effortlessly, their translucent wings refracting the prismatic light into mesmerizing patterns.\",\n    \"Gravitational Geckos, masters of anti-gravity, traverse the inverted cliffs of ZenML World, defying the laws of physics with their extraordinary abilities.\",\n    \"Plasma Phoenixes, majestic creatures of pure energy, soar above the chromatic canyons of ZenML World, their fiery trails painting the sky in a dazzling display of colors.\",\n    \"Along the prismatic shores of ZenML World, Crystalline Crabs scuttle and burrow, their transparent exoskeletons refracting the light into a kaleidoscope of hues.\",\n]\n\ncorpus = [preprocess_text(sentence) for sentence in corpus]\n\nquestion1 = \"What are Plasma Phoenixes?\"\nanswer1 = answer_question(question1, corpus)\nprint(f\"Question: {question1}\")\nprint(f\"Answer: {answer1}\")\n\nquestion2 = (\n    \"What kinds of creatures live on the prismatic shores of ZenML World?\"\n)\nanswer2 = answer_question(question2, corpus)\nprint(f\"Question: {question2}\")\nprint(f\"Answer: {answer2}\")\n\nirrelevant_question_3 = \"What is the capital of Panglossia?\"\nanswer3 = answer_question(irrelevant_question_3, corpus)\nprint(f\"Question: {irrelevant_question_3}\")\nprint(f\"Answer: {answer3}\")\n```\n\nThis outputs the following:\n\n```shell\nQuestion: What are Plasma Phoenixes?\nAnswer: Plasma Phoenixes are majestic creatures made of pure energy that soar above the chromatic canyons of Zenml World. They leave fiery trails behind them, painting the sky with dazzling displays of colors.\nQuestion: What kinds of creatures live on the prismatic shores of ZenML World?\nAnswer: On the prismatic shores of ZenML World, you can find crystalline crabs scuttling and burrowing with their transparent exoskeletons, which refract light into a kaleidoscope of hues.\nQuestion: What is the capital of Panglossia?\nAnswer: The capital of Panglossia is not mentioned in the provided context.\n```\n\nThe implementation above is by no means sophisticated or performant, but it's\nsimple enough that you can see all the moving parts. Our tokenization process\nconsists of splitting the text into individual words. \n\nThe way we check for similarity between the question / query and the chunks of\ntext is extremely naive and inefficient. The similarity between the query and\nthe current chunk is calculated using the [Jaccard similarity\ncoefficient]("}
{"input": "https://www.statology.org/jaccard-similarity/). This coefficient\nmeasures the similarity between two sets and is defined as the size of the\nintersection divided by the size of the union of the two sets. So we count the\nnumber of words that are common between the query and the chunk and divide it by\nthe total number of unique words in both the query and the chunk. There are much\nbetter ways of measuring the similarity between two pieces of text, such as\nusing embeddings or other more sophisticated techniques, but this example is\nkept simple for illustrative purposes.\n\nThe rest of this guide will showcase a more performant and scalable way of\nperforming the same task using ZenML. If you ever are unsure why we're doing\nsomething, feel free to return to this example for the high-level overview.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Understand the Retrieval-Augmented Generation (RAG) technique and its\n  benefits.\n---\n\n# Understanding Retrieval-Augmented Generation (RAG)\n\nLLMs are powerful but not without their limitations. They are prone to generating incorrect responses, especially when it's unclear what the input prompt is asking for. They are also limited in the amount of text they can understand and generate. While some LLMs can handle more than 1 million tokens of input, most open-source models can handle far less. Your use case also might not require all the complexity and cost associated with running a large LLM.\n\nRAG, [originally proposed in 2020](https://arxiv.org/abs/2005.11401v4) by researchers at Facebook, is a technique that supplements the inbuilt abilities of foundation models like LLMs with a retrieval mechanism. This mechanism retrieves relevant documents from a large corpus and uses them to generate a response. This approach combines the strengths of retrieval-based and generation-based models, allowing you to leverage the power of LLMs while addressing their limitations.\n\n## What exactly happens in a RAG pipeline?\n\n![](../../../.gitbook/assets/rag-process-whole.png)\n\nIn a RAG pipeline, we use a retriever to find relevant documents from a large corpus and then uses a generator to produce a response based on the retrieved documents. This approach is particularly useful for tasks that require contextual understanding and long-form generation, such as question answering, summarization, and dialogue generation.\n\nRAG helps with the context limitations mentioned above by providing a way to retrieve relevant documents that can be used to generate a response. This retrieval step can help ensure that the generated response is grounded in relevant information, reducing the likelihood of generating incorrect or inappropriate responses. It also helps with the token limitations by allowing the generator to focus on a smaller set of relevant documents, rather than having to process an entire large corpus.\n\nGiven the costs associated with running LLMs, RAG can also be more cost-effective than using a pure generation-based approach, as it allows you to focus the generator's resources on a smaller set of relevant documents. This can be particularly important when working with large corpora or when deploying models to resource-constrained environments.\n\n## When is RAG a good choice?\n\n![](../../../.gitbook/assets/rag-when.png)\n\nRAG is a good choice when you need to generate long-form responses that require contextual understanding and when you have access to a large corpus of relevant documents. It can be particularly useful for tasks like question answering, summarization, and dialogue generation, where the generated response needs to be grounded in relevant information.\n\nIt's often the first thing that you'll want to try when dipping your toes into the world of LLMs. This is because it provides a sensible way to get a feel for how the process works, and it doesn't require as much data or computational resources as other approaches."}
{"input": " It's also a good choice when you need to balance the benefits of LLMs with the limitations of the current generation of models.\n\n## How does RAG fit into the ZenML ecosystem?\n\nIn ZenML, you can set up RAG pipelines that combine the strengths of retrieval-based and generation-based models. This allows you to leverage the power of LLMs while addressing their limitations. ZenML provides tools for data ingestion, index store management, and tracking RAG-associated artifacts, making it easy to set up and manage RAG pipelines.\n\nZenML also provides a way to scale beyond the limitations of simple RAG pipelines, as we shall see in later sections of this guide. While you might start off with something simple, at a later point you might want to transition to a more complex setup that involves finetuning embeddings, reranking retrieved documents, or even finetuning the LLM itself. ZenML provides tools for all of these scenarios, making it easy to scale your RAG pipelines as needed.\n\nZenML allows you to track all the artifacts associated with your RAG pipeline, from hyperparameters and model weights to metadata and performance metrics, as well as all the RAG or LLM-specific artifacts like chains, agents, tokenizers and vector stores. These can all be tracked in the [Model Control Plane](../../../how-to/use-the-model-control-plane/README.md) and thus visualized in the [ZenML Pro](https://zenml.io/pro) dashboard.\n\nBy bringing all of the above into a simple ZenML pipeline we achieve a clearly delineated set of steps that can be run and rerun to set up our basic RAG pipeline. This is a great starting point for building out more complex RAG pipelines, and it's a great way to get started with LLMs in a sensible way.\n\nA summary of some of the advantages that ZenML brings to the table here includes:\n\n* **Reproducibility**: You can rerun the pipeline to update the index store with new documents or to change the parameters of the chunking process and so on. Previous versions of the artifacts will be preserved, and you can compare the performance of different versions of the pipeline.\n* **Scalability**: You can easily scale the pipeline to handle larger corpora of documents by deploying it on a cloud provider and using a more scalable vector store.\n* **Tracking artifacts and associating them with metadata**: You can track the artifacts generated by the pipeline and associate them with metadata that provides additional context and insights into the pipeline. This metadata and these artifacts are then visible in the ZenML dashboard, allowing you to monitor the performance of the pipeline and debug any issues that arise.\n* **Maintainability** - Having your pipeline in a clear, modular format makes it easier to maintain and update. You can easily add new steps, change the parameters of existing steps, and experiment with different configurations to see how they affect the performance of the pipeline.\n* **Collabor"}
{"input": "ation** - You can share the pipeline with your team and collaborate on it together. You can also use the ZenML dashboard to share insights and findings with your team, making it easier to work together on the pipeline.\n\nIn the next section, we'll showcase the components of a basic RAG pipeline. This will give you a taste of how you can leverage the power of LLMs in your MLOps workflows using ZenML. Subsequent sections will cover more advanced topics like reranking retrieved documents, finetuning embeddings, and finetuning the LLM itself.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Evaluate the performance of your reranking model.\n---\n\n# Evaluating reranking performance\n\nWe've already set up an evaluation pipeline, so adding reranking evaluation is relatively straightforward. In this section, we'll explore how to evaluate the performance of your reranking model using ZenML.\n\n### Evaluating Reranking Performance\n\nThe simplest first step in evaluating the reranking model is to compare the retrieval performance before and after reranking. You can use the same metrics we discussed in the [evaluation section](../evaluation/) to assess the performance of the reranking model.\n\n![](../../../.gitbook/assets/reranking-evaluation.png)\n\nIf you recall, we have a hand-crafted set of queries and relevant documents that we use to evaluate the performance of our retrieval system. We also have a set that was [generated by LLMs](../evaluation/retrieval.md#automated-evaluation-using-synthetic-generated-queries). The actual retrieval test is implemented as follows:\n\n```python\ndef perform_retrieval_evaluation(\n    sample_size: int, use_reranking: bool\n) -> float:\n    \"\"\"Helper function to perform the retrieval evaluation.\"\"\"\n    dataset = load_dataset(\"zenml/rag_qa_embedding_questions\", split=\"train\")\n    sampled_dataset = dataset.shuffle(seed=42).select(range(sample_size))\n\n    total_tests = len(sampled_dataset)\n    failures = 0\n\n    for item in sampled_dataset:\n        generated_questions = item[\"generated_questions\"]\n        question = generated_questions[\n            0\n        ]  # Assuming only one question per item\n        url_ending = item[\"filename\"].split(\"/\")[\n            -1\n        ]  # Extract the URL ending from the filename\n\n        # using the method above to query similar documents\n        # we pass in whether we want to use reranking or not\n        _, _, urls = query_similar_docs(question, url_ending, use_reranking)\n\n        if all(url_ending not in url for url in urls):\n            logging.error(\n                f\"Failed for question: {question}. Expected URL ending: {url_ending}. Got: {urls}\"\n            )\n            failures += 1\n\n    logging.info(f\"Total tests: {total_tests}. Failures: {failures}\")\n    failure_rate = (failures / total_tests) * 100\n    return round(failure_rate, 2)\n```\n\nThis function takes a sample size and a flag indicating whether to use reranking and evaluates the retrieval performance based on the generated questions and relevant documents. It queries similar documents for each question and checks whether the expected URL ending is present in the retrieved URLs. The failure rate is calculated as the percentage of failed tests over the total number of tests.\n\nThis function is then called in two separate evaluation steps: one for the retrieval system without reranking and one for the retrieval system with reranking.\n\n```python\n@step\ndef retrieval_evaluation_full"}
{"input": "(\n    sample_size: int = 100,\n) -> Annotated[float, \"full_failure_rate_retrieval\"]:\n    \"\"\"Executes the retrieval evaluation step without reranking.\"\"\"\n    failure_rate = perform_retrieval_evaluation(\n        sample_size, use_reranking=False\n    )\n    logging.info(f\"Retrieval failure rate: {failure_rate}%\")\n    return failure_rate\n\n\n@step\ndef retrieval_evaluation_full_with_reranking(\n    sample_size: int = 100,\n) -> Annotated[float, \"full_failure_rate_retrieval_reranking\"]:\n    \"\"\"Executes the retrieval evaluation step with reranking.\"\"\"\n    failure_rate = perform_retrieval_evaluation(\n        sample_size, use_reranking=True\n    )\n    logging.info(f\"Retrieval failure rate with reranking: {failure_rate}%\")\n    return failure_rate\n```\n\nBoth of these steps return the failure rate of the respective retrieval systems. If we want, we can look into the logs of those steps (either on the dashboard or in the terminal) to see specific examples that failed. For example:\n\n```\n...\nLoading default flashrank model for language en\nDefault Model: ms-marco-MiniLM-L-12-v2\nLoading FlashRankRanker model ms-marco-MiniLM-L-12-v2\nLoading model FlashRank model ms-marco-MiniLM-L-12-v2...\nRunning pairwise ranking..\nFailed for question:  Based on the provided ZenML documentation text, here's a question\n that can be asked: \"How do I develop a custom alerter as described on the Feast page, \n and where can I find the 'How to use it?' guide?\". Expected URL ending: feature-stores.\n  Got: ['https://docs.zenml.io/stacks-and-components/component-guide/alerters/custom', \n  'https://docs.zenml.io/v/docs/stacks-and-components/component-guide/alerters/custom', \n  'https://docs.zenml.io/v/docs/reference/how-do-i', 'https://docs.zenml.io/stacks-and-components/component-guide/alerters', \n  'https://docs.zenml.io/stacks-and-components/component-guide/alerters/slack']\n\nLoading default flashrank model for language en\nDefault Model: ms-marco-MiniLM-L-12-v2\nLoading FlashRankRanker model ms-marco-MiniLM-L-12-v2\nLoading model FlashRank model ms-marco-MiniLM-L-12-v2...\nRunning pairwise ranking..\nStep retrieval_evaluation_full_with_reranking has finished in 4m20s.\n```\n\nWe can see here a specific example of a failure in the reranking evaluation. It's quite a good one because we can see that the question asked was actually an anomaly in the sense that the LLM has generated two questions and included its meta-discussion of the two questions it generated. Obviously this is"}
{"input": " not a representative question for the dataset, and if we saw a lot of these we might want to take some time to both understand why the LLM is generating these questions and how we can filter them out.\n\n### Visualising our reranking performance\n\nSince ZenML can display visualizations in its dashboard, we can showcase the results of our experiments in a visual format. For example, we can plot the failure rates of the retrieval system with and without reranking to see the impact of reranking on the performance.\n\nOur documentation explains how to set up your outputs so that they appear as visualizations in the ZenML dashboard. You can find more information [here](../../../how-to/visualize-artifacts/README.md). There are lots of options, but we've chosen to plot our failure rates as a bar chart and export them as a `PIL.Image` object. We also plotted the other evaluation scores so as to get a quick global overview of our performance.\n\n```python\n# passing the results from all our previous evaluation steps\n\n@step(enable_cache=False)\ndef visualize_evaluation_results(\n    small_retrieval_eval_failure_rate: float,\n    small_retrieval_eval_failure_rate_reranking: float,\n    full_retrieval_eval_failure_rate: float,\n    full_retrieval_eval_failure_rate_reranking: float,\n    failure_rate_bad_answers: float,\n    failure_rate_bad_immediate_responses: float,\n    failure_rate_good_responses: float,\n    average_toxicity_score: float,\n    average_faithfulness_score: float,\n    average_helpfulness_score: float,\n    average_relevance_score: float,\n) -> Optional[Image.Image]:\n    \"\"\"Visualizes the evaluation results.\"\"\"\n    step_context = get_step_context()\n    pipeline_run_name = step_context.pipeline_run.name\n\n    normalized_scores = [\n        score / 20\n        for score in [\n            small_retrieval_eval_failure_rate,\n            small_retrieval_eval_failure_rate_reranking,\n            full_retrieval_eval_failure_rate,\n            full_retrieval_eval_failure_rate_reranking,\n            failure_rate_bad_answers,\n        ]\n    ]\n\n    scores = normalized_scores + [\n        failure_rate_bad_immediate_responses,\n        failure_rate_good_responses,\n        average_toxicity_score,\n        average_faithfulness_score,\n        average_helpfulness_score,\n        average_relevance_score,\n    ]\n\n    labels = [\n        \"Small Retrieval Eval Failure Rate\",\n        \"Small Retrieval Eval Failure Rate Reranking\",\n        \"Full Retrieval Eval Failure Rate\",\n        \"Full Retrieval Eval Failure Rate Reranking\",\n        \"Failure Rate Bad Answers\",\n        \"Failure Rate Bad Immediate Responses\",\n        \"Failure Rate Good Responses\",\n        \"Average Toxicity Score\",\n        \"Average Faithfulness Score\",\n        \"Average Helpfulness Score\",\n        \"Average Relevance Score\",\n    ]\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots(figsize=(10, "}
{"input": "6))\n\n    # Plot the horizontal bar chart\n    y_pos = np.arange(len(labels))\n    ax.barh(y_pos, scores, align=\"center\")\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels)\n    ax.invert_yaxis()  # Labels read top-to-bottom\n    ax.set_xlabel(\"Score\")\n    ax.set_xlim(0, 5)\n    ax.set_title(f\"Evaluation Metrics for {pipeline_run_name}\")\n\n    # Adjust the layout\n    plt.tight_layout()\n\n    # Save the plot to a BytesIO object\n    buf = io.BytesIO()\n    plt.savefig(buf, format=\"png\")\n    buf.seek(0)\n\n    image = Image.open(buf)\n\n    return image\n```\n\nFor one of my runs of the evaluation pipeline, this looked like the following in the dashboard:\n\n![Evaluation metrics for our RAG pipeline](../../../.gitbook/assets/reranker\\_evaluation\\_metrics.png)\n\nYou can see that for the full retrieval evaluation we do see an improvement. Our small retrieval test, which as of writing only included five questions, showed a considerable degradation in performance. Since these were specific examples where we knew the answers, this would be something we'd want to look into to see why the reranking model was not performing as expected.\n\nWe can also see that regardless of whether reranking was performed or not, the retrieval scores aren't great. This is a good indication that we might want to look into the retrieval model itself (i.e. our embeddings) to see if we can improve its performance. This is what we'll turn to next as we explore finetuning our embeddings to improve retrieval performance.\n\n### Try it out!\n\nTo see how this works in practice, you can run the evaluation pipeline using the project code. The reranking is included as part of the pipeline, so providing you've run the main `rag` pipeline, you can run the evaluation pipeline to see how the reranking model is performing.\n\nTo run the evaluation pipeline, first clone the project repository:\n\n```bash\ngit clone https://github.com/zenml-io/zenml-projects.git\n```\n\nThen navigate to the `llm-complete-guide` directory and follow the instructions in the `README.md` file to run the evaluation pipeline. (You'll have to have first run the main pipeline to generate the embeddings.)\n\nTo run the evaluation pipeline, you can use the following command:\n\n```bash\npython run.py --evaluation\n```\n\nThis will run the evaluation pipeline and output the results to the dashboard. As always, you can inspect the progress, logs, and results in the dashboard!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to implement reranking in ZenML.\n---\n\n# Implementing Reranking in ZenML\n\nWe already have a working RAG pipeline, so inserting a reranker into the\npipeline is relatively straightforward. The reranker will take the retrieved\ndocuments from the initial retrieval step and reorder them in terms of the query\nthat was used to retrieve them.\n\n![](/docs/book/.gitbook/assets/reranking-workflow.png)\n\n## How and where to add reranking\n\nWe'll use the [`rerankers`](https://github.com/AnswerDotAI/rerankers/) package\nto handle the reranking process in our RAG inference pipeline. It's a relatively\nlow-cost (in terms of technical debt and complexity) and lightweight dependency\nto add into our pipeline. It offers an interface to most of the model types that\nare commonly used for reranking and means we don't have to worry about the\nspecifics of each model.\n\nThis package provides a `Reranker` abstract class that you can use to define\nyour own reranker. You can also use the provided implementations to add\nreranking to your pipeline. The reranker takes the query and a list of retrieved\ndocuments as input and outputs a reordered list of documents based on the\nreranking scores. Here's a toy example:\n\n```python\nfrom rerankers import Reranker\n\nranker = Reranker('cross-encoder')\n\ntexts = [\n    \"I like to play soccer\",\n    \"I like to play football\",\n    \"War and Peace is a great book\"\n    \"I love dogs\",\n    \"Ginger cats aren't very smart\",\n    \"I like to play basketball\",\n]\n\nresults = ranker.rank(query=\"What's your favorite sport?\", docs=texts)\n```\n\nAnd results will look something like this:\n\n```\nRankedResults(\n    results=[\n        Result(doc_id=5, text='I like to play basketball', score=-0.46533203125, rank=1),\n        Result(doc_id=0, text='I like to play soccer', score=-0.7353515625, rank=2),\n        Result(doc_id=1, text='I like to play football', score=-0.9677734375, rank=3),\n        Result(doc_id=2, text='War and Peace is a great book', score=-5.40234375, rank=4),\n        Result(doc_id=3, text='I love dogs', score=-5.5859375, rank=5),\n        Result(doc_id=4, text=\"Ginger cats aren't very smart\", score=-5.94921875, rank=6)\n    ],\n    query=\"What's your favorite sport?\",\n    has_scores=True\n)\n```\n\nWe can see that the reranker has reordered the documents based on the reranking\nscores, with the most relevant document appearing at the top"}
{"input": " of the list. The\ntexts about sport are at the top and the less relevant ones about animals are\ndown at the bottom.\n\nWe specified that we want a `cross-encoder` reranker, but you can also use other\nreranker models from the Hugging Face Hub, use API-driven reranker models (from\nJina or Cohere, for example), or even define your own reranker model. Read\n[their documentation](https://github.com/AnswerDotAI/rerankers/) to see how to\nuse these different configurations.\n\nIn our case, we can simply add a helper function that can optionally be invoked\nwhen we want to use the reranker:\n\n```python\n\ndef rerank_documents(\n    query: str, documents: List[Tuple], reranker_model: str = \"flashrank\"\n) -> List[Tuple[str, str]]:\n    \"\"\"Reranks the given documents based on the given query.\"\"\"\n    ranker = Reranker(reranker_model)\n    docs_texts = [f\"{doc[0]} PARENT SECTION: {doc[2]}\" for doc in documents]\n    results = ranker.rank(query=query, docs=docs_texts)\n    # pair the texts with the original urls in `documents`\n    # `documents` is a tuple of (content, url)\n    # we want the urls to be returned\n    reranked_documents_and_urls = []\n    for result in results.results:\n        # content is a `rerankers` Result object\n        index_val = result.doc_id\n        doc_text = result.text\n        doc_url = documents[index_val][1]\n        reranked_documents_and_urls.append((doc_text, doc_url))\n    return reranked_documents_and_urls\n```\n\nThis function takes a query and a list of documents (each document is a tuple of\ncontent and URL) and reranks the documents based on the query. It returns a list\nof tuples, where each tuple contains the reranked document text and the URL of\nthe original document. We use the `flashrank` model from the `rerankers` package\nby default as it appeared to be a good choice for our use case during\ndevelopment.\n\nThis function then gets used in tests in the following way:\n\n```python\ndef query_similar_docs(\n    question: str,\n    url_ending: str,\n    use_reranking: bool = False,\n    returned_sample_size: int = 5,\n) -> Tuple[str, str, List[str]]:\n    \"\"\"Query similar documents for a given question and URL ending.\"\"\"\n    embedded_question = get_embeddings(question)\n    db_conn = get_db_conn()\n    num_docs = 20 if use_reranking else returned_sample_size\n    # get (content, url) tuples for the top n similar documents\n    top_similar_docs = get_topn_similar_docs(\n        embedded_question, db_conn, n=num_docs, include_metadata"}
{"input": "=True\n    )\n\n    if use_reranking:\n        reranked_docs_and_urls = rerank_documents(question, top_similar_docs)[\n            :returned_sample_size\n        ]\n        urls = [doc[1] for doc in reranked_docs_and_urls]\n    else:\n        urls = [doc[1] for doc in top_similar_docs]  # Unpacking URLs\n\n    return (question, url_ending, urls)\n```\n\nWe get the embeddings for the question being passed into the function and\nconnect to our PostgreSQL database. If we're using reranking, we get the top 20\ndocuments similar to our query and rerank them using the `rerank_documents`\nhelper function. We then extract the URLs from the reranked documents and return\nthem. Note that we only return 5 URLs, but in the case of reranking we get a\nlarger number of documents and URLs back from the database to pass to our\nreranker, but in the end we always choose the top five reranked documents to\nreturn.\n\nNow that we've added reranking to our pipeline, we can evaluate the performance\nof our reranker and see how it affects the quality of the retrieved documents.\n\n## Code Example\n\nTo explore the full code, visit the [Complete\nGuide](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/)\nrepository and for this section, particularly [the `eval_retrieval.py` file](https://github.com/zenml-io/zenml-projects/blob/main/llm-complete-guide/steps/eval_retrieval.py).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Add reranking to your RAG inference for better retrieval performance.\n---\n\nRerankers are a crucial component of retrieval systems that use LLMs. They help\nimprove the quality of the retrieved documents by reordering them based on\nadditional features or scores. In this section, we'll explore how to add a\nreranker to your RAG inference pipeline in ZenML.\n\nIn previous sections, we set up the overall workflow, from data ingestion and\npreprocessing to embeddings generation and retrieval. We then set up some basic\nevaluation metrics to assess the performance of our retrieval system. A reranker\nis a way to squeeze a bit of extra performance out of the system by reordering\nthe retrieved documents based on additional features or scores.\n\n![](/docs/book/.gitbook/assets/reranking-workflow.png)\n\nAs you can see, reranking is an optional addition we make to what we've already\nset up. It's not strictly necessary, but it can help improve the relevance and\nquality of the retrieved documents, which in turn can lead to better responses\nfrom the LLM. Let's dive in!\n\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Understand how reranking works.\n---\n\n## What is reranking?\n\nReranking is the process of refining the initial ranking of documents retrieved\nby a retrieval system. In the context of Retrieval-Augmented Generation (RAG),\nreranking plays a crucial role in improving the relevance and quality of the\nretrieved documents that are used to generate the final output.\n\nThe initial retrieval step in RAG typically uses a sparse retrieval method, such\nas BM25 or TF-IDF, to quickly find a set of potentially relevant documents based\non the input query. However, these methods rely on lexical matching and may not\ncapture the semantic meaning or context of the query effectively.\n\nRerankers, on the other hand, are designed to reorder the retrieved documents by\nconsidering additional features, such as semantic similarity, relevance scores,\nor domain-specific knowledge. They aim to push the most relevant and informative\ndocuments to the top of the list, ensuring that the LLM has access to the best\npossible context for generating accurate and coherent responses.\n\n## Types of Rerankers\n\nThere are different types of rerankers that can be used in RAG, each with its\nown strengths and trade-offs:\n\n1. **Cross-Encoders**: Cross-encoders are a popular choice for reranking in RAG.\n   They take the concatenated query and document as input and output a relevance\n   score. Examples include BERT-based models fine-tuned for passage ranking\n   tasks. Cross-encoders can capture the interaction between the query and\n   document effectively but are computationally expensive.\n\n2. **Bi-Encoders**: Bi-encoders, also known as dual encoders, use separate\n   encoders for the query and document. They generate embeddings for the query\n   and document independently and then compute the similarity between them.\n   Bi-encoders are more efficient than cross-encoders but may not capture the\n   query-document interaction as effectively.\n\n3. **Lightweight Models**: Lightweight rerankers, such as distilled models or\n   small transformer variants, aim to strike a balance between effectiveness and\n   efficiency. They are faster and have a smaller footprint compared to large\n   cross-encoders, making them suitable for real-time applications.\n\n## Benefits of Reranking in RAG\n\nReranking offers several benefits in the context of RAG:\n\n1. **Improved Relevance**: By considering additional features and scores,\n   rerankers can identify the most relevant documents for a given query,\n   ensuring that the LLM has access to the most informative context for\n   generating accurate responses.\n\n2. **Semantic Understanding**: Rerankers can capture the semantic meaning and\n   context of the query and documents, going beyond simple keyword matching.\n   This enables the retrieval of documents that are semantically similar to the\n   query, even if they don't contain exact keyword matches.\n\n3. **"}
{"input": "Domain Adaptation**: Rerankers can be fine-tuned on domain-specific data to\n   incorporate domain knowledge and improve performance in specific verticals or\n   industries.\n\n4. **Personalization**: Rerankers can be personalized based on user preferences,\n   historical interactions, or user profiles, enabling the retrieval of\n   documents that are more tailored to individual users' needs.\n\nIn the next section, we'll dive into how to implement reranking in ZenML and\nintegrate it into your RAG inference pipeline.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Add reranking to your RAG inference for better retrieval performance.\n---\n\nRerankers are a crucial component of retrieval systems that use LLMs. They help\nimprove the quality of the retrieved documents by reordering them based on\nadditional features or scores. In this section, we'll explore how to add a\nreranker to your RAG inference pipeline in ZenML.\n\nIn previous sections, we set up the overall workflow, from data ingestion and\npreprocessing to embeddings generation and retrieval. We then set up some basic\nevaluation metrics to assess the performance of our retrieval system. A reranker\nis a way to squeeze a bit of extra performance out of the system by reordering\nthe retrieved documents based on additional features or scores.\n\n![](/docs/book/.gitbook/assets/reranking-workflow.png)\n\nAs you can see, reranking is an optional addition we make to what we've already\nset up. It's not strictly necessary, but it can help improve the relevance and\nquality of the retrieved documents, which in turn can lead to better responses\nfrom the LLM. Let's dive in!\n\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Creating a full picture of a ML model using the Model Control Plane\n---\n\n# Track ML models\n\n![Walkthrough of ZenML Model Control Plane (Dashboard available only on ZenML Pro)](../../.gitbook/assets/mcp_walkthrough.gif)\n\nAs discussed in the [Core Concepts](../../getting-started/core-concepts.md), ZenML also contains the notion of a `Model`, which consists of many model versions (the iterations of the model). These concepts are exposed in the `Model Control Plane` (MCP for short).\n\n## What is a ZenML Model?\n\nBefore diving in, let's take some time to build an understanding of what we mean when we say `Model` in ZenML terms. A `Model` is simply an entity that groups pipelines, artifacts, metadata, and other crucial business data into a unified entity. In this sense, a ZenML Model is a concept that more broadly encapsulates your ML product's business logic. You may even think of a ZenML Model as a \"project\" or a \"workspace\"\n\n{% hint style=\"warning\" %}\nPlease note that one of the most common artifacts that is associated with a Model in ZenML is the so-called technical model, which is the actually model file/files that holds the weight and parameters of a machine learning training result. However, this is not the only artifact that is relevant; artifacts such as the training data and the predictions this model produces in production are also linked inside a ZenML Model.\n{% endhint %}\n\nModels are first-class citizens in ZenML and as such viewing and using them is unified and centralized in the ZenML API, the ZenML client as well as on the [ZenML Pro](https://zenml.io/pro) dashboard.\n\nThese models can be viewed within ZenML:\n\n{% tabs %}\n{% tab title=\"OSS (CLI)\" %}\n`zenml model list` can be used to list all models.\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe [ZenML Pro](https://zenml.io/pro) dashboard has additional capabilities, that include visualizing these models in the dashboard.\n\n<figure><img src=\"../../.gitbook/assets/mcp_model_list.png\" alt=\"\"><figcaption><p>ZenML Model Control Plane.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\n## Configuring a model in a pipeline\n\nThe easiest way to use a ZenML model is to pass a `Model` object as part of a pipeline run. This can be done easily at a pipeline or a step level, or via a [YAML config](../production-guide/configure-pipeline.md).\n\nOnce you configure a pipeline this way, **all** artifacts generated during pipeline runs are automatically **linked** to the specified model. This connecting of artifacts provides lineage tracking and transparency into what data and models are used during training, evaluation, and inference.\n\n```python\nfrom zenml import pipeline\nfrom zen"}
{"input": "ml import Model\n\nmodel = Model(\n    # The name uniquely identifies this model\n    # It usually represents the business use case\n    name=\"iris_classifier\",\n    # The version specifies the version\n    # If None or an unseen version is specified, it will be created\n    # Otherwise, a version will be fetched.\n    version=None, \n    # Some other properties may be specified\n    license=\"Apache 2.0\",\n    description=\"A classification model for the iris dataset.\",\n)\n\n# The step configuration will take precedence over the pipeline\n@step(model=model)\ndef svc_trainer(...) -> ...:\n    ...\n\n# This configures it for all steps within the pipeline\n@pipeline(model=model)\ndef training_pipeline(gamma: float = 0.002):\n    # Now this pipeline will have the `iris_classifier` model active.\n    X_train, X_test, y_train, y_test = training_data_loader()\n    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)\n\nif __name__ == \"__main__\":\n    training_pipeline()\n\n# In the YAML the same can be done; in this case, the \n#  passing to the decorators is not needed\n# model: \n  # name: iris_classifier\n  # license: \"Apache 2.0\"\n  # description: \"A classification model for the iris dataset.\"\n\n```\n\nThe above will establish a **link between all artifacts that pass through this ZenML pipeline and this model**. This includes the **technical model** which is what comes out of the `svc_trainer` step. You will be able to see all associated artifacts and pipeline runs, all within one view.\n\nFurthermore, this pipeline run and all other pipeline runs that are configured with this model configuration will be linked to this model as well.\n\nYou can see all versions of a model, and associated artifacts and run like this:\n\n{% tabs %}\n{% tab title=\"OSS (CLI)\" %}\n`zenml model version list <MODEL_NAME>` can be used to list all versions of a particular model.\n\nThe following commands can be used to list the various pipeline runs associated with a model:\n\n* `zenml model version runs <MODEL_NAME> <MODEL_VERSIONNAME>`\n\nThe following commands can be used to list the various artifacts associated with a model:\n\n* `zenml model version data_artifacts <MODEL_NAME> <MODEL_VERSIONNAME>`\n* `zenml model version model_artifacts <MODEL_NAME> <MODEL_VERSIONNAME>`\n* `zenml model version deployment_artifacts <MODEL_NAME> <MODEL_VERSIONNAME>`\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe [ZenML Pro](https://zenml.io/pro) dashboard has additional capabilities, that include visualizing all associated runs and artifacts for a model version:\n\n<figure><img src=\"../../.gitbook/assets/mcp_model_versions_list.png\" alt=\"ZenML Model Versions List.\"><figcaption"}
{"input": "><p>ZenML Model versions List.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\n## Fetching the model in a pipeline\n\nWhen configured at the pipeline or step level, the model will be available through the [StepContext](../../how-to/track-metrics-metadata/fetch-metadata-within-pipeline.md) or [PipelineContext](../../how-to/track-metrics-metadata/fetch-metadata-within-pipeline.md).\n\n```python\nfrom zenml import get_step_context, get_pipeline_context, step, pipeline\n\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Annotated[ClassifierMixin, \"trained_model\"]:\n    # This will return the model specified in the \n    # @pipeline decorator. In this case, the production version of \n    # the `iris_classifier` will be returned in this case.\n    model = get_step_context().model\n    ...\n\n@pipeline(\n    model=Model(\n        # The name uniquely identifies this model\n        name=\"iris_classifier\",\n        # Pass the stage you want to get the right model\n        version=\"production\", \n    ),\n)\ndef training_pipeline(gamma: float = 0.002):\n    # Now this pipeline will have the production `iris_classifier` model active.\n    model = get_pipeline_context().model\n\n    X_train, X_test, y_train, y_test = training_data_loader()\n    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)\n```\n\n## Logging metadata to the `Model` object\n\n[Just as one can associate metadata with artifacts](manage-artifacts.md#logging-metadata-for-an-artifact), models too can take a dictionary of key-value pairs to capture their metadata. This is achieved using the `log_model_metadata` method:\n\n```python\nfrom zenml import get_step_context, step, log_model_metadata \n\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Annotated[ClassifierMixin, \"sklearn_classifier\"],:\n    # Train and score model\n    ...\n    model.fit(dataset[0], dataset[1])\n    accuracy = model.score(dataset[0], dataset[1])\n\n    model = get_step_context().model\n    \n    log_model_metadata(\n        # Model name can be omitted if specified in the step or pipeline context\n        model_name=\"iris_classifier\",\n        # Passing None or omitting this will use the `latest` version\n        version=None,\n        # Metadata should be a dictionary of JSON-serializable values\n        metadata={\"accuracy\": float(accuracy)}\n        # A dictionary of dictionaries can also be passed to group metadata\n        #  in the dashboard\n        # metadata = {\"metrics\": {\"accuracy\": accuracy}}\n    )\n```\n\n{%"}
{"input": " tabs %}\n{% tab title=\"Python\" %}\n```python\nfrom zenml.client import Client\n\n# Get an artifact version (in this the latest `iris_classifier`)\nmodel_version = Client().get_model_version('iris_classifier')\n\n# Fetch it's metadata\nmodel_version.run_metadata[\"accuracy\"].value\n```\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe [ZenML Pro](https://zenml.io/pro) dashboard offers advanced visualization features for artifact exploration, including a dedicated artifacts tab with metadata visualization:\n\n<figure><img src=\"../../.gitbook/assets/dcp_metadata.png\" alt=\"\"><figcaption><p>ZenML Artifact Control Plane.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\nChoosing [log metadata with artifacts](manage-artifacts.md#logging-metadata-for-an-artifact) or model versions depends on the scope and purpose of the information you wish to capture. Artifact metadata is best for details specific to individual outputs, while model version metadata is suitable for broader information relevant to the overall model. By utilizing ZenML's metadata logging capabilities and special types, you can enhance the traceability, reproducibility, and analysis of your ML workflows.\n\nOnce metadata has been logged to a model, we can retrieve it easily with the client:\n\n```python\nfrom zenml.client import Client\nclient = Client()\nmodel = client.get_model_version(\"my_model\", \"my_version\")\nprint(model.run_metadata[\"metadata_key\"].value)\n```\n\nFor further depth, there is an [advanced metadata logging guide](../../how-to/track-metrics-metadata/README.md) that goes more into detail about logging metadata in ZenML.\n\n## Using the stages of a model\n\nA model's versions can exist in various stages. These are meant to signify their lifecycle state:\n\n* `staging`: This version is staged for production.\n* `production`: This version is running in a production setting.\n* `latest`: The latest version of the model.\n* `archived`: This is archived and no longer relevant. This stage occurs when a model moves out of any other stage.\n\n{% tabs %}\n{% tab title=\"Python SDK\" %}\n```python\nfrom zenml import Model\n\n# Get the latest version of a model\nmodel = Model(\n    name=\"iris_classifier\",\n    version=\"latest\"\n)\n\n# Get `my_version` version of a model\nmodel = Model(\n    name=\"iris_classifier\",\n    version=\"my_version\",\n)\n\n# Pass the stage into the version field\n# to get the `staging` model\nmodel = Model(\n    name=\"iris_classifier\",\n    version=\"staging\",\n)\n\n# This will set this version to production\nmodel.set_stage(stage=\"production\", force=True)\n```\n{% endtab %}\n\n{% tab title=\"CLI\" %}\n```shell\n# List staging models\nzenml model version list <MODEL_NAME> --stage staging \n\n# Update to production\nzenml model version update <MODEL"}
{"input": "_NAME> <MODEL_VERSIONNAME> -s production \n```\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe [ZenML Pro](https://zenml.io/pro) dashboard has additional capabilities, that include easily changing the stage:\n\n![ZenML Pro Transition Model Stages](../../.gitbook/assets/dcp\\_transition\\_stage.gif)\n{% endtab %}\n{% endtabs %}\n\nZenML Model and versions are some of the most powerful features in ZenML. To understand them in a deeper way, read the [dedicated Model Management](../../how-to/use-the-model-control-plane/README.md) guide.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Start with the basics of steps and pipelines.\n---\n\n# Create an ML pipeline\n\nIn the quest for production-ready ML models, workflows can quickly become complex. Decoupling and standardizing stages such as data ingestion, preprocessing, and model evaluation allows for more manageable, reusable, and scalable processes. ZenML pipelines facilitate this by enabling each stage\u2014represented as **Steps**\u2014to be modularly developed and then integrated smoothly into an end-to-end **Pipeline**.\n\nLeveraging ZenML, you can create and manage robust, scalable machine learning (ML) pipelines. Whether for data preparation, model training, or deploying predictions, ZenML standardizes and streamlines the process, ensuring reproducibility and efficiency.\n\n<figure><img src=\"../../.gitbook/assets/pipeline_showcase.png\" alt=\"\"><figcaption><p>ZenML pipelines are simple Python code</p></figcaption></figure>\n\n{% hint style=\"info\" %}\nBefore starting this guide, make sure you have [installed ZenML](../../getting-started/installation.md):\n\n```shell\npip install \"zenml[server]\"\nzenml up  # Will launch the dashboard locally\n```\n{% endhint %}\n\n## Start with a simple ML pipeline\n\nLet's jump into an example that demonstrates how a simple pipeline can be set up in ZenML, featuring actual ML components to give you a better sense of its application.\n\n```python\nfrom zenml import pipeline, step\n\n@step\ndef load_data() -> dict:\n    \"\"\"Simulates loading of training data and labels.\"\"\"\n\n    training_data = [[1, 2], [3, 4], [5, 6]]\n    labels = [0, 1, 0]\n    \n    return {'features': training_data, 'labels': labels}\n\n@step\ndef train_model(data: dict) -> None:\n    \"\"\"\n    A mock 'training' process that also demonstrates using the input data.\n    In a real-world scenario, this would be replaced with actual model fitting logic.\n    \"\"\"\n    total_features = sum(map(sum, data['features']))\n    total_labels = sum(data['labels'])\n    \n    print(f\"Trained model using {len(data['features'])} data points. \"\n          f\"Feature sum is {total_features}, label sum is {total_labels}\")\n\n@pipeline\ndef simple_ml_pipeline():\n    \"\"\"Define a pipeline that connects the steps.\"\"\"\n    dataset = load_data()\n    train_model(dataset)\n\nif __name__ == \"__main__\":\n    run = simple_ml_pipeline()\n    # You can now use the `run` object to see steps, outputs, etc.\n```\n\n{% hint style=\"info\" %}\n* **`@step`** is a decorator that converts its function into a step that can be used within a pipeline\n* **`@pipeline`** defines a function as a pipeline and within this function, the steps are called and their outputs link them together.\n{% endhint %}\n\n"}
{"input": "Copy this code into a new file and name it `run.py`. Then run it with your command line:\n\n{% code overflow=\"wrap\" %}\n```bash\n$ python run.py\n\nInitiating a new run for the pipeline: simple_ml_pipeline.\nExecuting a new run.\nUsing user: hamza@zenml.io\nUsing stack: default\n  orchestrator: default\n  artifact_store: default\nStep load_data has started.\nStep load_data has finished in 0.385s.\nStep train_model has started.\nTrained model using 3 data points. Feature sum is 21, label sum is 1\nStep train_model has finished in 0.265s.\nRun simple_ml_pipeline-2023_11_23-10_51_59_657489 has finished in 1.612s.\nPipeline visualization can be seen in the ZenML Dashboard. Run zenml up to see your pipeline!\n```\n{% endcode %}\n\n### Explore the dashboard\n\nOnce the pipeline has finished its execution, use the `zenml up` command to view the results in the ZenML Dashboard. Using that command will open up the browser automatically.\n\n<figure><img src=\"../../.gitbook/assets/landingpage.png\" alt=\"\"><figcaption><p>Landing Page of the Dashboard</p></figcaption></figure>\n\nUsually, the dashboard is accessible at [http://127.0.0.1:8237/](http://127.0.0.1:8237/). Log in with the default username **\"default\"** (password not required) and see your recently run pipeline. Browse through the pipeline components, such as the execution history and artifacts produced by your steps. Use the DAG visualization to understand the flow of data and to ensure all steps are completed successfully.\n\n<figure><img src=\"../../.gitbook/assets/DAGofRun.png\" alt=\"\"><figcaption><p>Diagram view of the run, with the runtime attributes of step 2.</p></figcaption></figure>\n\nFor further insights, explore the logging and artifact information associated with each step, which can reveal details about the data and intermediate results.\n\nIf you have closed the browser tab with the ZenML dashboard, you can always reopen it by running `zenml show` in your terminal.\n\n## Understanding steps and artifacts\n\nWhen you ran the pipeline, each individual function that ran is shown in the DAG visualization as a `step` and is marked with the function name. Steps are connected with `artifacts`, which are simply the objects that are returned by these functions and input into downstream functions. This simple logic lets us break down our entire machine learning code into a sequence of tasks that pass data between each other.\n\nThe artifacts produced by your steps are automatically stored and versioned by ZenML. The code that produced these artifacts is also automatically tracked. The parameters and all other configuration is also automatically captured.\n\nSo you can see, by simply structuring your code within some functions"}
{"input": " and adding some decorators, we are one step closer to having a more tracked and reproducible codebase!\n\n## Expanding to a Full Machine Learning Workflow\n\nWith the fundamentals in hand, let\u2019s escalate our simple pipeline to a complete ML workflow. For this task, we will use the well-known Iris dataset to train a Support Vector Classifier (SVC).\n\nLet's start with the imports.\n\n```python\nfrom typing_extensions import Annotated  # or `from typing import Annotated on Python 3.9+\nfrom typing import Tuple\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.svm import SVC\n\nfrom zenml import pipeline, step\n```\n\nMake sure to install the requirements as well:\n\n```bash\npip install matplotlib\nzenml integration install sklearn -y\n```\n\nIn this case, ZenML has an integration with `sklearn` so you can use the ZenML CLI to install the right version directly.\n\n{% hint style=\"info\" %}\nThe `zenml integration install sklearn` command is simply doing a `pip install` of `sklearn` behind the scenes. If something goes wrong, one can always use `zenml integration requirements sklearn` to see which requirements are compatible and install using pip (or any other tool) directly. (If no specific requirements are mentioned for an integration then this means we support using all possible versions of that integration/package.)\n{% endhint %}\n\n### Define a data loader with multiple outputs\n\nA typical start of an ML pipeline is usually loading data from some source. This step will sometimes have multiple outputs. To define such a step, use a `Tuple` type annotation. Additionally, you can use the `Annotated` annotation to assign [custom output names](manage-artifacts.md#giving-names-to-your-artifacts). Here we load an open-source dataset and split it into a train and a test dataset.\n\n```python\nimport logging\n\n@step\ndef training_data_loader() -> Tuple[\n    # Notice we use a Tuple and Annotated to return \n    # multiple named outputs\n    Annotated[pd.DataFrame, \"X_train\"],\n    Annotated[pd.DataFrame, \"X_test\"],\n    Annotated[pd.Series, \"y_train\"],\n    Annotated[pd.Series, \"y_test\"],\n]:\n    \"\"\"Load the iris dataset as a tuple of Pandas DataFrame / Series.\"\"\"\n    logging.info(\"Loading iris...\")\n    iris = load_iris(as_frame=True)\n    logging.info(\"Splitting train and test...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n```\n\n{% hint style=\"info\" %}\nZenML records the root python logging handler's output into the artifact"}
{"input": " store as a side-effect of running a step. Therefore, when writing steps, use the `logging` module to record logs, to ensure that these logs then show up in the ZenML dashboard.\n{% endhint %}\n\n### Create a parameterized training step\n\nHere we are creating a training step for a support vector machine classifier with `sklearn`. As we might want to adjust the hyperparameter `gamma` later on, we define it as an input value to the step as well.\n\n```python\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Tuple[\n    Annotated[ClassifierMixin, \"trained_model\"],\n    Annotated[float, \"training_acc\"],\n]:\n    \"\"\"Train a sklearn SVC classifier.\"\"\"\n\n    model = SVC(gamma=gamma)\n    model.fit(X_train.to_numpy(), y_train.to_numpy())\n\n    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n    print(f\"Train accuracy: {train_acc}\")\n\n    return model, train_acc\n```\n\n{% hint style=\"info\" %}\nIf you want to run the step function outside the context of a ZenML pipeline, all you need to do is call the step function outside of a ZenML pipeline. For example:\n\n```python\nsvc_trainer(X_train=..., y_train=...)\n```\n{% endhint %}\n\nNext, we will combine our two steps into a pipeline and run it. As you can see, the parameter gamma is configurable as a pipeline input as well.\n\n```python\n@pipeline\ndef training_pipeline(gamma: float = 0.002):\n    X_train, X_test, y_train, y_test = training_data_loader()\n    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)\n\n\nif __name__ == \"__main__\":\n    training_pipeline(gamma=0.0015)\n```\n\n{% hint style=\"info\" %}\nBest Practice: Always nest the actual execution of the pipeline inside an `if __name__ == \"__main__\"` condition. This ensures that loading the pipeline from elsewhere does not also run it.\n\n```python\nif __name__ == \"__main__\":\n    training_pipeline()\n```\n{% endhint %}\n\nRunning `python run.py` should look somewhat like this in the terminal:\n\n<pre class=\"language-sh\" data-line-numbers><code class=\"lang-sh\"><strong>Registered new pipeline with name `training_pipeline`.\n</strong>.\n.\n.\nPipeline run `training_pipeline-2023_04_29-09_19_54_273710` has finished in 0.236s.\n</code></pre>\n\nIn the dashboard, you should now be able to see this new run, along with its runtime configuration and a visualization of the training data.\n\n<figure><img src=\"../../.gitbook/assets/RunWithVisualization.png\" alt=\"\"><figcaption><p>Run"}
{"input": " created by the code in this section along with a visualization of the ground-truth distribution.</p></figcaption></figure>\n\n### Configure with a YAML file\n\nInstead of configuring your pipeline runs in code, you can also do so from a YAML file. This is best when we do not want to make unnecessary changes to the code; in production this is usually the case.\n\nTo do this, simply reference the file like this:\n\n```python\n# Configure the pipeline\ntraining_pipeline = training_pipeline.with_options(\n    config_path='/local/path/to/config.yaml'\n)\n# Run the pipeline\ntraining_pipeline()\n```\n\nThe reference to a local file will change depending on where you are executing the pipeline and code from, so please bear this in mind. It is best practice to put all config files in a configs directory at the root of your repository and check them into git history.\n\nA simple version of such a YAML file could be:\n\n```yaml\nparameters:\n    gamma: 0.01\n```\n\nPlease note that this would take precedence over any parameters passed in the code.\n\nIf you are unsure how to format this config file, you can generate a template config file from a pipeline.\n\n```python\ntraining_pipeline.write_run_configuration_template(path='/local/path/to/config.yaml')\n```\n\nCheck out [this section](../../how-to/use-configuration-files/README.md) for advanced configuration options.\n\n## Full Code Example\n\nThis section combines all the code from this section into one simple script that you can use to run easily:\n\n<details>\n\n<summary>Code Example of this Section</summary>\n\n```python\nfrom typing_extensions import Tuple, Annotated\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.svm import SVC\n\nfrom zenml import pipeline, step\n\n\n@step\ndef training_data_loader() -> Tuple[\n    Annotated[pd.DataFrame, \"X_train\"],\n    Annotated[pd.DataFrame, \"X_test\"],\n    Annotated[pd.Series, \"y_train\"],\n    Annotated[pd.Series, \"y_test\"],\n]:\n    \"\"\"Load the iris dataset as tuple of Pandas DataFrame / Series.\"\"\"\n    iris = load_iris(as_frame=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n\n\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Tuple[\n    Annotated[ClassifierMixin, \"trained_model\"],\n    Annotated[float, \"training_acc\"],\n]:\n    \"\"\"Train a sklearn SVC classifier and log to MLflow.\"\"\"\n    model = SVC(gamma=gamma)\n    model.fit(X_train.to_numpy(), y_train"}
{"input": ".to_numpy())\n    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n    print(f\"Train accuracy: {train_acc}\")\n    return model, train_acc\n\n\n@pipeline\ndef training_pipeline(gamma: float = 0.002):\n    X_train, X_test, y_train, y_test = training_data_loader()\n    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)\n\n\nif __name__ == \"__main__\":\n    training_pipeline()\n```\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Kickstart your journey into MLOps with the essentials of ZenML.\n---\n\n# \ud83d\udc23 Starter guide\n\nWelcome to the ZenML Starter Guide! If you're an MLOps engineer aiming to build robust ML platforms, or a data scientist interested in leveraging the power of MLOps, this is the perfect place to begin. Our guide is designed to provide you with the foundational knowledge of the ZenML framework and equip you with the initial tools to manage the complexity of machine learning operations.\n\n<figure><img src=\"../../.gitbook/assets/abstractions_showcase.png\" alt=\"\"><figcaption><p>Embarking on MLOps can be intricate. ZenML simplifies the journey.</p></figcaption></figure>\n\nThroughout this guide, we'll cover essential topics including:\n\n- [Creating your first ML pipeline](create-an-ml-pipeline.md)\n- [Understanding caching between pipeline steps](cache-previous-executions.md)\n- [Fetching objects after pipelines have run](fetching-pipelines.md)\n- [Managing data and data versioning](manage-artifacts.md)\n- [Tracking your machine learning models](track-ml-models.md)\n- [Structuring your pipelines, models, and artifacts](structuring-a-project.md)\n\nBefore jumping in, make sure you have a Python environment ready and `virtualenv` installed to follow along with ease.\nBy the end, you will have completed a [starter project](starter-project.md), marking the beginning of your journey into MLOps with ZenML.\n\nLet this guide be not only your introduction to ZenML but also a foundational asset in your MLOps toolkit. Prepare your development environment, and let's get started!\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Kickstart your journey into MLOps with the essentials of ZenML.\n---\n\n# \ud83d\udc23 Starter guide\n\nWelcome to the ZenML Starter Guide! If you're an MLOps engineer aiming to build robust ML platforms, or a data scientist interested in leveraging the power of MLOps, this is the perfect place to begin. Our guide is designed to provide you with the foundational knowledge of the ZenML framework and equip you with the initial tools to manage the complexity of machine learning operations.\n\n<figure><img src=\"../../.gitbook/assets/abstractions_showcase.png\" alt=\"\"><figcaption><p>Embarking on MLOps can be intricate. ZenML simplifies the journey.</p></figcaption></figure>\n\nThroughout this guide, we'll cover essential topics including:\n\n* [Creating your first ML pipeline](create-an-ml-pipeline.md)\n* [Understanding caching between pipeline steps](cache-previous-executions.md)\n* [Managing data and data versioning](manage-artifacts.md)\n* [Tracking your machine learning models](track-ml-models.md)\n\nBefore jumping in, make sure you have a Python environment ready and `virtualenv` installed to follow along with ease. By the end, you will have completed a [starter project](starter-project.md), marking the beginning of your journey into MLOps with ZenML.\n\nLet this guide be not only your introduction to ZenML but also a foundational asset in your MLOps toolkit. Prepare your development environment, and let's get started!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Iterating quickly with ZenML through caching.\n---\n\n# Cache previous executions\n\nDeveloping machine learning pipelines is iterative in nature. ZenML speeds up development in this work with step caching.\n\nIn the logs of your previous runs, you might have noticed at this point that rerunning the pipeline a second time will use caching on the first step:\n\n```bash\nStep training_data_loader has started.\nUsing cached version of training_data_loader.\nStep svc_trainer has started.\nTrain accuracy: 0.3416666666666667\nStep svc_trainer has finished in 0.932s.\n```\n\n![DAG of a cached pipeline run](../../.gitbook/assets/CachedDag.png)\n\nZenML understands that nothing has changed between subsequent runs, so it re-uses the output of the previous run (the outputs are persisted in the [artifact store](../../component-guide/artifact-stores/artifact-stores.md)). This behavior is known as **caching**.\n\nIn ZenML, caching is enabled by default. Since ZenML automatically tracks and versions all inputs, outputs, and parameters of steps and pipelines, steps will not be re-executed within the **same pipeline** on subsequent pipeline runs as long as there is **no change** in the inputs, parameters, or code of a step.\n\n{% hint style=\"warning\" %}\nThe caching does not automatically detect changes within the file system or on external APIs. Make sure to **manually** set caching to `False` on steps that depend on **external inputs, file-system changes,** or if the step should run regardless of caching.\n\n```python\n@step(enable_cache=False)\ndef load_data_from_external_system(...) -> ...:\n    # This step will always be run\n```\n{% endhint %}\n\n## Configuring the caching behavior of your pipelines\n\nWith caching as the default behavior, there will be times when you need to disable it.\n\nThere are levels at which you can take control of when and where caching is used.\n\n```mermaid\ngraph LR\n  A[\"Pipeline Settings\"] -->|overwritten by| B[\"Step Settings\"] \n  B[\"Step Settings\"] -->|overwritten by| C[\"Changes in Code, Inputs or Parameters\"] \n```\n\n### Caching at the pipeline level\n\nOn a pipeline level, the caching policy can be set as a parameter within the `@pipeline` decorator as shown below:\n\n```python\n@pipeline(enable_cache=False)\ndef first_pipeline(....):\n    \"\"\"Pipeline with cache disabled\"\"\"\n```\n\nThe setting above will disable caching for all steps in the pipeline unless a step explicitly sets `enable_cache=True` ( see below).\n\n{% hint style=\"info\" %}\nWhen writing your pipelines, be explicit. This makes it clear when looking at the code if caching is enabled or disabled for any given pipeline.\n{% endhint %}\n\n#### Dynamically configuring caching for a pipeline run\n\nSometimes you want to have control over caching at runtime instead"}
{"input": " of defaulting to the hard-coded pipeline and step decorator settings. ZenML offers a way to override all caching settings at runtime:\n\n```python\nfirst_pipeline = first_pipeline.with_options(enable_cache=False)\n```\n\nThe code above disables caching for all steps of your pipeline, no matter what you have configured in the `@step` or `@pipeline` decorators.\n\nThe `with_options` function allows you to configure all sorts of things this way. We will learn more about it in the [coming chapters](../production-guide/configure-pipeline.md)!\n\n### Caching at a step-level\n\nCaching can also be explicitly configured at a step level via a parameter of the `@step` decorator:\n\n```python\n@step(enable_cache=False)\ndef import_data_from_api(...):\n    \"\"\"Import most up-to-date data from public api\"\"\"\n    ...\n```\n\nThe code above turns caching off for this step only.\n\nYou can also use `with_options` with the step, just as in the pipeline:\n\n```python\nimport_data_from_api = import_data_from_api.with_options(enable_cache=False)\n\n# use in your pipeline directly\n```\n\n## Code Example\n\nThis section combines all the code from this section into one simple script that you can use to see caching easily:\n\n<details>\n\n<summary>Code Example of this Section</summary>\n\n```python\nfrom typing_extensions import Tuple, Annotated\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.svm import SVC\n\nfrom zenml import pipeline, step\nfrom zenml.logger import get_logger\n\nlogger = get_logger(__name__)\n\n\n@step\ndef training_data_loader() -> Tuple[\n    Annotated[pd.DataFrame, \"X_train\"],\n    Annotated[pd.DataFrame, \"X_test\"],\n    Annotated[pd.Series, \"y_train\"],\n    Annotated[pd.Series, \"y_test\"],\n]:\n    \"\"\"Load the iris dataset as tuple of Pandas DataFrame / Series.\"\"\"\n    iris = load_iris(as_frame=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n\n\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Tuple[\n    Annotated[ClassifierMixin, \"trained_model\"],\n    Annotated[float, \"training_acc\"],\n]:\n    \"\"\"Train a sklearn SVC classifier and log to MLflow.\"\"\"\n    model = SVC(gamma=gamma)\n    model.fit(X_train.to_numpy(), y_train.to_numpy())\n    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n    print(f\"Train accuracy: {train_acc}\")\n    return model,"}
{"input": " train_acc\n\n\n@pipeline\ndef training_pipeline(gamma: float = 0.002):\n    X_train, X_test, y_train, y_test = training_data_loader()\n    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)\n\n\nif __name__ == \"__main__\":\n    training_pipeline()\n\n    # Step one will use cache, step two will rerun. \n    # ZenML will detect a different value for the\n    # `gamma` input of the second step and disable caching.\n    logger.info(\"\\n\\nFirst step cached, second not due to parameter change\")\n    training_pipeline(gamma=0.0001)\n\n    # This will disable cache for the second step.\n    logger.info(\"\\n\\nFirst step cached, second not due to settings\")\n    svc_trainer = svc_trainer.with_options(enable_cache=False)\n    training_pipeline()\n\n    # This will disable cache for all steps.\n    logger.info(\"\\n\\nCaching disabled for the entire pipeline\")\n    training_pipeline.with_options(enable_cache=False)()\n```\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Put your new knowledge into action with a simple starter project\n---\n\n# A starter project\n\nBy now, you have understood some of the basic pillars of a MLOps system:\n\n* [Pipelines and steps](create-an-ml-pipeline.md)\n* [Artifacts](manage-artifacts.md)\n* [Models](track-ml-models.md)\n\nWe will now put this into action with a simple starter project.\n\n## Get started\n\nStart with a fresh virtual environment with no dependencies. Then let's install our dependencies:\n\n```bash\npip install \"zenml[templates,server]\" notebook\nzenml integration install sklearn -y\n```\n\nWe will then use [ZenML templates](../../how-to/setting-up-a-project-repository/using-project-templates.md) to help us get the code we need for the project:\n\n```bash\nmkdir zenml_starter\ncd zenml_starter\nzenml init --template starter --template-with-defaults\n\n# Just in case, we install the requirements again\npip install -r requirements.txt\n```\n\n<details>\n\n<summary>Above doesn't work? Here is an alternative</summary>\n\nThe starter template is the same as the [ZenML quickstart](https://github.com/zenml-io/zenml/tree/main/examples/quickstart). You can clone it like so:\n\n```bash\ngit clone --depth 1 git@github.com:zenml-io/zenml.git\ncd zenml/examples/quickstart\npip install -r requirements.txt\nzenml init\n```\n\n</details>\n\n## What you'll learn\n\nYou can either follow along in the [accompanying Jupyter notebook](https://github.com/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb), or just keep reading the [README file for more instructions](https://github.com/zenml-io/zenml/blob/main/examples/quickstart/README.md).\n\nEither way, at the end you would run three pipelines that are exemplary:\n\n* A feature engineering pipeline that loads data and prepares it for training.\n* A training pipeline that loads the preprocessed dataset and trains a model.\n* A batch inference pipeline that runs predictions on the trained model with new data.\n\nAnd voil\u00e0! You're now well on your way to be an MLOps expert. As a next step, try introducing the [ZenML starter template](https://github.com/zenml-io/template-starter) to your colleagues and see the benefits of a standard MLOps framework in action!\n\n## Conclusion and next steps\n\nThis marks the end of the first chapter of your MLOps journey with ZenML. Make sure you do your own experimentation with ZenML to master the basics. When ready, move on to the [production guide](../production-guide/), which is the next part of the series.\n\n<figure><img src=\"https://static.scarf.sh"}
{"input": "/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Understand and adjust how ZenML versions your data.\n---\n\n# Manage artifacts\n\nData sits at the heart of every machine learning workflow. Managing and versioning this data correctly is essential for reproducibility and traceability within your ML pipelines. ZenML takes a proactive approach to data versioning, ensuring that every artifact\u2014be it data, models, or evaluations\u2014is automatically tracked and versioned upon pipeline execution.\n\n![Walkthrough of ZenML Artifact Control Plane (Dashboard available only on ZenML Pro)](../../.gitbook/assets/dcp\\_walkthrough.gif)\n\nThis guide will delve into artifact versioning and management, showing you how to efficiently name, organize, and utilize your data with the ZenML framework.\n\n## Managing artifacts produced by ZenML pipelines\n\nArtifacts, the outputs of your steps and pipelines, are automatically versioned and stored in the artifact store. Configuring these artifacts is pivotal for transparent and efficient pipeline development.\n\n### Giving names to your artifacts\n\nAssigning custom names to your artifacts can greatly enhance their discoverability and manageability. As best practice, utilize the `Annotated` object within your steps to give precise, human-readable names to outputs:\n\n```python\nfrom typing_extensions import Annotated\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\nfrom zenml import pipeline, step\n\n# Using Annotated to name our dataset\n@step\ndef training_data_loader() -> Annotated[pd.DataFrame, \"iris_dataset\"]:\n    \"\"\"Load the iris dataset as pandas dataframe.\"\"\"\n    iris = load_iris(as_frame=True)\n    return iris.get(\"frame\")\n\n\n@pipeline\ndef feature_engineering_pipeline():\n    training_data_loader()\n\n\nif __name__ == \"__main__\":\n    feature_engineering_pipeline()\n```\n\n{% hint style=\"info\" %}\nUnspecified artifact outputs default to a naming pattern of `{pipeline_name}::{step_name}::output`. For visual exploration in the ZenML dashboard, it's best practice to give significant outputs clear custom names.\n{% endhint %}\n\nArtifacts named `iris_dataset` can then be found swiftly using various ZenML interfaces:\n\n{% tabs %}\n{% tab title=\"OSS (CLI)\" %}\nTo list artifacts: `zenml artifact list`\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe [ZenML Pro](https://zenml.io/pro) dashboard offers advanced visualization features for artifact exploration.\n\n<figure><img src=\"../../.gitbook/assets/dcp_artifacts_list.png\" alt=\"\"><figcaption><p>ZenML Artifact Control Plane.</p></figcaption></figure>\n\n{% hint style=\"info\" %}\nTo prevent visual clutter, make sure to assign names to your most important artifacts that you would like to explore visually.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n### Versioning artifacts manually\n\nZenML automatically versions all created artifacts using auto-incremented numbering. I.e., if you have defined a step creating an artifact named `iris_dataset` as"}
{"input": " shown above, the first execution of the step will create an artifact with this name and version \"1\", the second execution will create version \"2\", and so on.\n\nWhile ZenML handles artifact versioning automatically, you have the option to specify custom versions using the [`ArtifactConfig`](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-model/#zenml.model.artifact\\_config.DataArtifactConfig). This may come into play during critical runs like production releases.\n\n```python\nfrom zenml import step, ArtifactConfig\n\n@step\ndef training_data_loader() -> (\n    Annotated[\n        pd.DataFrame, \n        # Add `ArtifactConfig` to control more properties of your artifact\n        ArtifactConfig(\n            name=\"iris_dataset\", \n            version=\"raw_2023\"\n        ),\n    ]\n):\n    ...\n```\n\nThe next execution of this step will then create an artifact with the name `iris_dataset` and version `raw_2023`. This is primarily useful if you are making a particularly important pipeline run (such as a release) whose artifacts you want to distinguish at a glance later.\n\n{% hint style=\"warning\" %}\nSince custom versions cannot be duplicated, the above step can only be run once successfully. To avoid altering your code frequently, consider using a [YAML config](../production-guide/configure-pipeline.md) for artifact versioning.\n{% endhint %}\n\nAfter execution, `iris_dataset` and its version `raw_2023` can be seen using:\n\n{% tabs %}\n{% tab title=\"OSS (CLI)\" %}\nTo list versions: `zenml artifact version list`\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe Cloud dashboard visualizes version history for your review.\n\n<figure><img src=\"../../.gitbook/assets/dcp_artifacts_versions_list.png\" alt=\"\"><figcaption><p>ZenML Data Versions List.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\n### Add metadata and tags to artifacts\n\nIf you would like to extend your artifacts with extra metadata or tags you can do so by following the patterns demonstrated below:\n\n```python\nfrom zenml import step, get_step_context, ArtifactConfig\nfrom typing_extensions import Annotated\n\n\n# below we annotate output with `ArtifactConfig` giving it a name,\n# run_metadata and tags. As a result, the created artifact\n# `artifact_name` will get configured with metadata and tags\n@step\ndef annotation_approach() -> (\n    Annotated[\n        str,\n        ArtifactConfig(\n            name=\"artifact_name\",\n            run_metadata={\"metadata_key\": \"metadata_value\"},\n            tags=[\"tag_name\"],\n        ),\n    ]\n):\n    return \"string\"\n\n\n# below we annotate output using functional approach with\n# run_metadata and tags. As a result, the created artifact \n# `artifact_name` will get configured with metadata and tags\n@step\ndef annotation_approach() -> Annotated[str"}
{"input": ", \"artifact_name\"]:\n    step_context = get_step_context()\n    step_context.add_output_metadata(\n        output_name=\"artifact_name\", metadata={\"metadata_key\": \"metadata_value\"}\n    )\n    step_context.add_output_tags(output_name=\"artifact_name\", tags=[\"tag_name\"])\n    return \"string\"\n\n\n# below we combine both approaches, so the artifact will get\n# metadata and tags from both sources\n@step\ndef annotation_approach() -> (\n    Annotated[\n        str,\n        ArtifactConfig(\n            name=\"artifact_name\",\n            run_metadata={\"metadata_key\": \"metadata_value\"},\n            tags=[\"tag_name\"],\n        ),\n    ]\n):\n    step_context = get_step_context()\n    step_context.add_output_metadata(\n        output_name=\"artifact_name\", metadata={\"metadata_key2\": \"metadata_value2\"}\n    )\n    step_context.add_output_tags(output_name=\"artifact_name\", tags=[\"tag_name2\"])\n    return \"string\"\n```\n\n### Consuming external artifacts within a pipeline\n\nWhile most pipelines start with a step that produces an artifact, it is often the case to want to consume artifacts external from the pipeline. The `ExternalArtifact` class can be used to initialize an artifact within ZenML with any arbitrary data type.\n\nFor example, let's say we have a Snowflake query that produces a dataframe, or a CSV file that we need to read. External artifacts can be used for this, to pass values to steps that are neither JSON serializable nor produced by an upstream step:\n\n```python\nimport numpy as np\nfrom zenml import ExternalArtifact, pipeline, step\n\n@step\ndef print_data(data: np.ndarray):\n    print(data)\n\n@pipeline\ndef printing_pipeline():\n    # One can also pass data directly into the ExternalArtifact\n    # to create a new artifact on the fly\n    data = ExternalArtifact(value=np.array([0]))\n\n    print_data(data=data)\n\n\nif __name__ == \"__main__\":\n    printing_pipeline()\n```\n\nOptionally, you can configure the `ExternalArtifact` to use a custom [materializer](../../how-to/handle-data-artifacts/handle-custom-data-types.md) for your data or disable artifact metadata and visualizations. Check out the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-artifacts/#zenml.artifacts.external\\_artifact.ExternalArtifact) for all available options.\n\n{% hint style=\"info\" %}\nUsing an `ExternalArtifact` for your step automatically disables caching for the step.\n{% endhint %}\n\n### Consuming artifacts produced by other pipelines\n\nIt is also common to consume an artifact downstream after producing it in an upstream pipeline or step. As we have learned in the [previous section](../../how-to/build-pipelines/fetching-pipelines.md#fetching-artifacts-directly), the `Client` can be used to fetch artifacts directly inside the pipeline code:\n\n```python\nfrom uuid import UUID\nimport pandas as pd\nfrom zen"}
{"input": "ml import step, pipeline\nfrom zenml.client import Client\n\n\n@step\ndef trainer(dataset: pd.DataFrame):\n    ...\n\n@pipeline\ndef training_pipeline():\n    client = Client()\n    # Fetch by ID\n    dataset_artifact = client.get_artifact_version(\n        name_id_or_prefix=UUID(\"3a92ae32-a764-4420-98ba-07da8f742b76\")\n    )\n\n    # Fetch by name alone - uses the latest version of this artifact\n    dataset_artifact = client.get_artifact_version(name_id_or_prefix=\"iris_dataset\")\n\n    # Fetch by name and version\n    dataset_artifact = client.get_artifact_version(\n        name_id_or_prefix=\"iris_dataset\", version=\"raw_2023\"\n    )\n\n    # Pass into any step\n    trainer(dataset=dataset_artifact)\n\n\nif __name__ == \"__main__\":\n    training_pipeline()\n```\n\n{% hint style=\"info\" %}\nCalls of `Client` methods like `get_artifact_version` directly inside the pipeline code makes use of ZenML's [late materialization](../../how-to/handle-data-artifacts/load-artifacts-into-memory.md) behind the scenes.\n{% endhint %}\n\nIf you would like to bypass materialization entirely and just download the data or files associated with a particular artifact version, you can use the `.download_files` method:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nartifact = client.get_artifact_version(name_id_or_prefix=\"iris_dataset\")\nartifact.download_files(\"path/to/save.zip\")\n```\n\nTake note that the path must have the `.zip` extension, as the artifact data will be saved as a zip file. Make sure to handle any exceptions that may arise from this operation.\n\n## Managing artifacts **not** produced by ZenML pipelines\n\nSometimes, artifacts can be produced completely outside of ZenML. A good example of this is the predictions produced by a deployed model.\n\n```python\n# A model is deployed, running in a FastAPI container\n# Let's use the ZenML client to fetch the latest model and make predictions\n\nfrom zenml.client import Client\nfrom zenml import save_artifact\n\n# Fetch the model from a registry or a previous pipeline\nmodel = ...\n\n# Let's make a prediction\nprediction = model.predict([[1, 1, 1, 1]])\n\n# We now store this prediction in ZenML as an artifact\n# This will create a new artifact version\nsave_artifact(prediction, name=\"iris_predictions\")\n```\n\nYou can also load any artifact stored within ZenML using the `load_artifact` method:\n\n```python\n# Loads the latest version\nload_artifact(\"iris_predictions\")\n```\n\n{% hint style=\"info\" %}\n`load_artifact` is simply short-hand for the following Client call:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nclient.get_artifact(\"iris_predictions\").load()\n```\n"}
{"input": "{% endhint %}\n\nEven if an artifact is created externally, it can be treated like any other artifact produced by ZenML steps - with all the functionalities described above!\n\n{% hint style=\"info\" %}\nIt is also possible to use these functions inside your ZenML steps. However, it is usually cleaner to return the artifacts as outputs of your step to save them, or to use External Artifacts to load them instead.\n{% endhint %}\n\n## Logging metadata for an artifact\n\nOne of the most useful ways of interacting with artifacts in ZenML is the ability to associate metadata with them. [As mentioned before](../../how-to/build-pipelines/fetching-pipelines.md#artifact-information), artifact metadata is an arbitrary dictionary of key-value pairs that are useful for understanding the nature of the data.\n\nAs an example, one can associate the results of a model training alongside a model artifact, the shape of a table alongside a `pandas` dataframe, or the size of an image alongside a PNG file.\n\nFor some artifacts, ZenML automatically logs metadata. As an example, for `pandas.Series` and `pandas.DataFrame` objects, ZenML logs the shape and size of the objects:\n\n{% tabs %}\n{% tab title=\"Python\" %}\n```python\nfrom zenml.client import Client\n\n# Get an artifact version (e.g. pd.DataFrame)\nartifact = Client().get_artifact_version('50ce903f-faa6-41f6-a95f-ff8c0ec66010')\n\n# Fetch it's metadata\nartifact.run_metadata[\"storage_size\"].value  # Size in bytes\nartifact.run_metadata[\"shape\"].value  # Shape e.g. (500,20)\n```\n{% endtab %}\n\n{% tab title=\"OSS (Dashboard)\" %}\nThe information regarding the metadata of an artifact can be found within the DAG visualizer interface on the OSS dashboard:\n\n<figure><img src=\"../../.gitbook/assets/dashboard_artifact_metadata.png\" alt=\"\"><figcaption><p>ZenML Artifact Control Plane.</p></figcaption></figure>\n{% endtab %}\n\n{% tab title=\"Cloud (Dashboard)\" %}\nThe [ZenML Pro](https://zenml.io/pro) dashboard offers advanced visualization features for artifact exploration, including a dedicated artifacts tab with metadata visualization:\n\n<figure><img src=\"../../.gitbook/assets/dcp_metadata.png\" alt=\"\"><figcaption><p>ZenML Artifact Control Plane.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\nA user can also add metadata to an artifact within a step directly using the `log_artifact_metadata` method:\n\n```python\nfrom zenml import step, log_artifact_metadata\n\n@step\ndef model_finetuner_step(\n    model: ClassifierMixin, dataset: Tuple[np.ndarray, np.ndarray]\n) -> Annotated[\n    ClassifierMixin, ArtifactConfig(name=\"my_model\", tags=[\"SVC\", \"trained\"])\n]:\n    \"\"\"Finetunes a given model on"}
{"input": " a given dataset.\"\"\"\n    model.fit(dataset[0], dataset[1])\n    accuracy = model.score(dataset[0], dataset[1])\n\n    \n    log_artifact_metadata(\n        # Artifact name can be omitted if step returns only one output\n        artifact_name=\"my_model\",\n        # Passing None or omitting this will use the `latest` version\n        version=None,\n        # Metadata should be a dictionary of JSON-serializable values\n        metadata={\"accuracy\": float(accuracy)}\n        # A dictionary of dictionaries can also be passed to group metadata\n        #  in the dashboard\n        # metadata = {\"metrics\": {\"accuracy\": accuracy}}\n    )\n    return model\n```\n\nFor further depth, there is an [advanced metadata logging guide](../../how-to/track-metrics-metadata/README.md) that goes more into detail about logging metadata in ZenML.\n\nAdditionally, there is a lot more to learn about artifacts within ZenML. Please read the [dedicated data management guide](../../how-to/handle-data-artifacts/) for more information.\n\n## Code example\n\nThis section combines all the code from this section into one simple script that you can use easily:\n\n<details>\n\n<summary>Code Example of this Section</summary>\n\n```python\nfrom typing import Optional, Tuple\nfrom typing_extensions import Annotated\n\nimport numpy as np\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom zenml import ArtifactConfig, pipeline, step, log_artifact_metadata\nfrom zenml import save_artifact, load_artifact\nfrom zenml.client import Client\n\n\n@step\ndef versioned_data_loader_step() -> (\n    Annotated[\n        Tuple[np.ndarray, np.ndarray],\n        ArtifactConfig(\n            name=\"my_dataset\",\n            tags=[\"digits\", \"computer vision\", \"classification\"],\n        ),\n    ]\n):\n    \"\"\"Loads the digits dataset as a tuple of flattened numpy arrays.\"\"\"\n    digits = load_digits()\n    return (digits.images.reshape((len(digits.images), -1)), digits.target)\n\n\n@step\ndef model_finetuner_step(\n    model: ClassifierMixin, dataset: Tuple[np.ndarray, np.ndarray]\n) -> Annotated[\n    ClassifierMixin,\n    ArtifactConfig(name=\"my_model\", is_model_artifact=True, tags=[\"SVC\", \"trained\"]),\n]:\n    \"\"\"Finetunes a given model on a given dataset.\"\"\"\n    model.fit(dataset[0], dataset[1])\n    accuracy = model.score(dataset[0], dataset[1])\n    log_artifact_metadata(metadata={\"accuracy\": float(accuracy)})\n    return model\n\n\n@pipeline\ndef model_finetuning_pipeline(\n    dataset_version: Optional[str] = None,\n    model_version: Optional[str] = None,\n):\n    client = Client()\n    # Either load a previous version of \"my_dataset\" or create a new one\n    if dataset_version:\n        dataset = client.get_artifact_version(\n            name_id"}
{"input": "_or_prefix=\"my_dataset\", version=dataset_version\n        )\n    else:\n        dataset = versioned_data_loader_step()\n\n    # Load the model to finetune\n    # If no version is specified, the latest version of \"my_model\" is used\n    model = client.get_artifact_version(\n        name_id_or_prefix=\"my_model\", version=model_version\n    )\n\n    # Finetune the model\n    # This automatically creates a new version of \"my_model\"\n    model_finetuner_step(model=model, dataset=dataset)\n\n\ndef main():\n    # Save an untrained model as first version of \"my_model\"\n    untrained_model = SVC(gamma=0.001)\n    save_artifact(\n        untrained_model, name=\"my_model\", version=\"1\", tags=[\"SVC\", \"untrained\"]\n    )\n\n    # Create a first version of \"my_dataset\" and train the model on it\n    model_finetuning_pipeline()\n\n    # Finetune the latest model on an older version of the dataset\n    model_finetuning_pipeline(dataset_version=\"1\")\n\n    # Run inference with the latest model on an older version of the dataset\n    latest_trained_model = load_artifact(\"my_model\")\n    old_dataset = load_artifact(\"my_dataset\", version=\"1\")\n    latest_trained_model.predict(old_dataset[0])\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis would create the following pipeline run DAGs:\n\n**Run 1:**\n\n<img src=\"../../.gitbook/assets/artifact_management_1.png\" alt=\"Create a first version of my_dataset\" data-size=\"original\">\n\n**Run 2:**\n\n<img src=\"../../.gitbook/assets/artifact_management_2.png\" alt=\"Uses a second version of my_dataset\" data-size=\"original\">\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Transitioning to remote artifact storage.\n---\n\n# Connecting remote storage\n\nIn the previous chapters, we've been working with artifacts stored locally on our machines. This setup is fine for individual experiments, but as we move towards a collaborative and production-ready environment, we need a solution that is more robust, shareable, and scalable. Enter remote storage!\n\nRemote storage allows us to store our artifacts in the cloud, which means they're accessible from anywhere and by anyone with the right permissions. This is essential for team collaboration and for managing the larger datasets and models that come with production workloads.\n\nWhen using a stack with remote storage, nothing changes except the fact that the artifacts get materialized in a central and remote storage location. This diagram explains the flow:\n\n<figure><img src=\"../../.gitbook/assets/local_run_with_remote_artifact_store.png\" alt=\"\"><figcaption><p>Sequence of events that happen when running a pipeline on a remote artifact store.</p></figcaption></figure>\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already?\n\nCheck out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Terraform modules](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register a cloud stack.\n{% endhint %}\n\n## Provisioning and registering a remote artifact store\n\nOut of the box, ZenML ships with [many different supported artifact store flavors](../../component-guide/artifact-stores/artifact-stores.md). For convenience, here are some brief instructions on how to quickly get up and running on the major cloud providers:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\nYou will need to install and set up the AWS CLI on your machine as a prerequisite, as covered in [the AWS CLI documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), before you register the S3 Artifact Store.\n\nThe Amazon Web Services S3 Artifact Store flavor is provided by the [S3 ZenML integration](../../component-guide/artifact-stores/s3.md), you need to install it on your local machine to be able to register an S3 Artifact Store and add it to your stack:\n\n```shell\nzenml integration install s3 -y\n```\n\n{% hint style=\"info\" %}\nHaving trouble with this command? You can use `poetry` or `pip` to install the requirements of any ZenML integration directly. In order to obtain the exact requirements of the AWS S3 integration you can use `zenml integration requirements s3`.\n{% endhint %}\n\nThe only configuration parameter mandatory for registering an S3 Artifact Store is the root path URI, which needs to point to an S3 bucket and take the form"}
{"input": " `s3://bucket-name`. In order to create a S3 bucket, refer to the [AWS documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html).\n\nWith the URI to your S3 bucket known, registering an S3 Artifact Store can be done as follows:\n\n```shell\n# Register the S3 artifact-store\nzenml artifact-store register cloud_artifact_store -f s3 --path=s3://bucket-name\n```\n\nFor more information, read the [dedicated S3 artifact store flavor guide](../../component-guide/artifact-stores/s3.md).\n{% endtab %}\n\n{% tab title=\"GCP\" %}\nYou will need to install and set up the Google Cloud CLI on your machine as a prerequisite, as covered in [the Google Cloud documentation](https://cloud.google.com/sdk/docs/install-sdk) , before you register the GCS Artifact Store.\n\nThe Google Cloud Storage Artifact Store flavor is provided by the [GCP ZenML integration](../../component-guide/artifact-stores/gcp.md), you need to install it on your local machine to be able to register a GCS Artifact Store and add it to your stack:\n\n```shell\nzenml integration install gcp -y\n```\n\n{% hint style=\"info\" %}\nHaving trouble with this command? You can use `poetry` or `pip` to install the requirements of any ZenML integration directly. In order to obtain the exact requirements of the GCP integrations you can use `zenml integration requirements gcp`.\n{% endhint %}\n\nThe only configuration parameter mandatory for registering a GCS Artifact Store is the root path URI, which needs to point to a GCS bucket and take the form `gs://bucket-name`. Please read [the Google Cloud Storage documentation](https://cloud.google.com/storage/docs/creating-buckets) on how to provision a GCS bucket.\n\nWith the URI to your GCS bucket known, registering a GCS Artifact Store can be done as follows:\n\n```shell\n# Register the GCS artifact store\nzenml artifact-store register cloud_artifact_store -f gcp --path=gs://bucket-name\n```\n\nFor more information, read the [dedicated GCS artifact store flavor guide](../../component-guide/artifact-stores/gcp.md).\n{% endtab %}\n\n{% tab title=\"Azure\" %}\nYou will need to install and set up the Azure CLI on your machine as a prerequisite, as covered in [the Azure documentation](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli), before you register the Azure Artifact Store.\n\nThe Microsoft Azure Artifact Store flavor is provided by the [Azure ZenML integration](../../component-guide/artifact-stores/azure.md), you need to install it on your local machine to be able to register an Azure Artifact Store and add it to your stack:\n\n```shell\nzenml integration install azure -y\n```\n\n{% hint style=\""}
{"input": "info\" %}\nHaving trouble with this command? You can use `poetry` or `pip` to install the requirements of any ZenML integration directly. In order to obtain the exact requirements of the Azure integration you can use `zenml integration requirements azure`.\n{% endhint %}\n\nThe only configuration parameter mandatory for registering an Azure Artifact Store is the root path URI, which needs to point to an Azure Blog Storage container and take the form `az://container-name` or `abfs://container-name`. Please read [the Azure Blob Storage documentation](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal) on how to provision an Azure Blob Storage container.\n\nWith the URI to your Azure Blob Storage container known, registering an Azure Artifact Store can be done as follows:\n\n```shell\n# Register the Azure artifact store\nzenml artifact-store register cloud_artifact_store -f azure --path=az://container-name\n```\n\nFor more information, read the [dedicated Azure artifact store flavor guide](../../component-guide/artifact-stores/azure.md).\n{% endtab %}\n\n{% tab title=\"Other\" %}\nYou can create a remote artifact store in pretty much any environment, including other cloud providers using a cloud-agnostic artifact storage such as [Minio](../../component-guide/artifact-stores/artifact-stores.md).\n\nIt is also relatively simple to create a [custom stack component flavor](../../how-to/stack-deployment/implement-a-custom-stack-component.md) for your use case.\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nHaving trouble with setting up infrastructure? Join the [ZenML community](https://zenml.io/slack) and ask for help!\n{% endhint %}\n\n## Configuring permissions with your first service connector\n\nWhile you can go ahead and [run your pipeline on your stack](remote-storage.md#running-a-pipeline-on-a-cloud-stack) if your local client is configured to access it, it is best practice to use a [service connector](../../how-to/auth-management/) for this purpose. Service connectors are quite a complicated concept (We have a whole [docs section](../../how-to/auth-management/) on them) - but we're going to be starting with a very basic approach.\n\nFirst, let's understand what a service connector does. In simple words, a\nservice connector contains credentials that grant stack components access to\ncloud infrastructure. These credentials are stored in the form of a\n[secret](../../how-to/interact-with-secrets.md),\nand are available to the ZenML server to use. Using these credentials, the\nservice connector brokers a short-lived token and grants temporary permissions\nto the stack component to access that infrastructure. This diagram represents\nthis process:\n\n<figure><img src=\"../../.gitbook/assets/ConnectorsDiagram.png\" alt=\"\"><figcaption><p>Service Connectors abstract away complexity and implement security best practices</"}
{"input": "p></figcaption></figure>\n\n{% tabs %}\n{% tab title=\"AWS\" %}\nThere are [many ways to create an AWS service connector](../../how-to/auth-management/aws-service-connector.md#authentication-methods), but for the sake of this guide, we recommend creating one by [using the IAM method](../../how-to/auth-management/aws-service-connector.md#aws-iam-role).\n\n```shell\nAWS_PROFILE=<AWS_PROFILE> zenml service-connector register cloud_connector --type aws --auto-configure\n```\n{% endtab %}\n\n{% tab title=\"GCP\" %}\nThere are [many ways to create a GCP service connector](../../how-to/auth-management/gcp-service-connector.md#authentication-methods), but for the sake of this guide, we recommend creating one by [using the Service Account method](../../how-to/auth-management/gcp-service-connector.md#gcp-service-account).\n\n```shell\nzenml service-connector register cloud_connector --type gcp --auth-method service-account --service_account_json=@<PATH_TO_SERVICE_ACCOUNT_JSON> --project_id=<PROJECT_ID> --generate_temporary_tokens=False\n```\n{% endtab %}\n\n{% tab title=\"Azure\" %}\nThere are [many ways to create an Azure service connector](../../how-to/auth-management/azure-service-connector.md#authentication-methods), but for the sake of this guide, we recommend creating one by [using the Service Principal method](../../how-to/auth-management/azure-service-connector.md#azure-service-principal).\n\n```shell\nzenml service-connector register cloud_connector --type azure --auth-method service-principal --tenant_id=<TENANT_ID> --client_id=<CLIENT_ID> --client_secret=<CLIENT_SECRET>\n```\n{% endtab %}\n{% endtabs %}\n\nOnce we have our service connector, we can now attach it to stack components. In this case, we are going to connect it to our remote artifact store:\n\n```shell\nzenml artifact-store connect cloud_artifact_store --connector cloud_connector\n```\n\nNow, every time you (or anyone else with access) uses the `cloud_artifact_store`, they will be granted a temporary token that will grant them access to the remote storage. Therefore, your colleagues don't need to worry about setting up credentials and installing clients locally!\n\n## Running a pipeline on a cloud stack\n\nNow that we have our remote artifact store registered, we can [register a new stack](understand-stacks.md#registering-a-stack) with it, just like we did in the previous chapter:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\nzenml stack register local_with_remote_storage -o default -a cloud_artifact_store\n```\n{% endtab %}\n\n{% tab title=\"Dashboard\" %}\n<figure><img src=\"../../.gitbook/assets/CreateStack.png\" alt=\"\"><figcaption><p>Register a new stack.</p></figcaption></figure>\n{% endtab %}\n{% end"}
{"input": "tabs %}\n\nNow, using the [code from the previous chapter](understand-stacks.md#run-a-pipeline-on-the-new-local-stack), we run a training pipeline:\n\nSet our `local_with_remote_storage` stack active:\n\n```shell\nzenml stack set local_with_remote_storage\n```\n\nLet us continue with the example from the previous page and run the training pipeline:\n\n```shell\npython run.py --training-pipeline\n```\n\nWhen you run that pipeline, ZenML will automatically store the artifacts in the specified remote storage, ensuring that they are preserved and accessible for future runs and by your team members. You can ask your colleagues to connect to the same [ZenML server](deploying-zenml.md), and you will notice that if they run the same pipeline, the pipeline would be partially cached, **even if they have not run the pipeline themselves before**.\n\nYou can list your artifact versions as follows:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\n# This will give you the artifacts from the last 15 minutes\nzenml artifact version list --created=\"gte:$(date -v-15M '+%Y-%m-%d %H:%M:%S')\"\n```\n{% endtab %}\n\n{% tab title=\"Cloud Dashboard\" %}\n[ZenML Pro](https://zenml.io/pro) features an [Artifact Control Plane](../starter-guide/manage-artifacts.md) to visualize artifact versions:\n\n<figure><img src=\"../../.gitbook/assets/dcp_artifacts_versions_list.png\" alt=\"\"><figcaption><p>See artifact versions in the cloud.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\nYou will notice above that some artifacts are stored locally, while others are stored in a remote storage location.\n\nBy connecting remote storage, you're taking a significant step towards building a collaborative and scalable MLOps workflow. Your artifacts are no longer tied to a single machine but are now part of a cloud-based ecosystem, ready to be shared and built upon.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrate using cloud resources.\n---\n\n# Orchestrate on the cloud\n\nUntil now, we've only run pipelines locally. The next step is to get free from our local machines and transition our pipelines to execute on the cloud. This will enable you to run your MLOps pipelines in a cloud environment, leveraging the scalability and robustness that cloud platforms offer.\n\nIn order to do this, we need to get familiar with two more stack components:\n\n* The [orchestrator](../../component-guide/orchestrators/orchestrators.md) manages the workflow and execution of your pipelines.\n* The [container registry](../../component-guide/container-registries/container-registries.md) is a storage and content delivery system that holds your Docker container images.\n\nThese, along with [remote storage](remote-storage.md), complete a basic cloud stack where our pipeline is entirely running on the cloud.\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already?\n\nCheck out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Terraform modules](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register a cloud stack.\n{% endhint %}\n\n## Starting with a basic cloud stack\n\nThe easiest cloud orchestrator to start with is the [Skypilot](https://skypilot.readthedocs.io/) orchestrator running on a public cloud. The advantage of Skypilot is that it simply provisions a VM to execute the pipeline on your cloud provider.\n\nCoupled with Skypilot, we need a mechanism to package your code and ship it to the cloud for Skypilot to do its thing. ZenML uses [Docker](https://www.docker.com/) to achieve this. Every time you run a pipeline with a remote orchestrator, [ZenML builds an image](../../how-to/setting-up-a-project-repository/connect-your-git-repository.md) for the entire pipeline (and optionally each step of a pipeline depending on your [configuration](../../how-to/customize-docker-builds/README.md)). This image contains the code, requirements, and everything else needed to run the steps of the pipeline in any environment. ZenML then pushes this image to the container registry configured in your stack, and the orchestrator pulls the image when it's ready to execute a step.\n\nTo summarize, here is the broad sequence of events that happen when you run a pipeline with such a cloud stack:\n\n<figure><img src=\"../../.gitbook/assets/cloud_orchestration_run.png\" alt=\"\"><figcaption><p>Sequence of events that happen when running a pipeline on a full cloud stack.</p></figcaption></figure>\n\n1. The user runs a pipeline on the"}
{"input": " client machine. This executes the `run.py` script where ZenML reads the `@pipeline` function and understands what steps need to be executed.\n2. The client asks the server for the stack info, which returns it with the configuration of the cloud stack.\n3. Based on the stack info and pipeline specification, the client builds and pushes an image to the `container registry`. The image contains the environment needed to execute the pipeline and the code of the steps.\n4. The client creates a run in the `orchestrator`. For example, in the case of the [Skypilot](https://skypilot.readthedocs.io/) orchestrator, it creates a virtual machine in the cloud with some commands to pull and run a Docker image from the specified container registry.\n5. The `orchestrator` pulls the appropriate image from the `container registry` as it's executing the pipeline (each step has an image).\n6. As each pipeline runs, it stores artifacts physically in the `artifact store`. Of course, this artifact store needs to be some form of cloud storage.\n7. As each pipeline runs, it reports status back to the ZenML server and optionally queries the server for metadata.\n\n## Provisioning and registering an orchestrator alongside a container registry\n\nWhile there are detailed docs on [how to set up a Skypilot orchestrator](../../component-guide/orchestrators/skypilot-vm.md) and a [container registry](../../component-guide/container-registries/container-registries.md) on each public cloud, we have put the most relevant details here for convenience:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\nIn order to launch a pipeline on AWS with the SkyPilot orchestrator, the first thing that you need to do is to install the AWS and Skypilot integrations:\n\n```shell\nzenml integration install aws skypilot_aws -y\n```\n\nBefore we start registering any components, there is another step that we have to execute. As we [explained in the previous section](remote-storage.md#configuring-permissions-with-your-first-service-connector), components such as orchestrators and container registries often require you to set up the right permissions. In ZenML, this process is simplified with the use of [Service Connectors](../../how-to/auth-management/README.md). For this example, we need to use the [IAM role authentication method of our AWS service connector](../../how-to/auth-management/aws-service-connector.md#aws-iam-role):\n\n```shell\nAWS_PROFILE=<AWS_PROFILE> zenml service-connector register cloud_connector --type aws --auto-configure\n```\n\nOnce the service connector is set up, we can register [a Skypilot orchestrator](../../component-guide/orchestrators/skypilot-vm.md):\n\n```shell\nzenml orchestrator register cloud_orchestrator -f vm_aws\nzenml orchestrator connect cloud_orch"}
{"input": "estrator --connector cloud_connector\n```\n\nThe next step is to register [an AWS container registry](../../component-guide/container-registries/aws.md). Similar to the orchestrator, we will use our connector as we are setting up the container registry:\n\n```shell\nzenml container-registry register cloud_container_registry -f aws --uri=<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com\nzenml container-registry connect cloud_container_registry --connector cloud_connector\n```\n\nWith the components registered, everything is set up for the next steps.\n\nFor more information, you can always check the [dedicated Skypilot orchestrator guide](../../component-guide/orchestrators/skypilot-vm.md).\n{% endtab %}\n\n{% tab title=\"GCP\" %}\nIn order to launch a pipeline on GCP with the SkyPilot orchestrator, the first thing that you need to do is to install the GCP and Skypilot integrations:\n\n```shell\nzenml integration install gcp skypilot_gcp -y\n```\n\nBefore we start registering any components, there is another step that we have to execute. As we [explained in the previous section](remote-storage.md#configuring-permissions-with-your-first-service-connector), components such as orchestrators and container registries often require you to set up the right permissions. In ZenML, this process is simplified with the use of [Service Connectors](../../how-to/auth-management/README.md). For this example, we need to use the [Service Account authentication feature of our GCP service connector](../../how-to/auth-management/gcp-service-connector.md#gcp-service-account):\n\n```shell\nzenml service-connector register cloud_connector --type gcp --auth-method service-account --service_account_json=@<PATH_TO_SERVICE_ACCOUNT_JSON> --project_id=<PROJECT_ID> --generate_temporary_tokens=False\n```\n\nOnce the service connector is set up, we can register [a Skypilot orchestrator](../../component-guide/orchestrators/skypilot-vm.md):\n\n```shell\nzenml orchestrator register cloud_orchestrator -f vm_gcp \nzenml orchestrator connect cloud_orchestrator --connect cloud_connector\n```\n\nThe next step is to register [a GCP container registry](../../component-guide/container-registries/gcp.md). Similar to the orchestrator, we will use our connector as we are setting up the container registry:\n\n```shell\nzenml container-registry register cloud_container_registry -f gcp --uri=gcr.io/<PROJECT_ID>\nzenml container-registry connect cloud_container_registry --connector cloud_connector\n```\n\nWith the components registered, everything is set up for the next steps.\n\nFor more information, you can always check the [dedicated Skypilot orchestrator guide](../../component-guide/orchestrators/skypilot-vm.md).\n{% endtab %}\n\n{% tab title=\""}
{"input": "Azure\" %}\nAs of [v0.60.0](https://github.com/zenml-io/zenml/releases/tag/0.60.0), alongside the switch to `pydantic` v2, due to an incompatibility between the new version `pydantic` and the `azurecli`, the `skypilot[azure]` flavor can not be installed at the same time. Therefore, for Azure users, an alternative is to use the [Kubernetes Orchestrator](../../component-guide/orchestrators/kubernetes.md). You can easily deploy a Kubernetes cluster in your subscription using the [Azure Kubernetes Service](https://azure.microsoft.com/en-us/products/kubernetes-service).\n\nIn order to launch a pipeline on Azure with the Kubernetes orchestrator, the first thing that you need to do is to install the Azure and Kubernetes integrations:\n\n```shell\nzenml integration install azure kubernetes -y\n```\n\nYou should also ensure you have [kubectl installed](https://kubernetes.io/docs/tasks/tools/).\n\nBefore we start registering any components, there is another step that we have to execute. As we [explained in the previous section](remote-storage.md#configuring-permissions-with-your-first-service-connector), components such as orchestrators and container registries often require you to set up the right permissions. In ZenML, this process is simplified with the use of [Service Connectors](../../how-to/auth-management/README.md). For this example, we will need to use the [Service Principal authentication feature of our Azure service connector](../../how-to/auth-management/azure-service-connector.md#azure-service-principal):\n\n```shell\nzenml service-connector register cloud_connector --type azure --auth-method service-principal --tenant_id=<TENANT_ID> --client_id=<CLIENT_ID> --client_secret=<CLIENT_SECRET>\n```\n\nOnce the service connector is set up, we can register [a Kubernetes orchestrator](../../component-guide/orchestrators/kubernetes.md):\n\n```shell\n# Ensure your service connector has access to the AKS cluster:\nzenml service-connector list-resources --resource-type kubernetes-cluster -e\nzenml orchestrator register cloud_orchestrator --flavor kubernetes\nzenml orchestrator connect cloud_orchestrator --connect cloud_connector\n```\n\nThe next step is to register [an Azure container registry](../../component-guide/container-registries/azure.md). Similar to the orchestrator, we will use our connector as we are setting up the container registry.\n\n```shell\nzenml container-registry register cloud_container_registry -f azure --uri=<REGISTRY_NAME>.azurecr.io\nzenml container-registry connect cloud_container_registry --connector cloud_connector\n```\n\nWith the components registered, everything is set up for the next steps.\n\nFor more information, you can always check the [dedicated Kubernetes orchestrator guide](../../component-guide/orchestrators/kubernetes.md).\n{% endtab %}\n"}
{"input": "{% endtabs %}\n\n{% hint style=\"info\" %}\nHaving trouble with setting up infrastructure? Try reading the [stack deployment](../../how-to/stack-deployment/README.md) section of the docs to gain more insight. If that still doesn't work, join the [ZenML community](https://zenml.io/slack) and ask!\n{% endhint %}\n\n## Running a pipeline on a cloud stack\n\nNow that we have our orchestrator and container registry registered, we can [register a new stack](understand-stacks.md#registering-a-stack), just like we did in the previous chapter:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\nzenml stack register minimal_cloud_stack -o cloud_orchestrator -a cloud_artifact_store -c cloud_container_registry\n```\n{% endtab %}\n{% endtabs %}\n\nNow, using the [code from the previous chapter](understand-stacks.md#run-a-pipeline-on-the-new-local-stack), we can run a training pipeline. First, set the minimal cloud stack active:\n\n```shell\nzenml stack set minimal_cloud_stack\n```\n\nand then, run the training pipeline:\n\n```shell\npython run.py --training-pipeline\n```\n\nYou will notice this time your pipeline behaves differently. After it has built the Docker image with all your code, it will push that image, and run a VM on the cloud. Here is where your pipeline will execute, and the logs will be streamed back to you. So with a few commands, we were able to ship our entire code to the cloud!\n\nCurious to see what other stacks you can create? The [Component Guide](../../component-guide/README.md) has an exhaustive list of various artifact stores, container registries, and orchestrators that are integrated with ZenML. Try playing around with more stack components to see how easy it is to switch between MLOps stacks with ZenML.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>"}
{"input": "---\ndescription: >-\n  Connect a Git repository to ZenML to track code changes and collaborate on\n  MLOps projects.\n---\n\n# Configure a code repository\n\nThroughout the lifecycle of a MLOps pipeline, it can get quite tiresome to always wait for a Docker build every time after running a pipeline (even if the local Docker cache is used). However, there is a way to just have one pipeline build and keep reusing it until a change to the pipeline environment is made: by connecting a code repository.\n\nWith ZenML, connecting to a Git repository optimizes the Docker build processes. It also has the added bonus of being a better way of managing repository changes and enabling better code collaboration. Here is how the flow changes when running a pipeline:\n\n<figure><img src=\"../../.gitbook/assets/run_with_repository.png\" alt=\"\"><figcaption><p>Sequence of events that happen when running a pipeline on a remote stack with a code repository</p></figcaption></figure>\n\n1. You trigger a pipeline run on your local machine. ZenML parses the `@pipeline` function to determine the necessary steps.\n2. The local client requests stack information from the ZenML server, which responds with the cloud stack configuration.\n3. The local client detects that we're using a code repository and requests the information from the git repo.\n4. Instead of building a new Docker image, the client checks if an existing image can be reused based on the current Git commit hash and other environment metadata.\n5. The client initiates a run in the orchestrator, which sets up the execution environment in the cloud, such as a VM.\n6. The orchestrator downloads the code directly from the Git repository and uses the existing Docker image to run the pipeline steps.\n7. Pipeline steps execute, storing artifacts in the cloud-based artifact store.\n8. Throughout the execution, the pipeline run status and metadata are reported back to the ZenML server.\n\nBy connecting a Git repository, you avoid redundant builds and make your MLOps processes more efficient. Your team can work on the codebase simultaneously, with ZenML handling the version tracking and ensuring that the correct code version is always used for each run.\n\n## Creating a GitHub Repository\n\nWhile ZenML supports [many different flavors of git repositories](../../how-to/setting-up-a-project-repository/connect-your-git-repository.md), this guide will focus on [GitHub](https://github.com). To create a repository on GitHub:\n\n1. Sign in to [GitHub](https://github.com/).\n2. Click the \"+\" icon and select \"New repository.\"\n3. Name your repository, set its visibility, and add a README or .gitignore if needed.\n4. Click \"Create repository.\"\n\nWe can now push our local code (from the [previous chapters](understand-stacks.md#run-a-pipeline-on-the-new-local-stack)) to GitHub with these commands:\n\n```sh\n# Initialize a Git repository\ngit init\n\n#"}
{"input": " Add files to the repository\ngit add .\n\n# Commit the files\ngit commit -m \"Initial commit\"\n\n# Add the GitHub remote\ngit remote add origin https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git\n\n# Push to GitHub\ngit push -u origin master\n```\n\nReplace `YOUR_USERNAME` and `YOUR_REPOSITORY_NAME` with your GitHub information.\n\n## Linking to ZenML\n\nTo connect your GitHub repository to ZenML, you'll need a GitHub Personal Access Token (PAT).\n\n<details>\n\n<summary>How to get a PAT for GitHub</summary>\n\n1. Go to your GitHub account settings and click on [Developer settings](https://github.com/settings/tokens?type=beta).\n2. Select \"Personal access tokens\" and click on \"Generate new token\".\n3.  Give your token a name and a description.\n\n    ![](../../.gitbook/assets/github-fine-grained-token-name.png)\n4.  We recommend selecting the specific repository and then giving `contents` read-only access.\n\n    ![](../../.gitbook/assets/github-token-set-permissions.png)\n\n    ![](../../.gitbook/assets/github-token-permissions-overview.png)\n5.  Click on \"Generate token\" and copy the token to a safe place.\n\n    ![](../../.gitbook/assets/copy-github-fine-grained-token.png)\n\n</details>\n\nNow, we can install the GitHub integration and register your repository:\n\n```sh\nzenml integration install github\nzenml code-repository register <REPO_NAME> --type=github \\\n--url=https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git \\\n--owner=YOUR_USERNAME --repository=YOUR_REPOSITORY_NAME \\\n--token=YOUR_GITHUB_PERSONAL_ACCESS_TOKEN\n```\n\nFill in `<REPO_NAME>`, `YOUR_USERNAME`, `YOUR_REPOSITORY_NAME`, and `YOUR_GITHUB_PERSONAL_ACCESS_TOKEN` with your details.\n\nYour code is now connected to your ZenML server. ZenML will automatically detect if your source files are being tracked by GitHub and store the commit hash for each subsequent pipeline run.\n\nYou can try this out by running our training pipeline again:\n\n```python\n# This will build the Docker image the first time\npython run.py --training-pipeline\n\n# This will skip Docker building\npython run.py --training-pipeline\n```\n\nYou can read more about [the ZenML Git Integration here](../../how-to/setting-up-a-project-repository/connect-your-git-repository.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Level up your skills in a production setting.\n---\n\n# \ud83d\udc14 Production guide\n\nThe ZenML production guide builds upon the [Starter guide](../starter-guide/README.md) and is the next step in the MLOps Engineer journey with ZenML. If you're an ML practitioner hoping to implement a proof of concept within your workplace to showcase the importance of MLOps, this is the place for you.\n\n<figure><img src=\"../../.gitbook/assets/stack_showcase.png\" alt=\"\"><figcaption><p>ZenML simplifies development of MLOps pipelines that can span multiple production stacks.</p></figcaption></figure>\n\nThis guide will focus on shifting gears from running pipelines _locally_ on your machine, to running them in _production_ in the cloud. We'll cover:\n\n* [Deploying ZenML](deploying-zenml.md)\n* [Understanding stacks](understand-stacks.md)\n* [Connecting remote storage](remote-storage.md)\n* [Orchestrating on the cloud](cloud-orchestration.md)\n* [Easy stack registration](easy-stack-registration.md)\n* [Configuring the pipeline to scale compute](configure-pipeline.md)\n* [Configure a code repository](connect-code-repository.md)\n\nLike in the starter guide, make sure you have a Python environment ready and `virtualenv` installed to follow along with ease. As now we are dealing with cloud infrastructure, you'll also want to select one of the major cloud providers (AWS, GCP, Azure), and make sure the respective CLIs are installed and authorized.\n\nBy the end, you will have completed an [end-to-end](end-to-end.md) MLOps project that you can use as inspiration for your own work. Let's get right into it!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying ZenML is the first step to production.\n---\n\n# Deploying ZenML\n\nWhen you first get started with ZenML, it is based on the following architecture on your machine:\n\n![Scenario 1: ZenML default local configuration](../../.gitbook/assets/Scenario1.png)\n\nThe SQLite database that you can see in this diagram is used to store all the metadata we produced in the previous guide (pipelines, models, artifacts, etc).\n\nIn order to move into production, you will need to deploy this server somewhere centrally outside of your machine. This allows different infrastructure components to interact with, alongside enabling you to collaborate with your team members:\n\n![Scenario 3: Deployed ZenML Server](../../.gitbook/assets/Scenario3.2.png)\n\n## Choosing how to deploy ZenML\n\nWhile there are many options on how to [deploy ZenML](../../getting-started/deploying-zenml/README.md), the two simplest ones are:\n\n### Option 1: Sign up for a free ZenML Pro Trial\n\n[ZenML Pro](https://zenml.io/pro) comes as a managed SaaS solution that offers a one-click deployment for your ZenML server. Click [here](https://cloud.zenml.io/?utm\\_source=docs\\&utm\\_medium=referral\\_link\\&utm\\_campaign=cloud\\_promotion\\&utm\\_content=signup\\_link) to start a free trial.\n\nOn top of the one-click SaaS experience, ZenML Pro also comes built-in with additional features and a new dashboard that might be beneficial to follow for this guide. You can always go back to self-hosting after your learning journey is complete.\n\n### Option 2: Self-host ZenML on your cloud provider\n\nAs ZenML is open source, it is easy to [self-host it](../../getting-started/deploying-zenml/README.md). There is even a [ZenML CLI](../../getting-started/deploying-zenml/deploy-with-zenml-cli.md) one-liner that deploys ZenML on a Kubernetes cluster, abstracting away all the infrastructure complexity. If you don't have an existing Kubernetes cluster, you can create it manually using the documentation for your cloud provider. For convenience, here are links for [AWS](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html), [Azure](https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli), and [GCP](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster#before\\_you\\_begin).\n\n{% hint style=\"warning\" %}\nOnce you have created your cluster, make sure that you configure your [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) client to connect to it.\n{% endhint %}\n\nYou're now ready to deploy ZenML!"}
{"input": " Run the following command:\n\n```bash\nzenml deploy\n```\n\nYou will be prompted to provide a name for your deployment and details like what cloud provider you want to deploy to, in addition to the username, password, and email you want to set for the default user \u2014 and that's it! It creates the database and any VPCs, permissions, and more that are needed.\n\n{% hint style=\"info\" %}\nIn order to be able to run the `deploy` command, you should have your cloud provider's CLI configured locally with permissions to create resources like MySQL databases and networks.\n{% endhint %}\n\nTo learn more about different options for [deploying ZenML, visit the deployment documentation](../../getting-started/deploying-zenml/README.md).\n\n## Connecting to a deployed ZenML\n\nYou can connect your local ZenML client with the ZenML Server using the ZenML CLI and the web-based login. This can be executed with the command:\n\n```bash\nzenml connect --url <SERVER_URL>\n```\n\nwhere SERVER\\_URL is the host address of your ZenML deployment (e.g. `https://mydeployment.zenml.com`)\n\n{% hint style=\"info\" %}\nHaving trouble connecting with a browser? There are other ways to connect. Read [here](../../how-to/connecting-to-zenml/README.md) for more details.\n{% endhint %}\n\nThis command will start a series of steps to validate the device from where you are connecting that will happen in your browser. After that, you're now locally connected to a remote ZenML. Nothing of your experience changes, except that all metadata that you produce will be tracked centrally in one place from now on.\n\n{% hint style=\"info\" %}\nYou can always go back to the local zenml experience by using `zenml disconnect`\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Level up your skills in a production setting.\n---\n\n# \ud83d\udc14 Production guide\n\nThe ZenML production guide builds upon the [Starter guide](../starter-guide/README.md) and is the next step in the MLOps Engineer journey with ZenML. If you're an ML practitioner hoping to implement a proof of concept within your workplace to showcase the importance of MLOps, this is the place for you.\n\n<figure><img src=\"../../.gitbook/assets/stack_showcase.png\" alt=\"\"><figcaption><p>ZenML simplifies development of MLOps pipelines that can span multiple production stacks.</p></figcaption></figure>\n\nThis guide will focus on shifting gears from running pipelines _locally_ on your machine, to running them in _production_ in the cloud. We'll cover:\n\n* [Deploying ZenML](deploying-zenml.md)\n* [Understanding stacks](understand-stacks.md)\n* [Connecting remote storage](remote-storage.md)\n* [Orchestrating on the cloud](cloud-orchestration.md)\n* [Configuring the pipeline to scale compute](configure-pipeline.md)\n* [Configure a code repository](connect-code-repository.md)\n\nLike in the starter guide, make sure you have a Python environment ready and `virtualenv` installed to follow along with ease. As now we are dealing with cloud infrastructure, you'll also want to select one of the major cloud providers (AWS, GCP, Azure), and make sure the respective CLIs are installed and authorized.\n\nBy the end, you will have completed an [end-to-end](end-to-end.md) MLOps project that you can use as inspiration for your own work. Let's get right into it!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Put your new knowledge in action with an end-to-end project\n---\n\n# An end-to-end project\n\nThat was awesome! We learned so many advanced MLOps production concepts:\n\n* The value of [deploying ZenML](deploying-zenml.md)\n* Abstracting infrastructure configuration into [stacks](understand-stacks.md)\n* [Connecting remote storage](remote-storage.md)\n* [Orchestrating on the cloud](cloud-orchestration.md)\n* [Configuring the pipeline to scale compute](configure-pipeline.md)\n* [Connecting a git repository](connect-code-repository.md)\n\nWe will now combine all of these concepts into an end-to-end MLOps project powered by ZenML.\n\n## Get started\n\nStart with a fresh virtual environment with no dependencies. Then let's install our dependencies:\n\n```bash\npip install \"zenml[templates,server]\" notebook\nzenml integration install sklearn -y\n```\n\nWe will then use [ZenML templates](../../how-to/setting-up-a-project-repository/using-project-templates.md) to help us get the code we need for the project:\n\n```bash\nmkdir zenml_batch_e2e\ncd zenml_batch_e2e\nzenml init --template e2e_batch --template-with-defaults\n\n# Just in case, we install the requirements again\npip install -r requirements.txt\n```\n\n<details>\n\n<summary>Above doesn't work? Here is an alternative</summary>\n\nThe e2e template is also available as a [ZenML example](https://github.com/zenml-io/zenml/tree/main/examples/e2e). You can clone it:\n\n```bash\ngit clone --depth 1 git@github.com:zenml-io/zenml.git\ncd zenml/examples/e2e\npip install -r requirements.txt\nzenml init\n```\n\n</details>\n\n## What you'll learn\n\nThe e2e project is a comprehensive project template to cover major use cases of ZenML: a collection of steps and pipelines and, to top it all off, a simple but useful CLI. It showcases the core ZenML concepts for supervised ML with batch predictions. It builds on top of the [starter project](../starter-guide/starter-project.md) with more advanced concepts.\n\nAs you progress through the e2e batch template, try running the pipelines on a [remote cloud stack](cloud-orchestration.md) on a tracked [git repository](connect-code-repository.md) to practice some of the concepts we have learned in this guide.\n\nAt the end, don't forget to share the [ZenML e2e template](https://github.com/zenml-io/template-e2e-batch) with your colleagues and see how they react!\n\n## Conclusion and next steps\n\nThe production guide has now hopefully landed you with an end-to-end MLOps project, powered by a ZenML server connected to"}
{"input": " your cloud infrastructure. You are now ready to dive deep into writing your own pipelines and stacks. If you are looking to learn more advanced concepts, the [how-to section](../../how-to/build-pipelines/README.md) is for you. Until then, we wish you the best of luck chasing your MLOps dreams!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Add more resources to your pipeline configuration.\n---\n\n# Configure your pipeline to add compute\n\nNow that we have our pipeline up and running in the cloud, you might be wondering how ZenML figured out what sort of dependencies to install in the Docker image that we just ran on the VM. The answer lies in the [runner script we executed (i.e. run.py)](https://github.com/zenml-io/zenml/blob/main/examples/quickstart/run.py#L215), in particular, these lines:\n\n```python\npipeline_args[\"config_path\"] = os.path.join(\n    config_folder, \"training_rf.yaml\"\n)\n# Configure the pipeline\ntraining_pipeline_configured = training_pipeline.with_options(**pipeline_args)\n# Create a run\ntraining_pipeline_configured()\n```\n\nThe above commands [configure our training pipeline](../starter-guide/create-an-ml-pipeline.md#configure-with-a-yaml-file) with a YAML configuration called `training_rf.yaml` (found [here in the source code](https://github.com/zenml-io/zenml/blob/main/examples/quickstart/configs/training\\_rf.yaml)). Let's learn more about this configuration file.\n\n{% hint style=\"info\" %}\nThe `with_options` command that points to a YAML config is only one way to configure a pipeline. We can also directly configure a pipeline or a step in the decorator:\n\n```python\n@pipeline(settings=...)\n```\n\nHowever, it is best to not mix configuration from code to ensure separation of concerns in our codebase.\n{% endhint %}\n\n## Breaking down our configuration YAML\n\nThe YAML configuration of a ZenML pipeline can be very simple, as in this case. Let's break it down and go through each section one by one:\n\n### The Docker settings\n\n```yaml\nsettings:\n  docker:\n    required_integrations:\n      - sklearn\n    requirements:\n      - pyarrow\n```\n\nThe first section is the so-called `settings` of the pipeline. This section has a `docker` key, which controls the [containerization process](cloud-orchestration.md#orchestrating-pipelines-on-the-cloud). Here, we are simply telling ZenML that we need `pyarrow` as a pip requirement, and we want to enable the `sklearn` integration of ZenML, which will in turn install the `scikit-learn` library. This Docker section can be populated with many different options, and correspond to the [DockerSettings](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-config/#zenml.config.docker\\_settings.DockerSettings) class in the Python SDK.\n\n### Associating a ZenML Model\n\nThe next section is about associating a [ZenML Model](../starter-guide/track-ml-models.md) with this pipeline.\n\n```yaml\n# Configuration of the Model Control Plane\nmodel:\n  name: breast_cancer_classifier\n  version:"}
{"input": " rf\n  license: Apache 2.0\n  description: A breast cancer classifier\n  tags: [\"breast_cancer\", \"classifier\"]\n```\n\nYou will see that this configuration lines up with the model created after executing these pipelines:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\n# List all versions of the breast_cancer_classifier\nzenml model version list breast_cancer_classifier\n```\n{% endtab %}\n\n{% tab title=\"Dashboard\" %}\n[ZenML Pro](https://www.zenml.io/pro) ships with a Model Control Plane dashboard where you can visualize all the versions:\n\n<figure><img src=\"../../.gitbook/assets/mcp_model_versions_list.png\" alt=\"\"><figcaption><p>All model versions listed</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\n### Passing parameters\n\nThe last part of the config YAML is the `parameters` key:\n\n```yaml\n# Configure the pipeline\nparameters:\n  model_type: \"rf\"  # Choose between rf/sgd\n```\n\nThis parameters key aligns with the parameters that the pipeline expects. In this case, the pipeline expects a string called `model_type` that will inform it which type of model to use:\n\n```python\n@pipeline\ndef training_pipeline(model_type: str):\n    ...\n```\n\nSo you can see that the YAML config is fairly easy to use and is an important part of the codebase to control the execution of our pipeline. You can read more about how to configure a pipeline in the [how to section](../../how-to/use-configuration-files/what-can-be-configured.md), but for now, we can move on to scaling our pipeline.\n\n## Scaling compute on the cloud\n\nWhen we ran our pipeline with the above config, ZenML used some sane defaults to pick the resource requirements for that pipeline. However, in the real world, you might want to add more memory, CPU, or even a GPU depending on the pipeline at hand.\n\nThis is as easy as adding the following section to your local `training_rf.yaml` file:\n\n```yaml\n# These are the resources for the entire pipeline, i.e., each step\nsettings:    \n  ...\n\n  # Adapt this to vm_gcp accordingly\n  orchestrator.vm_aws:\n    memory: 32 # in GB\n        \n...    \nsteps:\n  model_trainer:\n    settings:\n      orchestrator.vm_aws:\n        cpus: 8\n```\n\nHere we are configuring the entire pipeline with a certain amount of memory, while for the trainer step we are additionally configuring 8 CPU cores. The `orchestrator.vm_aws` key corresponds to the [`SkypilotBaseOrchestratorSettings`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-skypilot/#zenml.integrations.skypilot.flavors.skypilot\\_orchestrator\\_base\\_vm\\_config"}
{"input": ".SkypilotBaseOrchestratorSettings) class in the Python SDK. You can adapt it to `vm_gcp` if you are using the GCP variant of skypilot.\n\n<details>\n\n<summary>Instructions for Microsoft Azure Users</summary>\n\nAs discussed [before](cloud-orchestration.md), we are using the [Kubernetes orchestrator](../../component-guide/orchestrators/kubernetes.md) for Azure users. In order to scale compute for the Kubernetes orchestrator, the\nYAML file needs to look like this:\n\n```yaml\n# These are the resources for the entire pipeline, i.e., each step\nsettings:    \n  ...\n\n  resources:\n    memory: \"32GB\"\n        \n...    \nsteps:\n  model_trainer:\n    settings:\n      resources:\n        memory: \"8GB\"\n```\n\n</details>\n\n{% hint style=\"info\" %}\nRead more about settings in ZenML [here](../../how-to/use-configuration-files/runtime-configuration.md) and\n[here](../../how-to/training-with-gpus/training-with-gpus.md)\n{% endhint %}\n\nNow let's run the pipeline again:\n\n```python\npython run.py --training-pipeline\n```\n\nNow you should notice the machine that gets provisioned on your cloud provider would have a different configuration as compared to last time. As easy as that!\n\nBear in mind that not every orchestrator supports `ResourceSettings` directly. To learn more, you can read about [`ResourceSettings` here](../../how-to/use-configuration-files/runtime-configuration.md), including the ability to [attach a GPU](../../how-to/training-with-gpus/training-with-gpus.md#1-specify-a-cuda-enabled-parent-image-in-your-dockersettings).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to switch the infrastructure backend of your code.\n---\n\n# Understanding stacks\n\nNow that we have ZenML deployed, we can take the next steps in making sure that our machine learning workflows are production-ready. As you were running [your first pipelines](../starter-guide/create-an-ml-pipeline.md), you might have already noticed the term `stack` in the logs and on the dashboard.\n\nA `stack` is the configuration of tools and infrastructure that your pipelines can run on. When you run ZenML code without configuring a stack, the pipeline will run on the so-called `default` stack.\n\n<figure><img src=\"../../.gitbook/assets/02_pipeline_local_stack.png\" alt=\"\"><figcaption><p>ZenML is the translation layer that allows your code to run on any of your stacks</p></figcaption></figure>\n\n### Separation of code from configuration and infrastructure\n\nAs visualized in the diagram above, there are two separate domains that are connected through ZenML. The left side shows the code domain. The user's Python code is translated into a ZenML pipeline. On the right side, you can see the infrastructure domain, in this case, an instance of the `default` stack. By separating these two domains, it is easy to switch the environment that the pipeline runs on without making any changes in the code. It also allows domain experts to write code/configure infrastructure without worrying about the other domain.\n\n### The `default` stack\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n`zenml stack describe` lets you find out details about your active stack:\n\n```bash\n...\n        Stack Configuration        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 COMPONENT_TYPE \u2502 COMPONENT_NAME \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ARTIFACT_STORE \u2502 default        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ORCHESTRATOR   \u2502 default        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n     'default' stack (ACTIVE)      \nStack 'default' with id '...' is owned by user default and is 'private'.\n...\n```\n\n`zenml stack list` lets you see all stacks that are registered in your zenml deployment.\n\n```bash\n...\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 STACK NAME \u2502 STACK ID  \u2502 SHARED \u2502 OWNER   \u2502 ARTIFACT_STORE \u2502 ORCHESTRATOR \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default    \u2502 ...       \u2502 \u2796     \u2502 default \u2502 default        \u2502 default      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n...\n```\n\n{% hint style=\"info\" %}\nAs you can see a stack can be **active** on your **client**. This simply means that any pipeline you run will be using the **active stack** as its environment.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n## Components of a stack\n\nAs you can see in the section above, a stack consists of multiple components. All stacks have at minimum an **orchestrator** and an **artifact store**.\n\n### Orchestrator\n\nThe **orchestrator** is responsible for executing the pipeline code. In the simplest case, this will be a simple Python thread on your machine. Let's explore this default orchestrator.\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n`zenml orchestrator list` lets you see all orchestrators that are registered in your zenml deployment.\n\n```bash\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME    \u2502 COMPONENT ID \u2502 FLAVOR \u2502 SHARED \u2502 OWNER   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default \u2502 ...          \u2502 local  \u2502 \u2796     \u2502 default \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endtab %}\n{% endtabs %}\n\n### Artifact store\n\nThe **artifact store** is responsible for persisting the step outputs. As we learned in the previous section, the step outputs are not passed along in memory, rather the outputs of each step are stored in the **artifact store** and then loaded from there when the next step needs them. By default this will also be on your own machine:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n`zenml artifact-store list` lets you see all artifact stores that are registered in your zenml deployment.\n\n```bash\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME    \u2502 COMPONENT ID \u2502 FL"}
{"input": "AVOR \u2502 SHARED \u2502 OWNER   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default \u2502 ...          \u2502 local  \u2502 \u2796     \u2502 default \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endtab %}\n{% endtabs %}\n\n### Other stack components\n\nThere are many more components that you can add to your stacks, like experiment trackers, model deployers, and more. You can see all supported stack component types in a single table view [here](../../component-guide/README.md)\n\nPerhaps the most important stack component after the orchestrator and the artifact store is the [container registry](../../component-guide/container-registries/container-registries.md). A container registry stores all your containerized images, which hold all your code and the environment needed to execute them. We will learn more about them in the next section!\n\n## Registering a stack\n\nJust to illustrate how to interact with stacks, let's create an alternate local stack. We start by first creating a local artifact store.\n\n### Create an artifact store\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nzenml artifact-store register my_artifact_store --flavor=local \n```\n\nLet's understand the individual parts of this command:\n\n* `artifact-store` : This describes the top-level group, to find other stack components simply run `zenml --help`\n* `register` : Here we want to register a new component, instead, we could also `update` , `delete` and more `zenml artifact-store --help` will give you all possibilities\n* `my_artifact_store` : This is the unique name that the stack component will have.\n* `--flavor=local`: A flavor is a possible implementation for a stack component. So in the case of an artifact store, this could be an s3-bucket or a local filesystem. You can find out all possibilities with `zenml artifact-store flavor --list`\n\nThis will be the output that you can expect from the command above.\n\n```bash\nUsing the default local database.\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered artifact_store `my_artifact_store`.bash\n```\n\nTo see the new artifact store that you just registered, just run:\n\n```bash\nzenml artifact-store describe my_artifact_store\n```\n{% endtab %}\n{% endtabs %}\n\n### Create a local stack\n\nWith the artifact store created, we can now create a new stack with this artifact store.\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nzenml stack register a_new_local_stack -o"}
{"input": " default -a my_artifact_store\n```\n\n* `stack` : This is the CLI group that enables interactions with the stacks\n* `register`: Here we want to register a new stack. Explore other operations with`zenml stack --help`.\n* `a_new_local_stack` : This is the unique name that the stack will have.\n* `--orchestrator` or `-o` are used to specify which orchestrator to use for the stack\n* `--artifact-store` or `-a` are used to specify which artifact store to use for the stack\n\nThe output for the command should look something like this:\n\n```bash\nUsing the default local database.\nRunning with active workspace: 'default' (repository)\nStack 'a_new_local_stack' successfully registered!\n```\n\nYou can inspect the stack with the following command:\n\n```bash\n zenml stack describe a_new_local_stack\n```\n\nWhich will give you an output like this:\n\n```bash\n         Stack Configuration          \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 COMPONENT_TYPE \u2502 COMPONENT_NAME    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ORCHESTRATOR   \u2502 default           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ARTIFACT_STORE \u2502 my_artifact_store \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n           'a_new_local_stack' stack           \nStack 'a_new_local_stack' with id '...' is owned by user default and is 'private'.\n```\n{% endtab %}\n{% endtabs %}\n\n### Switch stacks with our VS Code extension\n\n![GIF of our VS code extension, showing some of the uses of the sidebar](../../.gitbook/assets/zenml-extension-shortened.gif)\n\nIf you are using [our VS Code\nextension](https://marketplace.visualstudio.com/items?itemName=ZenML.zenml-vscode),\nyou can easily view and switch your stacks by opening the sidebar (click on the\nZenML icon). You can then click on the stack you want to switch to as well as\nview the stack components it's made up of.\n\n### Run a pipeline on the new local stack\n\nLet's use the pipeline in our starter project from the [previous guide](../starter-guide/starter-project.md) to see it in action.\n\nIf you have not already, clone the starter template:\n\n```bash\npip install \"zenml[templates,server]\" notebook\nzenml integration install sklearn -y\nmkdir zenml_starter\ncd zenml_starter\nzenml init --template starter --template-with-defaults\n\n# Just in case, we install the requirements again\npip install -r requirements.txt\n```\n\n<details>\n\n<summary>Above doesn't work?"}
{"input": " Here is an alternative</summary>\n\nThe starter template is the same as the [ZenML quickstart](https://github.com/zenml-io/zenml/tree/main/examples/quickstart). You can clone it like so:\n\n```bash\ngit clone --depth 1 git@github.com:zenml-io/zenml.git\ncd zenml/examples/quickstart\npip install -r requirements.txt\nzenml init\n```\n\n</details>\n\nTo run a pipeline using the new stack:\n\n1.  Set the stack as active on your client\n\n    ```bash\n    zenml stack set a_new_local_stack\n    ```\n2.  Run your pipeline code:\n\n    ```bash\n    python run.py --training-pipeline\n    ```\n\nKeep this code handy as we'll be using it in the next chapters!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Managing the lifecycle of a ZenML pipeline with Continuous Integration and\n  Delivery\n---\n\n# Set up CI/CD\n\nUntil now, we have been executing ZenML pipelines locally. While this is a good mode of operating pipelines, in\nproduction it is often desirable to mediate runs through a central workflow engine baked into your CI.\n\nThis allows data scientists to experiment with data processing and model training locally and then have code changes\nautomatically tested and validated through the standard pull request/merge request peer review process. Changes that\npass the CI and code review are then deployed automatically to production. Here is how this could look like:\n\n![Pipeline being run on staging/production stack through ci/cd](../../.gitbook/assets/ci-cd-overall.png)\n\n## Breaking it down\n\nTo illustrate this, let's walk through how this process could be set up with\na GitHub Repository. Basically we'll be using Github Actions in order to set up\na proper CI/CD workflow.\n\n{% hint style=\"info\" %}\nTo see this in action, check out the [ZenML Gitflow Repository](https://github.com/zenml-io/zenml-gitflow/). This\nrepository showcases how ZenML can be used for machine learning with a GitHub workflow that automates CI/CD with\ncontinuous model training and continuous model deployment to production. The repository is also meant to be used as a\ntemplate: you can fork it and easily adapt it to your own MLOps stack, infrastructure, code and data.{% endhint %}\n\n### Configure an API Key in ZenML\n\nIn order to facilitate machine-to-machine connection you need to create an API key within ZenML. Learn more about those\n[here](https://docs.zenml.io/how-to/connecting-to-zenml/connect-with-a-service-account).\n\n```bash\nzenml service-account create github_action_api_key\n```\n\nThis will return the API Key to you like this. This will not be shown to you again, so make sure to copy it here for\nuse in the next section.\n\n```bash\nCreated service account 'github_action_api_key'.\nSuccessfully created API key `default`.\nThe API key value is: 'ZENKEY_...'\nPlease store it safely as it will not be shown again.\nTo configure a ZenML client to use this API key, run:\n\n...\n```\n\n### Set up your secrets in Github\n\nFor our Github Actions we will need to set up some\nsecrets [for our repository](https://docs.github.com/en/actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository).\nSpecifically, you should use github secrets to store the `ZENML_API_KEY` that you created above.\n\n![create_gh_secret.png](../../.gitbook/assets/create_gh_secret.png)\n\nThe other values that are loaded from secrets into the\nenvironment [here](https://github.com"}
{"input": "/zenml-io/zenml-gitflow/blob/main/.github/workflows/pipeline_run.yaml#L14-L23)\ncan also be set explicitly or as variables.\n\n### (Optional) Set up different stacks for Staging and Production\n\nYou might not necessarily want to use the same stack with the same resources for your staging and production use.\n\nThis step is optional, all you'll need for certain is a stack that runs remotely (remote orchestration and artifact\nstorage). The rest is up to you. You might for example want to parametrize your pipeline to use different data sources\nfor the respective environments. You can also use different [configuration files](../../how-to/configuring-zenml/configuring-zenml.md)\nfor the different environments to configure the [Model](../../how-to/use-the-model-control-plane/README.md), the \n[DockerSettings](../../how-to/customize-docker-builds/docker-settings-on-a-pipeline.md), the [ResourceSettings like\naccelerators](../../how-to/training-with-gpus/training-with-gpus.md) differently for the different environments.\n\n### Trigger a pipeline on a Pull Request (Merge Request)\n\nOne way to ensure only fully working code makes it into production, you should use a staging environment to test all\nthe changes made to your code base and verify they work as intended. To do so automatically you should set up a\ngithub action workflow that runs your pipeline for you when you make changes to it.\n[Here](https://github.com/zenml-io/zenml-gitflow/blob/main/.github/workflows/pipeline_run.yaml) is an example that you\ncan use.\n\nTo only run the Github Action on a PR, you can configure the yaml like this\n\n```yaml\non:\n  pull_request:\n    branches: [ staging, main ]\n```\n\nWhen the workflow starts we want to set some important values. Here is a simplified version that you can use.\n\n```yaml\njobs:\n  run-staging-workflow:\n    runs-on: run-zenml-pipeline\n    env:\n      ZENML_HOST: ${{ secrets.ZENML_HOST }}  # Put your server url here\n      ZENML_API_KEY: ${{ secrets.ZENML_API_KEY }}  # Retrieves the api key for use  \n      ZENML_STACK: stack_name  #  Use this to decide which stack is used for staging\n      ZENML_GITHUB_SHA: ${{ github.event.pull_request.head.sha }}\n      ZENML_GITHUB_URL_PR: ${{ github.event.pull_request._links.html.href }}\n```\n\nAfter configuring these values so they apply to your specific situation the rest of the template should work as is for\nyou. Specifically you will need to install all requirements, connect to your ZenML Server, set an active stack\nand run a pipeline within your github action.\n\n```yaml\nsteps:\n  - name: Check out repository code\n    uses: actions/"}
{"input": "checkout@v3\n\n  - uses: actions/setup-python@v4\n    with:\n      python-version: '3.9'\n\n  - name: Install requirements\n    run: |\n      pip3 install -r requirements.txt\n\n  - name: Connect to ZenML server\n    run: |\n      zenml connect --url $ZENML_HOST --api-key $ZENML_API_KEY\n\n  - name: Set stack\n    run: |\n      zenml stack set ${{ env.ZENML_STACK }}\n\n\n  - name: Run pipeline\n    run: |\n      python run.py \\\n        --pipeline end-to-end \\\n        --dataset production \\\n        --version ${{ env.ZENML_GITHUB_SHA }} \\\n        --github-pr-url ${{ env.ZENML_GITHUB_URL_PR }}\n```\n\nWhen you push to a branch now, that is within a Pull Request, this action will run automatically.\n\n### (Optional) Comment Metrics onto the PR\n\nFinally you can configure your github action workflow to leave a report based on the pipeline that was run. \nCheck out the template for this [here](https://github.com/zenml-io/zenml-gitflow/blob/main/.github/workflows/pipeline_run.yaml#L87-L99.\n\n![Comment left on Pull Request](../../.gitbook/assets/github-action-pr-comment.png)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Taking your ZenML workflow to the next level.\n---\n\n# \u2601\ufe0f Cloud guide\n\nThis section of the guide consists of easy to follow guides on how to connect the major public clouds to your ZenML deployment. We achieve this by configuring a [stack](../production-guide/understand-stacks.md).\n\nA `stack` is the configuration of tools and infrastructure that your pipelines can run on. When you run a pipeline, ZenML performs a different series of actions depending on the stack.\n\n<figure><img src=\"../../.gitbook/assets/vpc_zenml.png\" alt=\"\"><figcaption><p>ZenML is the translation layer that allows your code to run on any of your stacks</p></figcaption></figure>\n\nNote, this guide focuses on the *registering* a stack, meaning that the resources required to run pipelines have already been *provisioned*. In order to provision the underlying infrastructure, you can either do so manually, via a IaC tool like Terraform, or use ZenML's sister project [MLStacks](https://mlstacks.zenml.io/).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: A guide to debug common issues and get help.\n---\n\n# \ud83d\udc1e Debug and solve issues\n\nIf you stumbled upon this page, chances are you're facing issues with using ZenML. This page documents suggestions and best practices to let you debug, get help, and solve issues quickly.\n\n### When to get help?\n\nWe suggest going through the following checklist before asking for help:\n\n*   Search on Slack using the built-in Slack search function at the top of the page.\n\n    ![Searching on Slack.](../.gitbook/assets/slack\\_search\\_bar.png)\n* Search on [GitHub issues](https://github.com/zenml-io/zenml/issues).\n*   Search the [docs](https://docs.zenml.io) using the search bar in the top right corner of the page.\n\n    ![Searching on docs page.](../.gitbook/assets/doc\\_search\\_bar.png)\n* Check out the [common errors](debug-and-solve-issues.md#most-common-errors) section below.\n* Understand the problem by studying the [additional logs](debug-and-solve-issues.md#41-additional-logs) and [client/server logs](debug-and-solve-issues.md#client-and-server-logs).\n\nChances are you'd find your answers there. If you can't find any clue, then it's time to post your question on [Slack](https://zenml.io/slack).\n\n### How to post on Slack?\n\nWhen posting on Slack it's useful to provide the following information (when applicable) so that we get a complete picture before jumping into solutions.\n\n#### 1. System Information\n\nLet us know relevant information about your system. We recommend running the following in your terminal and attaching the output to your question.\n\n```shell\nzenml info -a -s\n```\n\nYou can optionally include information about specific packages where you're having problems by using the `-p` option. For example, if you're having problems with the `tensorflow` package, you can run:\n\n```shell\nzenml info -p tensorflow\n```\n\nThe output should look something like this:\n\n```yaml\nZENML_LOCAL_VERSION: 0.40.2\nZENML_SERVER_VERSION: 0.40.2\nZENML_SERVER_DATABASE: mysql\nZENML_SERVER_DEPLOYMENT_TYPE: alpha\nZENML_CONFIG_DIR: /Users/my_username/Library/Application Support/zenml\nZENML_LOCAL_STORE_DIR: /Users/my_username/Library/Application Support/zenml/local_stores\nZENML_SERVER_URL: https://someserver.zenml.io\nZENML_ACTIVE_REPOSITORY_ROOT: /Users/my_username/coding/zenml/repos/zenml\nPYTHON_VERSION: 3.9.13\nENVIRONMENT: native\nSYSTEM_INFO: {'os': 'mac', 'mac_version': '13.2'}\nACTIVE_WORKSPACE: default\nACTIVE_STACK: default"}
{"input": "\nACTIVE_USER: some_user\nTELEMETRY_STATUS: disabled\nANALYTICS_CLIENT_ID: xxxxxxx-xxxxxxx-xxxxxxx\nANALYTICS_USER_ID: xxxxxxx-xxxxxxx-xxxxxxx\nANALYTICS_SERVER_ID: xxxxxxx-xxxxxxx-xxxxxxx\nINTEGRATIONS: ['airflow', 'aws', 'azure', 'dash', 'evidently', 'facets', 'feast', 'gcp', 'github',\n'graphviz', 'huggingface', 'kaniko', 'kubeflow', 'kubernetes', 'lightgbm', 'mlflow',\n'neptune', 'neural_prophet', 'pillow', 'plotly', 'pytorch', 'pytorch_lightning', 's3', 'scipy',\n'sklearn', 'slack', 'spark', 'tensorboard', 'tensorflow', 'vault', 'wandb', 'whylogs', 'xgboost']\n```\n\nSystem information provides more context to your issue and also eliminates the need for anyone to ask when they're trying to help. This increases the chances of your question getting answered and saves everyone's time.\n\n#### 2. What happened?\n\nTell us briefly:\n\n* What were you trying to achieve?\n* What did you expect to happen?\n* What actually happened?\n\n#### 3. How to reproduce the error?\n\nWalk us through how to reproduce the same error you had step-by-step, whenever possible. Use the format you prefer. Write it in text or record a video, whichever lets you get the issue at hand across to us!\n\n#### 4. Relevant log output\n\nAs a general rule of thumb, always attach relevant log outputs and the full\nerror traceback to help us understand what happened under the hood. If the full\nerror traceback does not fit into a text message, attach a file or use a service\nlike [Pastebin](https://pastebin.com/) or [Github's Gist](https://gist.github.com/).\n\nAlong with the error traceback, we recommend to always share the output of the following commands:\n\n* `zenml status`\n* `zenml stack describe`\n\nWhen applicable, also attach logs of the orchestrator. For example, if you're using the Kubeflow orchestrator, include the logs of the pod that was running the step that failed.\n\nUsually, the default log you see in your terminal is sufficient, in the event it's not, then it's useful to provide additional logs. Additional logs are not shown by default, you'll have to toggle an environment variable for it. Read the next section to find out how.\n\n**4.1 Additional logs**\n\nWhen the default logs are not helpful, ambiguous, or do not point you to the root of the issue, you can toggle the value of the `ZENML_LOGGING_VERBOSITY` environment variable to change the type of logs shown. The default value of"}
{"input": " `ZENML_LOGGING_VERBOSITY` environment variable is:\n\n```\nZENML_LOGGING_VERBOSITY=INFO\n```\n\nYou can pick other values such as `WARN`, `ERROR`, `CRITICAL`, `DEBUG` to change what's shown in the logs. And export the environment variable in your terminal. For example in Linux:\n\n```shell\nexport ZENML_LOGGING_VERBOSITY=DEBUG\n```\n\nRead more about how to set environment variables for:\n\n* For [Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/).\n* For [macOS](https://youngstone89.medium.com/setting-up-environment-variables-in-mac-os-28e5941c771c).\n* For [Windows](https://www.computerhope.com/issues/ch000549.htm).\n\n### Client and server logs\n\nWhen facing a ZenML Server-related issue, you can view the logs of the server to introspect deeper. To achieve this, run:\n\n```shell\nzenml logs\n```\n\nThe logs from a healthy server should look something like this:\n\n```shell\nINFO:asyncio:Syncing pipeline runs...\n2022-10-19 09:09:18,195 - zenml.zen_stores.metadata_store - DEBUG - Fetched 4 steps for pipeline run '13'. (metadata_store.py:315)\n2022-10-19 09:09:18,359 - zenml.zen_stores.metadata_store - DEBUG - Fetched 0 inputs and 4 outputs for step 'importer'. (metadata_store.py:427)\n2022-10-19 09:09:18,461 - zenml.zen_stores.metadata_store - DEBUG - Fetched 0 inputs and 4 outputs for step 'importer'. (metadata_store.py:427)\n2022-10-19 09:09:18,516 - zenml.zen_stores.metadata_store - DEBUG - Fetched 2 inputs and 2 outputs for step 'normalizer'. (metadata_store.py:427)\n2022-10-19 09:09:18,606 - zenml.zen_stores.metadata_store - DEBUG - Fetched 0 inputs and 4 outputs for step 'importer'. (metadata_store.py:427)\n```\n\n### Most common errors\n\nThis section documents frequently encountered errors among users and solutions to each.\n\n#### Error initializing rest store\n\nTypically, the error presents itself as:\n\n```bash\nRuntimeError: Error initializing rest store with URL 'http://127.0.0.1:8237': HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: /api/v1/login (Caused by \nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9abb198550>: Failed to establish"}
{"input": " a new connection: [Errno 61] Connection refused'))\n```\n\nIf you restarted your machine after deploying ZenML then you have to run `zenml up` again after each restart. Local ZenML deployments don't survive machine restarts.\n\n#### Column 'step\\_configuration' cannot be null\n\n```bash\nsqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1048, \"Column 'step_configuration' cannot be null\")\n```\n\nThis happens when a step configuration is too long. We changed the limit from 4K to 65K chars, but it could still happen if you have excessively long strings in your config.\n\n#### 'NoneType' object has no attribute 'name'\n\nThis is also a common error you might encounter when you do not have the necessary stack components registered on the stack. For example:\n\n```shell\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 /home/dnth/Documents/zenml-projects/nba-pipeline/run_pipeline.py:24 in <module>                  \u2502\n\u2502                                                                                                  \u2502\n\u2502    21 \u2502   reference_data_splitter,                                                               \u2502\n\u2502    22 \u2502   TrainingSplitConfig,                                                                   \u2502\n\u2502    23 )                                                                                          \u2502\n\u2502 \u2771  24 from steps.trainer import random_forest_trainer                                            \u2502\n\u2502    25 from steps.encoder import encode_columns_and_clean                                         \u2502\n\u2502    26 from steps.importer import (                                                               \u2502\n\u2502    27 \u2502   import_season_schedule,                                                                \u2502\n\u2502                                                                                                  \u2502\n\u2502 /home/dnth/Documents/zenml-projects/nba-pipeline/steps/trainer.py:24 in <module>                 \u2502\n\u2502                                                                                                  \u2502\n\u2502   21 \u2502   max_depth: int = 10000                                                                  \u2502\n\u2502   22 \u2502   target_col: str = \"FG3M\"                                                                \u2502\n\u2502   23                                                                                             \u2502\n\u2502 \u2771 24 @step(enable_cache=False, experiment_tracker=experiment_tracker.name)                       \u2502\n\u2502   25 def random_forest_trainer(                                                                  \u2502\n\u2502   26 \u2502   train_df_x: pd.DataFrame,                                                               \u2502\n\u2502   27 \u2502   train_df_y: pd.DataFrame,                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nAttributeError: 'NoneType' object has no attribute 'name'\n```\n\nIn the above error snippet, the `step` on line 24 expects an experiment tracker but could not find it on the stack. To solve it, register an experiment tracker of your choice on the stack. For instance:\n\n```shell\nzenml experiment-tracker register mlflow_tracker --flavor=mlflow\n```\n\nand update your stack with the experiment tracker:\n\n```shell\nzenml stack update -e ml"}
{"input": "flow_tracker\n```\n\nThis also applies to all other [stack components](../component-guide/README.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Managing your secrets with ZenML.\n---\n\n# \ud83d\udd10 Interact with secrets\n\n## How to create a secret\n\n{% tabs %}\n{% tab title=\"CLI\" %}\nTo create a secret with a name `<SECRET_NAME>` and a key-value pair, you can run the following CLI command:\n\n```shell\nzenml secret create <SECRET_NAME> \\\n    --<KEY_1>=<VALUE_1> \\\n    --<KEY_2>=<VALUE_2>\n\n# Another option is to use the '--values' option and provide key-value pairs in either JSON or YAML format.\nzenml secret create <SECRET_NAME> \\\n    --values='{\"key1\":\"value2\",\"key2\":\"value2\"}'\n```\n\nAlternatively, you can create the secret in an interactive session (in which ZenML will query you for the secret keys and values) by passing the `--interactive/-i` parameter:\n\n```shell\nzenml secret create <SECRET_NAME> -i\n```\n\nFor secret values that are too big to pass as a command line argument, or have special characters, you can also use the special `@` syntax to indicate to ZenML that the value needs to be read from a file:\n\n```bash\nzenml secret create <SECRET_NAME> \\\n   --key=@path/to/file.txt \\\n   ...\n   \n# Alternatively, you can utilize the '--values' option by specifying a file path containing key-value pairs in either JSON or YAML format.\nzenml secret create <SECRET_NAME> \\\n    --values=@path/to/file.txt\n```\n\nThe CLI also includes commands that can be used to list, update and delete secrets. A full guide on using the CLI to create, access, update and delete secrets is available [here](https://sdkdocs.zenml.io/latest/cli/#zenml.cli--secrets-management).\n\n**Interactively register missing secrets for your stack**\n\nIf you're using components with [secret references](interact-with-secrets.md#reference-secrets-in-stack-component-attributes-and-settings) in your stack, you need to make sure that all the referenced secrets exist. To make this process easier, you can use the following CLI command to interactively register all secrets for a stack:\n\n```shell\nzenml stack register-secrets [<STACK_NAME>]\n```\n{% endtab %}\n\n{% tab title=\"Python SDK\" %}\nThe ZenML client API offers a programmatic interface to create, e.g.:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nclient.create_secret(\n    name=\"my_secret\",\n    values={\n        \"username\": \"admin\",\n        \"password\": \"abc123\"\n    }\n)\n```\n\nOther Client methods used for secrets management include `get_secret` to fetch a secret by name or id, `update_secret` to update an existing secret, `list_secrets` to query the secrets store using a variety of filtering and sorting criteria, and `delete"}
{"input": "_secret` to delete a secret. The full Client API reference is available [here](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/).\n{% endtab %}\n{% endtabs %}\n\n## Set scope for secrets\n\nZenML secrets can be scoped to a workspace or a user. This allows you to create secrets that are only accessible within a specific workspace or to one user.\n\nBy default, all created secrets are scoped to the active workspace. To create a secret and scope it to your active user instead, you can pass the `--scope` argument to the CLI command:\n\n```shell\nzenml secret create <SECRET_NAME> \\\n    --scope user \\\n    --<KEY_1>=<VALUE_1> \\\n    --<KEY_2>=<VALUE_2>\n```\n\nScopes also act as individual namespaces. When you are referencing a secret by name in your pipelines and stacks, ZenML will first look for a secret with that name scoped to the active user, and if it doesn't find one, it will look for one in the active workspace.\n\n## Accessing registered secrets\n\n### Reference secrets in stack component attributes and settings\n\nSome of the components in your stack require you to configure them with sensitive information like passwords or tokens, so they can connect to the underlying infrastructure. Secret references allow you to configure these components in a secure way by not specifying the value directly but instead referencing a secret by providing the secret name and key. Referencing a secret for the value of any string attribute of your stack components, simply specify the attribute using the following syntax: `{{<SECRET_NAME>.<SECRET_KEY>}}`\n\nFor example:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\n# Register a secret called `mlflow_secret` with key-value pairs for the\n# username and password to authenticate with the MLflow tracking server\n\n# Using central secrets management\nzenml secret create mlflow_secret \\\n    --username=admin \\\n    --password=abc123\n    \n\n# Then reference the username and password in our experiment tracker component\nzenml experiment-tracker register mlflow \\\n    --flavor=mlflow \\\n    --tracking_username={{mlflow_secret.username}} \\\n    --tracking_password={{mlflow_secret.password}} \\\n    ...\n```\n{% endtab %}\n{% endtabs %}\n\nWhen using secret references in your stack, ZenML will validate that all secrets and keys referenced in your stack components exist before running a pipeline. This helps us fail early so your pipeline doesn't fail after running for some time due to some missing secret.\n\nThis validation by default needs to fetch and read every secret to make sure that both the secret and the specified key-value pair exist. This can take quite some time and might fail if you don't have permission to read secrets.\n\nYou can use the environment variable `ZENML_SECRET_VALIDATION_LEVEL` to disable or control the degree to which ZenML validates your secrets:\n\n* Setting it"}
{"input": " to `NONE` disables any validation.\n* Setting it to `SECRET_EXISTS` only validates the existence of secrets. This might be useful if the machine you're running on only has permission to list secrets but not actually read their values.\n* Setting it to `SECRET_AND_KEY_EXISTS` (the default) validates both the secret existence as well as the existence of the exact key-value pair.\n\n### Fetch secret values in a step\n\nIf you are using [centralized secrets management](interact-with-secrets.md), you can access secrets directly from within your steps through the ZenML `Client` API. This allows you to use your secrets for querying APIs from within your step without hard-coding your access keys:\n\n```python\nfrom zenml import step\nfrom zenml.client import Client\n\n\n@step\ndef secret_loader() -> None:\n    \"\"\"Load the example secret from the server.\"\"\"\n    # Fetch the secret from ZenML.\n    secret = Client().get_secret( < SECRET_NAME >)\n\n    # `secret.secret_values` will contain a dictionary with all key-value\n    # pairs within your secret.\n    authenticate_to_some_api(\n        username=secret.secret_values[\"username\"],\n        password=secret.secret_values[\"password\"],\n    )\n    ...\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Trigger a pipeline from another pipeline.\n---\n\n# Trigger a pipeline from another pipeline\n\n{% hint style=\"info\" %}\nThis is a [ZenML Pro](https://zenml.io/pro) only feature. Please [sign up here](https://cloud.zenml.io) get access.\nOSS users can only trigger a pipeline by calling the pipeline function inside their runner script.\n{% endhint %}\n\nTriggering a pipeline from another **only** works if you've created at least one run template for that pipeline.\n\n```python\nimport pandas as pd\nfrom zenml import pipeline, step\nfrom zenml.client import Client\nfrom zenml.config.pipeline_run_configuration import PipelineRunConfiguration\n\n@step  \ndef trainer(data_artifact_id: str):\n    df = load_artifact(data_artifact_id)\n\n@pipeline\ndef training_pipeline():\n    trainer()\n\n@step  \ndef load_data() -> pd.Dataframe:\n    ...\n\n@step  \ndef trigger_pipeline(df: UnmaterializedArtifact):\n    # By using UnmaterializedArtifact we can get the ID of the artifact\n    run_config = PipelineRunConfiguration(steps={\"trainer\": {\"parameters\": {\"data_artifact_id\": df.id}}})\n    Client().trigger_pipeline(\"training_pipeline\", run_configuration=run_config)\n\n@pipeline  \ndef loads_data_and_triggers_training():\n    df = load_data()\n    trigger_pipeline(df)  # Will trigger the other pipeline\n```\n\nRead more about the [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) and [`trigger_pipeline`](https://sdkdocs.zenml.io/0.60.0/core_code_docs/core-client/#zenml.client.Client) function object in the [SDK Docs](https://sdkdocs.zenml.io/).\n\nRead more about Unmaterialized Artifacts [here](../handle-data-artifacts/unmaterialized-artifacts.md).\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Learn how to run a pipeline directly from the Python Client</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-client.md\">orchestrators.md</a></td></tr><tr><td>Learn how to run a pipeline from the REST API</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-rest-api.md\">orchestrators.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-"}
{"input": "4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  There are numerous ways to trigger a pipeline, apart from\n  calling the runner script.\n---\n\n# \ud83d\udea8 Trigger a pipeline\n\nA pipeline can be run via Python like this:\n\n```python\n@step  # Just add this decorator\ndef load_data() -> dict:\n    training_data = [[1, 2], [3, 4], [5, 6]]\n    labels = [0, 1, 0]\n    return {'features': training_data, 'labels': labels}\n\n\n@step\ndef train_model(data: dict) -> None:\n    total_features = sum(map(sum, data['features']))\n    total_labels = sum(data['labels'])\n\n    # Train some model here\n\n    print(f\"Trained model using {len(data['features'])} data points. \"\n          f\"Feature sum is {total_features}, label sum is {total_labels}\")\n\n\n@pipeline  # This function combines steps together \ndef simple_ml_pipeline():\n    dataset = load_data()\n    train_model(dataset)\n```\n\nYou can now run this pipeline by simply calling the function:\n\n```python\nsimple_ml_pipeline()\n```\n\nHowever, there are other ways to trigger a pipeline, specifically a pipeline with a remote stack (remote\norchestrator, artifact store, and container registry).\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Trigger a pipeline from Python SDK</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-client.md\">trigger-a-pipeline-from-client.md</a></td></tr><tr><td>Trigger a pipeline from another</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-another.md\">trigger-a-pipeline-from-another.md</a></td></tr><tr><td>Trigger a pipeline from the REST API</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-rest-api.md\">trigger-a-pipeline-from-rest-api.md</a></td></tr></tbody></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Trigger a pipeline using the ZenML Client.\n---\n\n# Trigger a pipeline from the Python Client\n\n{% hint style=\"info\" %}\nThis is a [ZenML Pro](https://zenml.io/pro) only feature. Please [sign up here](https://cloud.zenml.io) get access.\nOSS users can only trigger a pipeline by calling the pipeline function inside their runner script.\n{% endhint %}\n\nTriggering a pipeline from the Python client **only** works if you've created at least one run template for that pipeline.\n\n\n```python\nfrom zenml.client import Client\nfrom zenml.config.pipeline_run_configuration import PipelineRunConfiguration\n\nif __name__ == \"__main__\":\n    run_config = PipelineRunConfiguration(steps={\"trainer\": {\"parameters\": {\"data_artifact_id\": data_artifact_id}}})\n    Client().trigger_pipeline(\"training_pipeline\", run_configuration=run_config)\n```\n\n\nRead more about the [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) and [`trigger_pipeline`](https://sdkdocs.zenml.io/0.60.0/core_code_docs/core-client/#zenml.client.Client) in the [SDK Docs](https://sdkdocs.zenml.io/).\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Learn how to run a pipeline directly from another</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-another.md\">orchestrators.md</a></td></tr><tr><td>Learn how to run a pipeline from the REST API</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-rest-api.md\">orchestrators.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Trigger a pipeline from the rest API.\n---\n\n# Trigger a pipeline from the REST API\n\n{% hint style=\"info\" %}\nThis is a [ZenML Pro](https://zenml.io/pro) only feature. Please [sign up here](https://cloud.zenml.io) get access.\nOSS users can only trigger a pipeline by calling the pipeline function inside their runner script.\n{% endhint %}\n\nTriggering a pipeline from the REST API **only** works if you've created at least one run template for that pipeline.\n\nAs a pre-requisite, you need a pipeline name. After you have it, there are three calls that need to be made in order to trigger a pipeline from the REST API:\n\n1. `GET /pipelines?name=<PIPELINE_NAME>` -> This returns a response, where a <PIPELINE_ID> can be copied\n2. `GET /run_templates?pipeline_id=<PIPELINE_ID>` -> This returns a list of responses where a <TEMPLATE_ID> can be chosen\n3. `POST /run_templates/<TEMPLATE_ID>/runs` -> This runs the pipeline. You can pass the [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) in the body\n\n## A worked example\n\n{% hint style=\"info\" %}\nLearn how to get a bearer token for the curl commands [here](../../reference/api-reference.md#using-a-bearer-token-to-access-the-api-programmatically).\n{% endhint %}\n\nHere is an example. Let's say would we like to re-run a pipeline called `training`. We first query the `/pipelines` endpoint:\n\n```shell\ncurl -X 'GET' \\\n  '<YOUR_ZENML_SERVER_URL>/api/v1/pipelines?hydrate=false&name=training' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer <YOUR_TOKEN>'\n```\n\n<figure><img src=\"../../.gitbook/assets/rest_api_step_1.png\" alt=\"\"><figcaption><p>Identifying the pipeline ID</p></figcaption></figure>\n\nWe can take the ID from any object in the list of responses. In this case, the <PIPELINE_ID> is `c953985e-650a-4cbf-a03a-e49463f58473` in the response.\n\nAfter this, we take the pipeline ID and call the `/run_templates?pipeline_id=<PIPELINE_ID>` API:\n\n```shell\ncurl -X 'GET' \\\n  '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates?hydrate=false&logical_operator=and&page=1&size=20&pipeline_id=b826b714-a9b3-461c-9a6e-1bde3df3241d' \\\n  -H 'accept: application/json' \\\n  -"}
{"input": "H 'Authorization: Bearer <YOUR_TOKEN>'\n```\n\nWe can now take the <TEMPLATE_ID> from this response. Here it is `b826b714-a9b3-461c-9a6e-1bde3df3241d`.\n\n<figure><img src=\"../../.gitbook/assets/rest_api_step_2.png\" alt=\"\"><figcaption><p>Identifying the template ID</p></figcaption></figure>\n\nFinally, we can use the template ID to trigger the pipeline with a different configuration:\n\n```shell\ncurl -X 'POST' \\\n  '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates/b826b714-a9b3-461c-9a6e-1bde3df3241d/runs' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer <YOUR_TOKEN>' \\\n  -d '{\n  \"steps\": {\"model_trainer\": {\"parameters\": {\"model_type\": \"rf\"}}}\n}'\n```\n\nA positive response means your pipeline has been re-triggered with a different config!\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Learn how to run a pipeline directly from another</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-another.md\">orchestrators.md</a></td></tr><tr><td>Learn how to run a pipeline directly from the Python client</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-client.md\">orchestrators.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Creating an external integration and contributing to ZenML\n---\n\n# Implement a custom integration\n\n![ZenML integrates with a number of tools from the MLOps landscape](../../.gitbook/assets/sam-side-by-side-full-text.png)\n\nOne of the main goals of ZenML is to find some semblance of order in the ever-growing MLOps landscape. ZenML already provides [numerous integrations](https://zenml.io/integrations) into many popular tools, and allows you to come up with ways to [implement your own stack component flavors](implement-a-custom-stack-component.md) in order to fill in any gaps that are remaining.\n\n_However, what if you want to make your extension of ZenML part of the main codebase, to share it with others?_ If you are such a person, e.g., a tooling provider in the ML/MLOps space, or just want to contribute a tooling integration to ZenML, this guide is intended for you.\n\n### Step 1: Plan out your integration\n\nIn [the previous page](implement-a-custom-stack-component.md), we looked at the categories and abstractions that core ZenML defines. In order to create a new integration into ZenML, you would need to first find the categories that your integration belongs to. The list of categories can be found [here](../../component-guide/README.md) as well.\n\nNote that one integration may belong to different categories: For example, the cloud integrations (AWS/GCP/Azure) contain [container registries](../../component-guide/container-registries/container-registries.md), [artifact stores](../../component-guide/artifact-stores/artifact-stores.md) etc.\n\n### Step 2: Create individual stack component flavors\n\nEach category selected above would correspond to a [stack component type](../../component-guide/README.md). You can now start developing individual stack component flavors for this type by following the detailed instructions on the respective pages.\n\nBefore you package your new components into an integration, you may want to use/test them as a regular custom flavor. For instance, if you are [developing a custom orchestrator](../../component-guide/orchestrators/custom.md) and your flavor class `MyOrchestratorFlavor` is defined in `flavors/my_flavor.py`, you can register it by using:\n\n```shell\nzenml orchestrator flavor register flavors.my_flavor.MyOrchestratorFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but"}
{"input": " usually it's better to not have to rely on this mechanism, and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml orchestrator flavor list\n```\n\nSee the docs on extensibility of the different components [here](../../component-guide/README.md) or get inspired by the many integrations that are already implemented such as [the MLflow experiment tracker](../../component-guide/experiment-trackers/mlflow.md).\n\n### Step 3: Create an integration class\n\nOnce you are finished with your flavor implementations, you can start the process of packaging them into your integration and ultimately the base ZenML package. Follow this checklist to prepare everything:\n\n**1. Clone Repo**\n\nOnce your stack components work as a custom flavor, you can now [clone the main zenml repository](https://github.com/zenml-io/zenml) and follow the [contributing guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md) to set up your local environment for develop.\n\n**2. Create the integration directory**\n\nAll integrations live within [`src/zenml/integrations/`](https://github.com/zenml-io/zenml/tree/main/src/zenml/integrations) in their own sub-folder. You should create a new folder in this directory with the name of your integration.\n\nAn example integration directory would be structured as follows:\n\n```\n/src/zenml/integrations/                        <- ZenML integration directory\n    <example-integration>                       <- Root integration directory\n        |\n        \u251c\u2500\u2500 artifact-stores                     <- Separated directory for  \n        |      \u251c\u2500\u2500 __init_.py                      every type\n        |      \u2514\u2500\u2500 <example-artifact-store>     <- Implementation class for the  \n        |                                          artifact store flavor\n        \u251c\u2500\u2500 flavors \n        |      \u251c\u2500\u2500 __init_.py \n        |      \u2514\u2500\u2500 <example-artifact-store-flavor>  <- Config class and flavor\n        |\n        \u2514\u2500\u2500 __init_.py                          <- Integration class \n```\n\n**3. Define the name of your integration in constants**\n\nIn [`zenml/integrations/constants.py`](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/constants.py), add:\n\n```python\nEXAMPLE_INTEGRATION = \"<name-of-integration>\"\n```\n\nThis will be the name of the integration when you run:\n\n```shell\n zenml integration install <name-of-integration>\n```\n\n**4. Create the integration class \\_\\_init\\_\\_.py**\n\nIn `src/zenml/integrations/<YOUR_INTEGRATION>/init__.py` you must now create a new class, which is a subclass of the `Integration` class, set some important attributes (`NAME` and `REQUIREMENTS`), and overwrite the `flavors"}
{"input": "` class method.\n\n```python\nfrom zenml.integrations.constants import <EXAMPLE_INTEGRATION>\nfrom zenml.integrations.integration import Integration\nfrom zenml.stack import Flavor\n\n# This is the flavor that will be used when registering this stack component\n#  `zenml <type-of-stack-component> register ... -f example-orchestrator-flavor`\nEXAMPLE_ORCHESTRATOR_FLAVOR = <\"example-orchestrator-flavor\">\n\n# Create a Subclass of the Integration Class\nclass ExampleIntegration(Integration):\n    \"\"\"Definition of Example Integration for ZenML.\"\"\"\n\n    NAME = <EXAMPLE_INTEGRATION>\n    REQUIREMENTS = [\"<INSERT PYTHON REQUIREMENTS HERE>\"]\n\n    @classmethod\n    def flavors(cls) -> List[Type[Flavor]]:\n        \"\"\"Declare the stack component flavors for the <EXAMPLE> integration.\"\"\"\n        from zenml.integrations.<example_flavor> import <ExampleFlavor>\n        \n        return [<ExampleFlavor>]\n        \nExampleIntegration.check_installation() # this checks if the requirements are installed\n```\n\nHave a look at the [MLflow Integration](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/mlflow/\\_\\_init\\_\\_.py) as an example for how it is done.\n\n**5. Import in all the right places**\n\nThe Integration itself must be imported within [`src/zenml/integrations/__init__.py`](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/\\_\\_init\\_\\_.py).\n\n### Step 4: Create a PR and celebrate :tada:\n\nYou can now [create a PR](https://github.com/zenml-io/zenml/compare) to ZenML and wait for the core maintainers to take a look. Thank you so much for your contribution to the codebase, rock on! \ud83d\udc9c\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploy a cloud stack using Terraform\n---\n\n# Deploy a cloud stack with Terraform\n\nZenML maintains a collection of [Terraform modules](https://registry.terraform.io/modules/zenml-io/zenml-stack)\ndesigned to streamline the provisioning of cloud resources and seamlessly\nintegrate them with ZenML Stacks. These modules simplify the setup process,\nallowing users to quickly provision cloud resources as well as configure and\nauthorize ZenML to utilize them for running pipelines and other AI/ML operations.\n\nBy leveraging these Terraform modules, users can ensure a more efficient and\nscalable deployment of their machine learning infrastructure, ultimately\nenhancing their development and operational workflows. The modules'\nimplementation can also be used as a reference for creating custom Terraform\nconfigurations tailored to specific cloud environments and requirements.\n\n{% hint style=\"info\" %}\nTerraform requires you to manage your infrastructure as code yourself. Among\nother things, this means that you will need to have Terraform installed on your\nmachine and you will need to manually manage the state of your infrastructure.\n\nIf you prefer a more automated approach, you can use [the 1-click stack deployment feature](deploy-a-cloud-stack.md)\nto deploy a cloud stack with ZenML with minimal knowledge of Terraform or cloud\ninfrastructure for that matter.\n\nIf you have the required infrastructure pieces already deployed on your cloud, \nyou can also use [the stack wizard to seamlessly register your stack](../../how-to/stack-deployment/register-a-cloud-stack.md).\n{% endhint %}\n\n## Pre-requisites\n\nTo use this feature, you need a deployed ZenML server instance that is reachable\nfrom the cloud provider where you wish to have the stack provisioned (this can't\nbe a local server started via `zenml up`). If you do not already have one set up,\nyou [can register for a free ZenML Pro account](https://cloud.zenml.io/signup)\nor you can learn about self-hosting a ZenML server [here](../../getting-started/deploying-zenml/README.md).\n\nOnce you are connected to your deployed ZenML server, you need to create\na service account and an API key for it. You will use the API key to give the\nTerraform module programmatic access to your ZenML server. You can find more\nabout service accounts and API keys [here](../connecting-to-zenml/connect-with-a-service-account.md).\nbut the process is as simple as running the following CLI command while\nconnected to your ZenML server:\n\n```shell\nzenml service-account create <account-name>\n```\n\nExample output:\n\n```shell\n$ zenml service-account create terraform-account\nCreated service account 'terraform-account'.\nSuccessfully created API key `default`.\nThe API key value is: 'ZENKEY_...'\nPlease store it safely as it will not be shown again.\nTo configure a ZenML client to use"}
{"input": " this API key, run:\n\nzenml connect --url https://842ed6a9-zenml.staging.cloudinfra.zenml.io --api-key \\\n    'ZENKEY_...'\n```\n\nFinally, you will need the following on the machine where you will be running\nTerraform:\n\n* [Terraform](https://www.terraform.io/downloads.html) installed on your machine\n(version at least 1.9).\n* the ZenML Terraform stack modules assume you are already locally authenticated\nwith your cloud provider through the provider's CLI or SDK tool and have\npermissions to create the resources that the modules will provision. This is\ndifferent depending on the cloud provider you are using and is covered in the\nfollowing sections.\n\n## How to use the Terraform stack deployment modules\n\nIf you are already knowledgeable with using Terraform and the cloud provider\nwhere you want to deploy the stack, this process will be straightforward. In a\nnutshell, you will need to:\n\n1. create a new Terraform configuration file (e.g., `main.tf`), preferably in a\nnew directory, with the content that looks like this (`<cloud provider>` can be\n`aws`, `gcp`, or `azure`):\n\n```hcl\nmodule \"zenml_stack\" {\n  source = \"zenml-io/zenml-stack/<cloud-provider>\"\n  version = \"x.y.z\"\n\n  # Required inputs\n  zenml_server_url = \"https://<zenml-server-url>\"\n  zenml_api_key = \"<your-api-key>\"\n  # Optional inputs\n  zenml_stack_name = \"<your-stack-name>\"\n  orchestrator = \"<your-orchestrator-type>\" # e.g., \"local\", \"sagemaker\", \"vertex\", \"azureml\", \"skypilot\"\n}\noutput \"zenml_stack_id\" {\n  value = module.zenml_stack.zenml_stack_id\n}\noutput \"zenml_stack_name\" {\n  value = module.zenml_stack.zenml_stack_name\n}\n```\n\nThere might be a few additional required or optional inputs depending on the\ncloud provider you are using. You can find the full list of inputs for each\nmodule in the [Terraform Registry](https://registry.terraform.io/modules/zenml-io/zenml-stack)\ndocumentation for the relevant module or you can read on in the following\nsections.\n\n2. Run the following commands in the directory where you have your Terraform\nconfiguration file:\n\n```shell\nterraform init\nterraform apply\n```\n\n{% hint style=\"warning\" %}\nThe directory where you keep the Terraform configuration file and where you run\nthe `terraform` commands is important. This is where Terraform will store the\nstate of your infrastructure. Make sure you do not delete this directory or the\nstate file it contains unless you are sure you no longer need to manage these\nresources with Terraform or after you"}
{"input": " have deprovisioned them up with\n`terraform destroy`.\n{% endhint %}\n\n3. Terraform will prompt you to confirm the changes it will make to your cloud\ninfrastructure. If you are happy with the changes, type `yes` and hit enter.\n\n4. Terraform will then provision the resources you have specified in your\nconfiguration file. Once the process is complete, you will see a message\nindicating that the resources have been successfully created and printing out\nthe ZenML stack ID and name:\n\n```shell\n...\nApply complete! Resources: 15 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nzenml_stack_id = \"04c65b96-b435-4a39-8484-8cc18f89b991\"\nzenml_stack_name = \"terraform-gcp-588339e64d06\"\n```\n\nAt this point, a ZenML stack has also been created and registered with your\nZenML server and you can start using it to run your pipelines:\n\n```shell\nzenml integration install <list-of-required-integrations>\nzenml stack set <zenml_stack_id>\n```\n\nYou can find more details specific to the cloud provider of your choice in the\nnext section:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n\nThe [original documentation for the ZenML AWS Terraform module](https://registry.terraform.io/modules/zenml-io/zenml-stack/aws/latest)\ncontains extensive information about required permissions, inputs, outputs and\nprovisioned resources. This is a summary of the key points from that\ndocumentation.\n\n### Authentication\n\nTo authenticate with AWS, you need to have [the AWS CLI](https://aws.amazon.com/cli/)\ninstalled on your machine and you need to have run `aws configure` to set up your\ncredentials.\n\n### Example Terraform Configuration\n\nHere is an example Terraform configuration file for deploying a ZenML stack on\nAWS:\n\n```hcl\nmodule \"zenml_stack\" {\n  source = \"zenml-io/zenml-stack/aws\"\n\n  # Required inputs\n  zenml_server_url = \"https://<zenml-server-url>\"\n  zenml_api_key = \"<your-api-key>\"\n\n  # Optional inputs\n  region = \"<your-aws-region>\"\n  orchestrator = \"<your-orchestrator-type>\" # e.g., \"local\", \"sagemaker\", \"skypilot\"\n}\noutput \"zenml_stack_id\" {\n  value = module.zenml_stack.zenml_stack_id\n}\noutput \"zenml_stack_name\" {\n  value = module.zenml_stack.zenml_stack_name\n}\n```\n\n### Stack Components\n\nThe Terraform module will create a ZenML stack configuration with the\nfollowing components:\n\n\n1. an S3 Artifact Store linked to a S3 bucket\n2. an ECR Container Registry linked to a ECR repository\n3. depending on the `orchestr"}
{"input": "ator` input variable:\n  * a local Orchestrator, if `orchestrator` is set to `local`. This can be used in combination with the SageMaker Step Operator to selectively run some steps locally and some on SageMaker.\n  * a SageMaker Orchestrator linked to the AWS account, if `orchestrator` is set to `sagemaker` (default)\n  * a SkyPilot Orchestrator linked to the AWS account, if `orchestrator` is set to `skypilot`\n4. a SageMaker Step Operator linked to the AWS account\n5. an AWS Service Connector configured with the IAM role credentials and used to\nauthenticate all ZenML components with your AWS account\n\nTo use the ZenML stack, you will need to install the required integrations:\n\n* for the local or SageMaker orchestrator:\n\n```shell\nzenml integration install aws s3\n```\n\n* for the SkyPilot orchestrator:\n\n```shell\nzenml integration install aws s3 skypilot_aws\n```\n\n{% endtab %}\n{% tab title=\"GCP\" %}\n\nThe [original documentation for the ZenML GCP Terraform module](https://registry.terraform.io/modules/zenml-io/zenml-stack/gcp/latest)\ncontains extensive information about required permissions, inputs, outputs and\nprovisioned resources. This is a summary of the key points from that\ndocumentation.\n\n### Authentication\n\nTo authenticate with GCP, you need to have [the `gcloud` CLI](https://cloud.google.com/sdk/gcloud)\ninstalled on your machine and you need to have run `gcloud init` or `gcloud auth login`\nto set up your credentials.\n\n### Example Terraform Configuration\n\nHere is an example Terraform configuration file for deploying a ZenML stack on\nAWS:\n\n```hcl\nmodule \"zenml_stack\" {\n  source = \"zenml-io/zenml-stack/gcp\"\n\n  # Required inputs\n  project_id = \"<your-gcp-project-id>\"\n  zenml_server_url = \"https://<zenml-server-url>\"\n  zenml_api_key = \"<your-api-key>\"\n\n  # Optional inputs\n  region = \"<your-gcp-region>\"\n  orchestrator = \"<your-orchestrator-type>\" # e.g., \"local\", \"vertex\", \"skypilot\" or \"airflow\"\n}\noutput \"zenml_stack_id\" {\n  value = module.zenml_stack.zenml_stack_id\n}\noutput \"zenml_stack_name\" {\n  value = module.zenml_stack.zenml_stack_name\n}\n```\n\n### Stack Components\n\nThe Terraform module will create a ZenML stack configuration with the\nfollowing components:\n\n1. an GCP Artifact Store linked to a GCS bucket\n2. an GCP Container Registry linked to a Google Artifact Registry\n3. depending on the `orchestrator` input variable:\n  *"}
{"input": " a local Orchestrator, if `orchestrator` is set to `local`. This can be used in combination with the Vertex AI Step Operator to selectively run some steps locally and some on Vertex AI.\n  * a Vertex AI Orchestrator linked to the GCP project, if `orchestrator` is set to `vertex` (default)\n  * a SkyPilot Orchestrator linked to the GCP project, if `orchestrator` is set to `skypilot`\n  * an Airflow Orchestrator linked to the Cloud Composer environment, if `orchestrator` is set to `airflow`\n4. a Google Cloud Build Image Builder linked to your GCP project\n5. a Vertex AI Step Operator linked to the GCP project\n6. a GCP Service Connector configured with the GCP service account credentials or the GCP Workload Identity Provider configuration and used to authenticate all ZenML components with the GCP resources\n\nTo use the ZenML stack, you will need to install the required integrations:\n\n* for the local and Vertex AI orchestrators:\n\n```shell\nzenml integration install gcp\n```\n\n* for the SkyPilot orchestrator:\n\n```shell\nzenml integration install gcp skypilot_gcp\n```\n\n* for the Airflow orchestrator:\n\n```shell\nzenml integration install gcp airflow\n```\n\n{% endtab %}\n{% tab title=\"Azure\" %}\n\nThe [original documentation for the ZenML Azure Terraform module](https://registry.terraform.io/modules/zenml-io/zenml-stack/azure/latest)\ncontains extensive information about required permissions, inputs, outputs and\nprovisioned resources. This is a summary of the key points from that\ndocumentation.\n\n### Authentication\n\nTo authenticate with Azure, you need to have [the Azure CLI](https://learn.microsoft.com/en-us/cli/azure/)\ninstalled on your machine and you need to have run `az login` to set up your\ncredentials.\n\n### Example Terraform Configuration\n\nHere is an example Terraform configuration file for deploying a ZenML stack on\nAWS:\n\n```hcl\nmodule \"zenml_stack\" {\n  source = \"zenml-io/zenml-stack/azure\"\n\n  # Required inputs\n  zenml_server_url = \"https://<zenml-server-url>\"\n  zenml_api_key = \"<your-api-key>\"\n\n  # Optional inputs\n  location = \"<your-azure-location>\"\n  orchestrator = \"<your-orchestrator-type>\" # e.g., \"local\", \"skypilot_azure\"\n}\noutput \"zenml_stack_id\" {\n  value = module.zenml_stack.zenml_stack_id\n}\noutput \"zenml_stack_name\" {\n  value = module.zenml_stack.zenml_stack_name\n}\n```\n\n### Stack Components\n\nThe Terraform module will create a ZenML stack configuration with the\nfollowing components:\n\n1. an"}
{"input": " Azure Artifact Store linked to an Azure Storage Account and Blob Container\n2. an ACR Container Registry linked to an Azure Container Registry\n3. depending on the `orchestrator` input variable:\n  * a local Orchestrator, if `orchestrator` is set to `local`. This can be used in combination with the AzureML Step Operator to selectively run some steps locally and some on AzureML.\n  * an Azure SkyPilot Orchestrator linked to the Azure subscription, if `orchestrator` is set to `skypilot` (default)\n  * an AzureML Orchestrator linked to an AzureML Workspace, if `orchestrator` is set to `azureml` \n4. an AzureML Step Operator linked to an AzureML Workspace\n5. an Azure Service Connector configured with Azure Service Principal\ncredentials and used to authenticate all ZenML components with the Azure resources\n\nTo use the ZenML stack, you will need to install the required integrations:\n\n* for the local and AzureML orchestrators:\n\n```shell\nzenml integration install azure\n```\n\n* for the SkyPilot orchestrator:\n\n```shell\nzenml integration install azure skypilot_azure\n```\n\n{% endtab %}\n{% endtabs %}\n\n## How to clean up the Terraform stack deployments\n\nCleaning up the resources provisioned by Terraform is as simple as running the\n`terraform destroy` command in the directory where you have your Terraform\nconfiguration file. This will remove all the resources that were provisioned by\nthe Terraform module and will also delete the ZenML stack that was registered\nwith your ZenML server.\n\n```shell\nterraform destroy\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Individually deploying different stack components.\n---\n\n# \u2692\ufe0f Deploying stacks and components using `mlstacks`\n\nThe first step in running your pipelines on remote infrastructure is to deploy all the components that you would need, like an [MLflow tracking server](../../component-guide/experiment-trackers/mlflow.md),\n[Kubeflow orchestrator](../../component-guide/orchestrators/kubeflow.md), and more to your cloud.\n\nThis can bring plenty of benefits like scalability, reliability, and collaboration. ZenML eases the path to production by providing a seamless way for all tools to interact with others through the use of abstractions. However, one of the most painful parts of this process, from what we see on our Slack and in general, is the deployment of these stack components.\n\n[`mlstacks`](https://mlstacks.zenml.io/) is a Python package that allows you to quickly spin up MLOps\ninfrastructure using Terraform. It is designed to be used with\n[ZenML](https://zenml.io), but can be used with any MLOps tool or platform. You\ncan deploy a modular MLOps stack for AWS, GCP or K3D using mlstacks. Each deployment type is designed to offer a great deal of flexibility in configuring the resources while preserving the ease of application through the use of sensible defaults.\n\nTo make even this process easier for our users, we have created the `deploy` command in `zenml`, which allows you to quickly get started with a full-fledged MLOps stack using only a few commands. You can choose to deploy individual stack components through the stack-component CLI or deploy a stack with multiple components together (a tad more manual steps).\n\nCheck out [the full documentation for the mlstacks package](https://mlstacks.zenml.io/) for more information.\n\n## When should I deploy something using mlstacks?\n\n{% hint style=\"info\" %}\n**MLStacks deploys resources using a Kubernetes cluster, which may be expensive and not for every user. In order to use stacks which are more basic and cheaper on the cloud, read [how to easily register a cloud stack](../../how-to/stack-deployment/register-a-cloud-stack.md)\nif you have existing infrastructure, or read [how to deploy a cloud stack in one click](../../how-to/stack-deployment/deploy-a-cloud-stack.md) or [how to deploy a cloud stack with Terraform](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform).**\n\nOr simply try running one of:\n\n```shell\nzenml stack register --provider aws\nzenml stack deploy --provider aws\n```\n{% endhint %}\n\nTo answer this question, here are some pros and cons in comparison to the stack-component deploy method which can help you choose what works best for you!\n\n{% tabs %}\n{% tab title=\"\ud83d\ude0d Pros\""}
{"input": " %}\n* Offers a lot of flexibility in what you deploy.\n* Deploying with `mlstacks` gives you a full MLOps stack as the output. Your\n  components and stack is automatically imported to ZenML. This saves you the\n  effort of manually registering all the components.\n{% endtab %}\n\n{% tab title=\"\ud83d\ude25 Cons\" %}\n* Currently only supports AWS, GCP, and K3D as providers.\n* Most stack deployments are Kubernetes-based which might be too heavy for your\n  needs.\n* Not all stack components are supported yet.\n{% endtab %}\n{% endtabs %}\n\nThe ZenML CLI has special subcommands that allow you to deploy individual stack components as well as whole stacks using MLStacks. These stacks will be useful for you if:\n\n* You are at the start of your MLOps journey, and would like to explore different tools.\n* You are looking for guidelines for production-grade deployments.\n\n## How does `mlstacks` work?\n\nMLStacks is built around the concept of a stack specification. A stack specification is a YAML file that describes the stack and includes references to component specification files. A component specification is a YAML file that describes a component. (Currently all deployments of components (in various combinations) must be defined within the context of a stack.)\n\nZenML handles the creation of stack specifications for you when you run one of the `deploy` subcommands using the CLI. A valid specification is generated and used by `mlstacks` to deploy your stack using Terraform. The Terraform definitions and state are stored in your global configuration directory along with any state files generated while deploying your stack.\n\nYour configuration directory could be in a number of different places depending on your operating system, but read more about it in the [Click docs](https://click.palletsprojects.com/en/8.1.x/api/#click.get\\_app\\_dir) to see which location applies to your situation.\n\n## Installing the mlstacks extra\n\nTo install `mlstacks`, either run `pip install mlstacks` or `pip install \"zenml[mlstacks]\"` to install it along with ZenML.\n\nMLStacks uses Terraform on the backend to manage infrastructure. You will need to have Terraform installed. Please visit [the Terraform docs](https://learn.hashicorp.com/tutorials/terraform/install-cli#install-terraform) for installation instructions.\n\nMLStacks also uses Helm to deploy Kubernetes resources. You will need to have Helm installed. Please visit [the Helm docs](https://helm.sh/docs/intro/install/#from-script) for installation instructions.\n\n## Deploying a stack\n\nDeploying an end-to-end stack through the ZenML CLI is only possible with the [deployment wizard which does not use `mlstacks`](../../how-to/stack-deployment/deploy-a-cloud-stack.md). However, you can use `mlstacks` directly to deploy various"}
{"input": " types of stacks and [import them into ZenML](https://mlstacks.zenml.io/reference/zenml).\n\n```shell\nzenml stack import -f <path-to-stack-file-generated-by-mlstacks.yaml>\n```\n\n## Deploying a stack component\n\nIf you have used ZenML before, you must be familiar with the flow of registering new stack components. It goes something like this:\n\n```shell\nzenml artifact-store register my_store --flavor=s3 --path=s3://my_bucket\n```\n\nCommands like these assume that you already have the stack component deployed. In this case, it would mean that you must already have a bucket called `my_bucket` on AWS S3 to be able to use this component.\n\nWe took inspiration from this design to build something that feels natural to use and is also sufficiently powerful to take care of the deployment of the respective stack components for you. This is where the \\<STACK\\_COMPONENT> `deploy` CLI comes in!\n\nThe `deploy` command allows you to deploy individual components of your MLOps stack with a single command \ud83d\ude80. You can also customize your components easily by passing in flags (more on that later).\n\n{% hint style=\"info\" %}\nTo install `mlstacks`, either run `pip install mlstacks` or `pip install \"zenml[mlstacks]\"` to install it along with ZenML.\n\nMLStacks uses Terraform on the backend to manage infrastructure. You will need to have Terraform installed. Please visit [the Terraform docs](https://learn.hashicorp.com/tutorials/terraform/install-cli#install-terraform) for installation instructions.\n\nMLStacks also uses Helm to deploy Kubernetes resources. You will need to have Helm installed. Please visit [the Helm docs](https://helm.sh/docs/intro/install/#from-script) for installation instructions.\n{% endhint %}\n\nFor example, to deploy an artifact store on a GCP account, you can run:\n\n{% code overflow=\"wrap\" %}\n```bash\n# after installing mlstacks\nzenml artifact-store deploy -f gcp -p gcp -r us-east1 -x project_id=zenml my_store\n```\n{% endcode %}\n\nThe command above takes in the following parameters:\n\n* **Name**: The name of the stack component. In this case, it is `my_store`.\n* **Flavor:** The flavor of the stack component to deploy. Here, we are deploying an artifact store with the `gcp` flavor.\n* **Provider:** The provider to deploy this stack component on. Currently, only **GCP, AWS, and K3D** are supported as providers.\n* **Region**: The region to deploy the stack component in.\n* **Extra Config:** Some components can be customized by the user and these settings are passed as flags to the command. In the example above, we pass the GCP project ID to select"}
{"input": " what project to deploy the component to.\n\nSuccessful execution of this command does the following:\n\n* It also automatically registers the deployed stack component with your ZenML server, so you don't have to worry about manually configuring components after the deployment! \ud83e\udd29\n\n{% hint style=\"info\" %}\nThe command currently uses your local credentials for GCP and AWS to provision resources. Integration with your ZenML connectors might be possible soon too!\n{% endhint %}\n\n<details>\n\n<summary>Want to know what happens in the background?</summary>\n\nThe stack component deploy CLI is powered by ZenML's [mlstacks](https://github.com/zenml-io/mlstacks) in the background. This allows you to configure and deploy select stack components.\n\nUsing the values you pass for the cloud, the CLI picks up the right modular recipe to use (one of AWS, GCP, or K3D) and then deploys that recipe with the specific stack component enabled.\n\n</details>\n\n### Destroying a stack component\n\nDestroying a stack component (i.e. deleting and destroying the underlying\ninfrastructure) is as easy as deploying one. You can run the following command\nto destroy the artifact store we created above:\n\n```bash\nzenml artifact-store destroy -p gcp my_store\n```\n\nThis will destroy the deployed infrastructure and prompt you if you also want to remove and deregister the component from your ZenML server.\n\n### \ud83c\udf68 Available flavors for stack components\n\nHere's a table of all the flavors that can be deployed through the CLI for every stack component. This is a list that will keep on growing and you can also contribute any flavor or stack component that you feel is missing. Refer to the [Contribution page](../../../../CONTRIBUTING.md) for steps on how to do that :smile:\n\n<details>\n\n<summary>How does flavor selection work in the background?</summary>\n\nWhenever you pass in a flavor to any stack-component deploy function, the combination of these two parameters is used to construct a variable name in the following format:\n\n```\nenable_<STACK_COMPONENT>_<FLAVOR>\n```\n\nThis variable is then passed as input to the underlying modular recipe. If you check the [`variables.tf`](https://github.com/zenml-io/mlstacks/blob/main/gcp-modular/variables.tf) file for a given recipe, you can find all the supported flavor-stack component combinations there.\n\n</details>\n\n| Component Type     | Flavor(s)                            |\n| ------------------ | ------------------------------------ |\n| Artifact Store     | s3, gcp, minio                       |\n| Container Registry | aws, gcp                             |\n| Experiment Tracker | mlflow                               |\n| Orchestrator       | kubernetes, kubeflow, tekton, vertex |\n| MLOps Platform     | zenml                                |\n| Model Deployer     | seldon                               |\n| Step Operator      | sagemaker, vertex                    |\n\n#### \u2728"}
{"input": " Customizing your stack components\n\nWith simplicity, we didn't want to compromise on the flexibility that this deployment method allows. As such, we have added the option to pass configuration specific to the stack components as key-value arguments to the deploy CLI. Here is an assortment of all possible configurations that can be set.\n\n<details>\n\n<summary>How do configuration flags work?</summary>\n\nThe flags that you pass to the deploy CLI are passed on as-is to the backing modular recipes as input variables. This means that all the flags need to be defined as variables in the respective recipe.\n\nFor example, if you take a look at the [`variables.tf`](https://github.com/zenml-io/mlstacks/blob/main/gcp-modular/variables.tf) file for a modular recipe, like the `gcp-modular` recipe, you can find variables like `mlflow_bucket` that you could potentially pass in.\n\nValidation for these flags does not exist yet at the CLI level, so you must be careful in naming them while calling `deploy`.\n\nAll these extra configuration options are passed in with the `-x` option. For example, we already saw this in action above when we passed in the GCP project ID to the artifact store deploy command.\n\n```bash\nzenml artifact-store deploy -f gcp -p gcp -r us-east1 -x project_id=zenml my_store\n```\n\nSimply pass in as many `-x` flags as you want to customize your stack component.\n\n</details>\n\n**Experiment Trackers**\n\nYou can assign an existing bucket to the MLflow experiment tracker by passing the `-x mlflow_bucket=...` configuration:\n\n```shell\nzenml experiment-tracker deploy mlflow_tracker --flavor=mlflow -p YOUR_DESIRED_PROVIDER -r YOUR_REGION -x mlflow_bucket=gs://my_bucket\n```\n\n**Artifact Stores**\n\nFor an artifact store, you can pass `bucket_name` as an argument to the command.\n\n```bash\nzenml artifact-store deploy s3_artifact_store --flavor=s3 --provider=aws -r YOUR_REGION -x bucket_name=my_bucket\n```\n\n**Container Registries**\n\nFor container registries, you can pass the repository name using `repo_name`:\n\n```bash\nzenml container-registry deploy aws_registry --flavor=aws -p aws -r YOUR_REGION -x repo_name=my_repo\n```\n\nThis is only useful for the AWS case since AWS requires a repository to be created before pushing images to it and the deploy command ensures that a repository with the name you provide is created. In case of GCP and other providers, you can choose the repository name at the same time as you are pushing the image via code. This is achieved through setting the `target_repo` attribute of [the `DockerSettings` object](../customize-docker-builds/README.md).\n\n#### Other configuration\n\n* In the case of GCP components, it"}
{"input": " is _required_ that you pass a project ID to the command as extra configuration when you're creating any GCP resource.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Reference secrets in stack component attributes and settings\n---\n\n# Reference secrets in stack configuration\n\nSome of the components in your stack require you to configure them with sensitive information like passwords or tokens, so they can connect to the underlying infrastructure. Secret references allow you to configure these components in a secure way by not specifying the value directly but instead referencing a secret by providing the secret name and key. Referencing a secret for the value of any string attribute of your stack components, simply specify the attribute using the following syntax: `{{<SECRET_NAME>.<SECRET_KEY>}}`\n\nFor example:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\n# Register a secret called `mlflow_secret` with key-value pairs for the\n# username and password to authenticate with the MLflow tracking server\n\n# Using central secrets management\nzenml secret create mlflow_secret \\\n    --username=admin \\\n    --password=abc123\n    \n\n# Then reference the username and password in our experiment tracker component\nzenml experiment-tracker register mlflow \\\n    --flavor=mlflow \\\n    --tracking_username={{mlflow_secret.username}} \\\n    --tracking_password={{mlflow_secret.password}} \\\n    ...\n```\n{% endtab %}\n{% endtabs %}\n\nWhen using secret references in your stack, ZenML will validate that all secrets and keys referenced in your stack components exist before running a pipeline. This helps us fail early so your pipeline doesn't fail after running for some time due to some missing secret.\n\nThis validation by default needs to fetch and read every secret to make sure that both the secret and the specified key-value pair exist. This can take quite some time and might fail if you don't have permission to read secrets.\n\nYou can use the environment variable `ZENML_SECRET_VALIDATION_LEVEL` to disable or control the degree to which ZenML validates your secrets:\n\n* Setting it to `NONE` disables any validation.\n* Setting it to `SECRET_EXISTS` only validates the existence of secrets. This might be useful if the machine you're running on only has permission to list secrets but not actually read their values.\n* Setting it to `SECRET_AND_KEY_EXISTS` (the default) validates both the secret existence as well as the existence of the exact key-value pair.\n\n### Fetch secret values in a step\n\nIf you are using [centralized secrets management](../interact-with-secrets.md), you can access secrets directly from within your steps through the ZenML `Client` API. This allows you to use your secrets for querying APIs from within your step without hard-coding your access keys:\n\n```python\nfrom zenml import step\nfrom zenml.client import Client\n\n\n@step\ndef secret_loader() -> None:\n    \"\"\"Load the example secret from the server.\"\"\"\n    # Fetch the secret from ZenML.\n    secret = Client().get_secret( < SECRET_NAME >)\n\n    # `secret.secret_values` will contain a dictionary with all key-value\n   "}
{"input": " # pairs within your secret.\n    authenticate_to_some_api(\n        username=secret.secret_values[\"username\"],\n        password=secret.secret_values[\"password\"],\n    )\n    ...\n```\n\n## See Also\n\n- [Interact with secrets](../interact-with-secrets.md): Learn how to create,\n  list, and delete secrets using the ZenML CLI and Python SDK.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Stacks are the configuration of your infrastructure.\n---\n\n# Managing stacks & components\n\nThe [stack](../../user-guide/production-guide/understand-stacks.md) is a fundamental component of the ZenML framework. Put simply, a stack represents the configuration of the infrastructure and tooling that defines where and how a pipeline executes.\n\nHowever, deploying and managing a MLOps stack is tricky \ud83d\ude2d\ud83d\ude35\u200d\ud83d\udcab. It is not trivial to set up all the different tools that you might need for your pipeline.\n\n* \ud83c\udf08 Each tool comes with a certain set of requirements. For example, a Kubeflow installation will require you to have a Kubernetes cluster, and so would a Seldon Core deployment.\n* \ud83e\udd14 Figuring out the defaults for infra parameters is not easy. Even if you have identified the backing infra that you need for a stack component, setting up reasonable defaults for parameters like instance size, CPU, memory, etc., needs a lot of experimentation to figure out.\n* \ud83d\udea7 Many times, standard tool installations don't work out of the box. For example, to run a custom pipeline in Vertex AI, it is not enough to just run an imported pipeline. You might also need a custom service account that is configured to perform tasks like reading secrets from your secret store or talking to other GCP services that your pipeline might need.\n* \ud83d\udd10 Some tools need an additional layer of installations to enable a more secure, production-grade setup. For example, a standard MLflow tracking server deployment comes without an authentication frontend which might expose all of your tracking data to the world if deployed as-is.\n* \ud83d\udde3\ufe0f All the components that you deploy must have the right permissions to be able to talk to each other. When you run your pipeline, it is inevitable that some components would need to communicate with the others. For example, your workloads running in a Kubernetes cluster might require access to the container registry or the code repository, and so on.\n* \ud83e\uddf9 Cleaning up your resources after you're done with your experiments is super important yet very challenging. Many of the components need a range of other resources to work which might slide past your radar if you're not careful. For example, if your Kubernetes cluster has made use of Load Balancers, you might still have one lying around in your account even after deleting the cluster, costing you money and frustration.\n\nAll of these points make taking your pipelines to production a more difficult task than it should be. We believe that the expertise in setting up these often-complex stacks shouldn't be a prerequisite to running your ML pipelines.\n\nThis docs section consists of information that makes it easier to provision, configure, and extend stacks and components in ZenML.\n\n<table data-view=\"cards\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th data-hidden data-card-target data-type=\"content-ref\"></th>\n    </tr"}
{"input": ">\n  </thead>\n  <tbody>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Deploy a cloud stack with ZenML</strong></mark></td>\n      <td>Description of deploying a cloud stack with ZenML.</td>\n      <td><a href=\"./deploy-a-cloud-stack.md\">./deploy-a-cloud-stack.md</a></td>\n    </tr>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Register a cloud stack</strong></mark></td>\n      <td>Description of registering a cloud stack.</td>\n      <td><a href=\"./register-a-cloud-stack.md\">./register-a-cloud-stack.md</a></td>\n    </tr>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Deploy a cloud stack with Terraform</strong></mark></td>\n      <td>Description of deploying a cloud stack with Terraform.</td>\n      <td><a href=\"./deploy-a-cloud-stack-with-terraform.md\">./deploy-a-cloud-stack-with-terraform.md</a></td>\n    </tr>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Deploy stack/components using mlstacks</strong></mark></td>\n      <td>Deploying an entire stack with ZenML's `mlstacks` package.</td>\n      <td><a href=\"./deploy-a-stack-using-mlstacks.md\">./deploy-a-stack-using-mlstacks.md</a></td>\n    </tr>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Reference secrets in stack configuration</strong></mark></td>\n      <td>Description of referencing secrets in stack configuration.</td>\n      <td><a href=\"./reference-secrets-in-stack-configuration.md\">./reference-secrets-in-stack-configuration.md</a></td>\n    </tr>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Implement a custom stack component</strong></mark></td>\n      <td>Creating your custom stack component solutions.</td>\n      <td><a href=\"./implement-a-custom-stack-component.md\">./implement-a-custom-stack-component.md</a></td>\n    </tr>\n    <tr>\n      <td><mark style=\"color:purple;\"><strong>Implement a custom integration</strong></mark></td>\n      <td>Description of implementing a custom integration.</td>\n      <td><a href=\"./implement-a-custom-integration.md\">./implement-a-custom-integration.md</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></"}
{"input": "figure>\n"}
{"input": "---\ndescription: How to write a custom stack component flavor\n---\n\n# Implement a custom stack component\n\nWhen building a sophisticated MLOps Platform, you will often need to come up with custom-tailored solutions for your infrastructure or tooling. ZenML is built around the values of composability and reusability which is why the stack component flavors in ZenML are designed to be modular and straightforward to extend.\n\nThis guide will help you understand what a flavor is, and how you can develop and use your own custom flavors in ZenML.\n\n## Understanding component flavors\n\nIn ZenML, a component type is a broad category that defines the functionality of a stack component. Each type can have multiple flavors, which are specific implementations of the component type. For instance, the type `artifact_store` can have flavors like `local`, `s3`, etc. Each flavor defines a unique implementation of functionality that an artifact store brings to a stack.\n\n## Base Abstractions\n\nBefore we get into the topic of creating custom stack component flavors, let us briefly discuss the three core abstractions related to stack components: the `StackComponent`, the `StackComponentConfig`, and the `Flavor`.\n\n### Base Abstraction 1: `StackComponent`\n\nThe `StackComponent` is the abstraction that defines the core functionality. As an example, check out the `BaseArtifactStore` definition below: The `BaseArtifactStore` inherits from `StackComponent` and establishes the public interface of all artifact stores. Any artifact store flavor needs to follow the standards set by this base class.\n\n```python\nfrom zenml.stack import StackComponent\n\n\nclass BaseArtifactStore(StackComponent):\n    \"\"\"Base class for all ZenML artifact stores.\"\"\"\n\n    # --- public interface ---\n\n    @abstractmethod\n    def open(self, path, mode = \"r\"):\n        \"\"\"Open a file at the given path.\"\"\"\n\n    @abstractmethod\n    def exists(self, path):\n        \"\"\"Checks if a path exists.\"\"\"\n\n    ...\n```\n\nAs each component defines a different interface, make sure to check out the base class definition of the component type that you want to implement and also check out the [documentation on how to extend specific stack components](implement-a-custom-stack-component.md#extending-specific-stack-components).\n\n{% hint style=\"info\" %}\nIf you would like to automatically track some metadata about your custom stack component with each pipeline run, you can do so by defining some additional methods in your stack component implementation class as shown in the [Tracking Custom Stack Component Metadata](../../how-to/track-metrics-metadata/fetch-metadata-within-steps.md) section.\n{% endhint %}\n\nSee the full code of the base `StackComponent` class [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/stack/stack\\_component.py#L301).\n\n### Base Abstraction 2: `StackComponentConfig`\n\nAs the name suggests, the `StackComponentConfig` is used to configure"}
{"input": " a stack component instance. It is separated from the actual implementation on purpose. This way, ZenML can use this class to validate the configuration of a stack component during its registration/update, without having to import heavy (or even non-installed) dependencies.\n\n{% hint style=\"info\" %}\nThe `config` and `settings` of a stack component are two separate, yet related entities. The `config` is the static part of your flavor's configuration, defined when you register your flavor. The `settings` are the dynamic part of your flavor's configuration that can be overridden at runtime.\n\nYou can read more about the differences [here](../use-configuration-files/runtime-configuration.md).\n{% endhint %}\n\nLet us now continue with the base artifact store example from above and take a look at the `BaseArtifactStoreConfig`:\n\n```python\nfrom zenml.stack import StackComponentConfig\n\n\nclass BaseArtifactStoreConfig(StackComponentConfig):\n    \"\"\"Config class for `BaseArtifactStore`.\"\"\"\n\n    path: str\n\n    SUPPORTED_SCHEMES: ClassVar[Set[str]]\n\n    ...\n```\n\nThrough the `BaseArtifactStoreConfig`, each artifact store will require users to define a `path` variable. Additionally, the base config requires all artifact store flavors to define a `SUPPORTED_SCHEMES` class variable that ZenML will use to check if the user-provided `path` is actually supported by the flavor.\n\nSee the full code of the base `StackComponentConfig` class [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/stack/stack\\_component.py#L44).\n\n### Base Abstraction 3: `Flavor`\n\nFinally, the `Flavor` abstraction is responsible for bringing the implementation of a `StackComponent` together with the corresponding `StackComponentConfig` definition and also defines the `name` and `type` of the flavor. As an example, check out the definition of the `local` artifact store flavor below:\n\n```python\nfrom zenml.enums import StackComponentType\nfrom zenml.stack import Flavor\n\n\nclass LocalArtifactStore(BaseArtifactStore):\n    ...\n\n\nclass LocalArtifactStoreConfig(BaseArtifactStoreConfig):\n    ...\n\n\nclass LocalArtifactStoreFlavor(Flavor):\n\n    @property\n    def name(self) -> str:\n        \"\"\"Returns the name of the flavor.\"\"\"\n        return \"local\"\n\n    @property\n    def type(self) -> StackComponentType:\n        \"\"\"Returns the flavor type.\"\"\"\n        return StackComponentType.ARTIFACT_STORE\n\n    @property\n    def config_class(self) -> Type[LocalArtifactStoreConfig]:\n        \"\"\"Config class of this flavor.\"\"\"\n        return LocalArtifactStoreConfig\n\n    @property\n    def implementation_class(self) -> Type[LocalArtifactStore]:\n        \"\"\"Implementation class of this flavor.\"\"\"\n        return LocalArtifactStore\n```\n\nSee the full code of the base `Flavor` class definition [here](https"}
{"input": "://github.com/zenml-io/zenml/blob/main/src/zenml/stack/flavor.py#L29).\n\n## Implementing a Custom Stack Component Flavor\n\nLet's recap what we just learned by reimplementing the `S3ArtifactStore` from the `aws` integration as a custom flavor.\n\nWe can start with the configuration class: here we need to define the `SUPPORTED_SCHEMES` class variable introduced by the `BaseArtifactStore`. We also define several additional configuration values that users can use to configure how the artifact store will authenticate with AWS:\n\n```python\nfrom zenml.artifact_stores import BaseArtifactStoreConfig\nfrom zenml.utils.secret_utils import SecretField\n\n\nclass MyS3ArtifactStoreConfig(BaseArtifactStoreConfig):\n    \"\"\"Configuration for the S3 Artifact Store.\"\"\"\n\n    SUPPORTED_SCHEMES: ClassVar[Set[str]] = {\"s3://\"}\n\n    key: Optional[str] = SecretField(default=None)\n    secret: Optional[str] = SecretField(default=None)\n    token: Optional[str] = SecretField(default=None)\n    client_kwargs: Optional[Dict[str, Any]] = None\n    config_kwargs: Optional[Dict[str, Any]] = None\n    s3_additional_kwargs: Optional[Dict[str, Any]] = None\n```\n\n{% hint style=\"info\" %}\nYou can pass sensitive configuration values as [secrets](../../how-to/interact-with-secrets.md) by defining them as type `SecretField` in the configuration class.\n{% endhint %}\n\nWith the configuration defined, we can move on to the implementation class, which will use the S3 file system to implement the abstract methods of the `BaseArtifactStore`:\n\n```python\nimport s3fs\n\nfrom zenml.artifact_stores import BaseArtifactStore\n\n\nclass MyS3ArtifactStore(BaseArtifactStore):\n    \"\"\"Custom artifact store implementation.\"\"\"\n\n    _filesystem: Optional[s3fs.S3FileSystem] = None\n\n    @property\n    def filesystem(self) -> s3fs.S3FileSystem:\n        \"\"\"Get the underlying S3 file system.\"\"\"\n        if self._filesystem:\n            return self._filesystem\n\n        self._filesystem = s3fs.S3FileSystem(\n            key=self.config.key,\n            secret=self.config.secret,\n            token=self.config.token,\n            client_kwargs=self.config.client_kwargs,\n            config_kwargs=self.config.config_kwargs,\n            s3_additional_kwargs=self.config.s3_additional_kwargs,\n        )\n        return self._filesystem\n\n    def open(self, path, mode: = \"r\"):\n        \"\"\"Custom logic goes here.\"\"\"\n        return self.filesystem.open(path=path, mode=mode)\n\n    def exists(self, path):\n        \"\"\"Custom logic goes here.\"\"\"\n        return self.filesystem.exists(path=path)\n```\n\n{% hint style=\"info\" %}\nThe configuration values defined in the corresponding configuration class are always available in the implementation class under `self.config`.\n{% endhint %}\n\nFinally, let's"}
{"input": " define a custom flavor that brings these two classes together. Make sure that you give your flavor a globally unique name here.\n\n```python\nfrom zenml.artifact_stores import BaseArtifactStoreFlavor\n\n\nclass MyS3ArtifactStoreFlavor(BaseArtifactStoreFlavor):\n    \"\"\"Custom artifact store implementation.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"The name of the flavor.\"\"\"\n        return 'my_s3_artifact_store'\n\n    @property\n    def implementation_class(self):\n        \"\"\"Implementation class for this flavor.\"\"\"\n        from ... import MyS3ArtifactStore\n\n        return MyS3ArtifactStore\n\n    @property\n    def config_class(self):\n        \"\"\"Configuration class for this flavor.\"\"\"\n        from ... import MyS3ArtifactStoreConfig\n\n        return MyS3ArtifactStoreConfig\n```\n\n{% hint style=\"info\" %}\nFor flavors that require additional dependencies, you should make sure to define your implementation, config, and flavor classes in separate Python files and to only import the implementation class inside the `implementation_class` property of the flavor class. Otherwise, ZenML will not be able to load and validate your flavor configuration without the dependencies installed.\n{% endhint %}\n\n## Managing a Custom Stack Component Flavor\n\nOnce you have defined your implementation, config, and flavor classes, you can register your new flavor through the ZenML CLI:\n\n```shell\nzenml artifact-store flavor register <path.to.MyS3ArtifactStoreFlavor>\n```\n\n{% hint style=\"info\" %}\nMake sure to point to the flavor class via dot notation!\n{% endhint %}\n\nFor example, if your flavor class `MyS3ArtifactStoreFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml artifact-store flavor register flavors.my_flavor.MyS3ArtifactStoreFlavor\n```\n\nAfterwards, you should see the new custom artifact store flavor in the list of available artifact store flavors:\n\n```shell\nzenml artifact-store flavor list\n```\n\nAnd that's it! You now have a custom stack component flavor that you can use in your stacks just like any other flavor you used before, e.g.:\n\n```shell\nzenml artifact-store register <ARTIFACT_STORE_NAME> \\\n    --flavor=my_s3_artifact_store \\\n    --path='some-path' \\\n    ...\n\nzenml stack register <STACK_NAME> \\\n    --artifact-store <ARTIFACT_STORE_NAME> \\\n    ...\n```\n\n## Tips and best practices\n\n* ZenML resolves the flavor classes by taking the path where you initialized ZenML (via `zenml init`) as the starting point of resolution. Therefore, you and your team should remember to execute `zenml init` in a consistent manner (usually at the root of the repository where the `.git` folder lives). If the `zenml init` command was not executed, the current working directory is used to find implementation classes, which"}
{"input": " could lead to unexpected behavior.\n* You can use the ZenML CLI to find which exact configuration values a specific flavor requires. Check out [this 3-minute video](https://www.youtube.com/watch?v=CQRVSKbBjtQ) for more information.\n* You can keep changing the `Config` and `Settings` of your flavor after registration. ZenML will pick up these \"live\" changes when running pipelines.\n* Note that changing the config in a breaking way requires an update of the component (not a flavor). E.g., adding a mandatory name to flavor X field will break a registered component of that flavor. This may lead to a completely broken state where one should delete the component and re-register it.\n* Always test your flavor thoroughly before using it in production. Make sure it works as expected and handles errors gracefully.\n* Keep your flavor code clean and well-documented. This will make it easier for others to use and contribute to your flavor.\n* Follow best practices for the language and libraries you're using. This will help ensure your flavor is efficient, reliable, and easy to maintain.\n* We recommend you develop new flavors by using existing flavors as a reference. A good starting point is the flavors defined in the [official ZenML integrations](https://github.com/zenml-io/zenml/tree/main/src/zenml/integrations).\n\n## Extending Specific Stack Components\n\nIf you would like to learn more about how to build a custom stack component flavor for a specific stack component type, check out the links below:\n\n| **Type of Stack Component**                                                                                                                                  | **Description**                                                   |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------- |\n| [Orchestrator](../../component-guide/orchestrators/custom.md)              | Orchestrating the runs of your pipeline                           |\n| [Artifact Store](../../component-guide/artifact-stores/custom.md)          | Storage for the artifacts created by your pipelines               |\n| [Container Registry](../../component-guide/container-registries/custom.md) | Store for your containers                                         |\n| [Step Operator](../../component-guide/step-operators/custom.md)            | Execution of individual steps in specialized runtime environments |\n| [Model Deployer](../../component-guide/model-deployers/custom.md)          | Services/platforms responsible for online model serving           |\n| [Feature Store](../../component-guide/feature-stores/custom.md)            | Management of your data/features                                  |\n| [Experiment Tracker](../../component-guide/experiment-trackers/custom.md)  | Tracking your ML experiments                                      |\n| [Alerter](../../component-guide/alerters/custom.md)                        | Sending alerts through specified channels                         |\n| [Annotator](../../component-guide/annotators/custom.md)                    | Annotating and labeling data                                      |\n| [Data Validator](../../component-guide/data-validators/custom.md)          | Validating and monitoring your data                               |\n\n<figure><img src=\"https://static.scarf"}
{"input": ".sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Seamlessly register a cloud stack by using existing infrastructure\n---\n\nIn ZenML, the [stack](../../user-guide/production-guide/understand-stacks.md) \nis a fundamental concept that represents the configuration of your \ninfrastructure. In a normal workflow, creating a stack requires you to first \ndeploy the necessary pieces of infrastructure and then define them as stack \ncomponents in ZenML with proper authentication.\n\nEspecially in a remote setting, this process can be challenging and \ntime-consuming, and it may create multi-faceted problems. This is why we \nimplemented a feature called the stack wizard, that allows you to **browse \nthrough your existing infrastructure and use it to register a ZenML cloud \nstack**.\n\n{% hint style=\"info\" %}\nIf you do not have the required infrastructure pieces already deployed\non your cloud, you can also use [the 1-click deployment tool to build your \ncloud stack](deploy-a-cloud-stack.md).\n\nAlternatively, if you prefer to have more control over where and how resources\nare provisioned in your cloud, you can [use one of our Terraform modules](deploy-a-cloud-stack-with-terraform.md)\nto manage your infrastructure as code yourself.\n{% endhint %}\n\n# How to use the Stack Wizard?\n\nThe stack wizard is available to you by both our CLI and our dashboard.\n\n{% tabs %}\n{% tab title=\"Dashboard\" %}\n\nIf you are using the dashboard, the stack wizard is available through \nthe stacks page. \n\n![The new stacks page](../../.gitbook/assets/stack-wizard-new-stack.png)\n\nHere you can click on \"+ New Stack\" and choose the option \"Use existing Cloud\".\n\n![New stack options](../../.gitbook/assets/stack-wizard-options.png)\n\nNext, you have to select the cloud provider that you want to work with.\n\n![Stack Wizard Cloud Selection](../../.gitbook/assets/stack-wizard-cloud-selection.png)\n\nChoose one of the possible authentication methods based on your provider and\nfill in the required fields.\n\n![Wizard Example](../../.gitbook/assets/stack-wizard-example.png)\n\n<details>\n\n<summary>AWS: Authentication methods</summary>\n\nThere are several different methods to authenticate with AWS:\n\n![Wizard Example](../../.gitbook/assets/stack-wizard-aws-auth.png)\n\n</details>\n\n{% hint style=\"warning\" %}\nOn the dashboard, the stack wizard only works with AWS. We are working on \nbringing support to GCP and Azure as well. If you would like to use these \nproviders, you can still use the CLI or stay tuned for further updates.\n{% endhint %}\n\nFrom this step forward, ZenML will show you different selections of resources \nthat you can use from your existing infrastructure so that you can create the \nrequired stack components such as an artifact store, an orchestrator, \nand a container registry.\n\n{% endtab %}\n{% tab title=\"CLI\" %}\n\nIn order to register a"}
{"input": " remote stack over the CLI with the stack wizard,\nyou can use the following command:\n\n```shell\nzenml stack register <STACK_NAME> -p {aws|gcp|azure}\n```\n\nTo register the cloud stack, the first thing that the wizard needs is a [service \nconnector](../auth-management/service-connectors-guide.md). You can either use \nan existing connector by providing its ID or name \n`-sc <SERVICE_CONNECTOR_ID_OR_NAME>` (CLI-Only) or the wizard will create one \nfor you.\n\n{% hint style=\"info\" %}\nSimilar to the service connector, if you use the CLI, you can also use existing \nstack components. However, this is only possible if these components are already \nconfigured with the same service connector that you provided through the \nparameter described above.\n{% endhint %}\n\n### Define Service Connector\n\nAs the very first step the configuration wizard will check if the selected \ncloud provider credentials can be acquired automatically from the local environment.\nIf the credentials are found, you will be offered to use them or proceed to \nmanual configuration.\n\n{% code title=\"Example prompt for AWS auto-configuration\" %}\n```\nAWS cloud service connector has detected connection \ncredentials in your environment.\nWould you like to use these credentials or create a new \nconfiguration by providing connection details? [y/n] (y):\n```\n{% endcode %}\n\nIf you decline auto-configuration next you might be offered the list of already \ncreated service connectors available on the server: pick one of them and proceed or pick \n`0` to create a new one.\n\n<details>\n<summary>AWS: Authentication methods</summary>\n\nIf you select `aws` as your cloud provider, and you haven't selected a connector\nor declined auto-configuration, you will be prompted to select an authentication \nmethod for your cloud connector.\n\n{% code title=\"Available authentication methods for AWS\" %}\n```\n                  Available authentication methods for AWS                   \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Choice  \u2503 Name                           \u2503 Required                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 [0]     \u2502 AWS Secret Key                 \u2502 aws_access_key_id  (AWS Access \u2502\n\u2502         \u2502                                \u2502 Key ID)                        \u2502\n\u2502         \u2502                                \u2502 aws_secret_access_key  (AWS    \u2502\n\u2502         \u2502                                \u2502 Secret Access Key)             \u2502\n\u2502         \u2502                                \u2502 region  (AWS Region)           \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [1]     \u2502 AWS STS Token                  \u2502 aws_access_key_id  (AWS Access \u2502\n\u2502         \u2502                                \u2502 Key ID)                        \u2502\n\u2502         \u2502                                \u2502 aws_secret_access_key  (AWS    \u2502\n\u2502         \u2502                                \u2502 Secret Access Key)             \u2502\n\u2502         \u2502                                \u2502 aws_session_token  (AWS        \u2502\n\u2502         \u2502                                \u2502 Session Token)                 \u2502\n\u2502         \u2502                                \u2502 region  (AWS Region)           \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [2]     \u2502 AWS IAM Role                   \u2502 aws_access_key_id  (AWS Access \u2502\n\u2502         \u2502                                \u2502 Key ID)                        \u2502\n\u2502         \u2502                                \u2502 aws_secret_access_key  (AWS    \u2502\n\u2502         \u2502                                \u2502 Secret Access Key)             \u2502\n\u2502         \u2502                                \u2502 region  (AWS Region)           \u2502\n\u2502         \u2502                                \u2502 role_arn  (AWS IAM Role ARN)   \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [3]     \u2502 AWS Session Token              \u2502 aws_access_key_id  (AWS Access \u2502\n\u2502         \u2502                                \u2502 Key ID)                        \u2502\n\u2502         \u2502                                \u2502 aws_secret_access_key  (AWS    \u2502\n\u2502         \u2502                                \u2502 Secret Access Key)             \u2502\n\u2502         \u2502                                \u2502 region  (AWS Region)           \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [4]     \u2502 AWS Federation Token           \u2502 aws_access_key_id  (AWS Access \u2502\n\u2502         \u2502                                \u2502 Key ID)                        \u2502\n\u2502         \u2502                                \u2502 aws_secret_access_key  (AWS    \u2502\n\u2502         \u2502                                \u2502 Secret Access Key)             \u2502\n\u2502         \u2502                                \u2502 region  (AWS Region)           \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n{% endcode %}\n</details>\n\n<details>\n<summary>GCP: Authentication methods</summary>\n\n\n\nIf you select `gcp` as your cloud provider, and you haven't selected a connector\nor declined auto-configuration, you will be prompted to select an authentication \nmethod for your cloud connector.\n\n{% code title=\"Available authentication methods for GCP\" %}\n```\n                  Available authentication methods for GCP                   \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Choice  \u2503 Name                           \u2503 Required                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 [0]     \u2502 GCP User Account               \u2502 user_account_json  (GCP User   \u2502\n\u2502         \u2502                                \u2502 Account Credentials JSON       \u2502\n\u2502         \u2502                                \u2502 optionally base64 encoded.)    \u2502\n\u2502         \u2502                                \u2502 project_id  (GCP Project ID    \u2502\n\u2502         \u2502                                \u2502 where the target resource is   \u2502\n\u2502         \u2502                                \u2502 located.)                      \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [1]     \u2502 GCP Service Account            \u2502 service_account_json  (GCP     \u2502\n\u2502         \u2502                                \u2502 Service Account Key JSON       \u2502\n\u2502         \u2502                                \u2502 optionally base64 encoded.)    \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [2]     \u2502 GCP External Account           \u2502 external_account_json  (GCP    \u2502\n\u2502         \u2502                                \u2502 External Account JSON          \u2502\n\u2502         \u2502                                \u2502 optionally base64 encoded.)    \u2502\n\u2502         \u2502                                \u2502 project_id  (GCP Project ID    \u2502\n\u2502         \u2502                                \u2502 where the target resource is   \u2502\n\u2502         \u2502                                \u2502 located.)                      \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [3]     \u2502 GCP Oauth 2.0 Token            \u2502 token  (GCP OAuth 2.0 Token)   \u2502\n\u2502         \u2502                                \u2502 project_id  (GCP Project ID    \u2502\n\u2502         \u2502                                \u2502 where the target resource is   \u2502\n\u2502         \u2502                                \u2502 located.)                      \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [4]     \u2502 GCP Service Account            \u2502 service_account_json  (GCP     \u2502\n\u2502         \u2502 Impersonation                  \u2502 Service Account Key JSON       \u2502\n\u2502         \u2502                                \u2502 optionally base64 encoded.)    \u2502\n\u2502         \u2502                                \u2502 target_principal  (GCP Service \u2502\n\u2502         \u2502                                \u2502 Account Email to impersonate)  \u2502\n\u2502         \u2502                                \u2502                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n{% endcode %}\n</details>\n\n<details>\n<summary>Azure: Authentication methods</summary>\n\nIf you select `azure` as your cloud provider, and you haven't selected a \nconnector or declined auto-configuration, you will be prompted to select an \nauthentication method"}
{"input": " for your cloud connector.\n\n{% code title=\"Available authentication methods for Azure\" %}\n```\n    Available authentication methods for AZURE                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Choice \u2503 Name                    \u2503 Required                           \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 [0]    \u2502 Azure Service Principal \u2502 client_secret  (Service principal  \u2502\n\u2502        \u2502                         \u2502 client secret)                     \u2502\n\u2502        \u2502                         \u2502 tenant_id  (Azure Tenant ID)       \u2502\n\u2502        \u2502                         \u2502 client_id  (Azure Client ID)       \u2502\n\u2502        \u2502                         \u2502                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [1]    \u2502 Azure Access Token      \u2502 token  (Azure Access Token)        \u2502\n\u2502        \u2502                         \u2502                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n{% endcode %}\n\n</details>\n\n### Defining cloud components\n\nNext, you will define three major components of your target stack:\n- artifact store\n- orchestrator\n- container registry\n\nAll three are crucial for a basic cloud stack. Extra components can be added later \nif they are needed.\n\nFor each component, you will be asked:\n- if you would like to reuse one of the existing components connected via a defined \nservice connector (if any)\n\n{% code title=\"Example Command Output for available orchestrator\" %}\n```\n                    Available orchestrator\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Choice           \u2503 Name                                               \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 [0]              \u2502 Create a new orchestrator                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [1]              \u2502 existing_orchestrator_1                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [2]              \u2502 existing_orchestrator_2                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n{% endcode %}\n\n- to create a new one from available to the service connector resources \n(if the existing not picked)\n\n{%"}
{"input": " code title=\"Example Command Output for Artifact Stores\" %}\n```\n                        Available GCP storages                            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Choice        \u2503 Storage                                               \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 [0]           \u2502 gs://***************************                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [1]           \u2502 gs://***************************                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n{% endcode %}\n\nBased on your selection, ZenML will create the stack component and ultimately \nregister the stack for you.\n\n{% endtab %}\n\n{% endtabs %}\n\nThere you have it! Through the wizard, you just registered a cloud stack \nand, you can start running your pipelines on a remote setting.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploy a cloud stack from scratch with a single click\n---\n\n# Deploy a cloud stack with a single click\n\nIn ZenML, the [stack](../../user-guide/production-guide/understand-stacks.md) \nis a fundamental concept that represents the configuration of your \ninfrastructure. In a normal workflow, creating a stack\nrequires you to first deploy the necessary pieces of infrastructure and then \ndefine them as stack components in ZenML with proper authentication.\n\nEspecially in a remote setting, this process can be challenging and \ntime-consuming, and it may create multi-faceted problems. This is why we \nimplemented a feature, that allows you to **deploy the necessary pieces of \ninfrastructure on your selected cloud provider and get you started on remote \nstack with a single click**.\n\n{% hint style=\"info\" %}\nIf you prefer to have more control over where and how resources are provisioned\nin your cloud, you can [use one of our Terraform modules](deploy-a-cloud-stack-with-terraform.md)\nto manage your infrastructure as code yourself.\n\nIf you have the required infrastructure pieces already deployed on your cloud, \nyou can also use [the stack wizard to seamlessly register your stack](../../how-to/stack-deployment/register-a-cloud-stack.md).\n{% endhint %}\n\n## How to use the 1-click deployment tool?\n\nThe first thing that you need in order to use this feature is a\ndeployed instance of ZenML (not a local server via `zenml up`). If you do \nnot already have it set up for you, feel free to learn how to do so\n[here](../../getting-started/deploying-zenml/README.md).\n\nOnce you are connected to your deployed ZenML instance, you can use the \n1-click deployment tool either through the dashboard or the CLI:\n\n{% tabs %}\n{% tab title=\"Dashboard\" %}\n\nIn order to create a remote stack over the dashboard go to the stacks page \non the dashboard and click \"+ New Stack\".\n\n![The new stacks page](../../.gitbook/assets/register_stack_button.png)\n\nSince we will be deploying it from scratch, select \"New Infrastructure\" on the\nnext page:\n\n![Options for registering a stack](../../.gitbook/assets/register_stack_page.png)\n\n![Choosing a cloud provider](../../.gitbook/assets/deploy_stack_selection.png)\n\n### AWS\n\nIf you choose `aws` as your provider, you will see a page where you will have \nto select a region and a name for your new stack:\n\n![Configuring the new stack](../../.gitbook/assets/deploy_stack_aws.png)\n\nOnce the configuration is finished, you will see a deployment page:\n\n![Deploying the new stack](../../.gitbook/assets/deploy_stack_aws_2.png)\n\nClicking on the \"Deploy in AWS\" button will redirect you to a Cloud Formation page on AWS Console. \n\n![Cloudformation page](../../.gitbook"}
{"input": "/assets/deploy_stack_aws_cloudformation_intro.png)\n\nYou will have to log in to your AWS account, review and confirm the \npre-filled configuration and create the stack.\n\n![Finalizing the new stack](../../.gitbook/assets/deploy_stack_aws_cloudformation.png)\n\n### GCP\n\nIf you choose `gcp` as your provider, you will see a page where you will have \nto select a region and a name for your new stack:\n\n![Deploy GCP Stack - Step 1](../../.gitbook/assets/deploy_stack_gcp.png)\n![Deploy GCP Stack - Step 1 Continued](../../.gitbook/assets/deploy_stack_gcp_2.png)\n\nOnce the configuration is finished, you will see a deployment page:\n\n![Deploy GCP Stack - Step 2](../../.gitbook/assets/deploy_stack_gcp_3.png)\n\nMake note of the configuration values provided to you in the ZenML dashboard. You will need these in the next step.\n\nClicking on the \"Deploy in GCP\" button will redirect you to a Cloud Shell session on GCP.\n\n![GCP Cloud Shell start page](../../.gitbook/assets/deploy_stack_gcp_cloudshell_start.png)\n\n{% hint style=\"warning\" %}\nThe Cloud Shell session will warn you that the ZenML GitHub repository is\nuntrusted. We recommend that you review\n[the contents of the repository](https://github.com/zenml-io/zenml/tree/main/infra/gcp)\nand then check the `Trust repo` checkbox to proceed with the deployment,\notherwise the Cloud Shell session will not be authenticated to access your\nGCP projects. You will also get a chance to review the scripts that will be\nexecuted in the Cloud Shell session before proceeding.\n{% endhint %}\n\n![GCP Cloud Shell intro](../../.gitbook/assets/deploy_stack_gcp_cloudshell_intro.png)\n\nAfter the Cloud Shell session starts, you will be guided through the process of\nauthenticating with GCP, configuring your deployment, and finally provisioning\nthe resources for your new GCP stack using Deployment Manager.\n\nFirst, you will be asked to create or choose an existing GCP project with billing\nenabled and to configure your terminal with the selected project:\n\n![GCP Cloud Shell tutorial step 1](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_1.png)\n\nNext, you will be asked to configure your deployment by pasting the configuration\nvalues that were provided to you earlier in the ZenML dashboard. You may need to switch back to the ZenML dashboard to copy these values if you did not do so earlier:\n\n![GCP Cloud Shell tutorial step 2](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_2.png)\n\n![Deploy GCP Stack pending](../../.gitbook/assets/deploy_stack_pending.png)\n\nYou can take this opportunity to review the script that will be executed at the\nnext step"}
{"input": ". You will notice that this script starts by enabling some necessary\nGCP service APIs and configuring some basic permissions for the service accounts\ninvolved in the stack deployment, and then deploys the stack using a GCP\nDeployment Manager template. You can proceed with the deployment by running the\nscript in your terminal:\n\n![GCP Cloud Shell tutorial step 3](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_3.png)\n\nThe script will deploy a GCP Deployment Manager template that provisions the\nnecessary resources for your new GCP stack and automatically registers the stack\nwith your ZenML server. You can monitor the progress of the deployment in your\nGCP console:\n\n![GCP Deployment Manager progress](../../.gitbook/assets/deploy_stack_gcp_dm_progress.png)\n\nOnce the deployment is complete, you may close the Cloud Shell session and return\nto the ZenML dashboard to view the newly created stack:\n\n![GCP Cloud Shell tutorial step 4](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_4.png)\n\n![GCP Stack dashboard output](../../.gitbook/assets/deploy_stack_gcp_dashboard_output.png)\n\n### Azure\n\n{% hint style=\"warning\" %}\nCurrently, the 1-click deployment for Azure is only supported in the CLI. We are\nworking on bringing support to the dashboard as well. Stay in touch for further\nupdates.\n{% endhint %}\n\n{% endtab %}\n{% tab title=\"CLI\" %}\n\nIn order to create a remote stack over the CLI, you can use the following \ncommand:\n\n```shell\nzenml stack deploy -p {aws|gcp|azure}\n```\n\n### AWS\n\nIf you choose `aws` as your provider, the command will walk you through\ndeploying a Cloud Formation stack on AWS. It will start by showing some\ninformation about the stack that will be created:\n\n![CLI AWS stack deploy](../../.gitbook/assets/deploy_stack_aws_cli.png)\n\nUpon confirmation, the command will redirect you to a Cloud Formation page on\nAWS Console where you will have to deploy the stack:\n\n![Cloudformation page](../../.gitbook/assets/deploy_stack_aws_cloudformation_intro.png)\n\nYou will have to log in to your AWS account, have permission to deploy an AWS \nCloud Formation stack, review and confirm the pre-filled configuration and \ncreate the stack.\n\n![Finalizing the new stack](../../.gitbook/assets/deploy_stack_aws_cloudformation.png)\n\nThe Cloud Formation stack will provision the necessary resources for your new\nAWS stack and automatically register the stack with your ZenML server. You can\nmonitor the progress of the stack in your AWS console:\n\n![AWS Cloud Formation progress](../../.gitbook/assets/deploy_stack_aws_cf_progress.png)\n\nOnce the provisioning is complete, you may close the AWS Cloud Formation page\nand return to the ZenML CLI to view the newly created stack:\n\n![AWS Stack CLI output]("}
{"input": "../../.gitbook/assets/deploy_stack_aws_cli_output.png)\n\n\n### GCP\n\nIf you choose `gcp` as your provider, the command will walk you through\ndeploying a Deployment Manager template on GCP. It will start by showing some\ninformation about the stack that will be created:\n\n![CLI GCP stack deploy](../../.gitbook/assets/deploy_stack_gcp_cli.png)\n\nUpon confirmation, the command will redirect you to a Cloud Shell session on GCP.\n\n![GCP Cloud Shell start page](../../.gitbook/assets/deploy_stack_gcp_cloudshell_start.png)\n\n{% hint style=\"warning\" %}\nThe Cloud Shell session will warn you that the ZenML GitHub repository is\nuntrusted. We recommend that you review\n[the contents of the repository](https://github.com/zenml-io/zenml/tree/main/infra/gcp)\nand then check the `Trust repo` checkbox to proceed with the deployment,\notherwise the Cloud Shell session will not be authenticated to access your\nGCP projects. You will also get a chance to review the scripts that will be\nexecuted in the Cloud Shell session before proceeding.\n{% endhint %}\n\n![GCP Cloud Shell intro](../../.gitbook/assets/deploy_stack_gcp_cloudshell_intro.png)\n\nAfter the Cloud Shell session starts, you will be guided through the process of\nauthenticating with GCP, configuring your deployment, and finally provisioning\nthe resources for your new GCP stack using Deployment Manager.\n\nFirst, you will be asked to create or choose an existing GCP project with billing\nenabled and to configure your terminal with the selected project:\n\n![GCP Cloud Shell tutorial step 1](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_1.png)\n\nNext, you will be asked to configure your deployment by pasting the configuration\nvalues that were provided to you in the ZenML CLI. You may need to switch back\nto the ZenML CLI to copy these values if you did not do so earlier:\n\n![GCP Cloud Shell tutorial step 2](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_2.png)\n\nYou can take this opportunity to review the script that will be executed at the\nnext step. You will notice that this script starts by enabling some necessary\nGCP service APIs and configuring some basic permissions for the service accounts\ninvolved in the stack deployment and then deploys the stack using a GCP\nDeployment Manager template. You can proceed with the deployment by running the\nscript in your terminal:\n\n![GCP Cloud Shell tutorial step 3](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_3.png)\n\nThe script will deploy a GCP Deployment Manager template that provisions the\nnecessary resources for your new GCP stack and automatically registers the stack\nwith your ZenML server. You can monitor the progress of the deployment in your\nGCP console:\n\n![GCP Deployment"}
{"input": " Manager progress](../../.gitbook/assets/deploy_stack_gcp_dm_progress.png)\n\nOnce the deployment is complete, you may close the Cloud Shell session and return\nto the ZenML CLI to view the newly created stack:\n\n![GCP Cloud Shell tutorial step 4](../../.gitbook/assets/deploy_stack_gcp_cloudshell_step_4.png)\n\n![GCP Stack CLI output](../../.gitbook/assets/deploy_stack_gcp_cli_output.png)\n\n### Azure\n\nIf you choose `azure` as your provider, the command will walk you through\ndeploying [the ZenML Azure Stack Terraform module](https://registry.terraform.io/modules/zenml-io/zenml-stack/azure).\nIt will start by showing some information about the stack that will be created:\n\n![CLI Azure stack deploy](../../.gitbook/assets/deploy_stack_azure_cli.png)\n\nUpon confirmation, the command will redirect you to a Cloud Shell session on Azure.\n\n![Azure Cloud Shell page](../../.gitbook/assets/deploy_stack_azure_cloudshell.png)\n\nAfter the Cloud Shell session starts, you will have to use Terraform to deploy\nthe stack, as instructed by the CLI.\n\nFirst, you will have to open a file named `main.tf` in the Cloud Shell session\nusing the editor of your choice (e.g. `vim`, `nano`) and paste in the Terraform\nconfiguration provided by the CLI. You may need to switch back to the ZenML CLI\nto copy these values if you did not do so earlier:\n\n![Azure Cloud Shell Terraform Configuration File](../../.gitbook/assets/deploy_stack_azure_cloudshell_create_file.png)\n\nThe Terraform file is a simple configuration that uses [the ZenML Azure Stack Terraform module](https://registry.terraform.io/modules/zenml-io/zenml-stack/azure)\nto deploy the necessary resources for your Azure stack and then automatically\nregister the stack with your ZenML server. You can read more about the module\nand its configuration options in the module's documentation.\n\nYou can proceed with the deployment by running the `terraform init` and\n`terraform apply` Terraform commands in your terminal:\n\n![Azure Cloud Shell Terraform Init](../../.gitbook/assets/deploy_stack_azure_cloudshell_terraform_init.png)\n![Azure Cloud Shell Terraform Apply](../../.gitbook/assets/deploy_stack_azure_cloudshell_terraform_apply.png)\n\nOnce the Terraform deployment is complete, you may close the Cloud Shell\nsession and return to the ZenML CLI to view the newly created stack:\n\n![Azure Cloud Shell Terraform Outputs](../../.gitbook/assets/deploy_stack_azure_cloudshell_terraform_ouputs.png)\n![Azure Stack CLI output](../../.gitbook/assets/deploy_stack_azure_cli_output.png)\n\n{% endtab %}\n{% endtabs %}\n\n## What will be deployed?\n\nHere is an overview of the infrastructure that the 1-click deployment will\nprepare for"}
{"input": " you based on your cloud provider:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n\n### Resources\n\n- An S3 bucket that will be used as a ZenML Artifact Store.\n- An ECR container registry that will be used as a ZenML Container Registry.\n- Permissions to use SageMaker as a ZenML Orchestrator.\n- An IAM user and IAM role with the minimum necessary permissions to access \nthe resources listed above.\n- An AWS access key used to give access to ZenML to connect to the above \nresources through a ZenML service connector.\n\n### Permissions\n\nThe configured IAM service account and AWS access key will grant ZenML the\nfollowing AWS permissions in your AWS account:\n\n* S3 Bucket:\n  * s3:ListBucket\n  * s3:GetObject\n  * s3:PutObject\n  * s3:DeleteObject\n* ECR Repository:\n  * ecr:DescribeRepositories\n  * ecr:ListRepositories\n  * ecr:DescribeRegistry\n  * ecr:BatchGetImage\n  * ecr:DescribeImages\n  * ecr:BatchCheckLayerAvailability\n  * ecr:GetDownloadUrlForLayer\n  * ecr:InitiateLayerUpload\n  * ecr:UploadLayerPart\n  * ecr:CompleteLayerUpload\n  * ecr:PutImage\n  * ecr:GetAuthorizationToken\n* SageMaker (Client):\n  * sagemaker:CreatePipeline\n  * sagemaker:StartPipelineExecution\n  * sagemaker:DescribePipeline\n  * sagemaker:DescribePipelineExecution\n* SageMaker (Jobs):\n  * AmazonSageMakerFullAccess\n\n{% endtab %}\n{% tab title=\"GCP\" %}\n\n### Resources\n\n- A GCS bucket that will be used as a ZenML Artifact Store.\n- A GCP Artifact Registry that will be used as a ZenML Container Registry.\n- Permissions to use Vertex AI as a ZenML Orchestrator.\n- Permissions to use GCP Cloud Builder as a ZenML Image Builder.\n- A GCP Service Account with the minimum necessary permissions to access \nthe resources listed above.\n- An GCP Service Account access key used to give access to ZenML to connect to\nthe above  resources through a ZenML service connector.\n\n### Permissions\n\nThe configured GCP service account and its access key will grant ZenML the\nfollowing GCP permissions in your GCP project:\n\n* GCS Bucket:\n  * roles/storage.objectUser\n* GCP Artifact Registry:\n  * roles/artifactregistry.createOnPushWriter\n* Vertex AI (Client):\n  * roles/aiplatform.user\n* Vertex AI (Jobs):\n  * roles/aiplatform.serviceAgent\n* Cloud Build (Client):\n  * roles/cloudbuild.builds.editor\n\n{% endtab %}\n{% tab title=\"Azure\" %}\n\n### Resources\n\n- An Azure Resource Group to contain all the"}
{"input": " resources required for the ZenML stack\n- An Azure Storage Account and Blob Storage Container that will be used as a ZenML Artifact Store.\n- An Azure Container Registry that will be used as a ZenML Container Registry.\n- An AzureML Workspace that will be used as a ZenML Orchestrator and ZenML Step Operator. A Key Vault and Application Insights instance will also be created in the same Resource Group and used to construct the AzureML Workspace.\n- An Azure Service Principal with the minimum necessary permissions to access\nthe above resources.\n- An Azure Service Principal client secret used to give access to ZenML to\nconnect to the above resources through a ZenML service connector.\n\n### Permissions\n\nThe configured Azure service principal and its client secret will grant ZenML\nthe following permissions in your Azure subscription:\n\n* permissions granted for the created Storage Account:\n  * Storage Blob Data Contributor\n* permissions granted for the created Container Registry:\n  * AcrPull\n  * AcrPush\n  * Contributor\n* permissions granted for the created AzureML Workspace:\n  * AzureML Compute Operator\n  * AzureML Data Scientist\n\n{% endtab %}\n{% endtabs %}\n\nThere you have it! With a single click, you just deployed a cloud stack \nand, you can start running your pipelines on a remote setting.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Specify pip dependencies and apt packages\n\n{% hint style=\"warning\" %}\nThe configuration for specifying pip and apt dependencies only works in the remote pipeline case, and is disregarded for local pipelines (i.e. pipelines that run locally without having to build a Docker image).\n{% endhint %}\n\nWhen a [pipeline is run with a remote orchestrator](../configure-python-environments/README.md) a [Dockerfile](https://docs.docker.com/engine/reference/builder/) is dynamically generated at runtime. It is then used to build the Docker image using the [image builder](../configure-python-environments/README.md#-configure-python-environments) component of your stack.\n\nBy default, ZenML automatically installs all packages required by your active ZenML stack. However, you can specify additional packages to be installed in various ways:\n\n* Install all the packages in your local Python environment (This will use the `pip` or `poetry` package manager to get a list of your local packages):\n\n```python\n# or use \"poetry_export\"\ndocker_settings = DockerSettings(replicate_local_python_environment=\"pip_freeze\")\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\nIf required, a custom command can be provided. This command must output a list of requirements following the format of the [requirements file](https://pip.pypa.io/en/stable/reference/requirements-file-format/):\n\n```python\ndocker_settings = DockerSettings(replicate_local_python_environment=[\n    \"poetry\",\n    \"export\",\n    \"--extras=train\",\n    \"--format=requirements.txt\"\n])\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n*   Specify a list of requirements in code:\n\n    ```python\n    docker_settings = DockerSettings(requirements=[\"torch==1.12.0\", \"torchvision\"])\n\n    @pipeline(settings={\"docker\": docker_settings})\n    def my_pipeline(...):\n        ...\n    ```\n*   Specify a requirements file:\n\n    ```python\n    docker_settings = DockerSettings(requirements=\"/path/to/requirements.txt\")\n\n    @pipeline(settings={\"docker\": docker_settings})\n    def my_pipeline(...):\n        ...\n    ```\n* Specify a list of [ZenML integrations](../../component-guide/README.md) that you're using in your pipeline:\n\n```python\nfrom zenml.integrations.constants import PYTORCH, EVIDENTLY\n\ndocker_settings = DockerSettings(required_integrations=[PYTORCH, EVIDENTLY])\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n*   Specify a list of apt packages in code:\n\n    ```python\n    docker_settings = DockerSettings(apt_packages=[\"git\"])\n\n    @pipeline(settings={\"docker\": docker_settings})\n    def my_pipeline(...):\n        ...\n    ```\n*   Prevent ZenML from automatically installing the requirements of your stack:\n\n    ```python\n    docker_settings = DockerSettings(inst"}
{"input": "all_stack_requirements=False)\n\n      @pipeline(settings={\"docker\": docker_settings})\n      def my_pipeline(...):\n          ...\n    ```\n* In some cases the steps of your pipeline will have conflicting requirements or some steps of your pipeline will require large dependencies that don't need to be installed to run the remaining steps of your pipeline. For this case, ZenML allows you to specify custom Docker settings for steps in your pipeline.\n\n```python\ndocker_settings = DockerSettings(requirements=[\"tensorflow\"])\n\n\n@step(settings={\"docker\": docker_settings})\ndef my_training_step(...):\n    ...\n```\n\n{% hint style=\"info\" %}\nYou can combine these methods but do make sure that your list of requirements does not overlap with the ones specified explicitly in the Docker settings.\n{% endhint %}\n\nDepending on the options specified in your Docker settings, ZenML installs the requirements in the following order (each step optional):\n\n* The packages installed in your local python environment\n* The packages required by the stack unless this is disabled by setting `install_stack_requirements=False`.\n* The packages specified via the `required_integrations`\n* The packages specified via the `requirements` attribute\n* You can specify additional arguments for the installer used to install your Python packages as follows:\n```python\n# This will result in a `pip install --timeout=1000 ...` call when installing packages in the\n# Docker image\ndocker_settings = DockerSettings(python_package_installer_args={\"timeout\": 1000})\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n* **Experimental**: If you want to use [`uv`](https://github.com/astral-sh/uv) for faster resolving and installation of your Python packages, you can use by it as follows:\n\n```python\ndocker_settings = DockerSettings(python_package_installer=\"uv\")\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n{% hint style=\"info\" %}\n`uv` is a relatively new project and not as stable as `pip` yet, which might lead to errors during package installation. If this happens, try switching the installer back to `pip` and see if that solves the issue.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Using Docker images to run your pipeline.\n---\n\n# Specify Docker settings for a pipeline\n\nWhen a [pipeline is run with a remote orchestrator](../configure-python-environments/README.md) a [Dockerfile](https://docs.docker.com/engine/reference/builder/) is dynamically generated at runtime. It is then used to build the Docker image using the [image builder](../configure-python-environments/README.md#image-builder-environment) component of your stack. The Dockerfile consists of the following steps:\n\n* **Starts from a parent image** that has **ZenML installed**. By default, this will use the [official ZenML image](https://hub.docker.com/r/zenmldocker/zenml/) for the Python and ZenML version that you're using in the active Python environment. If you want to use a different image as the base for the following steps, check out [this guide](./docker-settings-on-a-pipeline.md#using-a-custom-parent-image).\n* **Installs additional pip dependencies**. ZenML will automatically detect which integrations are used in your stack and install the required dependencies. If your pipeline needs any additional requirements, check out our [guide on including custom dependencies](specify-pip-dependencies-and-apt-packages.md).\n* **Optionally copies your source files**. Your source files need to be available inside the Docker container so ZenML can execute your step code. Check out [this section](./which-files-are-built-into-the-image.md) for more information on how you can customize how ZenML handles your source files in Docker images.\n* **Sets user-defined environment variables.**\n\nThe process described above is automated by ZenML and covers the most basic use cases. This section covers various ways to customize the Docker build process to fit your needs.\n\nFor a full list of configuration options, check out [the DockerSettings object on the SDKDocs](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings).\n\n## How to configure settings for a pipeline\n\nCustomizing the Docker builds for your pipelines and steps is done using the [DockerSettings](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings) class which you can import like this:\n\n```python\nfrom zenml.config import DockerSettings\n```\n\nThere are many ways in which you can supply these settings:\n\n* Configuring them on a pipeline applies the settings to all steps of that pipeline:\n\n```python\nfrom zenml.config import DockerSettings\ndocker_settings = DockerSettings()\n\n# Either add it to the decorator\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline() -> None:\n    my_step()\n\n# Or configure the pipelines options\nmy_pipeline = my_pipeline.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n* Configuring them on a step gives you more fine"}
{"input": "-grained control and enables you to build separate specialized Docker images for different steps of your pipelines:\n\n```python\ndocker_settings = DockerSettings()\n\n# Either add it to the decorator\n@step(settings={\"docker\": docker_settings})\ndef my_step() -> None:\n    pass\n\n# Or configure the step options\nmy_step = my_step.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n* Using a YAML configuration file as described [here](../use-configuration-files/README.md):\n\n```yaml\nsettings:\n    docker:\n        ...\n\nsteps:\n  step_name:\n    settings:\n        docker:\n            ...\n```\n\nCheck out [this page](../use-configuration-files/configuration-hierarchy.md) for more information on the hierarchy and precedence of the various ways in which you can supply the settings.\n\n### Specifying Docker build options\n\nIf you want to specify build options that get passed to the build method of the [image builder](../configure-python-environments/README.md#image-builder-environment). For the default local image builder, these options get passed to the [`docker build` command](https://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.build).\n\n```python\ndocker_settings = DockerSettings(build_config={\"build_options\": {...}})\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n{% hint style=\"info\" %}\nIf you're running your pipelines on MacOS with ARM architecture, the local Docker caching does not work unless you specify the target platform of the image:\n```python\ndocker_settings = DockerSettings(build_config={\"build_options\": {\"platform\": \"linux/amd64\"}})\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n{% endhint %}\n\n### Using a custom parent image\n\nBy default, ZenML performs all the steps described above on top of the [official ZenML image](https://hub.docker.com/r/zenmldocker/zenml/) for the Python and ZenML version in the active Python environment. To have more control over the entire environment used to execute your pipelines, you can either specify a custom pre-built parent image or a Dockerfile that ZenML uses to build a parent image for you.\n\n{% hint style=\"info\" %}\nIf you're going to use a custom parent image (either pre-built or by specifying a Dockerfile), you need to make sure that it has Python, pip, and ZenML installed for it to work. If you need a starting point, you can take a look at the Dockerfile that ZenML uses [here](https://github.com/zenml-io/zenml/blob/main/docker/base.Dockerfile).\n{% endhint %}\n\n#### Using a pre-built parent image\n\nTo use a static parent image (e.g., with internal dependencies installed) that doesn't need to be rebuilt on every pipeline run, specify it in the Docker settings for your pipeline:\n\n"}
{"input": "```python\ndocker_settings = DockerSettings(parent_image=\"my_registry.io/image_name:tag\")\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\nTo use this image directly to run your steps without including any code or installing any requirements on top of it, skip the Docker builds by specifying it in the Docker settings:\n\n```python\ndocker_settings = DockerSettings(\n    parent_image=\"my_registry.io/image_name:tag\",\n    skip_build=True\n)\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n{% hint style=\"warning\" %}\nThis is an advanced feature and may cause unintended behavior when running your pipelines. If you use this, ensure your code files are correctly included in the image you specified.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Using Docker images to run your pipeline.\n---\n\n# \ud83d\udc33 Customize Docker Builds\n\nZenML executes pipeline steps sequentially in the active Python environment when running locally. However, with remote [orchestrators](../../user-guide/production-guide/cloud-orchestration.md) or [step operators](../../component-guide/step-operators/step-operators.md), ZenML builds [Docker](https://www.docker.com/) images to run your pipeline in an isolated, well-defined environment.\n\nThis section discusses how to control this dockerization process.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Use your own Docker files\n\nIn some cases, you might not want full control over the resulting Docker image but want to build a parent image dynamically each time a pipeline is executed. To make this process easier, ZenML allows you to specify a custom Dockerfile as well as `build context` directory and build options. ZenML then builds an intermediate image based on the Dockerfile you specified and uses the intermediate image as the parent image.\n\nHere is how the build process looks like:\n\n* **No `Dockerfile` specified**: If any of the options regarding requirements, environment variables or copying files require us to build an image, ZenML will build this image. Otherwise the `parent_image` will be used to run the pipeline.\n* **`Dockerfile` specified**: ZenML will first build an image based on the specified `Dockerfile`. If any of the options regarding requirements, environment variables or copying files require an additional image built on top of that, ZenML will build a second image. If not, the image build from the specified `Dockerfile` will be used to run the pipeline.\n\nDepending on the configuration of the [`DockerSettings`](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings) object, requirements will be installed in the following order (each step optional): \n\n* The packages installed in your local Python environment.\n* The packages specified via the `requirements` attribute.\n* The packages specified via the `required_integrations` and potentially stack requirements.\n\n{% hint style=\"info\" %}\nDepending on the configuration of your Docker settings, this intermediate image might also be used directly to execute your pipeline steps.\n{% endhint %}\n\n```python\ndocker_settings = DockerSettings(\n    dockerfile=\"/path/to/dockerfile\",\n    build_context_root=\"/path/to/build/context\",\n    parent_image_build_config={\n        \"build_options\": ...\n        \"dockerignore\": ...\n    }\n)\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "# Use code repositories to speed up Docker build times\n\nWhile reusing Docker builds is useful, it can be limited. This is because specifying a custom build when running a pipeline will **not run the code on your client machine** but will use the code **included in the Docker images of the build**. As a consequence, even if you make local code changes, reusing a build will _always_ execute the code bundled in the Docker image, rather than the local code. Therefore, if you would like to reuse a Docker build AND make sure your local code changes are also downloaded into the image, you need to disconnect your code from the build.\n\nYou can do so by connecting a git repository. Registering a code repository lets you avoid building images each time you run a pipeline **and** quickly iterate on your code. When running a pipeline that is part of a local code repository checkout, ZenML can instead build the Docker images without including any of your source files, and download the files inside the container before running your code. This greatly speeds up the building process and also allows you to reuse images that one of your colleagues might have built for the same stack.\n\nZenML will **automatically figure out which builds match your pipeline and reuse the appropriate build id**. Therefore, you **do not** need to explicitly pass in the build id when you have a clean repository state and a connected git repository. This approach is **highly recommended**. See an end to end example [here](../../user-guide/production-guide/connect-code-repository.md).\n\n{% hint style=\"warning\" %}\nIn order to benefit from the advantages of having a code repository in a project, you need to make sure that **the relevant integrations are installed for your ZenML installation.**. For instance, let's assume you are working on a project with ZenML and one of your team members has already registered a corresponding code repository of type `github` for it. If you do `zenml code-repository list`, you would also be able to see this repository. However, in order to fully use this repository, you still need to install the corresponding integration for it, in this example the `github` integration.\n\n```sh\nzenml integration install github\n```\n{% endhint %}\n\n## Detecting local code repository checkouts\n\nOnce you have registered one or more code repositories, ZenML will check whether the files you use when running a pipeline are tracked inside one of those code repositories. This happens as follows:\n\n* First, the [source root](./which-files-are-built-into-the-image.md) is computed\n* Next, ZenML checks whether this source root directory is included in a local checkout of one of the registered code repositories\n\n## Tracking code version for pipeline runs\n\nIf a [local code repository checkout](#detecting-local-code-repository-checkouts) is detected when running a pipeline, ZenML will store a reference to the current commit for"}
{"input": " the pipeline run, so you'll be able to know exactly which code was used. Note that this reference is only tracked if your local checkout is clean (i.e. it does not contain any untracked or uncommitted files). This is to ensure that your pipeline is actually running with the exact code stored at the specific code repository commit.\n\n## Tips and best practices\n\nIt is also important to take some additional points into consideration:\n\n* The file download is only possible if the local checkout is clean (i.e. it does not contain any untracked or uncommitted files) and the latest commit has been pushed to the remote repository. This is necessary as otherwise, the file download inside the Docker container will fail.\n* If you want to disable or enforce the downloading of files, check out [this docs page](./docker-settings-on-a-pipeline.md) for the available options.\n\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Which files are built into the image\n\nZenML determines the root directory of your source files in the following order:\n\n* If you've initialized zenml (`zenml init`) in your current working directory or one of its parent directories, the repository root directory will be used.\n* Otherwise, the parent directory of the Python file you're executing will be the source root. For example, running `python /path/to/file.py`, the source root would be `/path/to`.\n\nYou can specify how the files inside this root directory are handled using the following three attributes on the [DockerSettings](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings):\n* `allow_download_from_code_repository`: If this is set to `True` and your files are inside a registered [code repository](../setting-up-a-project-repository/connect-your-git-repository.md) and the repository has no local changes, the files will be downloaded from the code repository and not included in the image.\n* `allow_download_from_artifact_store`: If the previous option is disabled or no code repository without local changes exists for the root directory, ZenML will archive and upload your code to the artifact store if this is set to `True`.\n* `allow_including_files_in_images`: If both previous options were disabled or not possible, ZenML will include your files in the Docker image if this option is enabled. This means a new Docker image has to be built each time you modify one of your code files.\n\n{% hint style=\"warning\" %}\nSetting all of the above attributes to `False` is not recommended and will most likely cause unintended and unanticipated behavior when running your pipelines. If you do this, you're responsible that all your files are at the correct paths in the Docker images that will be used to run your pipeline steps.\n{% endhint %}\n\n## Control which files get downloaded\n\nWhen downloading files either from a code repository or the artifact store, ZenML downloads all contents of the root directory into the Docker container. To exclude files, track your code in a Git repository use a [gitignore](https://git-scm.com/docs/gitignore/en) to specify which files should be excluded.\n\n## Control which files get included\n\nWhen including files in the image, ZenML copies all contents of the root directory into the Docker image. To exclude files and keep the image smaller, use a [.dockerignore file](https://docs.docker.com/engine/reference/builder/#dockerignore-file) in either of the following ways:\n\n* Have a file called `.dockerignore` in your source root directory.\n* Explicitly specify a `.dockerignore` file to use:\n\n    ```python\n    docker_settings = DockerSettings(build_config={\"dockerignore\": \"/path/to/.dockerignore\"})\n\n    @pipeline(settings={\"docker\": docker_settings})\n    def my_pipeline(...):\n        ...\n    ```\n\n<!-- For scarf -->\n<figure><img alt"}
{"input": "=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: You have the option to customize the Docker settings at a step level.\n---\n\n# Docker settings on a step\n\nBy default every step of a pipeline uses the same Docker image that is defined at the [pipeline level](./docker-settings-on-a-pipeline.md). Sometimes your steps will have special requirements that make it necessary to define a different Docker image for one or many steps. This can easily be accomplished by adding the [DockerSettings](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings) to the step decorator directly.\n\n```python\nfrom zenml import step\nfrom zenml.config import DockerSettings\n\n@step(\n  settings={\n    \"docker\": DockerSettings(\n      parent_image=\"pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime\"\n    )\n  }\n)\ndef training(...):\n\t...\n```\n\nAlternatively, this can also be done within the configuration file.\n\n```yaml\nsteps:\n  training:\n    settings:\n      docker:\n        parent_image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime\n        required_integrations:\n          - gcp\n          - github\n        requirements:\n          - zenml  # Make sure to include ZenML for other parent images\n          - numpy\n```\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Defining the image builder.\n---\n\n# \ud83d\udc33 Define where an image is built\n\nZenML executes pipeline steps sequentially in the active Python environment when running locally. However, with remote [orchestrators](../../component-guide/orchestrators/orchestrators.md) or [step operators](../../component-guide/step-operators/step-operators.md), ZenML builds [Docker](https://www.docker.com/) images to run your pipeline in an isolated, well-defined environment.\n\nBy default, execution environments are created locally in the client environment using the local Docker client. However, this requires Docker installation and permissions. ZenML offers [image builders](../../component-guide/image-builders/image-builders.md), a special [stack component](../../component-guide/README.md), allowing users to build and push Docker images in a different specialized _image builder environment_.\n\nNote that even if you don't configure an image builder in your stack, ZenML still uses the [local image builder](../../component-guide/image-builders/local.md) to retain consistency across all builds. In this case, the image builder environment is the same as the [client environment](../configure-python-environments/README.md#client-environment-or-the-runner-environment).\n\nYou don't need to directly interact with any image builder in your code. As long as the image builder that you want to\nuse is part of your active [ZenML stack](/docs/book/user-guide/production-guide/understand-stacks.md), it will be used\nautomatically by any component that needs to build container images.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Displaying visualizations in the dashboard.\n---\n\n# Giving the ZenML Server Access to Visualizations\n\nIn order for the visualizations to show up on the dashboard, the following must be true:\n\n## Configuring a Service Connector\n\nVisualizations are usually stored alongside the artifact, in the [artifact store](../../component-guide/artifact-stores/artifact-stores.md). Therefore, if a user would like to see the visualization displayed on the ZenML dashboard, they must give access to the server to connect to the artifact store.\n\nThe [service connector](../auth-management/README.md) documentation goes deeper into the concept of service connectors and how they can be configured to give the server permission to access the artifact store. For a concrete example, see the [AWS S3](../../component-guide/artifact-stores/s3.md) artifact store documentation.\n\n{% hint style=\"info\" %}\nWhen using the default/local artifact store with a deployed ZenML, the server naturally does not have access to your local files. In this case, the visualizations are also not displayed on the dashboard.\n\nPlease use a service connector enabled and remote artifact store alongside a deployed ZenML to view visualizations.\n{% endhint %}\n\n## Configuring Artifact Stores\n\nIf all visualizations of a certain pipeline run are not showing up in the dashboard, it might be that your ZenML server does not have the required dependencies or permissions to access that artifact store. See the [custom artifact store docs page](../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores) for more information.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Types of visualizations in ZenML.\n---\n\n# Types of visualizations\n\nZenML automatically saves visualizations of many common data types and allows you to view these visualizations in the ZenML dashboard:\n\n![ZenML Artifact Visualizations](../../.gitbook/assets/artifact_visualization_dashboard.png)\n\nAlternatively, any of these visualizations can also be displayed in Jupyter notebooks using the `artifact.visualize()` method:\n\n![output.visualize() Output](../../.gitbook/assets/artifact_visualization_evidently.png)\n\nSome examples of default visualizations are:\n\n- A statistical representation of a [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) Dataframe represented as a png image.\n- Drift detection reports by [Evidently](../../component-guide/data-validators/evidently.md), [Great Expectations](../../component-guide/data-validators/great-expectations.md), and [whylogs](../../component-guide/data-validators/whylogs.md).\n- A [Hugging Face](https://zenml.io/integrations/huggingface) datasets viewer embedded as a HTML iframe.\n\n![output.visualize() output for the Hugging Face datasets viewer](../../.gitbook/assets/artifact_visualization_huggingface.gif)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>"}
{"input": "---\ndescription: Configuring ZenML to display data visualizations in the dashboard.\n---\n\n# Visualize artifacts\n\nIt is easy  to associate visualizations of data and artifacts in ZenML:\n\n![ZenML Artifact Visualizations](../../.gitbook/assets/artifact_visualization_dashboard.png)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>"}
{"input": "---\ndescription: Creating your own visualizations.\n---\n\n# Creating Custom Visualizations\n\nIt is simple to associate a custom visualization with an artifact in ZenML, if\nthe visualization is one of the supported visualization types.\nCurrently, the following visualization types are supported:\n\n* **HTML:** Embedded HTML visualizations such as data validation reports,\n* **Image:** Visualizations of image data such as Pillow images (e.g. `PIL.Image`) or certain numeric numpy arrays,\n* **CSV:** Tables, such as the pandas DataFrame `.describe()` output,\n* **Markdown:** Markdown strings or pages.\n\nThere are three ways how you can add custom visualizations to the dashboard:\n\n* If you are already handling HTML, Markdown, or CSV data in one of your steps, you can have them visualized in just a few lines of code by casting them to a [special class](#visualization-via-special-return-types) inside your step.\n* If you want to automatically extract visualizations for all artifacts of a certain data type, you can define type-specific visualization logic by [building a custom materializer](#visualization-via-materializers).\n* If you want to create any other custom visualizations, you can [create a custom return type class with corresponding materializer](#how-to-think-about-creating-a-custom-visualization) and build and return this custom return type from one of your steps.\n\n## Visualization via Special Return Types\n\nIf you already have HTML, Markdown, or CSV data available as a string inside your step, you can simply cast them to one of the following types and return them from your step:\n\n* `zenml.types.HTMLString` for strings in HTML format, e.g., `\"<h1>Header</h1>Some text\"`,\n* `zenml.types.MarkdownString` for strings in Markdown format, e.g., `\"# Header\\nSome text\"`,\n* `zenml.types.CSVString` for strings in CSV format, e.g., `\"a,b,c\\n1,2,3\"`.\n\n### Example:\n\n```python\nfrom zenml.types import CSVString\n\n@step\ndef my_step() -> CSVString:\n    some_csv = \"a,b,c\\n1,2,3\"\n    return CSVString(some_csv)\n```\n\nThis would create the following visualization in the dashboard:\n\n![CSV Visualization Example](../../.gitbook/assets/artifact_visualization_csv.png)\n\n## Visualization via Materializers\n\nIf you want to automatically extract visualizations for all artifacts of a certain data type, you can do so by overriding the `save_visualizations()` method of the corresponding materializer. See the [materializer docs page](../handle-data-artifacts/handle-custom-data-types.md#optional-how-to-visualize-the-artifact) for more information on how to create custom materializers that do this.\n\nOr, see a code example on [GitHub](https://github.com/zenml-io/zenml/blob/main/src/zen"}
{"input": "ml/integrations/huggingface/materializers/huggingface_datasets_materializer.py) where we visualize Hugging Face datasets by embedding their preview viewer.\n\n## How to think about creating a custom visualization\n\nBy combining the ideas behind the above two visualization approaches, you can visualize virtually anything you want inside your ZenML dashboard in three simple steps:\n\n1. Create a **custom class** that will hold the data that you want to visualize.\n2. [Build a custom **materializer**](../handle-data-artifacts/handle-custom-data-types.md#custom-materializers) for this custom class with the visualization logic implemented in the `save_visualizations()` method.\n3. Return your custom class from any of your ZenML steps.\n\n### Example: Facets Data Skew Visualization\n\nAs an example, have a look at the models, materializers, and steps of the [Facets Integration](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-facets), which can be used to visualize the data skew between multiple Pandas DataFrames:\n\n![Facets Visualization](../../.gitbook/assets/facets-visualization.png)\n\n**1. Custom Class** The [FacetsComparison](https://sdkdocs.zenml.io/0.42.0/integration\\_code\\_docs/integrations-facets/#zenml.integrations.facets.models.FacetsComparison) is the custom class that holds the data required for the visualization.\n\n```python\nclass FacetsComparison(BaseModel):\n    datasets: List[Dict[str, Union[str, pd.DataFrame]]]\n```\n\n**2. Materializer** The [FacetsMaterializer](https://sdkdocs.zenml.io/0.42.0/integration\\_code\\_docs/integrations-facets/#zenml.integrations.facets.materializers.facets\\_materializer.FacetsMaterializer) is a custom materializer that only handles this custom class and contains the corresponding visualization logic.\n\n```python\nclass FacetsMaterializer(BaseMaterializer):\n\n    ASSOCIATED_TYPES = (FacetsComparison,)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS\n\n    def save_visualizations(\n        self, data: FacetsComparison\n    ) -> Dict[str, VisualizationType]:\n        html = ...  # Create a visualization for the custom type \n        visualization_path = os.path.join(self.uri, VISUALIZATION_FILENAME)\n        with fileio.open(visualization_path, \"w\") as f:\n            f.write(html)\n        return {visualization_path: VisualizationType.HTML}\n```\n\n**3. Step** There are three different steps in the `facets` integration that can be used to create `FacetsComparison`s for different sets of inputs. E.g., the `facets_visualization_step` below takes two DataFrames as inputs and builds a `FacetsComparison` object out of them:\n\n```python\n@step\ndef facets"}
{"input": "_visualization_step(\n    reference: pd.DataFrame, comparison: pd.DataFrame\n) -> FacetsComparison:  # Return the custom type from your step\n    return FacetsComparison(\n        datasets=[\n            {\"name\": \"reference\", \"table\": reference},\n            {\"name\": \"comparison\", \"table\": comparison},\n        ]\n    )\n```\n\n{% hint style=\"info\" %}\nThis is what happens now under the hood when you add the `facets_visualization_step` into your pipeline:\n\n1. The step creates and returns a `FacetsComparison`.\n2. When the step finishes, ZenML will search for a materializer class that can handle this type, finds the `FacetsMaterializer`, and calls the `save_visualizations()` method which creates the visualization and saves it into your artifact store as an HTML file.\n3. When you open your dashboard and click on the artifact inside the run DAG, the visualization HTML file is loaded from the artifact store and displayed.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Disabling visualizations.\n---\n\n# Disabling Visualizations\n\nIf you would like to disable artifact visualization altogether, you can set `enable_artifact_visualization` at either pipeline or step level:\n\n```python\n@step(enable_artifact_visualization=False)\ndef my_step():\n    ...\n\n@pipeline(enable_artifact_visualization=False)\ndef my_pipeline():\n    ...\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to deploy ZenML pipelines on a Kubernetes cluster.\n---\n\n# Kubernetes\n\nThe ZenML Kubernetes Orchestrator allows you to run your ML pipelines on a Kubernetes cluster without writing Kubernetes code. It's a lightweight alternative to more complex orchestrators like Airflow or Kubeflow.\n\n{% hint style=\"info\" %}\nIf you only want to run individual steps of your pipeline in Kubernetes, check out our [Kubernetes Step Operator](../../component-guide/step-operators/kubernetes.md).\n{% endhint %}\n\n## Prerequisites\n\nTo use the Kubernetes Orchestrator, you'll need:\n\n- ZenML `kubernetes` integration installed (`zenml integration install kubernetes`)\n- Docker installed and running\n- `kubectl` installed\n- A remote artifact store and container registry in your ZenML stack\n- A deployed Kubernetes cluster\n- A configured `kubectl` context pointing to the cluster (optional, see below)\n\n## Deploying the Orchestrator\n\nYou can deploy the orchestrator from the ZenML CLI:\n\n```bash\nzenml orchestrator deploy k8s_orchestrator --flavor=kubernetes --provider=<YOUR_PROVIDER>\n```\n\n## Configuring the Orchestrator\n\nThere are two ways to configure the orchestrator:\n\n1. Using a [Service Connector](../../how-to/auth-management/service-connectors-guide.md) to connect to the remote cluster. This is the recommended approach, especially for cloud-managed clusters. No local `kubectl` context is needed.\n\n```bash\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubernetes\nzenml service-connector list-resources --resource-type kubernetes-cluster -e\nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\n2. Configuring `kubectl` with a context pointing to the remote cluster and setting the `kubernetes_context` in the orchestrator config:\n\n```bash\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=kubernetes \\\n    --kubernetes_context=<KUBERNETES_CONTEXT>\n\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\n## Running a Pipeline\n\nOnce configured, you can run any ZenML pipeline using the Kubernetes Orchestrator:\n\n```bash\npython your_pipeline.py\n```\n\nThis will create a Kubernetes pod for each step in your pipeline. You can interact with the pods using `kubectl` commands.\n\nFor more advanced configuration options and additional details, refer to the [full Kubernetes Orchestrator documentation](../../component-guide/orchestrators/kubernetes.md).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f"}
{"input": "0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Use your favorite tools with ZenML.\n---\n\n# \ud83d\udc68\u200d\ud83c\udfa4 Popular integrations\n\nZenML is designed to work seamlessly with your favorite tools. This guide will\nshow you how to integrate ZenML with some of the most popular tools in the data\nscience and machine learning ecosystem.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to use the MLflow Experiment Tracker with ZenML.\n---\n\n# MLflow Experiment Tracker\n\nThe ZenML MLflow Experiment Tracker integration and stack component allows you to log and visualize information from your pipeline steps using MLflow, without having to write extra MLflow code.\n\n## Prerequisites\n\nTo use the MLflow Experiment Tracker, you'll need:\n\n- ZenML `mlflow` integration installed (`zenml integration install mlflow -y`)\n- An MLflow deployment, either local (scenario 1) or remote with proxied artifact storage (scenario 5)\n\n## Configuring the Experiment Tracker\n\nThere are two main MLflow deployment scenarios:\n\n1. Local (scenario 1): Use a local artifact store, only suitable for running ZenML locally. No extra configuration needed.\n\n```bash\nzenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow\nzenml stack register custom_stack -e mlflow_experiment_tracker ... --set\n```\n\n2. Remote with proxied artifact storage (scenario 5): Can be used with any stack components. Requires authentication configuration.\n\nFor remote, you'll need to configure authentication using one of:\n- Basic authentication (not recommended for production)\n- ZenML secrets (recommended)\n\nTo use ZenML secrets:\n\n```bash\nzenml secret create mlflow_secret \\\n   --username=<USERNAME> \\\n   --password=<PASSWORD>\n   \nzenml experiment-tracker register mlflow \\\n   --flavor=mlflow \\\n   --tracking_username={{mlflow_secret.username}} \\\n   --tracking_password={{mlflow_secret.password}} \\\n   ...\n```\n\n## Using the Experiment Tracker\n\nTo log information with MLflow in a pipeline step:\n\n1. Enable the experiment tracker using the `@step` decorator \n2. Use MLflow's logging or auto-logging capabilities as usual\n\n```python\nimport mlflow\n\n@step(experiment_tracker=\"<MLFLOW_TRACKER_STACK_COMPONENT_NAME>\")\ndef train_step(...):\n   mlflow.tensorflow.autolog()\n   \n   mlflow.log_param(...)\n   mlflow.log_metric(...)\n   mlflow.log_artifact(...)\n   \n   ...\n```\n\n## Viewing Results\n\nYou can find the URL to the MLflow experiment for a ZenML run:\n\n```python\nlast_run = client.get_pipeline(\"<PIPELINE_NAME>\").last_run\ntrainer_step = last_run.get_step(\"<STEP_NAME>\")\ntracking_url = trainer_step.run_metadata[\"experiment_tracker_url\"].value\n```\n\nThis will link to your deployed MLflow instance UI, or the local MLflow experiment file.\n\n## Additional Configuration\n\nYou can further configure the experiment tracker using `MLFlowExperimentTrackerSettings`:\n\n```python\nfrom zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import MLFlowExperimentTrackerSettings\n\nmlflow_settings = MLFlowExperimentTrackerSettings(\n   nested=True,\n   tags={\"key\": \"value\"}  \n)\n\n@step(\n   experiment_tracker=\"<MLFLOW_TRACKER_STACK_COMPONENT_NAME>\",\n"}
{"input": "   settings={\n       \"experiment_tracker.mlflow\": mlflow_settings\n   }  \n)\n```\n\nFor more details and advanced options, see the [full MLflow Experiment Tracker documentation](../../component-guide/experiment-trackers/mlflow.md).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Use Skypilot with ZenML.\n---\n\n# Skypilot\n\nThe ZenML SkyPilot VM Orchestrator allows you to provision and manage VMs on any supported cloud provider (AWS, GCP, Azure, Lambda Labs) for running your ML pipelines. It simplifies the process and offers cost savings and high GPU availability.\n\n## Prerequisites\n\nTo use the SkyPilot VM Orchestrator, you'll need:\n\n- ZenML SkyPilot integration for your cloud provider installed (`zenml integration install <PROVIDER> skypilot_<PROVIDER>`)\n- Docker installed and running\n- A remote artifact store and container registry in your ZenML stack\n- A remote ZenML deployment\n- Appropriate permissions to provision VMs on your cloud provider\n- A service connector configured to authenticate with your cloud provider (not needed for Lambda Labs)\n\n## Configuring the Orchestrator\n\nConfiguration steps vary by cloud provider:\n\nAWS, GCP, Azure:\n1. Install the SkyPilot integration and connectors extra for your provider\n2. Register a service connector with credentials that have SkyPilot's required permissions \n3. Register the orchestrator and connect it to the service connector\n4. Register and activate a stack with the new orchestrator\n\n```bash\nzenml service-connector register <PROVIDER>-skypilot-vm -t <PROVIDER> --auto-configure\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_<PROVIDER>  \nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector <PROVIDER>-skypilot-vm\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\nLambda Labs:\n1. Install the SkyPilot Lambda integration \n2. Register a secret with your Lambda Labs API key\n3. Register the orchestrator with the API key secret\n4. Register and activate a stack with the new orchestrator\n\n```bash\nzenml secret create lambda_api_key --scope user --api_key=<KEY>\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_lambda --api_key={{lambda_api_key.api_key}}\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\n## Running a Pipeline\n\nOnce configured, you can run any ZenML pipeline using the SkyPilot VM Orchestrator. Each step will run in a Docker container on a provisioned VM.\n\n## Additional Configuration\n\nYou can further configure the orchestrator using cloud-specific `Settings` objects:\n\n```python\nfrom zenml.integrations.skypilot_<PROVIDER>.flavors.skypilot_orchestrator_<PROVIDER>_vm_flavor import Skypilot<PROVIDER>OrchestratorSettings\n\nskypilot_settings = Skypilot<PROVIDER>"}
{"input": "OrchestratorSettings(\n   cpus=\"2\",\n   memory=\"16\", \n   accelerators=\"V100:2\",\n   use_spot=True,\n   region=<REGION>,\n   ...  \n)\n\n@pipeline(\n   settings={\n       \"orchestrator.vm_<PROVIDER>\": skypilot_settings\n   }\n)\n```\n\nThis allows specifying VM size, spot usage, region, and more.\n\nYou can also configure resources per step:\n\n```\nhigh_resource_settings = Skypilot<PROVIDER>OrchestratorSettings(...)\n\n@step(settings={\"orchestrator.vm_<PROVIDER>\": high_resource_settings})  \ndef resource_intensive_step():\n   ...\n```\n\nFor more details and advanced options, see the [full SkyPilot VM Orchestrator documentation](../../component-guide/orchestrators/skypilot-vm.md).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: A simple guide to create an AWS stack to run your ZenML pipelines\n---\n\n# Run on AWS\n\nThis page aims to quickly set up a minimal production stack on AWS. With just a few simple steps, you will set up an IAM role with specifically-scoped permissions that ZenML can use to authenticate with the relevant AWS resources.\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full AWS ZenML cloud stack already?\n\nCheck out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML AWS Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack.\n{% endhint %}\n\n\n## 1) Set up credentials and local environment\n\nTo follow this guide, you need:\n\n* An active AWS account with necessary permissions for AWS S3, SageMaker, ECR, and ECS.\n* ZenML [installed](../../getting-started/installation.md)\n* AWS CLI installed and configured with your AWS credentials. You can follow the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).\n\nOnce ready, navigate to the AWS console:\n\n1. Choose an AWS region In the AWS console, choose the region where you want to deploy your ZenML stack resources. Make note of the region name (e.g., `us-east-1`, `eu-west-2`, etc.) as you will need it in subsequent steps.\n2. Create an IAM role\n\nFor this, you'll need to find out your AWS account ID. You can find this by running:\n\n```shell\naws sts get-caller-identity --query Account --output text\n```\n\nThis will output your AWS account ID. Make a note of this as you will need it in the next steps. (If you're doing anything more esoteric with your AWS account and IAM roles, this might not work for you. The account ID here that we're trying to get is the root account ID that you use to log in to the AWS console.)\n\nThen create a file named `assume-role-policy.json` with the following content:\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::<YOUR_ACCOUNT_ID>:root\",\n        \"Service\": \"sagemaker.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n```\n\nMake sure to replace the placeholder `<YOUR_ACCOUNT_ID>` with your actual AWS account ID that we found earlier.\n\nNow create a new IAM role that ZenML will use to access AWS resources. We'll use `"}
{"input": "zenml-role` as a role name in this example, but you can feel free to choose something else if you prefer. Run the following command to create the role:\n\n```shell\naws iam create-role --role-name zenml-role --assume-role-policy-document file://assume-role-policy.json\n```\n\nBe sure to take note of the information that is output to the terminal, as you will need it in the next steps, especially the Role ARN.\n\n1. Attach policies to the role\n\nAttach the following policies to the role to grant access to the necessary AWS services:\n\n* `AmazonS3FullAccess`\n* `AmazonEC2ContainerRegistryFullAccess`\n* `AmazonSageMakerFullAccess`\n\n```shell\naws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\naws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\naws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n```\n\n1. If you have not already, install the AWS and S3 ZenML integrations:\n\n```shell\nzenml integration install aws s3 -y\n```\n\n## 2) Create a Service Connector within ZenML\n\nCreate an AWS Service Connector within ZenML. The service connector will allow ZenML and other ZenML components to authenticate themselves with AWS using the IAM role.\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\nzenml service-connector register aws_connector \\\n  --type aws \\\n  --auth-method iam-role \\\n  --role_arn=<ROLE_ARN> \\\n  --region=<YOUR_REGION> \\\n  --aws_access_key_id=<YOUR_ACCESS_KEY_ID> \\\n  --aws_secret_access_key=<YOUR_SECRET_ACCESS_KEY>\n```\n\nReplace `<ROLE_ARN>` with the ARN of the IAM role you created in the previous step, `<YOUR_REGION>` with the respective value and use your AWS access key ID and secret access key that we noted down earlier.\n{% endtab %}\n{% endtabs %}\n\n## 3) Create Stack Components\n\n### Artifact Store (S3)\n\nAn [artifact store](../../user-guide/production-guide/remote-storage.md) is used for storing and versioning data flowing through your pipelines.\n\n1. Before you run anything within the ZenML CLI, create an AWS S3 bucket. If you already have one, you can skip this step. (Note: the bucket name should be unique, so you might need to try a few times to find a unique name.)\n\n```shell\naws s3api create-bucket --bucket your-bucket-name\n```\n\nOnce this is done, you can create the ZenML stack component as follows:\n\n1. Register an S3"}
{"input": " Artifact Store with the connector\n\n```shell\nzenml artifact-store register cloud_artifact_store -f s3 --path=s3://bucket-name --connector aws_connector\n```\n\nMore details [here](../../component-guide/artifact-stores/s3.md).\n\n### Orchestrator (SageMaker Pipelines)\n\nAn [orchestrator](../../user-guide/production-guide/cloud-orchestration.md) is the compute backend to run your pipelines.\n\n1. Before you run anything within the ZenML CLI, head on over to AWS and create a SageMaker domain (Skip this if you already have one). The instructions for creating a domain can be found [in the AWS core documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html).\n\nA SageMaker domain is a central management unit for all SageMaker users and resources within a region. It provides a single sign-on (SSO) experience and enables users to create and manage SageMaker resources, such as notebooks, training jobs, and endpoints, within a collaborative environment.\n\nWhen you create a SageMaker domain, you specify the configuration settings, such as the domain name, user profiles, and security settings. Each user within a domain gets their own isolated workspace, which includes a JupyterLab interface, a set of compute resources, and persistent storage.\n\nThe SageMaker orchestrator in ZenML requires a SageMaker domain to run pipelines because it leverages the SageMaker Pipelines service, which is part of the SageMaker ecosystem. SageMaker Pipelines allows you to define, execute, and manage end-to-end machine learning workflows using a declarative approach.\n\nBy creating a SageMaker domain, you establish the necessary environment and permissions for the SageMaker orchestrator to interact with SageMaker Pipelines and other SageMaker resources seamlessly. The domain acts as a prerequisite for using the SageMaker orchestrator in ZenML.\n\nOnce this is done, you can create the ZenML stack component as follows:\n\n1. Register a SageMaker Pipelines orchestrator stack component:\n\nYou'll need the IAM role ARN that we noted down earlier to register the orchestrator. This is the 'execution role' ARN you need to pass to the orchestrator.\n\n```shell\nzenml orchestrator register sagemaker-orchestrator --flavor=sagemaker --region=<YOUR_REGION> --execution_role=<ROLE_ARN>\n```\n\n**Note**: The SageMaker orchestrator utilizes the AWS configuration for operation and does not require direct connection via a service connector for authentication, as it relies on your AWS CLI configurations or environment variables.\n\nMore details [here](../../component-guide/orchestrators/sagemaker.md).\n\n### Container Registry (ECR)\n\nA [container registry](../../component-guide/container-registries/container-registries.md) is used to store Docker images for your pipelines.\n\n1. You'll need to create a repository in ECR. If you already have one, you can skip this"}
{"input": " step.\n\n```shell\naws ecr create-repository --repository-name zenml --region <YOUR_REGION>\n```\n\nOnce this is done, you can create the ZenML stack component as follows:\n\n1. Register an ECR container registry stack component:\n\n```shell\nzenml container-registry register ecr-registry --flavor=aws --uri=<ACCOUNT_ID>.dkr.ecr.<YOUR_REGION>.amazonaws.com --connector aws-connector\n```\n\nMore details [here](../../component-guide/container-registries/aws.md).\n\n## 4) Create stack\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\nexport STACK_NAME=aws_stack\n\nzenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} \\\n    -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set\n```\n\n{% hint style=\"info\" %}\nIn case you want to also add any other stack components to this stack, feel free to do so.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"Dashboard\" %}\n<figure><img src=\"../../.gitbook/assets/Create_Stack.png\" alt=\"\"><figcaption><p>Combine the three stack components and you have your AWS stack. Feel free to add any other component of your choice as well.</p></figcaption></figure>\n{% endtab %}\n{% endtabs %}\n\n## 5) And you're already done!\n\nJust like that, you now have a fully working AWS stack ready to go. Feel free to take it for a spin by running a pipeline on it.\n\nDefine a ZenML pipeline:\n\n```python\nfrom zenml import pipeline, step\n\n@step\ndef hello_world() -> str:\n    return \"Hello from SageMaker!\"\n\n@pipeline\ndef aws_sagemaker_pipeline():\n    hello_world()\n\nif __name__ == \"__main__\":\n    aws_sagemaker_pipeline()\n```\n\nSave this code to run.py and execute it. The pipeline will use AWS S3 for artifact storage, Amazon SageMaker Pipelines for orchestration, and Amazon ECR for container registry.\n\n```shell\npython run.py\n```\n\n<figure><img src=\"../../.gitbook/assets/run_with_repository.png\" alt=\"\"><figcaption><p>Sequence of events that happen when running a pipeline on a remote stack with a code repository</p></figcaption></figure>\n\nRead more in the [production guide](../../user-guide/production-guide/production-guide.md).\n\n## Cleanup\n\n{% hint style=\"warning\" %}\nMake sure you no longer need the resources before deleting them. The instructions and commands that follow are DESTRUCTIVE.\n{% endhint %}\n\nDelete any AWS resources you no longer use to avoid additional charges. You'll want to do the following:\n\n```shell\n# delete the S3 bucket\naws s3 rm s3://your-bucket-name --recursive\naws s3api delete-bucket --bucket your-bucket-name\n\n# delete the"}
{"input": " SageMaker domain\naws sagemaker delete-domain --domain-id <DOMAIN_ID>\n\n# delete the ECR repository\naws ecr delete-repository --repository-name zenml-repository --force\n\n# detach policies from the IAM role\naws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\naws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\naws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n\n# delete the IAM role\naws iam delete-role --role-name zenml-role\n```\n\nMake sure to run these commands in the same AWS region where you created the resources.\n\nBy running these cleanup commands, you will delete the S3 bucket, SageMaker domain, ECR repository, and IAM role, along with their associated policies. This will help you avoid any unnecessary charges for resources you no longer need.\n\nRemember to be cautious when deleting resources and ensure that you no longer require them before running the deletion commands.\n\n## Conclusion\n\nIn this guide, we walked through the process of setting up an AWS stack with ZenML to run your machine learning pipelines in a scalable and production-ready environment. The key steps included:\n\n1. Setting up credentials and the local environment by creating an IAM role with the necessary permissions.\n2. Creating a ZenML service connector to authenticate with AWS services using the IAM role.\n3. Configuring stack components, including an S3 artifact store, a SageMaker Pipelines orchestrator, and an ECR container registry.\n4. Registering the stack components and creating a ZenML stack.\n\nBy following these steps, you can leverage the power of AWS services, such as S3 for artifact storage, SageMaker Pipelines for orchestration, and ECR for container management, all within the ZenML framework. This setup allows you to build, deploy, and manage machine learning pipelines efficiently and scale your workloads based on your requirements.\n\nThe benefits of using an AWS stack with ZenML include:\n\n* Scalability: Leverage the scalability of AWS services to handle large-scale machine learning workloads.\n* Reproducibility: Ensure reproducibility of your pipelines with versioned artifacts and containerized environments.\n* Collaboration: Enable collaboration among team members by using a centralized stack and shared resources.\n* Flexibility: Customize and extend your stack components based on your specific needs and preferences.\n\nNow that you have a functional AWS stack set up with ZenML, you can explore more advanced features and capabilities offered by ZenML. Some next steps to consider:\n\n* Dive deeper into ZenML's [production guide](../../user-guide/production-guide/production-guide.md) to learn best practices for deploying and managing production-ready pipelines.\n* Explore ZenML"}
{"input": "'s [integrations](../../component-guide/README.md) with other popular tools and frameworks in the machine learning ecosystem.\n* Join the [ZenML community](https://zenml.io/slack) to connect with other users, ask questions, and get support.\n\nBy leveraging the power of AWS and ZenML, you can streamline your machine learning workflows, improve collaboration, and deploy production-ready pipelines with ease. Happy experimenting and building!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: A simple guide to create an Azure stack to run your ZenML pipelines\n---\n\n# Run on Azure\n\nThis page aims to quickly set up a minimal production stack on Azure. With \njust a few simple steps, you will set up a resource group, a service principal \nwith correct permissions and the relevant ZenML stack and components.\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full Azure ZenML cloud stack already?\n\nCheck out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Azure Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack.\n{% endhint %}\n\nTo follow this guide, you need:\n\n* An active Azure account.\n* ZenML [installed](../../getting-started/installation.md).\n* ZenML `azure` integration installed with `zenml integration install azure`.\n\n## 1. Set up proper credentials\n\nYou can start by [creating a service principal by creating an app registration](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb) \non Azure:\n\n1. Go to the App Registrations on the Azure portal.\n2. Click on `+ New registration`,\n3. Give it a name and click register.\n\n![Azure App Registrations](../../.gitbook/assets/azure_1.png)\n\n![Azure App Registrations](../../.gitbook/assets/azure_2.png)\n\nOnce you create the service principal, you will get an Application ID and \nTenant ID as they will be needed later.\n\nNext, go to your service principal and click on the `Certificates & secrets` in\nthe `Manage` menu. Here, you have to create a client secret. Note down the \nsecret value as it will be needed later.\n\n![Azure App Registrations](../../.gitbook/assets/azure_3.png)\n\n## 2. Create a service connector\n\nOnce you have created the service principal and the client secret, you can \ngo ahead and create [a ZenML Azure Service Connector](../../how-to/auth-management/azure-service-connector.md).\n\n```bash\nzenml service-connector register azure_connector --type azure \\\n  --auth-method service-principal \\\n  --client_secret=<CLIENT_SECRET> \\\n  --tenant_id=<TENANT_ID> \\\n  --client_id=<APPLICATION_ID>\n```\n\nYou will use this service connector later on to connect your components with \nproper authentication.\n\n## 3. Create a resource group and the AzureML instance\n\nNow, you have to [create a resource group on Azure](https://learn.microsoft.com/en-us/azure/"}
{"input": "azure-resource-manager/management/manage-resource-groups-portal).\nTo do this, go to the Azure portal and go to the `Resource Groups` page and \nclick `+ Create`. \n\n![Azure Resource Groups](../../.gitbook/assets/azure_4.png)\n\nOnce the resource group is created, go to the overview page of your new resource\ngroup and click `+ Create`. This will open up the marketplace where you can \nselect a variety of resource to create. Look for `Azure Machine Learning`.\n\n![Azure Marketplace](../../.gitbook/assets/azure_5.png)\n\nSelect it, and you will start the process of creating an AzureML workspace. \nAs you can see from the `Workspace details`, AzureML workspaces come equipped \nwith a storage account, key vault, and application insights. It is highly \nrecommended that you create a container registry as well.\n\n![AzureML Workspace Details](../../.gitbook/assets/azure_6.png)\n\n## 4. Create Stack Components\n\nIn order to run any workflows on Azure using ZenML, you need an artifact store,\nan orchestrator and a container registry.\n\n### Artifact Store (Azure Blob Storage)\n\nFor the artifact store, we will be using the storage account attached to our\nAzureML workspace. But before registering the component itself, you have to \ncreate a container for blob storage. To do this, go to the corresponding \nstorage account in your workspace and create a new container:\n\n![Azure Blob Storage](../../.gitbook/assets/azure_7.png)\n\nOnce you create the container, you can go ahead, register your artifact \nstore using its path and connect it to your service connector:\n\n```bash \nzenml artifact-store register azure_artifact_store -f azure \\\n  --path=<PATH_TO_YOUR_CONTAINER> \\ \n  --connector azure_connector\n```\n\nFor more information regarding Azure Blob Storage artifact stores, feel free to\n[check the docs](../../component-guide/artifact-stores/azure.md).\n\n### Orchestrator (AzureML)\n\nAs for the orchestrator, no additional setup is needed. Simply use the following\ncommand to register it and connect it your service connector:\n\n```bash\nzenml orchestrator register azure_orchestrator -f azureml \\\n    --subscription_id=<YOUR_AZUREML_SUBSCRIPTION_ID> \\\n    --resource_group=<NAME_OF_YOUR_RESOURCE_GROUP> \\\n    --workspace=<NAME_OF_YOUR_AZUREML_WORKSPACE> \\ \n    --connector azure_connector\n```\n\nFor more information regarding AzureML orchestrator, feel free to\n[check the docs](../../component-guide/orchestrators/azureml.md).\n\n### Container Registry (Azure Container Registry)\n\nSimilar to the orchestrator, you can register and connect your container \nregistry using the following command:\n\n```bash\nzenml container-registry register azure_container_registry -f azure \\\n  --uri=<URI_TO_YOUR_AZURE_CONTAINER_REGISTRY> \\ \n"}
{"input": "  --connector azure_connector\n```\n\nFor more information regarding Azure container registries, feel free to\n[check the docs](../../component-guide/container-registries/azure.md).\n\n## 5. Create a Stack\n\nNow, you can use the registered components to create an Azure ZenML stack:\n\n```shell\nzenml stack register azure_stack \\\n    -o azure_orchestrator \\\n    -a azure_artifact_store \\\n    -c azure_container_registry \\\n    --set\n```\n\n## 6. ...and you are done.\n\nJust like that, you now have a fully working Azure stack ready to go. \nFeel free to take it for a spin by running a pipeline on it.\n\nDefine a ZenML pipeline:\n\n```python\nfrom zenml import pipeline, step\n\n@step\ndef hello_world() -> str:\n    return \"Hello from Azure!\"\n\n@pipeline\ndef azure_pipeline():\n    hello_world()\n\nif __name__ == \"__main__\":\n    azure_pipeline()\n```\n\nSave this code to run.py and execute it. The pipeline will use Azure Blob \nStorage for artifact storage, AzureML for orchestration, and an Azure container \nregistry.\n\n```shell\npython run.py\n```\n\nNow that you have a functional Azure stack set up with ZenML, you can explore \nmore advanced features and capabilities offered by ZenML. Some next steps to \nconsider:\n\n* Dive deeper into ZenML's [production guide](../../user-guide/production-guide/production-guide.md) to learn best practices for deploying and managing production-ready pipelines.\n* Explore ZenML's [integrations](../../component-guide/README.md) with other popular tools and frameworks in the machine learning ecosystem.\n* Join the [ZenML community](https://zenml.io/slack) to connect with other users, ask questions, and get support.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: A simple guide to quickly set up a minimal stack on GCP.\n---\n\n# Set up a minimal GCP stack\n\nThis page aims to quickly set up a minimal production stack on GCP. With just a few simple steps you will set up a service account with specifically-scoped permissions that ZenML can use to authenticate with the relevant GCP resources.\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full GCP ZenML cloud stack already?\n\nCheck out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML GCP Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack.\n{% endhint %}\n\n\n{% hint style=\"warning\" %}\nWhile this guide focuses on Google Cloud, we are seeking contributors to create a similar guide for other cloud providers. If you are interested, please create a [pull request over on GitHub](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md).\n{% endhint %}\n\n### 1) Choose a GCP project\n\nIn the Google Cloud console, on the project selector page, select or [create a Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects). Make sure a billing account is attached to this project to allow the use of some APIs.\n\nThis is how you would do it from the CLI if this is preferred.\n\n```bash\ngcloud projects create <PROJECT_ID> --billing-project=<BILLING_PROJECT>\n```\n\n{% hint style=\"info\" %}\nIf you don't plan to keep the resources that you create in this procedure, create a new project. After you finish these steps, you can delete the project, thereby removing all resources associated with the project.\n{% endhint %}\n\n### 2) Enable GCloud APIs\n\nThe [following APIs](https://console.cloud.google.com/flows/enableapi?apiid=cloudfunctions,cloudbuild.googleapis.com,artifactregistry.googleapis.com,run.googleapis.com,logging.googleapis.com\\\\\\&redirect=https://cloud.google.com/functions/docs/create-deploy-gcloud&\\\\\\_ga=2.103703808.1862683951.1694002459-205697788.1651483076&\\\\\\_gac=1.161946062.1694011263.Cj0KCQjwxuCnBhDLARIsAB-cq1ouJZlVKAVPMsXnYrgQVF2t1Q2hUjgiHVpHXi2N0NlJvG3j3y-PPh8aAoSIEALw\\\\\\_wcB) will need to be enabled within your chosen"}
{"input": " GCP project.\n\n* Cloud Functions API # For the vertex orchestrator\n* Cloud Run Admin API # For the vertex orchestrator\n* Cloud Build API # For the container registry\n* Artifact Registry API # For the container registry\n* Cloud Logging API # Generally needed\n\n### 3) Create a dedicated service account\n\nThe service account should have these following roles.\n\n* AI Platform Service Agent\n* Storage Object Admin\n\nThese roles give permissions for full CRUD on storage objects and full permissions for compute within VertexAI.\n\n### 4) Create a JSON Key for your service account\n\nThis [json file](https://cloud.google.com/iam/docs/keys-create-delete) will allow the service account to assume the identity of this service account. You will need the filepath of the downloaded file in the next step.\n\n```bash\nexport JSON_KEY_FILE_PATH=<JSON_KEY_FILE_PATH>\n```\n\n### 5) Create a Service Connector within ZenML\n\nThe service connector will allow ZenML and other ZenML components to authenticate themselves with GCP.\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nzenml integration install gcp \\\n&& zenml service-connector register gcp_connector \\\n--type gcp \\\n--auth-method service-account \\\n--service_account_json=@${JSON_KEY_FILE_PATH} \\\n--project_id=<GCP_PROJECT_ID>\n```\n{% endtab %}\n{% endtabs %}\n\n### 6) Create Stack Components\n\n#### Artifact Store\n\nBefore you run anything within the ZenML CLI, head on over to GCP and create a GCS bucket, in case you don't already have one that you can use. Once this is done, you can create the ZenML stack component as follows:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nexport ARTIFACT_STORE_NAME=gcp_artifact_store\n\n# Register the GCS artifact-store and reference the target GCS bucket\nzenml artifact-store register ${ARTIFACT_STORE_NAME} --flavor gcp \\\n    --path=gs://<YOUR_BUCKET_NAME>\n\n# Connect the GCS artifact-store to the target bucket via a GCP Service Connector\nzenml artifact-store connect ${ARTIFACT_STORE_NAME} -i\n```\n\n{% hint style=\"info\" %}\nHead on over to our [docs](../../component-guide/artifact-stores/gcp.md) to learn more about artifact stores and how to configure them.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n#### Orchestrator\n\nThis guide will use Vertex AI as the orchestrator to run the pipelines. As a serverless service Vertex is a great choice for quick prototyping of your MLOps stack. The orchestrator can be switched out at any point in the future for a more use-case- and budget-appropriate solution.\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nexport ORCHESTRATOR_NAME=gcp_vertex_orchestrator\n\n# Register the G"}
{"input": "CS artifact-store and reference the target GCS bucket\nzenml orchestrator register ${ORCHESTRATOR_NAME} --flavor=vertex \n  --project=<PROJECT_NAME> --location=europe-west2\n\n# Connect the GCS orchestrator to the target gcp project via a GCP Service Connector\nzenml orchestrator connect ${ORCHESTRATOR_NAME} -i\n```\n\n{% hint style=\"info\" %}\nHead on over to our [docs](../../component-guide/orchestrators/vertex.md) to learn more about orchestrators and how to configure them.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n#### Container Registry\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nexport CONTAINER_REGISTRY_NAME=gcp_container_registry\n\nzenml container-registry register ${CONTAINER_REGISTRY_NAME} --flavor=gcp --uri=<GCR-URI>\n\n# Connect the GCS orchestrator to the target gcp project via a GCP Service Connector\nzenml container-registry connect ${CONTAINER_REGISTRY_NAME} -i\n```\n\n{% hint style=\"info\" %}\nHead on over to our [docs](../../component-guide/container-registries/container-registries.md) to learn more about container registries and how to configure them.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n### 7) Create Stack\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```bash\nexport STACK_NAME=gcp_stack\n\nzenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} \\\n    -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set\n```\n\n{% hint style=\"info\" %}\nIn case you want to also add any other stack components to this stack, feel free to do so.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n## And you're already done!\n\nJust like that, you now have a fully working GCP stack ready to go. Feel free to take it for a spin by running a pipeline on it.\n\n## Cleanup\n\nIf you do not want to use any of the created resources in the future, simply delete the project you created.\n\n```bash\ngcloud project delete <PROJECT_ID_OR_NUMBER>\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Run your ML pipelines on Kubeflow Pipelines.\n---\n\n# Kubeflow\n\nThe ZenML Kubeflow Orchestrator allows you to run your ML pipelines on Kubeflow Pipelines without writing Kubeflow code. \n\n## Prerequisites\n\nTo use the Kubeflow Orchestrator, you'll need:\n\n- ZenML `kubeflow` integration installed (`zenml integration install kubeflow`)\n- Docker installed and running\n- `kubectl` installed (optional, see below)\n- A Kubernetes cluster with Kubeflow Pipelines installed (see deployment guide for your cloud provider)\n- A remote artifact store and container registry in your ZenML stack\n- A remote ZenML server deployed to the cloud\n- The name of your Kubernetes context pointing to the remote cluster (optional, see below)\n\n## Configuring the Orchestrator\n\nThere are two ways to configure the orchestrator:\n\n1. Using a [Service Connector](../../how-to/auth-management/service-connectors-guide.md) to connect to the remote cluster (recommended for cloud-managed clusters). No local `kubectl` context needed.\n\n```bash\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubeflow\nzenml service-connector list-resources --resource-type kubernetes-cluster -e  \nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>\nzenml stack update -o <ORCHESTRATOR_NAME>\n```\n\n2. Configuring `kubectl` with a context pointing to the remote cluster and setting `kubernetes_context` in the orchestrator config:\n\n```bash  \nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=kubeflow \\\n    --kubernetes_context=<KUBERNETES_CONTEXT>\n    \nzenml stack update -o <ORCHESTRATOR_NAME>\n```\n\n## Running a Pipeline\n\nOnce configured, you can run any ZenML pipeline using the Kubeflow Orchestrator:\n\n```python\npython your_pipeline.py\n```\n\nThis will create a Kubernetes pod for each step in your pipeline. You can view pipeline runs in the Kubeflow UI.\n\n## Additional Configuration\n\nYou can further configure the orchestrator using `KubeflowOrchestratorSettings`:\n\n```python\nfrom zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor import KubeflowOrchestratorSettings\n\nkubeflow_settings = KubeflowOrchestratorSettings(\n   client_args={},  \n   user_namespace=\"my_namespace\",\n   pod_settings={\n       \"affinity\": {...},\n       \"tolerations\": [...]\n   }\n)\n\n@pipeline(\n   settings={\n       \"orchestrator.kubeflow\": kubeflow_settings\n   }\n)\n```\n\nThis allows specifying client arguments, user namespace, pod affinity/tolerations, and more.\n\n## Multi-Tenancy"}
{"input": " Deployments\n\nFor multi-tenant Kubeflow deployments, specify the `kubeflow_hostname` ending in `/pipeline` when registering the orchestrator:\n\n```bash\nzenml orchestrator register <NAME> \\\n   --flavor=kubeflow \\\n   --kubeflow_hostname=<KUBEFLOW_HOSTNAME> # e.g. https://mykubeflow.example.com/pipeline\n```\n\nAnd provide the namespace, username and password in the orchestrator settings:\n\n```python\nkubeflow_settings = KubeflowOrchestratorSettings(\n   client_username=\"admin\",\n   client_password=\"abc123\", \n   user_namespace=\"namespace_name\"\n)\n\n@pipeline(\n   settings={\n       \"orchestrator.kubeflow\": kubeflow_settings\n   }\n)\n```\n\nFor more advanced options and details, refer to the [full Kubeflow Orchestrator documentation](../../component-guide/orchestrators/kubeflow.md).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: >-\n  Tracking your code and avoiding unnecessary Docker builds by connecting your\n  git repo.\n---\n\n# Connect your git repository\n\nA code repository in ZenML refers to a remote storage location for your code. Some commonly known code repository platforms include [GitHub](https://github.com/) and [GitLab](https://gitlab.com/).\n\nCode repositories enable ZenML to keep track of the code version that you use for your pipeline runs. Additionally, running a pipeline that is tracked in a registered code repository can [speed up the Docker image building for containerized stack components](../customize-docker-builds/use-code-repositories-to-speed-up-docker-build-times.md) by eliminating the need to rebuild Docker images each time you change one of your source code files.\n\nLearn more about how code repositories benefit development [here](../customize-docker-builds/use-code-repositories-to-speed-up-docker-build-times.md).\n\n## Registering a code repository\n\nIf you are planning to use one of the [available implementations of code repositories](connect-your-git-repository.md#available-implementations), first, you need to install the corresponding ZenML integration:\n\n```\nzenml integration install <INTEGRATION_NAME>\n```\n\nAfterward, code repositories can be registered using the CLI:\n\n```shell\nzenml code-repository register <NAME> --type=<TYPE> [--CODE_REPOSITORY_OPTIONS]\n```\n\nFor concrete options, check out the section on the [`GitHubCodeRepository`](connect-your-git-repository.md#github), the [`GitLabCodeRepository`](connect-your-git-repository.md#gitlab) or how to develop and register a [custom code repository implementation](connect-your-git-repository.md#developing-a-custom-code-repository).\n\n## Available implementations\n\nZenML comes with builtin implementations of the code repository abstraction for the `GitHub` and `GitLab` platforms, but it's also possible to use a [custom code repository implementation](connect-your-git-repository.md#developing-a-custom-code-repository).\n\n### GitHub\n\nZenML provides built-in support for using GitHub as a code repository for your ZenML pipelines. You can register a GitHub code repository by providing the URL of the GitHub instance, the owner of the repository, the name of the repository, and a GitHub Personal Access Token (PAT) with access to the repository.\n\nBefore registering the code repository, first, you have to install the corresponding integration:\n\n```sh\nzenml integration install github\n```\n\nAfterward, you can register a GitHub code repository by running the following CLI command:\n\n```shell\nzenml code-repository register <NAME> --type=github \\\n--url=<GITHUB_URL> --owner=<OWNER> --repository=<REPOSITORY> \\\n--token=<GITHUB_TOKEN>\n```\n\nwhere \\<REPOSITORY> is the name of the code repository you are registering, \\<OWNER> is the owner of the repository, \\<NAME> is the name of the repository,"}
{"input": " \\<GITHUB\\_TOKEN> is your GitHub Personal Access Token and \\<GITHUB\\_URL> is the URL of the GitHub instance which defaults to `https://github.com.` You will need to set a URL if you are using GitHub Enterprise.\n\nAfter registering the GitHub code repository, ZenML will automatically detect if your source files are being tracked by GitHub and store the commit hash for each pipeline run.\n\n<details>\n\n<summary>How to get a token for GitHub</summary>\n\n1. Go to your GitHub account settings and click on [Developer settings](https://github.com/settings/tokens?type=beta).\n2. Select \"Personal access tokens\" and click on \"Generate new token\".\n3.  Give your token a name and a description.\n\n    ![](../../.gitbook/assets/github-fine-grained-token-name.png)\n4.  We recommend selecting the specific repository and then giving `contents` read-only access.\n\n    ![](../../.gitbook/assets/github-token-set-permissions.png)\n\n    ![](../../.gitbook/assets/github-token-permissions-overview.png)\n5.  Click on \"Generate token\" and copy the token to a safe place.\n\n    ![](../../.gitbook/assets/copy-github-fine-grained-token.png)\n\n</details>\n\n### GitLab\n\nZenML also provides built-in support for using GitLab as a code repository for your ZenML pipelines. You can register a GitLab code repository by providing the URL of the GitLab project, the group of the project, the name of the project, and a GitLab Personal Access Token (PAT) with access to the project.\n\nBefore registering the code repository, first, you have to install the corresponding integration:\n\n```sh\nzenml integration install gitlab\n```\n\nAfterward, you can register a GitLab code repository by running the following CLI command:\n\n```shell\nzenml code-repository register <NAME> --type=gitlab \\\n--url=<GITLAB_URL> --group=<GROUP> --project=<PROJECT> \\\n--token=<GITLAB_TOKEN>\n```\n\nwhere `<NAME>` is the name of the code repository you are registering, `<GROUP>` is the group of the project, `<PROJECT>` is the name of the project, \\<GITLAB\\_TOKEN> is your GitLab Personal Access Token, and \\<GITLAB\\_URL> is the URL of the GitLab instance which defaults to `https://gitlab.com.` You will need to set a URL if you have a self-hosted GitLab instance.\n\nAfter registering the GitLab code repository, ZenML will automatically detect if your source files are being tracked by GitLab and store the commit hash for each pipeline run.\n\n<details>\n\n<summary>How to get a token for GitLab</summary>\n\n1. Go to your GitLab account settings and click on [Access Tokens](https://gitlab.com/-/profile/personal\\_access\\_tokens).\n2.  Name the token and select"}
{"input": " the scopes that you need (e.g. `read_repository`, `read_user`, `read_api`)\n\n    ![](../../.gitbook/assets/gitlab-generate-access-token.png)\n3.  Click on \"Create personal access token\" and copy the token to a safe place.\n\n    ![](../../.gitbook/assets/gitlab-copy-access-token.png)\n\n</details>\n\n## Developing a custom code repository\n\nIf you're using some other platform to store your code, and you still want to use a code repository in ZenML, you can implement and register a custom code repository.\n\nFirst, you'll need to subclass and implement the abstract methods of the `zenml.code_repositories.BaseCodeRepository` class:\n\n```python\nclass BaseCodeRepository(ABC):\n    \"\"\"Base class for code repositories.\"\"\"\n\n    @abstractmethod\n    def login(self) -> None:\n        \"\"\"Logs into the code repository.\"\"\"\n\n    @abstractmethod\n    def download_files(\n            self, commit: str, directory: str, repo_sub_directory: Optional[str]\n    ) -> None:\n        \"\"\"Downloads files from the code repository to a local directory.\n\n        Args:\n            commit: The commit hash to download files from.\n            directory: The directory to download files to.\n            repo_sub_directory: The subdirectory in the repository to\n                download files from.\n        \"\"\"\n\n    @abstractmethod\n    def get_local_context(\n            self, path: str\n    ) -> Optional[\"LocalRepositoryContext\"]:\n        \"\"\"Gets a local repository context from a path.\n\n        Args:\n            path: The path to the local repository.\n\n        Returns:\n            The local repository context object.\n        \"\"\"\n```\n\nAfter you're finished implementing this, you can register it as follows:\n\n```shell\n# The `CODE_REPOSITORY_OPTIONS` are key-value pairs that your implementation will receive\n# as configuration in its __init__ method. This will usually include stuff like the username\n# and other credentials necessary to authenticate with the code repository platform.\nzenml code-repository register <NAME> --type=custom --source=my_module.MyRepositoryClass \\\n    [--CODE_REPOSITORY_OPTIONS]\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Recommended repository structure and best practices.\n---\n\n# Best practices\n\nWhile it doesn't matter how you structure your ZenML project, here is a recommended project structure the core team often uses:\n\n```markdown\n.\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 steps\n\u2502   \u251c\u2500\u2500 loader_step\n\u2502   \u2502   \u251c\u2500\u2500 .dockerignore (optional)\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile (optional)\n\u2502   \u2502   \u251c\u2500\u2500 loader_step.py\n\u2502   \u2502   \u2514\u2500\u2500 requirements.txt (optional)\n\u2502   \u2514\u2500\u2500 training_step\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 pipelines\n\u2502   \u251c\u2500\u2500 training_pipeline\n\u2502   \u2502   \u251c\u2500\u2500 .dockerignore (optional)\n\u2502   \u2502   \u251c\u2500\u2500 config.yaml (optional)\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile (optional)\n\u2502   \u2502   \u251c\u2500\u2500 training_pipeline.py\n\u2502   \u2502   \u2514\u2500\u2500 requirements.txt (optional)\n\u2502   \u2514\u2500\u2500 deployment_pipeline\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 notebooks\n\u2502   \u2514\u2500\u2500 *.ipynb\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 .zen\n\u2514\u2500\u2500 run.py\n```\n\nAll ZenML [Project templates](using-project-templates.md#generating-project-from-a-project-template) are modeled around this basic structure.\n\n#### Steps\n\nKeep your steps in separate Python files. This allows you to optionally keep their utils, dependencies, and Dockerfiles separate.\n\n#### Logging\n\nZenML records the root python logging handler's output into the artifact store as a side-effect of running a step. Therefore, when writing steps, use the `logging` module to record logs, to ensure that these logs then show up in the ZenML dashboard.\n\n```python\n# Use ZenML handler\nfrom zenml.logger import get_logger\n\nlogger = get_logger(__name__)\n...\n\n@step\ndef training_data_loader():\n    # This will show up in the dashboard\n    logger.info(\"My logs\")\n```\n\n#### Pipelines\n\nJust like steps, keep your pipelines in separate Python files. This allows you to optionally keep their utils, dependencies, and Dockerfiles separate.\n\nIt is recommended that you separate the pipeline execution from the pipeline definition so that importing the pipeline does not immediately run it.\n\n{% hint style=\"warning\" %}\nDo not give pipelines or pipeline instances the name \"pipeline\". Doing this will overwrite the imported `pipeline` and decorator and lead to failures at later stages if more pipelines are decorated there.\n{% endhint %}\n\n{% hint style=\"info\" %}\nPipeline names are their unique identifiers, so using the same name for different pipelines will create a mixed history where two runs of a pipeline are two very different entities.\n{% endhint %}\n\n#### .dockerignore\n\nContainerized orchestrators and step operators load your complete project files into a Docker image for execution. To speed up the process and reduce Docker image sizes, exclude all unnecessary files (like data, virtual environments, git repos, etc.) within"}
{"input": " the `.dockerignore`.\n\n#### Dockerfile (optional)\n\nBy default, ZenML uses the official [zenml Docker image](https://hub.docker.com/r/zenmldocker/zenml) as a base for all pipeline and step builds. You can use your own `Dockerfile` to overwrite this behavior. Learn more [here](../customize-docker-builds/README.md).\n\n#### Notebooks\n\nCollect all your notebooks in one place.\n\n#### .zen\n\nBy running `zenml init` at the root of your project, you define the project scope for ZenML. In ZenML terms, this will be called your \"source's root\". This will be used to resolve import paths and store configurations.\n\nAlthough this is optional, it is recommended that you do this for all of your projects.\n\n{% hint style=\"warning\" %}\nAll of your import paths should be relative to the source's root.\n{% endhint %}\n\n#### run.py\n\nPutting your pipeline runners in the root of the repository ensures that all imports that are defined relative to the project root resolve for the pipeline runner. In case there is no `.zen` defined this also defines the implicit source's root.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Rocketstart your ZenML journey!\n---\n\n# Project templates\n\nWhat would you need to get a quick understanding of the ZenML framework and start building your ML pipelines? The answer is one of ZenML project templates to cover major use cases of ZenML: a collection of steps and pipelines and, to top it all off, a simple but useful CLI. This is exactly what the ZenML templates are all about!\n\n## List of available project templates\n\n<table data-full-width=\"true\"><thead><tr><th width=\"281.33333333333337\">Project Template [Short name]</th><th width=\"200\">Tags</th><th>Description</th></tr></thead><tbody><tr><td><a href=\"https://github.com/zenml-io/template-starter\">Starter template</a> [<code>starter</code>]</td><td><code>basic</code> <code>scikit-learn</code></td><td>All the basic ML ingredients you need to get you started with ZenML: parameterized steps, a model training pipeline, a flexible configuration and a simple CLI. All created around a representative and versatile model training use-case implemented with the scikit-learn library.</td></tr><tr><td><a href=\"https://github.com/zenml-io/template-e2e-batch\">E2E Training with Batch Predictions</a> [<code>e2e_batch</code>]</td><td><code>etl</code> <code>hp-tuning</code> <code>model-promotion</code> <code>drift-detection</code> <code>batch-prediction</code> <code>scikit-learn</code></td><td>This project template is a good starting point for anyone starting with ZenML. It consists of two pipelines with the following high-level steps: load, split, and preprocess data; run HP tuning; train and evaluate model performance; promote model to production; detect data drift; run batch inference.</td></tr><tr><td><a href=\"https://github.com/zenml-io/template-nlp\">NLP Training Pipeline</a> [<code>nlp</code>]</td><td><code>nlp</code> <code>hp-tuning</code> <code>model-promotion</code> <code>training</code> <code>pytorch</code> <code>gradio</code> <code>huggingface</code></td><td>This project template is a simple NLP training pipeline that walks through tokenization, training, HP tuning, evaluation and deployment for a BERT or GPT-2 based model and testing locally it with gradio</td></tr></tbody></table>\n\n{% hint style=\"info\" %}\nDo you have a personal project powered by ZenML that you would like to see here? At ZenML, we"}
{"input": " are looking for design partnerships and collaboration to help us better understand the real-world scenarios in which MLOps is being used and to build the best possible experience for our users. If you are interested in sharing all or parts of your project with us in the form of a ZenML project template, please [join our Slack](https://zenml.io/slack/) and leave us a message!\n{% endhint %}\n\n## Generating project from a project template\n\nFirst, to use the templates, you need to have ZenML and its `templates` extras installed:\n\n```bash\npip install zenml[templates]\n```\n\nNow, you can generate a project from one of the existing templates by using the `--template` flag with the `zenml init` command:\n\n```bash\nzenml init --template <short_name_of_template>\n# example: zenml init --template e2e_batch\n```\n\nRunning the command above will result in input prompts being shown to you. If you would like to rely on default values for the ZenML project template - you can add `--template-with-defaults` to the same command, like this:\n\n```bash\nzenml init --template <short_name_of_template> --template-with-defaults\n# example: zenml init --template e2e_batch --template-with-defaults\n```\n\n## Creating your own ZenML template\n\nCreating your own ZenML template is a great way to standardize and share your ML workflows across different projects or teams. ZenML uses [Copier](https://copier.readthedocs.io/en/stable/) to manage its project templates. Copier is a library that allows you to generate projects from templates. It's simple, versatile, and powerful.\n\nHere's a step-by-step guide on how to create your own ZenML template:\n\n1. **Create a new repository for your template.** This will be the place where you store all the code and configuration files for your template.\n2. **Define your ML workflows as ZenML steps and pipelines.** You can start by copying the code from one of the existing ZenML templates (like the [starter template](https://github.com/zenml-io/template-starter)) and modifying it to fit your needs.\n3. **Create a `copier.yml` file.** This file is used by Copier to define the template's parameters and their default values. You can learn more about this config file [in the copier docs](https://copier.readthedocs.io/en/stable/creating/).\n4. **Test your template.** You can use the `copier` command-line tool to generate a new project from your template and check if everything works as expected:\n\n```bash\ncopier copy https://github.com/your-username/your-template.git your-project\n```\n\nReplace `https://github.com/your-username/your-template.git` with the URL of your template repository, and `"}
{"input": "your-project` with the name of the new project you want to create.\n\n5. **Use your template with ZenML.** Once your template is ready, you can use it with the `zenml init` command:\n\n```bash\nzenml init --template https://github.com/your-username/your-template.git\n```\n\nReplace `https://github.com/your-username/your-template.git` with the URL of your template repository.\n\nIf you want to use a specific version of your template, you can use the `--template-tag` option to specify the git tag of the version you want to use:\n\n```bash\nzenml init --template https://github.com/your-username/your-template.git --template-tag v1.0.0\n```\n\nReplace `v1.0.0` with the git tag of the version you want to use.\n\nThat's it! Now you have your own ZenML project template that you can use to quickly set up new ML projects. Remember to keep your template up-to-date with the latest best practices and changes in your ML workflows.\n\nOur [Production Guide](../../user-guide/production-guide/README.md) documentation is built around the `E2E Batch` project template codes. Most examples will be based on it, so we highly recommend you to install the `e2e_batch` template with `--template-with-defaults` flag before diving deeper into this documentation section, so you can follow this guide along using your own local environment.\n\n```bash\nmkdir e2e_batch\ncd e2e_batch\nzenml init --template e2e_batch --template-with-defaults\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Setting your team up for success with a project repository.\n---\n\n# \ud83d\ude38 Setting up a project repository\n\nZenML code typically lives in a `git` repository. Setting this repository up correctly can make a huge impact on collaboration and\ngetting the maximum out of your ZenML deployment. This section walks users through some of the options available to create a project\nrepository with ZenML.\n\n<figure><img src=\"../../.gitbook/assets/Remote_with_code_repository.png\" alt=\"\"><figcaption><p>A visual representation of how the code repository fits into the general ZenML architecture.</p></figcaption></figure>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Tracking your metadata.\n---\n\n# Special Metadata Types\n\nZenML supports several special metadata types to capture specific kinds of information. Here are examples of how to use the special types `Uri`, `Path`, `DType`, and `StorageSize`:\n\n```python\nfrom zenml.metadata.metadata_types import StorageSize, DType\nfrom zenml import log_artifact_metadata\n\nlog_artifact_metadata(\n    metadata={\n        \"dataset_source\": Uri(\"gs://my-bucket/datasets/source.csv\"),\n        \"preprocessing_script\": Path(\"/scripts/preprocess.py\"),\n        \"column_types\": {\n            \"age\": DType(\"int\"),\n            \"income\": DType(\"float\"),\n            \"score\": DType(\"int\")\n        },\n        \"processed_data_size\": StorageSize(2500000)\n    }\n)\n```\n\nIn this example:\n\n* `Uri` is used to indicate a dataset source URI.\n* `Path` is used to specify the filesystem path to a preprocessing script.\n* `DType` is used to describe the data types of specific columns.\n* `StorageSize` is used to indicate the size of the processed data in bytes.\n\nThese special types help standardize the format of metadata and ensure that it is logged in a consistent and interpretable manner.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Attach metadata to steps\n\nYou might want to log metadata and have that be attached to a specific step during the course of your work. This is possible by using the `log_step_metadata` method. This method allows you to attach a dictionary of key-value pairs as metadata to a step. The metadata can be any JSON-serializable value, including custom classes such as `Uri`, `Path`, `DType`, and `StorageSize`.\n\nYou can call this method from within a step or from outside. If you call it from within it will attach the metadata to the step and run that is currently being executed.\n\n```python\nfrom zenml import step, log_step_metadata, ArtifactConfig, get_step_context\nfrom typing import Annotated\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.base import ClassifierMixin\n\n@step\ndef train_model(dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig(name=\"sklearn_classifier\", is_model_artifact=True)]:\n    \"\"\"Train a model\"\"\"\n    # Fit the model and compute metrics\n    classifier = RandomForestClassifier().fit(dataset)\n    accuracy, precision, recall = ...\n\n    # Log metadata at the step level\n    # This associates the metadata with the ZenML step run\n    log_step_metadata(\n        metadata={\n            \"evaluation_metrics\": {\n                \"accuracy\": accuracy,\n                \"precision\": precision,\n                \"recall\": recall\n            }\n        },\n    )\n    return classifier\n```\n\nIf you call it from outside you can attach the metadata to a specific step run from any pipeline and step. This is useful if you want to attach the metadata after you've run the step.\n\n```python\nfrom zenml import log_step_metadata\n# run some step\n\n# subsequently log the metadata for the step\nlog_step_metadata(\n    metadata={\n        \"some_metadata\": {\"a_number\": 3}\n    },\n    pipeline_name_id_or_prefix=\"my_pipeline\",\n    step_name=\"my_step\",\n    run_id=\"my_step_run_id\"\n)\n```\n\n## Fetching logged metadata\n\nOnce metadata has been logged in an [artifact](attach-metadata-to-an-artifact.md), [model](attach-metadata-to-a-model.md), we can easily fetch the metadata with the ZenML Client:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nstep = client.get_pipeline_run().steps[\"step_name\"]\n\nprint(step.run_metadata[\"metadata_key\"].value)\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Accessing meta information in real-time within your pipeline.\n---\n\n# Fetch metadata within steps\n\n## Using the `StepContext`\n\nTo find information about the pipeline or step that is currently running, you can use the `zenml.get_step_context()` function to access the `StepContext` of your step:\n\n```python\nfrom zenml import step, get_step_context\n\n@step\ndef my_step():\n    step_context = get_step_context()\n    pipeline_name = step_context.pipeline.name\n    run_name = step_context.pipeline_run.name\n    step_name = step_context.step_run.name\n```\n\nFurthermore, you can also use the `StepContext` to find out where the outputs of your current step will be stored and which [Materializer](../handle-data-artifacts/handle-custom-data-types.md) class will be used to save them:\n\n```python\n@step\ndef my_step():\n    step_context = get_step_context()\n    # Get the URI where the output will be saved.\n    uri = step_context.get_output_artifact_uri()\n\n    # Get the materializer that will be used to save the output.\n    materializer = step_context.get_output_materializer() \n```\n\n{% hint style=\"info\" %}\nSee the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-new/#zenml.new.steps.step\\_context.StepContext) for more information on which attributes and methods the `StepContext` provides.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to fetch metadata during pipeline composition.\n---\n\n# Fetch metadata during pipeline composition\n\n### Pipeline configuration using the `PipelineContext`\n\nTo find information about the pipeline configuration during pipeline composition, you\ncan use the `zenml.get_pipeline_context()` function to access the `PipelineContext` of\nyour pipeline:\n\n```python\nfrom zenml import get_pipeline_context, pipeline\n\n...\n\n@pipeline(\n    extra={\n        \"complex_parameter\": [\n            (\"sklearn.tree\", \"DecisionTreeClassifier\"),\n            (\"sklearn.ensemble\", \"RandomForestClassifier\"),\n        ]\n    }\n)\ndef my_pipeline():\n    context = get_pipeline_context()\n\n    after = []\n    search_steps_prefix = \"hp_tuning_search_\"\n    for i, model_search_configuration in enumerate(\n        context.extra[\"complex_parameter\"]\n    ):\n        step_name = f\"{search_steps_prefix}{i}\"\n        cross_validation(\n            model_package=model_search_configuration[0],\n            model_class=model_search_configuration[1],\n            id=step_name\n        )\n        after.append(step_name)\n    select_best_model(\n        search_steps_prefix=search_steps_prefix, \n        after=after,\n    )\n```\n\n{% hint style=\"info\" %}\nSee the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.new.pipelines.pipeline_context.PipelineContext) for more information on which attributes and methods the `PipelineContext` provides.\n{% endhint %}\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Tracking metrics and metadata\n---\n\n# \ud83d\udcc8 Track metrics and metadata\n\nLogging metrics and metadata is standardized in ZenML. The most common pattern is to use the `log_xxx` methods, e.g.:\n\n* Log metadata to a [model](attach-metadata-to-a-model.md): `log_model_metadata`\n* Log metadata to an [artifact](attach-metadata-to-an-artifact.md): `log_artifact_metadata`\n* Log metadata to a [step](attach-metadata-to-steps.md): `log_step_metadata`\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to group key-value pairs in the dashboard.\n---\n\n# Grouping Metadata in the Dashboard\n\n![Metadata in the dashboard](../../.gitbook/assets/metadata-in-dashboard.png)\n\nWhen logging metadata passing a dictionary of dictionaries in the `metadata` parameter will group the metadata into cards in the ZenML dashboard. This feature helps organize metadata into logical sections, making it easier to visualize and understand.\n\nHere's an example of grouping metadata into cards:\n\n```python\nfrom zenml.metadata.metadata_types import StorageSize\n\nlog_artifact_metadata(\n    metadata={\n        \"model_metrics\": {\n            \"accuracy\": 0.95,\n            \"precision\": 0.92,\n            \"recall\": 0.90\n        },\n        \"data_details\": {\n            \"dataset_size\": StorageSize(1500000),\n            \"feature_columns\": [\"age\", \"income\", \"score\"]\n        }\n    }\n)\n```\n\nIn the ZenML dashboard, \"model\\_metrics\" and \"data\\_details\" would appear as separate cards, each containing their respective key-value pairs.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: >-\n  Attach any metadata as key-value pairs to your models for future reference and\n  auditability.\n---\n\n# Attach metadata to a model\n\n## Logging Metadata for Models\n\nWhile artifact metadata is specific to individual outputs of steps, model metadata encapsulates broader and more general information that spans across multiple artifacts. For example, evaluation results or the name of a customer for whom the model is intended could be logged with the model.\n\nHere's an example of logging metadata for a model:\n\n```python\nfrom zenml import step, log_model_metadata, ArtifactConfig, get_step_context\nfrom typing import Annotated\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.base import ClassifierMixin\n\n@step\ndef train_model(dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig(name=\"sklearn_classifier\", is_model_artifact=True)]:\n    \"\"\"Train a model\"\"\"\n    # Fit the model and compute metrics\n    classifier = RandomForestClassifier().fit(dataset)\n    accuracy, precision, recall = ...\n\n    # Log metadata for the model\n    # This associates the metadata with the ZenML model, not the artifact\n    log_model_metadata(\n        metadata={\n            \"evaluation_metrics\": {\n                \"accuracy\": accuracy,\n                \"precision\": precision,\n                \"recall\": recall\n            }\n        },\n        # Omitted model_name will use the model in the current context\n        model_name=\"zenml_model_name\",\n        # Omitted model_version will default to 'latest'\n        model_version=\"zenml_model_version\",\n    )\n    return classifier\n```\n\nIn this example, the metadata is associated with the model rather than the specific classifier artifact. This is particularly useful when the metadata reflects an aggregation or summary of various steps and artifacts in the pipeline.\n\n## Fetching logged metadata\n\nOnce metadata has been logged in an [artifact](attach-metadata-to-an-artifact.md), model, or [step](attach-metadata-to-steps.md), we can easily fetch the metadata with the ZenML Client:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nmodel = client.get_model_version(\"my_model\", \"my_version\")\n\nprint(model.run_metadata[\"metadata_key\"].value)\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to log metadata for artifacts and models in ZenML.\n---\n\n# Attach metadata to an artifact\n\n![Metadata in the dashboard](../../.gitbook/assets/metadata-in-dashboard.png)\n\nMetadata plays a critical role in ZenML, providing context and additional information about various entities within the platform. Anything which is `metadata` in ZenML can be compared in the dashboard.\n\nThis guide will explain how to log metadata for artifacts and models in ZenML and detail the types of metadata that can be logged.\n\n## Logging Metadata for Artifacts\n\nArtifacts in ZenML are outputs of steps within a pipeline, such as datasets, models, or evaluation results. Associating metadata with artifacts can help users understand the nature and characteristics of these outputs.\n\nTo log metadata for an artifact, you can use the `log_artifact_metadata` method. This method allows you to attach a dictionary of key-value pairs as metadata to an artifact. The metadata can be any JSON-serializable value, including custom classes such as `Uri`, `Path`, `DType`, and `StorageSize`. Find out more about these different types [here](../track-metrics-metadata/logging-metadata.md).&#x20;\n\nHere's an example of logging metadata for an artifact:\n\n```python\nfrom zenml import step, log_artifact_metadata\nfrom zenml.metadata.metadata_types import StorageSize\n\n@step\ndef process_data_step(dataframe: pd.DataFrame) ->  Annotated[pd.DataFrame, \"processed_data\"],:\n    \"\"\"Process a dataframe and log metadata about the result.\"\"\"\n    # Perform processing on the dataframe...\n    processed_dataframe = ...\n\n    # Log metadata about the processed dataframe\n    log_artifact_metadata(\n        artifact_name=\"processed_data\",\n        metadata={\n            \"row_count\": len(processed_dataframe),\n            \"columns\": list(processed_dataframe.columns),\n            \"storage_size\": StorageSize(processed_dataframe.memory_usage().sum())\n        }\n    )\n    return processed_dataframe\n```\n\n## Fetching logged metadata\n\nOnce metadata has been logged in an artifact, or [step](../track-metrics-metadata/attach-metadata-to-a-model.md), we can easily fetch the metadata with the ZenML Client:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nartifact = client.get_artifact_version(\"my_artifact\", \"my_version\")\n\nprint(artifact.run_metadata[\"metadata_key\"].value)\n```\n\n## Grouping Metadata in the Dashboard\n\nWhen logging metadata passing a dictionary of dictionaries in the `metadata` parameter will group the metadata into cards in the ZenML dashboard. This feature helps organize metadata into logical sections, making it easier to visualize and understand.\n\nHere's an example of grouping metadata into cards:\n\n```python\nfrom zenml.metadata.metadata_types import StorageSize\n\nlog_artifact_metadata(\n    metadata={\n        \"model_metrics\": {\n            \"accuracy\": 0.95,\n            \"precision\": 0.92,\n            \"recall\": 0.90"}
{"input": "\n        },\n        \"data_details\": {\n            \"dataset_size\": StorageSize(1500000),\n            \"feature_columns\": [\"age\", \"income\", \"score\"]\n        }\n    }\n)\n```\n\nIn the ZenML dashboard, \"model\\_metrics\" and \"data\\_details\" would appear as separate cards, each containing their respective key-value pairs.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Connect in with your User (interactive)\n\nYou can authenticate your clients with the ZenML Server using the ZenML CLI and the web based login. This can be executed with the command:\n\n```bash\nzenml connect --url https://...\n```\n\nThis command will start a series of steps to validate the device from where you are connecting that will happen in your browser. You can choose whether to mark your respective device as trusted or not. If you choose not to click `Trust this device`, a 24-hour token will be issued for authentication services. Choosing to trust the device will issue a 30-day token instead.\n\nTo see all devices you've permitted, use the following command:\n\n```bash\nzenml authorized-device list\n```\n\nAdditionally, the following command allows you to more precisely inspect one of these devices:\n\n```bash\nzenml authorized-device describe <DEVICE_ID>  \n```\n\nFor increased security, you can invalidate a token using the `zenml device lock` command followed by the device ID. This helps provide an extra layer of security and control over your devices.\n\n```\nzenml authorized-device lock <DEVICE_ID>  \n```\n\nTo keep things simple, we can summarize the steps:\n\n1. Use the `zenml connect --url` command to start a device flow and connect to a zenml server.\n2. Choose whether to trust the device when prompted.\n3. Check permitted devices with `zenml devices list`.\n4. Invalidate a token with `zenml device lock ...`.\n\n### Important notice\n\nUsing the ZenML CLI is a secure and comfortable way to interact with your ZenML tenants. It's important to always ensure that only trusted devices are used to maintain security and privacy.\n\nDon't forget to manage your device trust levels regularly for optimal security. Should you feel a device trust needs to be revoked, lock the device immediately. Every token issued is a potential gateway to access your data, secrets and infrastructure.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Various means of connecting to ZenML.\n---\n\n# \ud83d\udd0c Connect to a server\n\nOnce [ZenML is deployed](../../user-guide/production-guide/deploying-zenml.md), there are various ways to connect to it.&#x20;\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Connect with a Service Account\n\nSometimes you may need to authenticate to a ZenML server from a non-interactive environment where the web login is not possible, like a CI/CD workload or a serverless function. In these cases, you can configure a service account and an API key and use the API key to authenticate to the ZenML server:\n\n```bash\nzenml service-account create <SERVICE_ACCOUNT_NAME>\n```\n\nThis command creates a service account and an API key for it. The API key is displayed as part of the command output and cannot be retrieved later. You can then use the issued API key to connect your ZenML client to the server through one of the following methods:\n\n* using the CLI:\n\n```bash\nzenml connect --url https://... --api-key <API_KEY>\n```\n\n* setting the `ZENML_STORE_URL` and `ZENML_STORE_API_KEY` environment variables when you set up your ZenML client for the first time. This method is particularly useful when you are using the ZenML client in an automated CI/CD workload environment like GitHub Actions or GitLab CI or in a containerized environment like Docker or Kubernetes:\n\n```bash\nexport ZENML_STORE_URL=https://...\nexport ZENML_STORE_API_KEY=<API_KEY>\n```\n\nTo see all the service accounts you've created and their API keys, use the following commands:\n\n```bash\nzenml service-account list\nzenml service-account api-key <SERVICE_ACCOUNT_NAME> list\n```\n\nAdditionally, the following command allows you to more precisely inspect one of these service accounts and an API key:\n\n```bash\nzenml service-account describe <SERVICE_ACCOUNT_NAME>\nzenml service-account api-key <SERVICE_ACCOUNT_NAME> describe <API_KEY_NAME>\n```\n\nAPI keys don't have an expiration date. For increased security, we recommend that you regularly rotate the API keys to prevent unauthorized access to your ZenML server. You can do this with the ZenML CLI:\n\n```bash\nzenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME>\n```\n\nRunning this command will create a new API key and invalidate the old one. The new API key is displayed as part of the command output and cannot be retrieved later. You can then use the new API key to connect your ZenML client to the server just as described above.\n\nWhen rotating an API key, you can also configure a retention period for the old API key. This is useful if you need to keep the old API key for a while to ensure that all your workloads have been updated to use the new API key. You can do this with the `--retain` flag. For example, to rotate an API key and keep the old one for 60 minutes, you can run the following command:\n\n```bash\nzenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME> \\\n      --retain 60\n```\n\nFor increased security, you"}
{"input": " can deactivate a service account or an API key using one of the following commands:\n\n```\nzenml service-account update <SERVICE_ACCOUNT_NAME> --active false\nzenml service-account api-key <SERVICE_ACCOUNT_NAME> update <API_KEY_NAME> \\\n      --active false\n```\n\nDeactivating a service account or an API key will prevent it from being used to authenticate and has immediate effect on all workloads that use it.\n\nTo keep things simple, we can summarize the steps:\n\n1. Use the `zenml service-account create` command to create a service account and an API key.\n2. Use the `zenml connect --url <url> --api-key <api-key>` command to connect your ZenML client to the server using the API key.\n3. Check configured service accounts with `zenml service-account list`.\n4. Check configured API keys with `zenml service-account api-key <SERVICE_ACCOUNT_NAME> list`.\n5. Regularly rotate API keys with `zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate`.\n6. Deactivate service accounts or API keys with `zenml service-account update` or `zenml service-account api-key <SERVICE_ACCOUNT_NAME> update`.\n\n### Important notice\n\nEvery API key issued is a potential gateway to access your data, secrets and infrastructure. It's important to regularly rotate API keys and deactivate or delete service accounts and API keys that are no longer needed.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: >-\n  The complete guide to managing Service Connectors and connecting ZenML to\n  external resources.\n---\n\n# Service Connectors guide\n\nThis documentation section contains everything that you need to use Service Connectors to connect ZenML to external resources. A lot of information is covered, so it might be useful to use the following guide to navigate it:\n\n* if you're only getting started with Service Connectors, we suggest starting by familiarizing yourself with the [terminology](service-connectors-guide.md#terminology).\n* check out the section on [Service Connector Types](service-connectors-guide.md#cloud-provider-service-connector-types) to understand the different Service Connector implementations that are available and when to use them.\n* jumping straight to the sections on [Registering Service Connectors](service-connectors-guide.md#register-service-connectors) can get you set up quickly if you are only looking for a quick way to evaluate Service Connectors and their features.\n* if all you need to do is connect a ZenML Stack Component to an external resource or service like a Kubernetes cluster, a Docker container registry, or an object storage bucket, and you already have some Service Connectors available, the section on [connecting Stack Components to resources](service-connectors-guide.md#connect-stack-components-to-resources) is all you need.\n\nIn addition to this guide, there is an entire section dedicated to [best security practices concerning the various authentication methods](best-security-practices.md) implemented by Service Connectors, such as which types of credentials to use in development or production and how to keep your security information safe. That section is particularly targeted at engineers with some knowledge of infrastructure, but it should be accessible to larger audiences.\n\n## Terminology\n\nAs with any high-level abstraction, some terminology is needed to express the concepts and operations involved. In spite of the fact that Service Connectors cover such a large area of application as authentication and authorization for a variety of resources from a range of different vendors, we managed to keep this abstraction clean and simple. In the following expandable sections, you'll learn more about Service Connector Types, Resource Types, Resource Names, and Service Connectors.\n\n<details>\n\n<summary>Service Connector Types</summary>\n\nThis term is used to represent and identify a particular Service Connector implementation and answer questions about its capabilities such as \"what types of resources does this Service Connector give me access to\", \"what authentication methods does it support\" and \"what credentials and other information do I need to configure for it\". This is analogous to the role Flavors play for Stack Components in that the Service Connector Type acts as the template from which one or more Service Connectors are created.\n\nFor example, the built-in AWS Service Connector Type shipped with ZenML supports a rich variety of authentication methods and provides access to AWS resources such as S3 buckets, EKS clusters and ECR registries.\n\nThe `zenml service-connector list-types` and `zenml service-"}
{"input": "connector describe-type` CLI commands can be used to explore the Service Connector Types available with your ZenML deployment. Extensive documentation is included covering supported authentication methods and Resource Types. The following are just some examples:\n\n```sh\nzenml service-connector list-types\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             NAME             \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 AUTH METHODS      \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Kubernetes Service Connector \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 password          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502                       \u2502 token             \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   Docker Service Connector   \u2502 \ud83d\udc33 docker     \u2502 \ud83d\udc33 docker-registry    \u2502 password          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   Azure Service Connector    \u2502 \ud83c\udde6 azure      \u2502 \ud83c\udde6 azure-generic      \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 blob-container     \u2502 service-principal \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 access-token      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502                   \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    AWS Service Connector     \u2502 \ud83d\udd36 aws        \u2502 \ud83d\udd36 aws-generic        \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key        \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token         \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 iam-role          \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502"}
{"input": " session-token     \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 federation-token  \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    GCP Service Connector     \u2502 \ud83d\udd35 gcp        \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account   \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 impersonation     \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe-type aws\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                \ud83d\udd36 AWS Service Connector (connector type: aws)                \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 secret-key                                                                \n \u2022 \ud83d\udd12 sts-token                                                                 \n \u2022 \ud83d\udd12 iam-role                                                                  \n \u2022 \ud83d\udd12 session-token                                                             \n \u2022 \ud83d\udd12 federation-token                                                          \n                                                                                \nResource types:                                                                 \n                                                                                \n \u2022 \ud83d\udd36 aws-generic                                                               \n \u2022 \ud83d\udce6 s3-bucket                                                                 \n \u2022 \ud83c\udf00 kubernetes-cluster                                                        \n \u2022 \ud83d\udc33 docker-registry                                                           \n                                                                                \nSupports auto-configuration: True                                               \n                                                                                \nAvailable locally: True                                                         \n                                                                                \nAvailable remotely: False                                                       \n                                                                                \nThe ZenML AWS Service Connector facilitates the authentication and access to    \nmanaged AWS services and resources. These encompass a range of resources,       \nincluding S3 buckets, ECR repositories, and EKS clusters. The connector provides\nsupport for various authentication methods, including explicit long-lived AWS   \nsecret keys, IAM roles, short-lived STS tokens and implicit authentication.     \n                                                                                \nTo ensure heightened security measures, this connector also enables"}
{"input": " the         \ngeneration of temporary STS security tokens that are scoped down to the minimum \npermissions necessary for accessing the intended resource. Furthermore, it      \nincludes automatic configuration and detection of credentials locally configured\nthrough the AWS CLI.                                                            \n                                                                                \nThis connector serves as a general means of accessing any AWS service by issuing\npre-authenticated boto3 sessions to clients. Additionally, the connector can    \nhandle specialized authentication for S3, Docker and Kubernetes Python clients. \nIt also allows for the configuration of local Docker and Kubernetes CLIs.       \n                                                                                \nThe AWS Service Connector is part of the AWS ZenML integration. You can either  \ninstall the entire integration or use a pypi extra to install it independently  \nof the integration:                                                             \n                                                                                \n \u2022 pip install \"zenml[connectors-aws]\" installs only prerequisites for the AWS    \n   Service Connector Type                                                       \n \u2022 zenml integration install aws installs the entire AWS ZenML integration      \n                                                                                \nIt is not required to install and set up the AWS CLI on your local machine to   \nuse the AWS Service Connector to link Stack Components to AWS resources and     \nservices. However, it is recommended to do so if you are looking for a quick    \nsetup that includes using the auto-configuration Service Connector features.    \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe-type aws --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551      \ud83c\udf00 AWS EKS Kubernetes cluster (resource type: kubernetes-cluster)       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods: implicit, secret-key, sts-token, iam-role,              \nsession-token, federation-token                                                 \n                                                                                \nSupports resource instances: True                                               \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 secret-key                                                                \n \u2022 \ud83d\udd12 sts-token                                                                 \n \u2022 \ud83d\udd12 iam-role                                                                  \n \u2022 \ud83d\udd12 session-token                                                             \n \u2022 \ud83d\udd12 federation-token                                                          \n                                                                                \nAllows users to access an EKS cluster as a standard Kubernetes cluster resource.\nWhen used by Stack Components, they are provided a pre-authenticated            \npython-kubernetes client instance.                                              \n                                                                                \nThe configured credentials must have at least the following AWS IAM permissions \nassociated with the ARNs of EKS clusters that the connector will be allowed to  \naccess (e.g. arn:aws:eks:{region}:{account}:cluster/* represents all the EKS    \nclusters available in the target AWS region).                                   \n"}
{"input": "                                                                                \n \u2022 eks:ListClusters                                                             \n \u2022 eks:DescribeCluster                                                          \n                                                                                \nIn addition to the above permissions, if the credentials are not associated with\nthe same IAM user or role that created the EKS cluster, the IAM principal must  \nbe manually added to the EKS cluster's aws-auth ConfigMap, otherwise the        \nKubernetes client will not be allowed to access the cluster's resources. This   \nmakes it more challenging to use the AWS Implicit and AWS Federation Token      \nauthentication methods for this resource. For more information, see this        \ndocumentation.                                                                  \n                                                                                \nIf set, the resource name must identify an EKS cluster using one of the         \nfollowing formats:                                                              \n                                                                                \n \u2022 EKS cluster name (canonical resource name): {cluster-name}                   \n \u2022 EKS cluster ARN: arn:aws:eks:{region}:{account}:cluster/{cluster-name}       \n                                                                                \nEKS cluster names are region scoped. The connector can only be used to access   \nEKS clusters in the AWS region that it is configured to use.                    \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe-type aws --auth-method secret-key\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                 \ud83d\udd12 AWS Secret Key (auth method: secret-key)                  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nSupports issuing temporary credentials: False                                   \n                                                                                \nLong-lived AWS credentials consisting of an AWS access key ID and secret access \nkey associated with an AWS IAM user or AWS account root user (not recommended). \n                                                                                \nThis method is preferred during development and testing due to its simplicity   \nand ease of use. It is not recommended as a direct authentication method for    \nproduction use cases because the clients have direct access to long-lived       \ncredentials and are granted the full set of permissions of the IAM user or AWS  \naccount root user associated with the credentials. For production, it is        \nrecommended to use the AWS IAM Role, AWS Session Token or AWS Federation Token  \nauthentication method instead.                                                  \n                                                                                \nAn AWS region is required and the connector may only be used to access AWS      \nresources in the specified region.                                              \n                                                                                \nIf you already have the local AWS CLI set up with these credentials, they will  \nbe automatically picked up when auto-configuration is used.                     \n                                                                                \nAttributes:                                                                     \n                                                                                \n \u2022 aws_access_key_id {string, secret, required}: AWS Access Key ID              \n \u2022 aws_secret_access_key {string, secret"}
{"input": ", required}: AWS Secret Access Key      \n \u2022 region {string, required}: AWS Region                                        \n \u2022 endpoint_url {string, optional}: AWS Endpoint URL                            \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\n</details>\n\n<details>\n\n<summary>Resource Types</summary>\n\nResource Types are a way of organizing resources into logical, well-known classes based on the standard and/or protocol used to access them, or simply based on their vendor. This creates a unified language that can be used to declare the types of resources that are provided by Service Connectors on one hand and the types of resources that are required by Stack Components on the other hand.\n\nFor example, we use the generic `kubernetes-cluster` resource type to refer to any and all Kubernetes clusters, since they are all generally accessible using the same standard libraries, clients and API regardless of whether they are Amazon EKS, Google GKE, Azure AKS or another flavor of managed or self-hosted deployment. Similarly, there is a generic `docker-registry` resource type that covers any and all container registries that implement the Docker/OCI interface, be it DockerHub, Amazon ECR, Google GCR, Azure ACR, K3D or something similar. Stack Components that need to connect to a Kubernetes cluster (e.g. the Kubernetes Orchestrator or the Seldon Model Deployer) can use the `kubernetes-cluster` resource type identifier to describe their resource requirements and remain agnostic of their vendor.\n\nThe term Resource Type is used in ZenML everywhere resources accessible through Service Connectors are involved. For example, to list all Service Connector Types that can be used to broker access to Kubernetes Clusters, you can pass the `--resource-type` flag to the CLI command:\n\n```sh\nzenml service-connector list-types --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             NAME             \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 AUTH METHODS      \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Kubernetes Service Connector \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 password          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502                       \u2502 token             \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   Azure Service Connector    \u2502 \ud83c\udde6 azure      \u2502 \ud83c\udde6 azure-generic      \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 blob-container     \u2502 service-principal \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 access-token      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502                   \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    AWS Service Connector     \u2502 \ud83d\udd36 aws        \u2502 \ud83d\udd36 aws-generic        \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key        \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token         \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 iam-role          \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 session-token     \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 federation-token  \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    GCP Service Connector     \u2502 \ud83d\udd35 gcp        \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account   \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 impersonation     \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nFrom the above, you can see that there are not one but four Service Connector Types that can connect ZenML to Kubernetes clusters. The first one is a generic implementation that can be used with any standard Kubernetes cluster, including those that run on-premise. The other three deal exclusively with Kubernetes services managed by the AWS, GCP and Azure cloud providers.\n\nConversely, to list all currently registered Service"}
{"input": " Connector instances that provide access to Kubernetes clusters, one might run:\n\n```sh\nzenml service-connector list --resource_type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME                  \u2502 ID                           \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME                \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-iam-multi-eu      \u2502 e33c9fac-5daa-48b2-87bb-0187 \u2502 \ud83d\udd36 aws        \u2502 \ud83d\udd36 aws-generic        \u2502 <multiple>                   \u2502 \u2796     \u2502 default \u2502            \u2502 region:eu-central-1 \u2503\n\u2503        \u2502                       \u2502 d3782cde                     \u2502               \u2502 \ud83d\udce6 s3-bucket          \u2502                              \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2503        \u2502                       \u2502                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502                              \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2503        \u2502                       \u2502                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502                              \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-iam-multi-us      \u2502 ed528d5a-d6cb-4fc4-bc52-c3d2 \u2502 \ud83d\udd36 aws        \u2502 \ud83d\udd36 aws-generic        \u2502 <multiple>                   \u2502 \u2796     \u2502 default \u2502            \u2502 region:us-east-1    \u2503\n\u2503        \u2502                       \u2502 d01643e5                     \u2502               \u2502 \ud83d\udce6 s3-bucket          \u2502                              \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2503        \u2502                       \u2502                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502                              \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2503       "}
{"input": " \u2502                       \u2502                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502                              \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 kube-auto             \u2502 da497715-7502-4cdd-81ed-289e \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 A5F8F4142FB12DDCDE9F21F6E9B0 \u2502 \u2796     \u2502 default \u2502            \u2502                     \u2503\n\u2503        \u2502                       \u2502 70664597                     \u2502               \u2502                       \u2502 7A18.gr7.us-east-1.eks.amazo \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2503        \u2502                       \u2502                              \u2502               \u2502                       \u2502 naws.com                     \u2502        \u2502         \u2502            \u2502                     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n<details>\n\n<summary>Resource Names (also known as Resource IDs)</summary>\n\nIf a Resource Type is used to identify a class of resources, we also need some way to uniquely identify each resource instance belonging to that class that a Service Connector can provide access to. For example, an AWS Service Connector can be configured to provide access to multiple S3 buckets identifiable by their bucket names or their `s3://bucket-name` formatted URIs. Similarly, an AWS Service Connector can be configured to provide access to multiple EKS Kubernetes clusters in the same AWS region, each uniquely identifiable by their EKS cluster name. This is what we call Resource Names.\n\nResource Names make it generally easy to identify a particular resource instance accessible through a Service Connector, especially when used together with the Service Connector name and the Resource Type. The following ZenML CLI command output shows a few examples featuring Resource Names for S3 buckets, EKS clusters, ECR registries and general Kubernetes clusters. As you can see, the way we name resources varies from implementation to implementation and resource type to resource type:\n\n```sh\nzenml service-connector list-resources\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following resources can be accessed by service connectors configured in"}
{"input": " your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 8d307b98-f125-4d7a-b5d5-924c07ba04bb \u2502 aws-session-docker    \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry    \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 d1e5ecf5-1531-4507-bbf5-be0a114907a5 \u2502 aws-session-s3        \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket          \u2502 s3://public-flavor-logos                                         \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://sagemaker-us-east-1-715803424590                            \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://spark-artifact-store                                        \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://spark-demo-as                                               \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://spark-demo-dataset                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 d2341762-28a3-4dfc-98b9-1ae9aaa93228 \u2502 aws-key-docker-eu     \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry    \u2502 715803424590.dkr.ecr.eu-central-1.amazonaws.com                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 0658a465-2921-4d6b-a495-2dc078036037 \u2502 aws-key-kube-zenhacks \u2502"}
{"input": " \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 049e7f5e-e14c-42b7-93d4-a273ef414e66 \u2502 eks-eu-central-1      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 kubeflowmultitenant                                              \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 zenbox                                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 b551f3ae-1448-4f36-97a2-52ce303f20c9 \u2502 kube-auto             \u2502 \ud83c\udf00 kubernetes  \u2502 \ud83c\udf00 kubernetes-cluster \u2502 A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nEvery Service Connector Type defines its own rules for how Resource Names are formatted. These rules are documented in the section belonging each resource type. For example:\n\n```sh\nzenml service-connector describe-type aws --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551        \ud83d\udc33 AWS ECR container registry (resource type: docker-registry)        \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods: implicit, secret-key, sts-token, iam-role,              \nsession-token, federation-token                                                 \n                                                                                \nSupports resource instances: False                                              \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 secret-key                                                                \n \u2022 \ud83d\udd12 sts-token                                                                 \n \u2022 \ud83d\udd12 iam-role                                                                  \n \u2022 \ud83d\udd12 session-token                                                             "}
{"input": "\n \u2022 \ud83d\udd12 federation-token                                                          \n                                                                                \nAllows users to access one or more ECR repositories as a standard Docker        \nregistry resource. When used by Stack Components, they are provided a           \npre-authenticated python-docker client instance.                                \n                                                                                \nThe configured credentials must have at least the following AWS IAM permissions \nassociated with the ARNs of one or more ECR repositories that the connector will\nbe allowed to access (e.g. arn:aws:ecr:{region}:{account}:repository/*          \nrepresents all the ECR repositories available in the target AWS region).        \n                                                                                \n \u2022 ecr:DescribeRegistry                                                         \n \u2022 ecr:DescribeRepositories                                                     \n \u2022 ecr:ListRepositories                                                         \n \u2022 ecr:BatchGetImage                                                            \n \u2022 ecr:DescribeImages                                                           \n \u2022 ecr:BatchCheckLayerAvailability                                              \n \u2022 ecr:GetDownloadUrlForLayer                                                   \n \u2022 ecr:InitiateLayerUpload                                                      \n \u2022 ecr:UploadLayerPart                                                          \n \u2022 ecr:CompleteLayerUpload                                                      \n \u2022 ecr:PutImage                                                                 \n \u2022 ecr:GetAuthorizationToken                                                    \n                                                                                \nThis resource type is not scoped to a single ECR repository. Instead, a         \nconnector configured with this resource type will grant access to all the ECR   \nrepositories that the credentials are allowed to access under the configured AWS\nregion (i.e. all repositories under the Docker registry URL                     \nhttps://{account-id}.dkr.ecr.{region}.amazonaws.com).                           \n                                                                                \nThe resource name associated with this resource type uniquely identifies an ECR \nregistry using one of the following formats (the repository name is ignored,    \nonly the registry URL/ARN is used):                                             \n                                                                                \n \u2022 ECR repository URI (canonical resource name):                                \n   [https://]{account}.dkr.ecr.{region}.amazonaws.com[/{repository-name}]       \n \u2022 ECR repository ARN:                                                          \n   arn:aws:ecr:{region}:{account-id}:repository[/{repository-name}]             \n                                                                                \nECR repository names are region scoped. The connector can only be used to access\nECR repositories in the AWS region that it is configured to use.                \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\n</details>\n\n<details>\n\n<summary>Service Connectors</summary>\n\nThe Service Connector is how you configure ZenML to authenticate and connect to one or more external resources. It stores the required configuration and security credentials and can optionally be scoped with a Resource Type and a Resource Name.\n\nDepending on the Service Connector Type implementation, a Service Connector instance can be configured in one of the following modes with regards to the types and number of resources that it has access to:\n\n* a **multi-type** Service Connector instance that can be configured once and used to gain access to multiple types of resources. This is only possible with Service Connector Types that support multiple Resource Types to begin with,"}
{"input": " such as those that target multi-service cloud providers like AWS, GCP and Azure. In contrast, a **single-type** Service Connector can only be used with a single Resource Type. To configure a multi-type Service Connector, you can simply skip scoping its Resource Type during registration.\n* a **multi-instance** Service Connector instance can be configured once and used to gain access to multiple resources of the same type, each identifiable by a Resource Name. Not all types of connectors and not all types of resources support multiple instances. Some Service Connectors Types like the generic Kubernetes and Docker connector types only allow **single-instance** configurations: a Service Connector instance can only be used to access a single Kubernetes cluster and a single Docker registry. To configure a multi-instance Service Connector, you can simply skip scoping its Resource Name during registration.\n\nThe following is an example of configuring a multi-type AWS Service Connector instance capable of accessing multiple AWS resources of different types:\n\n```sh\nzenml service-connector register aws-multi-type --type aws --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u280b Registering service connector 'aws-multi-type'...\nSuccessfully registered service connector `aws-multi-type` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://aws-ia-mwaa-715803424590                \u2503\n\u2503                       \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2503                       \u2502 s3://zenml-public-datasets                   \u2503\n\u2503                       \u2502 s3://zenml-public-swagger-spec               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %"}
{"input": "}\n\nThe following is an example of configuring a multi-instance AWS S3 Service Connector instance capable of accessing multiple AWS S3 buckets:\n\n```sh\nzenml service-connector register aws-s3-multi-instance --type aws --auto-configure --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-s3-multi-instance'...\nSuccessfully registered service connector `aws-s3-multi-instance` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://aws-ia-mwaa-715803424590         \u2503\n\u2503               \u2502 s3://zenfiles                         \u2503\n\u2503               \u2502 s3://zenml-demos                      \u2503\n\u2503               \u2502 s3://zenml-generative-chat            \u2503\n\u2503               \u2502 s3://zenml-public-datasets            \u2503\n\u2503               \u2502 s3://zenml-public-swagger-spec        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe following is an example of configuring a single-instance AWS S3 Service Connector instance capable of accessing a single AWS S3 bucket:\n\n```sh\nzenml service-connector register aws-s3-zenfiles --type aws --auto-configure --resource-type s3-bucket --resource-id s3://zenfiles\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u283c Registering service connector 'aws-s3-zenfiles'...\nSuccessfully registered service connector `aws-s3-zenfiles` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Explore Service Connector Types\n\nService Connector Types are not only templates used to instantiate Service Connectors, they also form a body of knowledge that documents best security practices and guides users through the complicated world of authentication and authorization.\n\nZenML ships with a handful of Service Connector Types that enable you right out-of-the-box to connect ZenML to cloud resources and services available from cloud providers such"}
{"input": " as AWS and GCP, as well as on-premise infrastructure. In addition to built-in Service Connector Types, ZenML can be easily extended with custom Service Connector implementations.\n\nTo discover the Connector Types available with your ZenML deployment, you can use the `zenml service-connector list-types` CLI command:\n\n```sh\nzenml service-connector list-types\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             NAME             \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 AUTH METHODS      \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Kubernetes Service Connector \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 password          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502                       \u2502 token             \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   Docker Service Connector   \u2502 \ud83d\udc33 docker     \u2502 \ud83d\udc33 docker-registry    \u2502 password          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   Azure Service Connector    \u2502 \ud83c\udde6 azure      \u2502 \ud83c\udde6 azure-generic      \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 blob-container     \u2502 service-principal \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 access-token      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502                   \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    AWS Service Connector     \u2502 \ud83d\udd36 aws        \u2502 \ud83d\udd36 aws-generic        \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key        \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token         \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-"}
{"input": "registry    \u2502 iam-role          \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 session-token     \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 federation-token  \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    GCP Service Connector     \u2502 \ud83d\udd35 gcp        \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account   \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token      \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 impersonation     \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n<details>\n\n<summary>Exploring the documentation embedded into Service Connector Types</summary>\n\nA lot more is hidden behind a Service Connector Type than a name and a simple list of resource types. Before using a Service Connector Type to configure a Service Connector, you probably need to understand what it is, what it can offer and what are the supported authentication methods and their requirements. All this can be accessed directly through the CLI. Some examples are included here.\n\nShowing information about the `gcp` Service Connector Type:\n\n```sh\nzenml service-connector describe-type gcp\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                \ud83d\udd35 GCP Service Connector (connector type: gcp)                \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 user-account                                                              \n \u2022 \ud83d\udd12 service-account                                                           \n \u2022 \ud83d\udd12 oauth2-token                                                              \n \u2022 \ud83d\udd12 impersonation                                                             \n                                                                                \nResource types:                                                                 \n                                                                                \n \u2022 \ud83d\udd35 gcp-generic                                                               \n \u2022 \ud83d\udce6 gcs-bucket                                                                \n \u2022 \ud83c\udf00 kubernetes-cluster                                                        \n \u2022"}
{"input": " \ud83d\udc33 docker-registry                                                           \n                                                                                \nSupports auto-configuration: True                                               \n                                                                                \nAvailable locally: True                                                         \n                                                                                \nAvailable remotely: True                                                        \n                                                                                \nThe ZenML GCP Service Connector facilitates the authentication and access to    \nmanaged GCP services and resources. These encompass a range of resources,       \nincluding GCS buckets, GCR container repositories and GKE clusters. The         \nconnector provides support for various authentication methods, including GCP    \nuser accounts, service accounts, short-lived OAuth 2.0 tokens and implicit      \nauthentication.                                                                 \n                                                                                \nTo ensure heightened security measures, this connector always issues short-lived\nOAuth 2.0 tokens to clients instead of long-lived credentials. Furthermore, it  \nincludes automatic configuration and detection of  credentials locally          \nconfigured through the GCP CLI.                                                 \n                                                                                \nThis connector serves as a general means of accessing any GCP service by issuing\nOAuth 2.0 credential objects to clients. Additionally, the connector can handle \nspecialized authentication for GCS, Docker and Kubernetes Python clients. It    \nalso allows for the configuration of local Docker and Kubernetes CLIs.          \n                                                                                \nThe GCP Service Connector is part of the GCP ZenML integration. You can either  \ninstall the entire integration or use a pypi extra to install it independently  \nof the integration:                                                             \n                                                                                \n \u2022 pip install \"zenml[connectors-gcp]\" installs only prerequisites for the GCP    \n   Service Connector Type                                                       \n \u2022 zenml integration install gcp installs the entire GCP ZenML integration      \n                                                                                \nIt is not required to install and set up the GCP CLI on your local machine to   \nuse the GCP Service Connector to link Stack Components to GCP resources and     \nservices. However, it is recommended to do so if you are looking for a quick    \nsetup that includes using the auto-configuration Service Connector features.    \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\nFetching details about the GCP `kubernetes-cluster` resource type (i.e. the GKE cluster):\n\n```sh\nzenml service-connector describe-type gcp --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551      \ud83c\udf00 GCP GKE Kubernetes cluster (resource type: kubernetes-cluster)       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods: implicit, user-account, service-account, oauth2-token,  \nimpersonation                                                                   \n                                                                                \nSupports resource instances: True                                               "}
{"input": "\n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 user-account                                                              \n \u2022 \ud83d\udd12 service-account                                                           \n \u2022 \ud83d\udd12 oauth2-token                                                              \n \u2022 \ud83d\udd12 impersonation                                                             \n                                                                                \nAllows Stack Components to access a GKE registry as a standard Kubernetes       \ncluster resource. When used by Stack Components, they are provided a            \npre-authenticated Python Kubernetes client instance.                            \n                                                                                \nThe configured credentials must have at least the following GCP permissions     \nassociated with the GKE clusters that it can access:                            \n                                                                                \n \u2022 container.clusters.list                                                      \n \u2022 container.clusters.get                                                       \n                                                                                \nIn addition to the above permissions, the credentials should include permissions\nto connect to and use the GKE cluster (i.e. some or all permissions in the      \nKubernetes Engine Developer role).                                              \n                                                                                \nIf set, the resource name must identify an GKE cluster using one of the         \nfollowing formats:                                                              \n                                                                                \n \u2022 GKE cluster name: {cluster-name}                                             \n                                                                                \nGKE cluster names are project scoped. The connector can only be used to access  \nGKE clusters in the GCP project that it is configured to use.                   \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\nDisplaying information about the `service-account` GCP authentication method:\n\n```sh\nzenml service-connector describe-type gcp --auth-method service-account\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551            \ud83d\udd12 GCP Service Account (auth method: service-account)             \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nSupports issuing temporary credentials: False                                   \n                                                                                \nUse a GCP service account and its credentials to authenticate to GCP services.  \nThis method requires a GCP service account and a service account key JSON       \ncreated for it.                                                                 \n                                                                                \nThe GCP connector generates temporary OAuth 2.0 tokens from the user account    \ncredentials and distributes them to clients. The tokens have a limited lifetime \nof 1 hour.                                                                      \n                                                                                \nA GCP project is required and the connector may only be used to access GCP      \nresources in the specified project.                                             \n                                                                                \nIf you already have the GOOGLE_APPLICATION_CREDENTIALS environment variable     \nconfigured to point to a service account key JSON file, it will be automatically\npicked up when auto-configuration is used.                                      \n                                                                                \nAttributes:                                                                     \n                                                                                \n \u2022 service_account_json {string, secret, required}: GCP Service Account Key JSON\n"}
{"input": " \u2022 project_id {string, required}: GCP Project ID where the target resource is   \n   located.                                                                     \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\n</details>\n\n### Basic Service Connector Types\n\nService Connector Types like the [Kubernetes Service Connector](kubernetes-service-connector.md) and [Docker Service Connector](docker-service-connector.md) can only handle one resource at a time: a Kubernetes cluster and a Docker container registry respectively. These basic Service Connector Types are the easiest to instantiate and manage, as each Service Connector instance is tied exactly to one resource (i.e. they are _single-instance_ connectors).\n\nThe following output shows two Service Connector instances configured from basic Service Connector Types:\n\n* a Docker Service Connector that grants authenticated access to the DockerHub registry and allows pushing/pulling images that are stored in private repositories belonging to a DockerHub account\n* a Kubernetes Service Connector that authenticates access to a Kubernetes cluster running on-premise and allows managing containerized workloads running there.\n\n```\n$ zenml service-connector list\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME           \u2502 ID                                   \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 dockerhub      \u2502 b485626e-7fee-4525-90da-5b26c72331eb \u2502 \ud83d\udc33 docker     \u2502 \ud83d\udc33 docker-registry    \u2502 docker.io     \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 kube-on-prem   \u2502 4315e8eb-fcbd-4938-a4d7-a9218ab372a1 \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 192.168.0.12  \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n```\n\n### Cloud provider Service Connector Types\n\nCloud service providers like AWS, GCP and Azure implement one or more authentication schemes that are unified across a wide range or resources and services, all managed under the same umbrella. This allows users to access many different resources with a single set of authentication credentials. Some authentication methods are straightforward to set up, but are only meant to be used for development and testing. Other authentication schemes are powered by extensive roles and permissions management systems and are targeted at production environments where security and operations at scale are big concerns. The corresponding cloud provider Service Connector Types are designed accordingly:\n\n* they support multiple types of resources (e.g. Kubernetes clusters, Docker registries, a form of object storage)\n* they usually include some form of \"generic\" Resource Type that can be used by clients to access types of resources that are not yet part of the supported set. When this generic Resource Type is used, clients and Stack Components that access the connector are provided some form of generic session, credentials or client that can be used to access any of the cloud provider resources. For example, in the AWS case, clients accessing the `aws-generic` Resource Type are issued a pre-authenticated `boto3` Session object that can be used to access any AWS service.\n* they support multiple authentication methods. Some of these allow clients direct access to long-lived, broad-access credentials and are only recommended for local development use. Others support distributing temporary API tokens automatically generated from long-lived credentials, which are safer for production use-cases, but may be more difficult to set up. A few authentication methods even support down-scoping the permissions of temporary API tokens so that they only allow access to the target resource and restrict access to everything else. This is covered at length [in the section on best practices for authentication methods](service-connectors-guide.md).\n* there is flexibility regarding the range of resources that a single cloud provider Service Connector instance configured with a single set of credentials can be scoped to access:\n  * a _multi-type Service Connector_ instance can access any type of resources from the range of supported Resource Types\n  * a _multi-instance Service Connector_ instance can access multiple resources of the same type\n  * a _single-instance Service Connector_ instance is scoped to access a single resource\n\nThe following output shows three different Service Connectors configured from the same GCP Service Connector Type using three different scopes but with the same credentials:\n\n* a multi-type GCP Service Connector that allows access to every possible resource accessible with the configured credentials"}
{"input": "\n* a multi-instance GCS Service Connector that allows access to multiple GCS buckets\n* a single-instance GCS Service Connector that only permits access to one GCS bucket\n\n```\n$ zenml service-connector list\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME                   \u2502 ID                                   \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME           \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcp-multi              \u2502 9d953320-3560-4a78-817c-926a3898064d \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 <multiple>              \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2503        \u2502                        \u2502                                      \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502                         \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                        \u2502                                      \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502                         \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                        \u2502                                      \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502                         \u2502        \u2502         \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcs-multi              \u2502 ff9c0723-7451-46b7-93ef-fcf3efde30fa \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udce6 gcs-bucket         \u2502 <multiple>              \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcs-langchain-slackbot \u2502 cf3953e9-414c-4875-ba00-24c62a0dc0c5 \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udce6 gcs-bucket         \u2502"}
{"input": " gs://langchain-slackbot \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n### Local and remote availability\n\n{% hint style=\"success\" %}\nYou only need to be aware of local and remote availability for Service Connector Types if you are explicitly looking to use a Service Connector Type without installing its package prerequisites or if you are implementing or using a custom Service Connector Type implementation with your ZenML deployment. In all other cases, you may safely ignore this section.\n{% endhint %}\n\nThe `LOCAL` and `REMOTE` flags in the `zenml service-connector list-types` output indicate if the Service Connector implementation is available locally (i.e. where the ZenML client and pipelines are running) and remotely (i.e. where the ZenML server is running).\n\n{% hint style=\"info\" %}\nAll built-in Service Connector Types are by default available on the ZenML server, but some built-in Service Connector Types require additional Python packages to be installed to be available in your local environment. See the section documenting each Service Connector Type to find what these prerequisites are and how to install them.\n{% endhint %}\n\nThe local/remote availability determines the possible actions and operations that can be performed with a Service Connector. The following are possible with a Service Connector Type that is available either locally or remotely:\n\n* Service Connector registration, update, and discovery (i.e. the `zenml service-connector register`, `zenml service-connector update`, `zenml service-connector list` and `zenml service-connector describe` CLI commands).\n* Service Connector verification: checking whether its configuration and credentials are valid and can be actively used to access the remote resources (i.e. the `zenml service-connector verify` CLI commands).\n* Listing the resources that can be accessed through a Service Connector (i.e. the `zenml service-connector verify` and `zenml service-connector list-resources` CLI commands)\n* Connecting a Stack Component to a remote resource via a Service Connector\n\nThe following operations are only possible with Service Connector Types that are locally available (with some notable exceptions covered in the information box that follows):\n\n* Service Connector auto-configuration and discovery of credentials stored by a local client, CLI, or SDK (e.g. aws or kubectl).\n* Using the configuration and credentials managed by a Service Connector to configure a local client, CLI, or SDK (e.g. docker or kubectl).\n"}
{"input": "* Running pipelines with a Stack Component that is connected to a remote resource through a Service Connector\n\n{% hint style=\"info\" %}\nOne interesting and useful byproduct of the way cloud provider Service Connectors are designed is the fact that you don't need to have the cloud provider Service Connector Type available client-side to be able to access some of its resources. Take the following situation for example:\n\n* the GCP Service Connector Type can provide access to GKE Kubernetes clusters and GCR Docker container registries.\n* however, you don't need the GCP Service Connector Type or any GCP libraries to be installed on the ZenML clients to connect to and use those Kubernetes clusters or Docker registries in your ML pipelines.\n* the Kubernetes Service Connector Type is enough to access any Kubernetes cluster, regardless of its provenance (AWS, GCP, etc.)\n* the Docker Service Connector Type is enough to access any Docker container registry, regardless of its provenance (AWS, GCP, etc.)\n{% endhint %}\n\n## Register Service Connectors\n\nWhen you reach this section, you probably already made up your mind about the type of infrastructure or cloud provider that you want to use to run your ZenML pipelines after reading through [the Service Connector Types section](service-connectors-guide.md#explore-service-connector-types), and you probably carefully weighed your [choices of authentication methods and best security practices](best-security-practices.md). Either that or you simply want to quickly try out a Service Connector to [connect one of the ZenML Stack components to an external resource](service-connectors-guide.md#connect-stack-components-to-resources).\n\nIf you are looking for a quick, assisted tour, we recommend using the interactive CLI mode to configure Service Connectors, especially if this is your first time doing it:\n\n```\nzenml service-connector register -i\n```\n\n<details>\n\n<summary>Interactive Service Connector registration example</summary>\n\n```sh\nzenml service-connector register -i\n```\n\n{% code title=\"Example Command Output\" %}\n```\nPlease enter a name for the service connector: gcp-interactive\nPlease enter a description for the service connector []: Interactive GCP connector example\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                      Available service connector types                       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \n                                                                                \n          \ud83c\udf00 Kubernetes Service Connector (connector type: kubernetes)          \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 password                                                                  \n \u2022 \ud83d\udd12 token                                                                     \n                                                                                \nResource types:                                                                 \n                                                                                \n \u2022 \ud83c\udf00 kubernetes-cluster                                                        \n                                                                                \nSupports auto"}
{"input": "-configuration: True                                               \n                                                                                \nAvailable locally: True                                                         \n                                                                                \nAvailable remotely: True                                                        \n                                                                                \nThis ZenML Kubernetes service connector facilitates authenticating and          \nconnecting to a Kubernetes cluster.                                             \n                                                                                \nThe connector can be used to access to any generic Kubernetes cluster by        \nproviding pre-authenticated Kubernetes python clients to Stack Components that  \nare linked to it and also allows configuring the local Kubernetes CLI (i.e.     \nkubectl).                                                                       \n                                                                                \nThe Kubernetes Service Connector is part of the Kubernetes ZenML integration.   \nYou can either install the entire integration or use a pypi extra to install it \nindependently of the integration:                                               \n                                                                                \n \u2022 pip install \"zenml[connectors-kubernetes]\" installs only prerequisites for the \n   Kubernetes Service Connector Type                                            \n \u2022 zenml integration install kubernetes installs the entire Kubernetes ZenML    \n   integration                                                                  \n                                                                                \nA local Kubernetes CLI (i.e. kubectl ) and setting up local kubectl             \nconfiguration contexts is not required to access Kubernetes clusters in your    \nStack Components through the Kubernetes Service Connector.                      \n                                                                                \n                                                                                \n              \ud83d\udc33 Docker Service Connector (connector type: docker)              \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 password                                                                  \n                                                                                \nResource types:                                                                 \n                                                                                \n \u2022 \ud83d\udc33 docker-registry                                                           \n                                                                                \nSupports auto-configuration: False                                              \n                                                                                \nAvailable locally: True                                                         \n                                                                                \nAvailable remotely: True                                                        \n                                                                                \nThe ZenML Docker Service Connector allows authenticating with a Docker or OCI   \ncontainer registry and managing Docker clients for the registry.                \n                                                                                \nThis connector provides pre-authenticated python-docker Python clients to Stack \nComponents that are linked to it.                                               \n                                                                                \nNo Python packages are required for this Service Connector. All prerequisites   \nare included in the base ZenML Python package. Docker needs to be installed on  \nenvironments where container images are built and pushed to the target container\nregistry.                                                                       \n\n[...]\n\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPlease select a service connector type (kubernetes, docker, azure, aws, gcp): gcp\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                           Available resource types                           \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \n                                                                                \n              \ud83d\udd35 Generic GCP resource (resource type: gcp-generic)              \n                                                                                \nAuthentication methods: implicit, user-account, service-account, oauth2-token,  \nimpersonation                                                                   \n                                                                                \nSupports resource instances: False                                              \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n"}
{"input": " \u2022 \ud83d\udd12 user-account                                                              \n \u2022 \ud83d\udd12 service-account                                                           \n \u2022 \ud83d\udd12 oauth2-token                                                              \n \u2022 \ud83d\udd12 impersonation                                                             \n                                                                                \nThis resource type allows Stack Components to use the GCP Service Connector to  \nconnect to any GCP service or resource. When used by Stack Components, they are \nprovided a Python google-auth credentials object populated with a GCP OAuth 2.0 \ntoken. This credentials object can then be used to create GCP Python clients for\nany particular GCP service.                                                     \n                                                                                \nThis generic GCP resource type is meant to be used with Stack Components that   \nare not represented by other, more specific resource type, like GCS buckets,    \nKubernetes clusters or Docker registries. For example, it can be used with the  \nGoogle Cloud Builder Image Builder stack component, or the Vertex AI            \nOrchestrator and Step Operator. It should be accompanied by a matching set of   \nGCP permissions that allow access to the set of remote resources required by the\nclient and Stack Component.                                                     \n                                                                                \nThe resource name represents the GCP project that the connector is authorized to\naccess.                                                                         \n                                                                                \n                                                                                \n                 \ud83d\udce6 GCP GCS bucket (resource type: gcs-bucket)                  \n                                                                                \nAuthentication methods: implicit, user-account, service-account, oauth2-token,  \nimpersonation                                                                   \n                                                                                \nSupports resource instances: True                                               \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 user-account                                                              \n \u2022 \ud83d\udd12 service-account                                                           \n \u2022 \ud83d\udd12 oauth2-token                                                              \n \u2022 \ud83d\udd12 impersonation                                                             \n                                                                                \nAllows Stack Components to connect to GCS buckets. When used by Stack           \nComponents, they are provided a pre-configured GCS Python client instance.      \n                                                                                \nThe configured credentials must have at least the following GCP permissions     \nassociated with the GCS buckets that it can access:                             \n                                                                                \n \u2022 storage.buckets.list                                                         \n \u2022 storage.buckets.get                                                          \n \u2022 storage.objects.create                                                       \n \u2022 storage.objects.delete                                                       \n \u2022 storage.objects.get                                                          \n \u2022 storage.objects.list                                                         \n \u2022 storage.objects.update                                                       \n                                                                                \nFor example, the GCP Storage Admin role includes all of the required            \npermissions, but it also includes additional permissions that are not required  \nby the connector.                                                               \n                                                                                \nIf set, the resource name must identify a GCS bucket using one of the following \nformats:                                                                        \n                                                                                \n \u2022 GCS bucket URI: gs://{bucket-name}                                           \n \u2022 GCS bucket name: {bucket-name}\n\n[...]\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPlease select a resource type or leave it empty to create a connector that can be used to access any of the supported resource types (gcp-generic, gcs-bucket, kubernetes-cluster, docker-registry). []: gcs-bucket\nWould you like to attempt auto-configuration to extract the"}
{"input": " authentication configuration from your local environment ? [y/N]: y\nService connector auto-configured successfully with the following configuration:\nService connector 'gcp-interactive' of type 'gcp' is 'private'.\n    'gcp-interactive' gcp Service     \n          Connector Details           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 gcp-interactive \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd35 gcp          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 user-account    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 gcs-bucket   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n          Configuration           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY          \u2502 VALUE      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id        \u2502 zenml-core \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 user_account_json \u2502 [HIDDEN]   \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\nNo labels are set for this service connector.\nThe service connector configuration has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 gcs-bucket \u2502 gs://annotation-gcp-store                       \u2503\n\u2503               \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503               \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503               \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503               \u2502 gs://zen"}
{"input": "ml-datasets                             \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\nWould you like to continue with the auto-discovered configuration or switch to manual ? (auto, manual) [auto]: \nThe following GCP GCS bucket instances are reachable through this connector:\n - gs://annotation-gcp-store\n - gs://zenml-bucket-sl\n - gs://zenml-core.appspot.com\n - gs://zenml-core_cloudbuild\n - gs://zenml-datasets\nPlease select one or leave it empty to create a connector that can be used to access any of them []: gs://zenml-datasets\nSuccessfully registered service connector `gcp-interactive` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-datasets \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\nRegardless of how you came here, you should already have some idea of the following:\n\n* the type of resources that you want to connect ZenML to. This may be a Kubernetes cluster, a Docker container registry or an object storage service like AWS S3 or GCS.\n* the Service Connector implementation (i.e. Service Connector Type) that you want to use to connect to those resources. This could be one of the cloud provider Service Connector Types like AWS and GCP that provide access to a broader range of services, or one of the basic Service Connector Types like Kubernetes or Docker that only target a specific resource.\n* the credentials and authentication method that you want to use\n\nOther questions that should be answered in this section:\n\n* are you just looking to connect a ZenML Stack Component to a single resource? or would you rather configure a wide-access ZenML Service Connector that gives ZenML and all its users access to a broader range of resource types and resource instances with a single set of credentials issued by your cloud provider?\n* have you already provisioned all the authentication prerequisites (e.g. service accounts, roles, permissions) and prepared the credentials you will need to configure the Service Connector? If you already have one of the cloud provider CLIs configured with credentials on your local host, you can easily use the Service Connector auto-configuration capabilities to get faster where you need to go.\n\nFor help answering these questions, you can also use the interactive CLI mode to register Service Connectors and/or consult the documentation dedicated to each individual Service Connector Type.\n\n### Auto-configuration\n\n"}
{"input": "Many Service Connector Types support using auto-configuration to discover and extract configuration information and credentials directly from your local environment. This assumes that you have already installed and set up the local CLI or SDK associated with the type of resource or cloud provider that you're willing to use. The Service Connector auto-configuration feature relies on these CLIs being configured with valid credentials to work properly. Some examples are listed here, but you should consult the documentation section for the Service Connector Type of choice to find out if and how auto-configuration is supported:\n\n* AWS uses the [`aws configure` CLI command](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)\n* GCP offers [the `gcloud auth application-default login` CLI command](https://cloud.google.com/docs/authentication/provide-credentials-adc#how\\_to\\_provide\\_credentials\\_to\\_adc)\n* Azure provides [the `az login` CLI command](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli)\n\n<details>\n\n<summary>Or simply try it and find out</summary>\n\n```sh\nzenml service-connector register kubernetes-auto --type kubernetes --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `kubernetes-auto` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 35.185.95.223  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector register aws-auto --type aws --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u283c Registering service connector 'aws-auto'...\nSuccessfully registered service connector `aws-auto` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://aws-ia-mwaa-715803424590                \u2503\n\u2503                       \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml"}
{"input": "-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector register gcp-auto --type gcp --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `gcp-auto` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://annotation-gcp-store                       \u2503\n\u2503                       \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 gcr.io/zenml-core                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### Scopes: multi-type, multi-instance, and single-instance\n\nThese terms are briefly explained in the [Terminology](service-connectors-guide.md#terminology)"}
{"input": " section: you can register a Service Connector that grants access to multiple types of resources, to multiple instances of the same Resource Type, or to a single resource.\n\nService Connectors created from basic Service Connector Types like Kubernetes and Docker are single-resource by default, while Service Connectors used to connect to managed cloud resources like AWS and GCP can take all three forms.\n\n<details>\n\n<summary>Example of registering Service Connectors with different scopes</summary>\n\nThe following example shows registering three different Service Connectors configured from the same AWS Service Connector Type using three different scopes but with the same credentials:\n\n* a multi-type AWS Service Connector that allows access to every possible resource accessible with the configured credentials\n* a multi-instance AWS Service Connector that allows access to multiple S3 buckets\n* a single-instance AWS Service Connector that only permits access to one S3 bucket\n\n```sh\nzenml service-connector register aws-multi-type --type aws --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u280b Registering service connector 'aws-multi-type'...\nSuccessfully registered service connector `aws-multi-type` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://aws-ia-mwaa-715803424590                \u2503\n\u2503                       \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2503                       \u2502 s3://zenml-public-datasets                   \u2503\n\u2503                       \u2502 s3://zenml-public-swagger-spec               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector register aws-s3-multi-instance --type aws"}
{"input": " --auto-configure --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-s3-multi-instance'...\nSuccessfully registered service connector `aws-s3-multi-instance` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://aws-ia-mwaa-715803424590         \u2503\n\u2503               \u2502 s3://zenfiles                         \u2503\n\u2503               \u2502 s3://zenml-demos                      \u2503\n\u2503               \u2502 s3://zenml-generative-chat            \u2503\n\u2503               \u2502 s3://zenml-public-datasets            \u2503\n\u2503               \u2502 s3://zenml-public-swagger-spec        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector register aws-s3-zenfiles --type aws --auto-configure --resource-type s3-bucket --resource-id s3://zenfiles\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u283c Registering service connector 'aws-s3-zenfiles'...\nSuccessfully registered service connector `aws-s3-zenfiles` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\nThe following might help understand the difference between scopes:\n\n* the difference between a multi-instance and a multi-type Service Connector is that the Resource Type scope is locked to a particular value during configuration for the multi-instance Service Connector\n* similarly, the difference between a multi-instance and a multi-type Service Connector is that the Resource Name (Resource ID) scope is locked to a particular value during configuration for the single-instance Service Connector\n\n### Service Connector Verification\n\nWhen registering Service Connectors, the authentication configuration and credentials are automatically verified to ensure that they can indeed be used to gain access to the target resources:\n\n* for multi-type Service Connectors, this verification means checking that the configured credentials can be used to authenticate successfully to the remote service"}
{"input": ", as well as listing all resources that the credentials have permission to access for each Resource Type supported by the Service Connector Type.\n* for multi-instance Service Connectors, this verification step means listing all resources that the credentials have permission to access in addition to validating that the credentials can be used to authenticate to the target service or platform.\n* for single-instance Service Connectors, the verification step simply checks that the configured credentials have permission to access the target resource.\n\nThe verification can also be performed later on an already registered Service Connector. Furthermore, for multi-type and multi-instance Service Connectors, the verification operation can be scoped to a Resource Type and a Resource Name.\n\n<details>\n\n<summary>Example of on-demand Service Connector verification</summary>\n\nThe following shows how a multi-type, a multi-instance and a single-instance Service Connector can be verified with multiple scopes after registration.\n\nFirst, listing the Service Connectors will clarify which scopes they are configured with:\n\n```sh\nzenml service-connector list\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME                  \u2502 ID                                   \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-multi-type        \u2502 373a73c2-8295-45d4-a768-45f5a0f744ea \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic        \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2503        \u2502                       \u2502                                      \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                       \u2502                                      \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                       \u2502                                      \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\n\u2503        \u2502 aws-s3-multi-instance \u2502 fa9325ab-ce01-4404-aec3-61a3af395d48 \u2502 \ud83d\udd36 aws \u2502 \ud83d\udce6 s3-bucket          \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-s3-zenfiles       \u2502 19edc05b-92db-49de-bc84-aa9b3fb8261a \u2502 \ud83d\udd36 aws \u2502 \ud83d\udce6 s3-bucket          \u2502 s3://zenfiles \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nVerifying the multi-type Service Connector displays all resources that can be accessed through the Service Connector. This is like asking \"are these credentials valid? can they be used to authenticate to AWS ? and if so, what resources can they access?\":\n\n```sh\nzenml service-connector verify aws-multi-type\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-multi-type' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://aws-ia-mwaa-715803424590                \u2503\n\u2503                       \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks"}
{"input": "-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nYou can scope the verification down to a particular Resource Type or all the way down to a Resource Name. This is the equivalent of asking \"are these credentials valid and which S3 buckets are they authorized to access ?\" and \"can these credentials be used to access this particular Kubernetes cluster in AWS ?\":\n\n```sh\nzenml service-connector verify aws-multi-type --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-multi-type' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://aws-ia-mwaa-715803424590         \u2503\n\u2503               \u2502 s3://zenfiles                         \u2503\n\u2503               \u2502 s3://zenml-demos                      \u2503\n\u2503               \u2502 s3://zenml-generative-chat            \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector verify aws-multi-type --resource-type kubernetes-cluster --resource-id zenhacks-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-multi-type' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nVerifying the multi-instance Service Connector displays all the resources that it can access. We can also scope the verification to a single resource:\n\n```sh\nzenml service-connector verify aws-s"}
{"input": "3-multi-instance\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-s3-multi-instance' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://aws-ia-mwaa-715803424590         \u2503\n\u2503               \u2502 s3://zenfiles                         \u2503\n\u2503               \u2502 s3://zenml-demos                      \u2503\n\u2503               \u2502 s3://zenml-generative-chat            \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector verify aws-s3-multi-instance --resource-id s3://zenml-demos\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-s3-multi-instance' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://zenml-demos \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nFinally, verifying the single-instance Service Connector is straight-forward and requires no further explanation:\n\n```sh\nzenml service-connector verify aws-s3-zenfiles\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-s3-zenfiles' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Configure local clients\n\nYet another neat feature built into some Service Container Types that is the opposite of [Service Connector auto-configuration](service-connectors-guide.md#auto-configuration) is the ability to configure local CLI and SDK utilities installed on your"}
{"input": " host, like the Docker or Kubernetes CLI (`kubectl`) with credentials issued by a compatible Service Connector.\n\nYou may need to exercise this feature to get direct CLI access to a remote service in order to manually manage some configurations or resources, to debug some workloads or to simply verify that the Service Connector credentials are actually working.\n\n{% hint style=\"warning\" %}\nWhen configuring local CLI utilities with credentials extracted from Service Connectors, keep in mind that most Service Connectors, particularly those used with cloud platforms, usually exercise the security best practice of issuing _temporary credentials such as API tokens._ The implication is that your local CLI may only be allowed access to the remote service for a short time before those credentials expire, then you need to fetch another set of credentials from the Service Connector.\n{% endhint %}\n\n<details>\n\n<summary>Examples of local CLI configuration</summary>\n\nThe following examples show how the local Kubernetes `kubectl` CLI can be configured with credentials issued by a Service Connector and then used to access a Kubernetes cluster directly:\n\n```sh\nzenml service-connector list-resources --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME       \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 9d953320-3560-4a78-817c-926a3898064d \u2502 gcp-user-multi       \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 4a550c82-aa64-4a48-9c7f-d5e127d77a44 \u2502 aws-multi-type       \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                                                                    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector login gcp-user-multi --resource-type kubernetes-cluster --resource-id zenml-test-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector login gcp-user-multi --resource-type kubernetes-cluster --resource-id zenml-test-cluster\n\u2807 Attempting to configure local client using service connector 'gcp-user-multi'...\nUpdated local kubeconfig with the cluster details. The current kubectl context was set to 'gke_zenml-core_zenml-test-cluster'.\nThe 'gcp-user-multi' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.\n\n# Verify that the local kubectl client is now configured to access the remote Kubernetes cluster\n$ kubectl cluster-info\nKubernetes control plane is running at https://35.185.95.223\nGLBCDefaultBackend is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nKubeDNS is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n```\n{% endcode %}\n\n```sh\nzenml service-connector login aws-multi-type --resource-type kubernetes-cluster --resource-id zenhacks-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector login aws-multi-type --resource-type kubernetes-cluster --resource-id zenhacks-cluster\n\u280f Attempting to configure local client using service connector 'aws-multi-type'...\nUpdated local kubeconfig with the cluster details. The current kubectl context was set to 'arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster'.\nThe 'aws-multi-type' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.\n\n# Verify that the local kubectl client is now configured to access the remote Kubernetes cluster\n$ kubectl cluster-info\nKubernetes control plane is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com\nCoreDNS is running at https://A5F8F4142"}
{"input": "FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```\n{% endcode %}\n\nThe same is possible with the local Docker client:\n\n```sh\nzenml service-connector verify aws-session-token --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-session-token' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME    \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 3ae3e595-5cbc-446e-be64-e54e854e0e3f \u2502 aws-session-token \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector login aws-session-token --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$zenml service-connector login aws-session-token --resource-type docker-registry\n\u280f Attempting to configure local client using service connector 'aws-session-token'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'aws-session-token' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n\n# Verify that the local Docker client is now configured to access the remote Docker container registry\n$ docker pull 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server\nUsing default tag: latest\nlatest: Pulling from"}
{"input": " zenml-server\ne9995326b091: Pull complete \nf3d7f077cdde: Pull complete \n0db71afa16f3: Pull complete \n6f0b5905c60c: Pull complete \n9d2154d50fd1: Pull complete \nd072bba1f611: Pull complete \n20e776588361: Pull complete \n3ce69736a885: Pull complete \nc9c0554c8e6a: Pull complete \nbacdcd847a66: Pull complete \n482033770844: Pull complete \nDigest: sha256:bf2cc3895e70dfa1ee1cd90bbfa599fa4cd8df837e27184bac1ce1cc239ecd3f\nStatus: Downloaded newer image for 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest\n715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest\n```\n{% endcode %}\n\n</details>\n\n## Discover available resources\n\nOne of the questions that you may have as a ZenML user looking to register and connect a Stack Component to an external resource is \"what resources do I even have access to ?\". Sure, you can browse through all the registered Service connectors and manually verify each one to find a particular resource that you are looking for, but this is counterproductive.\n\nA better way is to ask ZenML directly questions such as:\n\n* what are the Kubernetes clusters that I can get access to through Service Connectors?\n* can I access this particular S3 bucket through one of the Service Connectors? Which one?\n\nThe `zenml service-connector list-resources` CLI command can be used exactly for this purpose.\n\n<details>\n\n<summary>Resource discovery examples</summary>\n\nIt is possible to show globally all the various resources that can be accessed through all available Service Connectors, and all Service Connectors that are in an error state. This operation is expensive and may take some time to complete, depending on the number of Service Connectors involved. The output also includes any errors that may have occurred during the discovery process:\n\n```sh\nzenml service-connector list-resources\n```\n\n{% code title=\"Example Command Output\" %}\n```\nFetching all service connector resources can take a long time, depending on the number of connectors configured in your workspace. Consider using the '--connector-type', '--resource-type' and '--resource-id' \noptions to narrow down the list of resources to fetch.\nThe following resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES                                                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 099fb152-cfb7-4af5-86a7-7b77c0961b21 \u2502 gcp-multi             \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udd35 gcp-generic        \u2502 zenml-core                                                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503                                      \u2502                       \u2502                \u2502 \ud83d\udce6 gcs-bucket         \u2502 gs://annotation-gcp-store                                                                               \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 gs://zenml-bucket-sl                                                                                    \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 gs://zenml-core.appspot.com                                                                             \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 gs://zenml-core_cloudbuild                                                                              \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 gs://zenml-datasets                                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503                                      \u2502                       \u2502                \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503                                      \u2502                       \u2502                \u2502 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 373a73c2-8295-45d4-a768-45f5a0f744ea \u2502 aws-multi-type        \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udd36 aws-generic        \u2502 us-east-1                                                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503                                      \u2502                       \u2502                \u2502 \ud83d\udce6 s3-bucket          \u2502 s3://aws-ia-mwaa-715803424590                                                                           \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenfiles                                                                                           \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenml-demos                                                                                        \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenml-generative-chat                                                                              \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenml-public-datasets                                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503                                      \u2502                       \u2502                \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503                                      \u2502                       \u2502                \u2502 \ud83d\udc33 docker-registry    \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 fa9325ab-ce01-4404-aec3-61a3af395d48 \u2502 aws-s3-multi-instance \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket          \u2502 s3://aws-ia-mwaa-715803424590                                                                           \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenfiles                                                                                           \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenml-demos                                                                                        \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenml-generative-chat                                                                              \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 s3://zenml-public-datasets                                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 19edc05b-92db-49de-bc84-aa9b3fb8261a \u2502 aws-s3-zenfiles       \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket          \u2502 s3://zenfiles                                                                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 c732c768-3992-4cbd-8738-d02cd7b6b340 \u2502 kubernetes-auto       \u2502 \ud83c\udf00 kubernetes  \u2502 \ud83c\udf00 kubernetes-cluster \u2502 \ud83d\udca5 error: connector 'kubernetes-auto' authorization failure: failed to verify Kubernetes cluster        \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 access: (401)                                                                                           \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 Reason: Unauthorized                                                                                    \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 HTTP response headers: HTTPHeaderDict({'Audit-Id': '20c96e65-3e3e-4e08-bae3-bcb72c527fbf',              \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 09 Jun 2023     \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 18:52:56 GMT', 'Content-Length': '129'})                                                                \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 HTTP response body:                                                                                     \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Unauthorized\",\"reason\":\" \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502 Unauthorized\",\"code\":401}                                                                               \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502                                                                                                         \u2503\n\u2503                                      \u2502                       \u2502                \u2502                       \u2502                                                                                                         \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nMore interesting is to scope the search to a particular Resource Type. This yields fewer, more accurate results, especially if you have many multi-type Service Connectors configured:\n\n```sh\nzenml service-connector list-resources --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME  \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES                                                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 099fb152-cfb7-4af5-86a7-7b77c0961b21 \u2502 gcp-multi       \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 373a73c2-8295-45d4-a768-45f5a0f744ea \u2502 aws-multi-type  \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                                                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 c732c768-3992-4cbd-8738-d02cd7b6b340 \u2502 kubernetes-auto \u2502 \ud83c\udf00 kubernetes  \u2502 \ud83c\udf00 kubernetes-cluster \u2502 \ud83d\udca5 error: connector 'kubernetes-auto' authorization failure: failed to verify Kubernetes cluster access:      \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 (401)                                                                                                         \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 Reason: Unauthorized                                                                                          \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 HTTP response headers: HTTPHeaderDict({'Audit-Id': '72558f83-e050-4fe3-93e5-9f7e66988a4c', 'Cache-Control':   \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 09 Jun 2023 18:59:02 GMT',             \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 'Content-Length': '129'})                                                                                     \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 HTTP response body:                                                                                           \u2503\n\u2503                                      \u2502"}
{"input": "                 \u2502                \u2502                       \u2502 {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Unauthorized\",\"reason\":\"Unauth \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502 orized\",\"code\":401}                                                                                           \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502                                                                                                               \u2503\n\u2503                                      \u2502                 \u2502                \u2502                       \u2502                                                                                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nFinally, you can ask for a particular resource, if you know its Resource Name beforehand:\n\n```sh\nzenml service-connector list-resources --resource-type s3-bucket --resource-id zenfiles\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe  's3-bucket' resource with name 'zenfiles' can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 373a73c2-8295-45d4-a768-45f5a0f744ea \u2502 aws-multi-type        \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 fa9325ab-ce01-4404-aec3-61a3af395d48 \u2502 aws-s3-multi-instance \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 19edc05b-92db-"}
{"input": "49de-bc84-aa9b3fb8261a \u2502 aws-s3-zenfiles       \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Connect Stack Components to resources\n\nService Connectors and the resources and services that they can authenticate to and grant access to are only useful because they are a means of providing Stack Components a better and easier way of accessing external resources.\n\nIf you are looking for a quick, assisted tour, we recommend using the interactive CLI mode to connect a Stack Component to a compatible Service Connector, especially if this is your first time doing it, e.g.:\n\n```\nzenml artifact-store connect <component-name> -i\nzenml orchestrator connect <component-name> -i\nzenml container-registry connect <component-name> -i\n```\n\nTo connect a Stack Component to an external resource or service, you first need to [register one or more Service Connectors](service-connectors-guide.md#register-service-connectors), or have someone else in your team with more infrastructure knowledge do it for you. If you already have that covered, you might want to ask ZenML \"which resources/services am I even authorized to access with the available Service Connectors?\". [The resource discovery feature](service-connectors-guide.md#end-to-end-examples) is designed exactly for this purpose. This last check is already included in the interactive ZenML CLI command used to connect a Stack Component to a remote resource.\n\n{% hint style=\"info\" %}\nNot all Stack Components support being connected to an external resource or service via a Service Connector. Whether a Stack Component can use a Service Connector to connect to a remote resource or service or not is shown in the Stack Component flavor details:\n\n```\n$ zenml artifact-store flavor describe s3\nConfiguration class: S3ArtifactStoreConfig\n\nConfiguration for the S3 Artifact Store.\n\n[...]\n\nThis flavor supports connecting to external resources with a Service\nConnector. It requires a 's3-bucket' resource. You can get a list of\nall available connectors and the compatible resources that they can\naccess by running:\n\n'zenml service-connector list-resources --resource-type s3-bucket'\nIf no compatible Service Connectors are yet registered, you can can\nregister a new one by running:\n\n'zenml service-connector register -i'\n\n```\n{% endhint %}\n\nFor Stack Components that do support Service Connectors, their flavor indicates the Resource Type and, optionally, Service Connector Type compatible with the Stack Component."}
{"input": " This can be used to figure out which resources are available and which Service Connectors can grant access to them. In some cases it is even possible to figure out the exact Resource Name based on the attributes already configured in the Stack Component, which is how ZenML can decide automatically which Resource Name to use in the interactive mode:\n\n```sh\nzenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles\nzenml service-connector list-resources --resource-type s3-bucket --resource-id s3://zenfiles\nzenml artifact-store connect s3-zenfiles --connector aws-multi-type\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered artifact_store `s3-zenfiles`.\n\n$ zenml service-connector list-resources --resource-type s3-bucket --resource-id zenfiles\nThe  's3-bucket' resource with name 'zenfiles' can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME       \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 4a550c82-aa64-4a48-9c7f-d5e127d77a44 \u2502 aws-multi-type       \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 66c0922d-db84-4e2c-9044-c13ce1611613 \u2502 aws-multi-instance   \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 65c82e59-cba0-4a01-b8f6-d75e8a1d0f55 \u2502 aws-single-instance  \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n"}
{"input": "\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n$ zenml artifact-store connect s3-zenfiles --connector aws-multi-type\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully connected artifact store `s3-zenfiles` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 4a550c82-aa64-4a48-9c7f-d5e127d77a44 \u2502 aws-multi-type \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe following is an example of connecting the same Stack Component to the remote resource using the interactive CLI mode:\n\n```sh\nzenml artifact-store connect s3-zenfiles -i\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following connectors have compatible resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 373a73c2-8295-45d4-a768-45f5a0f744ea \u2502 aws-multi-type        \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles "}
{"input": " \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 fa9325ab-ce01-4404-aec3-61a3af395d48 \u2502 aws-s3-multi-instance \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 19edc05b-92db-49de-bc84-aa9b3fb8261a \u2502 aws-s3-zenfiles       \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\nPlease enter the name or ID of the connector you want to use: aws-s3-zenfiles\nSuccessfully connected artifact store `s3-zenfiles` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME  \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 19edc05b-92db-49de-bc84-aa9b3fb8261a \u2502 aws-s3-zenfiles \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n## End-to-end examples\n\nTo get an idea of what a complete end-to-end journey looks like, from registering Service Connector all the way to configuring Stacks and Stack Components and running pipelines that access remote resources through Service Connectors, take a look at the following full-fledged examples:\n\n* [the AWS Service Connector end-to-end examples]("}
{"input": "aws-service-connector.md)\n* [the GCP Service Connector end-to-end examples](gcp-service-connector.md)\n* [the Azure Service Connector end-to-end examples](azure-service-connector.md)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Configuring Docker Service Connectors to connect ZenML to Docker container registries.\n---\n\n# Docker Service Connector\n\nThe ZenML Docker Service Connector allows authenticating with a Docker or OCI container registry and managing Docker clients for the registry. This connector provides pre-authenticated python-docker Python clients to Stack Components that are linked to it.\n\n```shell\nzenml service-connector list-types --type docker\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503           NAME           \u2502 TYPE      \u2502 RESOURCE TYPES     \u2502 AUTH METHODS \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Docker Service Connector \u2502 \ud83d\udc33 docker \u2502 \ud83d\udc33 docker-registry \u2502 password     \u2502 \u2705    \u2502 \u2705     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n## Prerequisites\n\nNo Python packages are required for this Service Connector. All prerequisites are included in the base ZenML Python package. Docker needs to be installed on environments where container images are built and pushed to the target container registry.\n\n## Resource Types\n\nThe Docker Service Connector only supports authenticating to and granting access to a Docker/OCI container registry. This type of resource is identified by the `docker-registry` Resource Type.\n\nThe resource name identifies a Docker/OCI registry using one of the following formats (the repository name is optional and ignored).\n\n* DockerHub: docker.io or `https://index.docker.io/v1/<repository-name>`\n* generic OCI registry URI: `https://host:port/<repository-name>`\n\n## Authentication Methods\n\nAuthenticating to Docker/OCI container registries is done with a username and password or access token. It is recommended to use API tokens instead of passwords, wherever this is available, for example in the case of DockerHub:\n\n```sh\nzenml service-connector register dockerhub --type docker -in\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nPlease enter a name for the service connector [dockerhub]: \nPlease enter a description for the service connector []: \nPlease select a service connector type (docker) [docker]: \nOnly one resource type is available for this connector (docker-registry).\nOnly one authentication method is available for this connector (password). Would you like to use it? [Y/n]: \nPlease enter the configuration"}
{"input": " for the Docker username and password/token authentication method.\n[username] Username {string, secret, required}: \n[password] Password {string, secret, required}: \n[registry] Registry server URL. Omit to use DockerHub. {string, optional}: \nSuccessfully registered service connector `dockerhub` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 docker.io      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n{% hint style=\"warning\" %}\nThis Service Connector does not support generating short-lived credentials from the username and password or token credentials configured in the Service Connector. In effect, this means that the configured credentials will be distributed directly to clients and used to authenticate directly to the target Docker/OCI registry service.\n{% endhint %}\n\n## Auto-configuration\n\n{% hint style=\"info\" %}\nThis Service Connector does not support auto-discovery and extraction of authentication credentials from local Docker clients. If this feature is useful to you or your organization, please let us know by messaging us in [Slack](https://zenml.io/slack) or [creating an issue on GitHub](https://github.com/zenml-io/zenml/issues).\n{% endhint %}\n\n## Local client provisioning\n\nThis Service Connector allows configuring the local Docker client with credentials:\n\n```sh\nzenml service-connector login dockerhub\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nAttempting to configure local client using service connector 'dockerhub'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'dockerhub' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n\n## Stack Components use\n\nThe Docker Service Connector can be used by all Container Registry stack component flavors to authenticate to a remote Docker/OCI container registry. This allows container images to be built and published to private container registries without the need to configure explicit Docker credentials in the target environment or the Stack Component.\n\n{% hint style=\"warning\" %}\nZenML does not yet support automatically configuring Docker credentials in container runtimes such as Kubernetes clusters (i.e. via imagePullSecrets) to allow container images to be pulled from the private container registries. This will be added in a future release.\n{% endhint %}\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when"}
{"input": "-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Configuring GCP Service Connectors to connect ZenML to GCP resources such as\n  GCS buckets, GKE Kubernetes clusters, and GCR container registries.\n---\n\n# GCP Service Connector\n\nThe ZenML GCP Service Connector facilitates the authentication and access to managed GCP services and resources. These encompass a range of resources, including GCS buckets, GAR and GCR container repositories, and GKE clusters. The connector provides support for various authentication methods, including GCP user accounts, service accounts, short-lived OAuth 2.0 tokens, and implicit authentication.\n\nTo ensure heightened security measures, this connector always issues [short-lived OAuth 2.0 tokens to clients instead of long-lived credentials](best-security-practices.md#generating-temporary-and-down-scoped-credentials) unless explicitly configured to do otherwise. Furthermore, it includes [automatic configuration and detection of credentials locally configured through the GCP CLI](service-connectors-guide.md#auto-configuration).\n\nThis connector serves as a general means of accessing any GCP service by issuing OAuth 2.0 credential objects to clients. Additionally, the connector can handle specialized authentication for GCS, Docker, and Kubernetes Python clients. It also allows for the configuration of local Docker and Kubernetes CLIs.\n\n```shell\n$ zenml service-connector list-types --type gcp\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS     \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 GCP Service Connector \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit         \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account     \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account  \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 external-account \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 oauth2-token     \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 impersonation    \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n"}
{"input": "## Prerequisites\n\nThe GCP Service Connector is part of the GCP ZenML integration. You can either install the entire integration or use a PyPI extra to install it independently of the integration:\n\n* `pip install \"zenml[connectors-gcp]\"` installs only prerequisites for the GCP Service Connector Type\n* `zenml integration install gcp` installs the entire GCP ZenML integration\n\nIt is not required to [install and set up the GCP CLI on your local machine](https://cloud.google.com/sdk/gcloud) to use the GCP Service Connector to link Stack Components to GCP resources and services. However, it is recommended to do so if you are looking for a quick setup that includes using the auto-configuration Service Connector features.\n\n{% hint style=\"info\" %}\nThe auto-configuration examples in this page rely on the GCP CLI being installed and already configured with valid credentials of one type or another. If you want to avoid installing the GCP CLI, we recommend using the interactive mode of the ZenML CLI to register Service Connectors:\n\n```\nzenml service-connector register -i --type gcp\n```\n{% endhint %}\n\n## Resource Types\n\n### Generic GCP resource\n\nThis resource type allows Stack Components to use the GCP Service Connector to connect to any GCP service or resource. When used by Stack Components, they are provided a Python google-auth credentials object populated with a GCP OAuth 2.0 token. This credentials object can then be used to create GCP Python clients for any particular GCP service.\n\nThis generic GCP resource type is meant to be used with Stack Components that are not represented by one of the other, more specific resource types like GCS buckets, Kubernetes clusters, or Docker registries. For example, it can be used with [the Google Cloud Image Builder](../../component-guide/image-builders/gcp.md) stack component, or [the Vertex AI Orchestrator](../../component-guide/orchestrators/vertex.md) and [Step Operator](../../component-guide/step-operators/vertex.md). It should be accompanied by a matching set of GCP permissions that allow access to the set of remote resources required by the client and Stack Component (see the documentation of each Stack Component for more details).\n\nThe resource name represents the GCP project that the connector is authorized to access.\n\n### GCS bucket\n\nAllows Stack Components to connect to GCS buckets. When used by Stack Components, they are provided a pre-configured GCS Python client instance.\n\nThe configured credentials must have at least the following [GCP permissions](https://cloud.google.com/iam/docs/permissions-reference) associated with the GCS buckets that it can access:\n\n* `storage.buckets.list`\n* `storage.buckets.get`\n* `storage.objects.create`\n* `storage.objects.delete`\n* `storage.objects.get`\n* `storage.objects.list`\n* `storage.objects.update`\n\nFor example,"}
{"input": " the GCP Storage Admin role includes all of the required permissions, but it also includes additional permissions that are not required by the connector.\n\nIf set, the resource name must identify a GCS bucket using one of the following formats:\n\n* GCS bucket URI (canonical resource name): gs://{bucket-name}\n* GCS bucket name: {bucket-name}\n\n### GKE Kubernetes cluster\n\nAllows Stack Components to access a GKE cluster as a standard Kubernetes cluster resource. When used by Stack Components, they are provided a pre-authenticated Python Kubernetes client instance.\n\nThe configured credentials must have at least the following [GCP permissions](https://cloud.google.com/iam/docs/permissions-reference) associated with the GKE clusters that it can access:\n\n* `container.clusters.list`\n* `container.clusters.get`\n\nIn addition to the above permissions, the credentials should include permissions to connect to and use the GKE cluster (i.e. some or all permissions in the Kubernetes Engine Developer role).\n\nIf set, the resource name must identify a GKE cluster using one of the following formats:\n\n* GKE cluster name: `{cluster-name}`\n\nGKE cluster names are project scoped. The connector can only be used to access GKE clusters in the GCP project that it is configured to use.\n\n### GAR container registry (including legacy GCR support)\n\n{% hint style=\"warning\" %}\n**Important Notice: Google Container Registry** [**is being replaced by Artifact Registry**](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr)**. Please start using Artifact Registry for your containers. As per Google's documentation, \"after May 15, 2024, Artifact Registry will host images for the gcr.io domain in Google Cloud projects without previous Container Registry usage. After March 18, 2025, Container Registry will be shut down.\".\n\nSupport for legacy GCR registries is still included in the GCP service connector. Users that already have GCP service connectors configured to access GCR registries may continue to use them without taking any action. However, it is recommended to transition to Google Artifact Registries as soon as possible by following [the GCP guide on this subject](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr) and making the following updates to ZenML GCP Service Connectors that are used to access GCR resources:\n\n* add the IAM permissions documented here to the GCP Service Connector credentials to enable them to access the Artifact Registries.\n* users may keep the gcr.io GCR URLs already configured in the GCP Service Connectors as well as those used in linked Container Registry stack components given that these domains are redirected by Google to GAR as covered in the GCR transition guide. Alternatively, users may update the GCP Service Connector configuration and/or the Container Registry stack components to use the replacement Artifact Registry URLs.\n\nThe GCP Service Connector will list the legacy GCR regist"}
{"input": "ries as accessible for a GCP project even if the GCP Service Connector credentials do not grant access to GCR registries. This is required for backwards-compatibility and will be removed in a future release.\n{% endhint %}\n\nAllows Stack Components to access a Google Artifact Registry as a standard Docker registry resource. When used by Stack Components, they are provided a pre-authenticated Python Docker client instance.\n\nThe configured credentials must have at least the following [GCP permissions](https://cloud.google.com/iam/docs/understanding-roles#artifact-registry-roles):\n\n* `artifactregistry.repositories.createOnPush`\n* `artifactregistry.repositories.downloadArtifacts`\n* `artifactregistry.repositories.get`\n* `artifactregistry.repositories.list`\n* `artifactregistry.repositories.readViaVirtualRepository`\n* `artifactregistry.repositories.uploadArtifacts`\n* `artifactregistry.locations.list`\n\nThe Artifact Registry Create-on-Push Writer role includes all of the above permissions.\n\nThis resource type also includes legacy GCR container registry support. When used with GCR registries, the configured credentials must have at least the following [GCP permissions](https://cloud.google.com/iam/docs/understanding-roles#cloud-storage-roles):\n\n* `storage.buckets.get`\n* `storage.multipartUploads.abort`\n* `storage.multipartUploads.create`\n* `storage.multipartUploads.list`\n* `storage.multipartUploads.listParts`\n* `storage.objects.create`\n* `storage.objects.delete`\n* `storage.objects.list`\n\nThe Storage Legacy Bucket Writer role includes all of the above permissions while at the same time restricting access to only the GCR buckets.\n\nIf set, the resource name must identify a GAR or GCR registry using one of the following formats:\n\n* Google Artifact Registry repository URI: `[https://]<region>-docker.pkg.dev/<project-id>/<registry-id>[/<repository-name>]`\n* Google Artifact Registry name: `projects/<project-id>/locations/<location>/repositories/<repository-id>`\n* (legacy) GCR repository URI: `[https://][us.|eu.|asia.]gcr.io/<project-id>[/<repository-name>]`\n\nThe connector can only be used to access GAR and GCR registries in the GCP\nproject that it is configured to use.\n\n## Authentication Methods\n\n### Implicit authentication\n\n[Implicit authentication](best-security-practices.md#implicit-authentication) to GCP services using [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc).\n\n{% hint style=\"warning\" %}\nThis method may constitute a security risk, because it can give users access to the same cloud resources and services that the ZenML Server itself is configured to access. For this reason, all implicit authentication methods are disabled by default and need to be explicitly enabled by setting the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable or the helm chart `enableImplicitAuthMethods` configuration option to `true` in the ZenML deployment.\n{% endhint %}\n\n"}
{"input": "This authentication method doesn't require any credentials to be explicitly configured. It automatically discovers and uses credentials from one of the following sources:\n\n* environment variables (GOOGLE\\_APPLICATION\\_CREDENTIALS)\n* local ADC credential files set up by running `gcloud auth application-default login` (e.g. `~/.config/gcloud/application_default_credentials.json`).\n* a GCP service account attached to the resource where the ZenML server is running. Only works when running the ZenML server on a GCP resource with a service account attached to it or when using Workload Identity (e.g. GKE cluster).\n\nThis is the quickest and easiest way to authenticate to GCP services. However, the results depend on how ZenML is deployed and the environment where it is used and is thus not fully reproducible:\n\n* when used with the default local ZenML deployment or a local ZenML server, the credentials are those set up on your machine (i.e. by running `gcloud auth application-default login` or setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to a service account key JSON file).\n* when connected to a ZenML server, this method only works if the ZenML server is deployed in GCP and will use the service account attached to the GCP resource where the ZenML server is running (e.g. a GKE cluster). The service account permissions may need to be adjusted to allow listing and accessing/describing the GCP resources that the connector is configured to access.\n\nNote that the discovered credentials inherit the full set of permissions of the local GCP CLI credentials or service account attached to the ZenML server GCP workload. Depending on the extent of those permissions, this authentication method might not be suitable for production use, as it can lead to accidental privilege escalation. Instead, it is recommended to use [the Service Account Key](gcp-service-connector.md#gcp-service-account) or [Service Account Impersonation](gcp-service-connector.md#gcp-service-account-impersonation) authentication methods to restrict the permissions that are granted to the connector clients.\n\nTo find out more about Application Default Credentials, [see the GCP ADC documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc).\n\nA GCP project is required and the connector may only be used to access GCP resources in the specified project. When used remotely in a GCP workload, the configured project has to be the same as the project of the attached service account.\n\n<details>\n\n<summary>Example configuration</summary>\n\nThe following assumes the local GCP CLI has already been configured with user account credentials by running the `gcloud auth application-default login` command:\n\n```sh\nzenml service-connector register gcp-implicit --type gcp --auth-method implicit --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `gcp-implicit` with access to the"}
{"input": " following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNo credentials are stored with the Service Connector:\n\n```sh\nzenml service-connector describe gcp-implicit\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-implicit' of type 'gcp' with id '0c49a7fe-5e87-41b9-adbe-3da0a0452e44' is owned by user '"}
{"input": "default' and is 'private'.\n                         'gcp-implicit' gcp Service Connector Details                          \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 0c49a7fe-5e87-41b9-adbe-3da0a0452e44                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 gcp-implicit                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd35 gcp                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 implicit                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd35 gcp-generic, \ud83d\udce6 gcs-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-05-19 08:04:51.037955                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-05-19 08:04:51.037958"}
{"input": "                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n       Configuration       \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY   \u2502 VALUE      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id \u2502 zenml-core \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### GCP User Account\n\n[Long-lived GCP credentials](best-security-practices.md#long-lived-credentials-api-keys-account-keys) consist of a GCP user account and its credentials.\n\nThis method requires GCP user account credentials like those generated by the `gcloud auth application-default login` command.\n\nBy default, the GCP connector [generates temporary OAuth 2.0 tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) from the user account credentials and distributes them to clients. The tokens have a limited lifetime of 1 hour. This behavior can be disabled by setting the `generate_temporary_tokens` configuration option to `False`, in which case, the connector will distribute the user account credentials JSON to clients instead (not recommended).\n\nThis method is preferred during development and testing due to its simplicity and ease of use. It is not recommended as a direct authentication method for production use cases because the clients are granted the full set of permissions of the GCP user account. For production, it is recommended to use the GCP Service Account or GCP Service Account Impersonation authentication methods.\n\nA GCP project is required and the connector may only be used to access GCP resources in the specified project.\n\nIf you already have the local GCP CLI set up with these credentials, they will be automatically picked up when auto-configuration is used (see the example below).\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nThe following assumes the local GCP CLI has been configured with GCP user account credentials by running the `gcloud auth application-default login` command:\n\n```sh\nzenml service-connector register gcp-user-account --type gcp --auth-method user-account --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `gcp-user-account` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe GCP user account credentials were lifted up from the local host:\n\n```sh\nzenml service-connector describe gcp-user-account\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-user-account' of type 'gcp' with id 'ddbce93f-df14-4861-a8a4-99a80972f3bc' is owned by user 'default' and is 'private'.\n                       'gcp-user-account' gcp Service Connector Details                        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503"}
{"input": " PROPERTY         \u2502 VALUE                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 ddbce93f-df14-4861-a8a4-99a80972f3bc                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 gcp-user-account                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd35 gcp                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 user-account                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd35 gcp-generic, \ud83d\udce6 gcs-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 17692951-614f-404f-a13a-4abb25bfa758                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-05-19 08:09:44.102934                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-05-19 08:09:44.102936                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n"}
{"input": "          Configuration           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY          \u2502 VALUE      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id        \u2502 zenml-core \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 user_account_json \u2502 [HIDDEN]   \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### GCP Service Account\n\n[Long-lived GCP credentials](best-security-practices.md#long-lived-credentials-api-keys-account-keys) consisting of a GCP service account and its credentials.\n\nThis method requires [a GCP service account](https://cloud.google.com/iam/docs/service-account-overview) and [a service account key JSON](https://cloud.google.com/iam/docs/service-account-creds#key-types) created for it.\n\nBy default, the GCP connector [generates temporary OAuth 2.0 tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) from the service account credentials and distributes them to clients. The tokens have a limited lifetime of 1 hour. This behavior can be disabled by setting the `generate_temporary_tokens` configuration option to `False`, in which case, the connector will distribute the service account credentials JSON to clients instead (not recommended).\n\nA GCP project is required and the connector may only be used to access GCP resources in the specified project.\n\nIf you already have the `GOOGLE_APPLICATION_CREDENTIALS` environment variable configured to point to a service account key JSON file, it will be automatically picked up when auto-configuration is used.\n\n<details>\n\n<summary>Example configuration</summary>\n\nThe following assumes a GCP service account was created, [granted permissions to access GCS buckets](gcp-service-connector.md#gcs-bucket) in the target project and a service account key JSON was generated and saved locally in the `connectors-devel@zenml-core.json` file:\n\n```sh\nzenml service-connector register gcp-service-account --type gcp --auth-method service-account --resource-type gcs-bucket --project_id=zenml-core --service_account_json=@connectors-devel@zenml-core.json\n```\n\n{% code title=\"Example Command Output\" %}\n```\nExpanding argument value service_account_json to contents of file connectors-devel@zenml-core.json.\nSuccessfully registered service connector `gcp-service-account` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe GCP service connector configuration and service account credentials:\n\n```sh\nzenml service-connector describe gcp-service-account\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-service-account' of type 'gcp' with id '4b3d41c9-6a6f-46da-b7ba-8f374c3f49c5' is owned by user 'default' and is 'private'.\n    'gcp-service-account' gcp Service Connector Details    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 4b3d41c9-6a6f-46da-b7ba-8f374c3f49c5 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 gcp-service-account                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd35 gcp                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 service-account                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 gcs-bucket                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 0d0a42bb-40a4-4f43-af9e-6342eeca3f28 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-05-19 08:15:48.056937           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-05-19 08:15:48.056940           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY             \u2502 VALUE      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id           \u2502 zenml-core \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 service_account_json \u2502 [HIDDEN]   \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### GCP Service Account impersonation\n\nGenerates [temporary STS credentials](best-security-practices.md#impersonating-accounts-and-assuming-roles) by [impersonating another GCP service account](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct#sa-impersonation).\n\nThe connector needs to be configured with the email address of the target GCP service account to be impersonated, accompanied by a GCP service account key JSON for the primary service account. The primary service account must have permission to generate tokens for the target service account (i.e. [the Service Account Token Creator role](https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate)). The connector will generate temporary OAuth 2.0 tokens upon request by using [GCP direct service account impersonation](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct#sa-impersonation). The tokens have a configurable limited lifetime of up to 1 hour.\n\n[The best practice implemented with this authentication scheme](best-security-practices"}
{"input": ".md#impersonating-accounts-and-assuming-roles) is to keep the set of permissions associated with the primary service account down to the bare minimum and grant permissions to the privilege-bearing service account instead.\n\nA GCP project is required and the connector may only be used to access GCP resources in the specified project.\n\nIf you already have the `GOOGLE_APPLICATION_CREDENTIALS` environment variable configured to point to the primary service account key JSON file, it will be automatically picked up when auto-configuration is used.\n\n<details>\n\n<summary>Configuration example</summary>\n\nFor this example, we have the following set up in GCP:\n\n* a primary `empty-connectors@zenml-core.iam.gserviceaccount.com` GCP service account with no permissions whatsoever aside from the \"Service Account Token Creator\" role that allows it to impersonate the secondary service account below. We also generate a service account key for this account.\n* a secondary `zenml-bucket-sl@zenml-core.iam.gserviceaccount.com` GCP service account that only has permission to access the `zenml-bucket-sl` GCS bucket\n\nFirst, let's show that the `empty-connectors` service account has no permission to access any GCS buckets or any other resources for that matter. We'll register a regular GCP Service Connector that uses the service account key (long-lived credentials) directly:\n\n```sh\nzenml service-connector register gcp-empty-sa --type gcp --auth-method service-account --service_account_json=@empty-connectors@zenml-core.json  --project_id=zenml-core\n```\n\n{% code title=\"Example Command Output\" %}\n```\nExpanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.\nSuccessfully registered service connector `gcp-empty-sa` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                                                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 \ud83d\udca5 error: connector authorization failure: failed to list GCS buckets: 403 GET                                            \u2503\n\u2503"}
{"input": "                       \u2502 https://storage.googleapis.com/storage/v1/b?project=zenml-core&projection=noAcl&prettyPrint=false:                        \u2503\n\u2503                       \u2502 empty-connectors@zenml-core.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud         \u2503\n\u2503                       \u2502 project. Permission 'storage.buckets.list' denied on resource (or it may not exist).                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 \ud83d\udca5 error: connector authorization failure: Failed to list GKE clusters: 403 Required \"container.clusters.list\"            \u2503\n\u2503                       \u2502 permission(s) for \"projects/20219041791\". [request_id: \"0x84808facdac08541\"                                               \u2503\n\u2503                       \u2502 ]                                                                                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                                                                                                         \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                                                                                                      \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                                                                                                      \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                                                                                                    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nVerifying access to individual resource types will fail:\n\n```sh\nzenml service-connector verify gcp-empty-sa --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nError: Service connector 'gcp-empty-sa' verification failed: connector authorization failure: Failed to list GKE clusters:\n403 Required \"container.clusters.list\" permission(s) for \"projects/20219041791\".\n```\n{% endcode %}\n\n```sh\nzenml service-connector verify gcp-empty-sa --resource-type gcs-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\nError: Service connector 'gcp-empty-sa' verification failed: connector authorization failure: failed to list GCS buckets:\n403 GET https://storage.googleapis.com/storage/v1/b?project=zenml-core&projection=noAcl&prettyPrint=false:\nempty-connectors@zenml-core.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud project.\n"}
{"input": "Permission 'storage.buckets.list' denied on resource (or it may not exist).\n```\n{% endcode %}\n\n```sh\nzenml service-connector verify gcp-empty-sa --resource-type gcs-bucket --resource-id zenml-bucket-sl\n```\n\n{% code title=\"Example Command Output\" %}\n```\nError: Service connector 'gcp-empty-sa' verification failed: connector authorization failure: failed to fetch GCS bucket\nzenml-bucket-sl: 403 GET https://storage.googleapis.com/storage/v1/b/zenml-bucket-sl?projection=noAcl&prettyPrint=false:\nempty-connectors@zenml-core.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\nPermission 'storage.buckets.get' denied on resource (or it may not exist).\n```\n{% endcode %}\n\nNext, we'll register a GCP Service Connector that actually uses account impersonation to access the `zenml-bucket-sl` GCS bucket and verify that it can actually access the bucket:\n\n```sh\nzenml service-connector register gcp-impersonate-sa --type gcp --auth-method impersonation --service_account_json=@empty-connectors@zenml-core.json  --project_id=zenml-core --target_principal=zenml-bucket-sl@zenml-core.iam.gserviceaccount.com --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl\n```\n\n{% code title=\"Example Command Output\" %}\n```\nExpanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.\nSuccessfully registered service connector `gcp-impersonate-sa` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### External Account (GCP Workload Identity)\n\nUse [GCP workload identity federation](https://cloud.google.com/iam/docs/workload-identity-federation) to authenticate to GCP services using AWS IAM credentials, Azure Active Directory credentials or generic OIDC tokens.\n\nThis authentication method only requires a GCP workload identity external account JSON file that only contains the configuration for the external account without any sensitive credentials. It allows implementing [a two layer authentication scheme](best-security-practices.md#impersonating-accounts-and-assuming-roles) that keeps the set of permissions associated with implicit credentials down to the bare minimum"}
{"input": " and grants permissions to the privilege-bearing GCP service account instead.\n\nThis authentication method can be used to authenticate to GCP services using credentials from other cloud providers or identity providers. When used with workloads running on AWS or Azure, it involves automatically picking up credentials from the AWS IAM or Azure AD identity associated with the workload and using them to authenticate to GCP services. This means that the result depends on the environment where the ZenML server is deployed and is thus not fully reproducible.\n\n{% hint style=\"warning\" %}\nWhen used with AWS or Azure implicit in-cloud authentication, this method may constitute a security risk, because it can give users access to the identity (e.g. AWS IAM role or Azure AD principal) implicitly associated with the environment where the ZenML server is running. For this reason, all implicit authentication methods are disabled by default and need to be explicitly enabled by setting the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable or the helm chart `enableImplicitAuthMethods` configuration option to `true` in the ZenML deployment.\n{% endhint %}\n\nBy default, the GCP connector generates temporary OAuth 2.0 tokens from the external account credentials and distributes them to clients. The tokens have a limited lifetime of 1 hour. This behavior can be disabled by setting the `generate_temporary_tokens` configuration option to `False`, in which case, the connector will distribute the external account credentials JSON to clients instead (not recommended).\n\nA GCP project is required and the connector may only be used to access GCP resources in the specified roject. This project must be the same as the one for which the external account was configured.\n\nIf you already have the GOOGLE\\_APPLICATION\\_CREDENTIALS environment variable configured to point to an external account key JSON file, it will be automatically picked up when auto-configuration is used.\n\n<details>\n\n<summary>Example configuration</summary>\n\nThe following assumes the following prerequisites are met, as covered in [the GCP documentation on how to configure workload identity federation with AWS](https://cloud.google.com/iam/docs/workload-identity-federation-with-other-clouds):\n\n* the ZenML server is deployed in AWS in an EKS cluster (or any other AWS compute environment)\n* the ZenML server EKS pods are associated with an AWS IAM role by means of an IAM OIDC provider, as covered in the [AWS documentation on how to associate a IAM role with a service account](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html). Alternatively, [the IAM role associated with the EKS/EC2 nodes](https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html) can be used instead. This AWS IAM role provides the implicit AWS IAM identity and credentials that will be used to authenticate to GCP services.\n* a GCP workload identity pool and AWS provider are configured for the GCP project where the target resources are"}
{"input": " located, as covered in [the GCP documentation on how to configure workload identity federation with AWS](https://cloud.google.com/iam/docs/workload-identity-federation-with-other-clouds).\n* a GCP service account is configured with permissions to access the target resources and granted the `roles/iam.workloadIdentityUser` role for the workload identity pool and AWS provider\n* a GCP external account JSON file is generated for the GCP service account. This is used to configure the GCP connector.\n\n```sh\nzenml service-connector register gcp-workload-identity --type gcp \\\n    --auth-method external-account --project_id=zenml-core \\\n    --external_account_json=@clientLibraryConfig-aws-zenml.json\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `gcp-workload-identity` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml"}
{"input": "-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNo sensitive credentials are stored with the Service Connector, just meta-information about the external provider and the external account:\n\n```sh\nzenml service-connector describe gcp-workload-identity -x\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-workload-identity' of type 'gcp' with id '37b6000e-3f7f-483e-b2c5-7a5db44fe66b' is\nowned by user 'default'.\n                        'gcp-workload-identity' gcp Service Connector Details                        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY               \u2502 VALUE                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID                     \u2502 37b6000e-3f7f-483e-b2c5-7a5db44fe66b                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME                   \u2502 gcp-workload-identity                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE                   \u2502 \ud83d\udd35 gcp                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD            \u2502 external-account                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES         \u2502 \ud83d\udd35 gcp-generic, \ud83d\udce6 gcs-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME          \u2502 <multiple>                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID              \u2502 1ff6557f-7f60-4e63-b73d-650e64f015b5                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION       \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN             \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES_SKEW_TOLERANCE \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER                  \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE              \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT             \u2502 2024-01-30 20:44:14.020514                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT             \u2502 2024-01-30 20:44:14.020516                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n                                              Configuration                                              \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE                                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id            \u2502 zenml-core                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 external_account_json \u2502 {                                                                             \u2503\n\u2503                       \u2502   \"type\": \"external_account\",                                                 \u2503\n\u2503                       \u2502   \"audience\":                                                                 \u2503\n\u2503                       \u2502 \"//iam.googleapis.com/projects/30267569827/locations/global/workloadIdentityP \u2503\n\u2503                       \u2502 ools/mypool/providers/myprovider\",                                            \u2503\n\u2503                       \u2502   \"subject_token_type\": \"urn:ietf:params:aws:token-type:aws4_request\",        \u2503\n\u2503                       \u2502   \"service_account_impersonation_url\":                                        \u2503\n\u2503                       \u2502 \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/myrole@  \u2503\n\u2503                       \u2502 zenml-core.iam.gserviceaccount.com:generate"}
{"input": "AccessToken\",                      \u2503\n\u2503                       \u2502   \"token_url\": \"https://sts.googleapis.com/v1/token\",                         \u2503\n\u2503                       \u2502   \"credential_source\": {                                                      \u2503\n\u2503                       \u2502     \"environment_id\": \"aws1\",                                                 \u2503\n\u2503                       \u2502     \"region_url\":                                                             \u2503\n\u2503                       \u2502 \"http://169.254.169.254/latest/meta-data/placement/availability-zone\",        \u2503\n\u2503                       \u2502     \"url\":                                                                    \u2503\n\u2503                       \u2502 \"http://169.254.169.254/latest/meta-data/iam/security-credentials\",           \u2503\n\u2503                       \u2502     \"regional_cred_verification_url\":                                         \u2503\n\u2503                       \u2502 \"https://sts.{region}.amazonaws.com?Action=GetCallerIdentity&Version=2011-06- \u2503\n\u2503                       \u2502 15\"                                                                           \u2503\n\u2503                       \u2502   }                                                                           \u2503\n\u2503                       \u2502 }                                                                             \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### GCP OAuth 2.0 token\n\nUses [temporary OAuth 2.0 tokens](best-security-practices.md#short-lived-credentials) explicitly configured by the user.\n\nThis method has the major limitation that the user must regularly generate new tokens and update the connector configuration as OAuth 2.0 tokens expire. On the other hand, this method is ideal in cases where the connector only needs to be used for a short period of time, such as sharing access temporarily with someone else in your team.\n\nUsing any of the other authentication methods will automatically generate and refresh OAuth 2.0 tokens for clients upon request.\n\nA GCP project is required and the connector may only be used to access GCP resources in the specified project.\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nFetching OAuth 2.0 tokens from the local GCP CLI is possible if the GCP CLI is already configured with valid credentials (i.e. by running `gcloud auth application-default login`). We need to force the ZenML CLI to use the OAuth 2.0 token authentication by passing the `--auth-method oauth2-token` option, otherwise, it would automatically pick up long-term credentials:\n\n```sh\nzenml service-connector register gcp-oauth2-token --type gcp --auto-configure --auth-method oauth2-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `gcp-oauth2-token` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe gcp-oauth2-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-oauth2-token' of type 'gcp' with id 'ec4d7d85-c71c-476b-aa76-95bf772c90da' is owned by user 'default' and is 'private'.\n                       'gcp-oauth2-token' gcp Service Connector"}
{"input": " Details                        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 ec4d7d85-c71c-476b-aa76-95bf772c90da                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 gcp-oauth2-token                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd35 gcp                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 oauth2-token                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd35 gcp-generic, \ud83d\udce6 gcs-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 4694de65-997b-4929-8831-b49d5e067b97                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 59m46s                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-05-19 09:04:33.557126                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-05-19 09"}
{"input": ":04:33.557127                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n       Configuration       \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY   \u2502 VALUE      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id \u2502 zenml-core \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 token      \u2502 [HIDDEN]   \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNote the temporary nature of the Service Connector. It will expire and become unusable in 1 hour:\n\n```sh\nzenml service-connector list --name gcp-oauth2-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME             \u2502 ID                                   \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcp-oauth2-token \u2502 ec4d7d85-c71c-476b-aa76-95bf772c90da \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502 59m35s     \u2502        \u2503\n\u2503        \u2502                  \u2502                                      \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                  \u2502                                      \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                  \u2502                                      \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Auto-configuration\n\nThe GCP Service Connector allows [auto-discovering and fetching credentials](service-connectors-guide.md#auto-configuration) and configuration [set up by the GCP CLI](https://cloud.google.com/sdk/gcloud) on your local host.\n\n<details>\n\n<summary>Auto-configuration example</summary>\n\nThe following is an example of lifting GCP user credentials granting access to the same set of GCP resources and services that the local GCP CLI is allowed to access. The GCP CLI should already be configured with valid credentials (i.e. by running `gcloud auth application-default login`). In this case, the [GCP user account authentication method](gcp-service-connector.md#gcp-user-account) is automatically detected:\n\n```sh\nzenml service-connector register gcp-auto --type gcp --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\nSuccessfully registered service connector `gcp-auto` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                      "}
{"input": " \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe gcp-auto\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-auto' of type 'gcp' with id 'fe16f141-7406-437e-a579-acebe618a293' is owned by user 'default' and is 'private'.\n                           'gcp-auto' gcp Service Connector Details                            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 fe16f141-7406-437e-a579-acebe618a293                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 gcp-auto                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd35 gcp                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 user-account                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd35 gcp-generic, \ud83d\udce6 gcs-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 5eca8f6e-291"}
{"input": "f-4958-ae2d-a3e847a1ad8a                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-05-19 09:15:12.882929                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-05-19 09:15:12.882930                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n          Configuration           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY          \u2502 VALUE      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 project_id        \u2502 zenml-core \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 user_account_json \u2502 [HIDDEN]   \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Local client provisioning\n\nThe local `gcloud` CLI, the Kubernetes `kubectl` CLI and the Docker CLI can be[ configured with credentials extracted from or generated by a compatible GCP Service Connector](service-connectors-guide.md#configure-local-clients). Please note that unlike the configuration made possible through the GCP CLI, the Kubernetes and Docker credentials issued by the GCP Service Connector have a short lifetime and will need to be regularly refreshed. This is a byproduct of implementing a high-security profile.\n\n{% hint style=\"info\" %}\nNote that the `gcloud` local client can only be configured with credentials issued by the GCP Service Connector if the connector is configured with the [GCP user account authentication method](gcp-service-connector.md#gcp"}
{"input": "-user-account) or the [GCP service account authentication method](gcp-service-connector.md#gcp-service-account) and if the `generate_temporary_tokens` option is set to true in the Service Connector configuration.\n\nOnly the `gcloud` local [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials) configuration will be updated by the GCP Service Connector configuration. This makes it possible to use libraries and SDKs that use the application default credentials to access GCP resources.\n{% endhint %}\n\n<details>\n\n<summary>Local CLI configuration examples</summary>\n\nThe following shows an example of configuring the local Kubernetes CLI to access a GKE cluster reachable through a GCP Service Connector:\n\n```sh\nzenml service-connector list --name gcp-user-account\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME             \u2502 ID                                   \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcp-user-account \u2502 ddbce93f-df14-4861-a8a4-99a80972f3bc \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2503        \u2502                  \u2502                                      \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                  \u2502                                      \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                  \u2502                                      \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe following lists all Kubernetes clusters accessible through the GCP Service Connector:\n\n```sh\nzenml service-connector verify gcp-user-account --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-user-account' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nCalling the login CLI command will configure the local Kubernetes `kubectl` CLI to access the Kubernetes cluster through the GCP Service Connector:\n\n```sh\nzenml service-connector login gcp-user-account --resource-type kubernetes-cluster --resource-id zenml-test-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2834 Attempting to configure local client using service connector 'gcp-user-account'...\nContext \"gke_zenml-core_zenml-test-cluster\" modified.\nUpdated local kubeconfig with the cluster details. The current kubectl context was set to 'gke_zenml-core_zenml-test-cluster'.\nThe 'gcp-user-account' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.\n```\n{% endcode %}\n\nTo verify that the local Kubernetes `kubectl` CLI is correctly configured, the following command can be used:\n\n```sh\nkubectl cluster-info\n```\n\n{% code title=\"Example Command Output\" %}\n```\nKubernetes control plane is running at https://35.185.95.223\nGLBCDefaultBackend is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nKubeDNS is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n```\n{% endcode %}\n\nA similar process is possible with GCR container registries:\n\n```sh\nzenml service-connector verify gcp-user-account --resource-type docker-registry --resource-id europe-west1-docker.pkg.dev/zenml-core/test\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'gcp-user-account' is correctly configured with valid credentials and has access to the following resources:\n\u250f"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 europe-west1-docker.pkg.dev/zenml-core/test \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector login gcp-user-account --resource-type docker-registry --resource-id europe-west1-docker.pkg.dev/zenml-core/test\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2826 Attempting to configure local client using service connector 'gcp-user-account'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'gcp-user-account' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n\nTo verify that the local Docker container registry client is correctly configured, the following command can be used:\n\n```sh\ndocker push europe-west1-docker.pkg.dev/zenml-core/test/zenml\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe push refers to repository [europe-west1-docker.pkg.dev/zenml-core/test/zenml]\nd4aef4f5ed86: Pushed \n2d69a4ce1784: Pushed \n204066eca765: Pushed \n2da74ab7b0c1: Pushed \n75c35abda1d1: Layer already exists \n415ff8f0f676: Layer already exists \nc14cb5b1ec91: Layer already exists \na1d005f5264e: Layer already exists \n3a3fd880aca3: Layer already exists \n149a9c50e18e: Layer already exists \n1f6d3424b922: Layer already exists \n8402c959ae6f: Layer already exists \n419599cb5288: Layer already exists \n8553b91047da: Layer already exists \nconnectors: digest: sha256:a4cfb18a5cef5b2201759a42dd9fe8eb2f833b788e9d8a6ebde194765b42fe46 size: 3256\n```\n{% endcode %}\n\nIt is also possible to update the local `gcloud"}
{"input": "` CLI configuration with credentials extracted from the GCP Service Connector:\n\n```sh\nzenml service-connector login gcp-user-account --resource-type gcp-generic\n```\n\n{% code title=\"Example Command Output\" %}\n```\nUpdated the local gcloud default application credentials file at '/home/user/.config/gcloud/application_default_credentials.json'\nThe 'gcp-user-account' GCP Service Connector connector was used to successfully configure the local Generic GCP resource client/SDK.\n```\n{% endcode %}\n\n</details>\n\n## Stack Components use\n\nThe[ GCS Artifact Store Stack Component](../../component-guide/artifact-stores/gcp.md) can be connected to a remote GCS bucket through a GCP Service Connector.\n\nThe [Google Cloud Image Builder Stack Component](../../component-guide/image-builders/gcp.md), [VertexAI Orchestrator](../../component-guide/orchestrators/vertex.md), and [VertexAI Step Operator](../../component-guide/step-operators/vertex.md) can be connected and use the resources of a target GCP project through a GCP Service Connector.\n\nThe GCP Service Connector can also be used with any Orchestrator or Model Deployer stack component flavor that relies on Kubernetes clusters to manage workloads. This allows GKE Kubernetes container workloads to be managed without the need to configure and maintain explicit GCP or Kubernetes `kubectl` configuration contexts and credentials in the target environment or in the Stack Component itself.\n\nSimilarly, Container Registry Stack Components can be connected to a Google Artifact Registry or GCR Container Registry through a GCP Service Connector. This allows container images to be built and published to GAR or GCR container registries without the need to configure explicit GCP credentials in the target environment or the Stack Component.\n\n## End-to-end examples\n\n<details>\n\n<summary>GKE Kubernetes Orchestrator, GCS Artifact Store and GCR Container Registry with a multi-type GCP Service Connector</summary>\n\nThis is an example of an end-to-end workflow involving Service Connectors that use a single multi-type GCP Service Connector to give access to multiple resources for multiple Stack Components. A complete ZenML Stack is registered and composed of the following Stack Components, all connected through the same Service Connector:\n\n* a [Kubernetes Orchestrator](../../component-guide/orchestrators/kubernetes.md) connected to a GKE Kubernetes cluster\n* a [GCS Artifact Store](../../component-guide/artifact-stores/gcp.md) connected to a GCS bucket\n* a [GCP Container Registry](../../component-guide/container-registries/gcp.md) connected to a Docker Google Artifact Registry\n* a local [Image Builder](../../component-guide/image-builders/local.md)\n\nAs a last step, a simple pipeline is run on the resulting Stack.\n\n1.  Configure the local GCP CLI with valid user account credentials with a wide range of permissions (i.e. by running `gcloud auth application-default login`) and install ZenML integration"}
{"input": " prerequisites:\n\n    ```sh\n    zenml integration install -y gcp\n    ```\n\n    ```sh\n    gcloud auth application-default login\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nCredentials saved to file: [/home/stefan/.config/gcloud/application_default_credentials.json]\n\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\n\nQuota project \"zenml-core\" was added to ADC which can be used by Google client libraries for billing\nand quota. Note that some services may still bill the project owning the resource.\n```\n````\n{% endcode %}\n\n2.  Make sure the GCP Service Connector Type is available\n\n    ```sh\n    zenml service-connector list-types --type gcp\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS    \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 GCP Service Connector \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit        \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 impersonation   \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n3.  Register a multi-type GCP Service Connector using auto-configuration\n\n    ```sh\n    zenml service-connector register gcp-demo-multi --type gcp --auto-configure\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nSuccessfully registered service connector `gcp-demo-multi` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n```\n**NOTE**: from this point forward, we don't need the local GCP CLI credentials or the local GCP CLI at all. The steps that follow can be run on any machine regardless of whether it has been configured and authorized to access the GCP project.\n```\n\n4\\. find out which GCS buckets, GAR registries, and GKE Kubernetes clusters we can gain access to. We'll use this information to configure the Stack Components in our minimal GCP stack: a GCS Artifact Store, a Kubernetes Orchestrator, and a GCP Container Registry.\n\n````\n```sh\nzenml service-connector list-resources --resource-type gcs-bucket\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nThe following 'gcs-bucket' resources can be accessed by service connectors configured in your workspace:\n\u250f"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 eeeabc13-9203-463b-aa52-216e629e903c \u2502 gcp-demo-multi \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                                      \u2502                \u2502                \u2502               \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                                      \u2502                \u2502                \u2502               \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                                      \u2502                \u2502                \u2502               \u2502 gs://zenml-datasets                             \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector list-resources --resource-type kubernetes-cluster\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nThe following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 eeeabc13-9203-463b-aa52-216e629e903c \u2502 gcp-demo-multi \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector list-resources --resource-type docker-registry\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nThe following 'docker-registry' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 eeeabc13-9203-463b-aa52-216e629e903c \u2502 gcp-demo-multi \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udc33 docker-registry \u2502 gcr.io/zenml-core                               \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                                      \u2502                \u2502                \u2502                    \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n5"}
{"input": ".  register and connect a GCS Artifact Store Stack Component to a GCS bucket:\n\n    ```sh\n    zenml artifact-store register gcs-zenml-bucket-sl --flavor gcp --path=gs://zenml-bucket-sl\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered artifact_store `gcs-zenml-bucket-sl`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml artifact-store connect gcs-zenml-bucket-sl --connector gcp-demo-multi\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully connected artifact store `gcs-zenml-bucket-sl` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 eeeabc13-9203-463b-aa52-216e629e903c \u2502 gcp-demo-multi \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n6.  register and connect a Kubernetes Orchestrator Stack Component to a GKE cluster:\n\n    ```sh\n    zenml orchestrator register gke-zenml-test-cluster --flavor kubernetes --synchronous=true \n    --kubernetes_namespace=zenml-workloads\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered orchestrator `gke-zenml-test-cluster`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml orchestrator connect gke-zenml-test-cluster --connector gcp-demo-multi\n```\n\n````\n\n{% code title"}
{"input": "=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully connected orchestrator `gke-zenml-test-cluster` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 eeeabc13-9203-463b-aa52-216e629e903c \u2502 gcp-demo-multi \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n7.  Register and connect a GCP Container Registry Stack Component to a GAR registry:\n\n    ```sh\n    zenml container-registry register gcr-zenml-core --flavor gcp --uri=europe-west1-docker.pkg.dev/zenml-core/test\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered container_registry `gcr-zenml-core`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml container-registry connect gcr-zenml-core --connector gcp-demo-multi\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully connected container registry `gcr-zenml-core` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE"}
{"input": " TYPE      \u2502 RESOURCE NAMES                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 eeeabc13-9203-463b-aa52-216e629e903c \u2502 gcp-demo-multi \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udc33 docker-registry \u2502 europe-west1-docker.pkg.dev/zenml-core/test \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n8.  Combine all Stack Components together into a Stack and set it as active (also throw in a local Image Builder for completion):\n\n    ```sh\n    zenml image-builder register local --flavor local\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered image_builder `local`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml stack register gcp-demo -a gcs-zenml-bucket-sl -o gke-zenml-test-cluster -c gcr-zenml-core -i local --set\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nStack 'gcp-demo' successfully registered!\nActive global stack set to:'gcp-demo'\n```\n````\n{% endcode %}\n\n9.  Finally, run a simple pipeline to prove that everything works as expected. We'll use the simplest pipelines possible for this example:\n\n    ```python\n    from zenml import pipeline, step\n\n\n    @step\n    def step_1() -> str:\n        \"\"\"Returns the `world` string.\"\"\"\n        return \"world\"\n\n\n    @step(enable_cache=False)\n    def step_2(input_one: str, input_two: str) -> None:\n        \"\"\"Combines the two strings at its input and prints them.\"\"\"\n        combined_str = f\"{input_one} {input_two}\"\n        print(combined_str)\n\n\n    @pipeline\n    def my_pipeline():\n        output_step_one = step_1()\n        step_2(input_one=\"hello\", input_two=output_step_one)\n\n\n    if __name__ == \"__main__\":\n        my_pipeline()\n    ```\n\n    Saving that to a `run.py` file and running it gives us:\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n$ python run.py \nBuilding"}
{"input": " Docker image(s) for pipeline simple_pipeline.\nBuilding Docker image europe-west1-docker.pkg.dev/zenml-core/test/zenml:simple_pipeline-orchestrator.\n- Including integration requirements: gcsfs, google-cloud-aiplatform>=1.11.0, google-cloud-build>=3.11.0, google-cloud-container>=2.21.0, google-cloud-functions>=1.8.3, google-cloud-scheduler>=2.7.3, google-cloud-secret-manager, google-cloud-storage>=2.9.0, kfp==1.8.16, kubernetes==18.20.0, shapely<2.0\nNo .dockerignore found, including all files inside build context.\nStep 1/8 : FROM zenmldocker/zenml:0.39.1-py3.8\nStep 2/8 : WORKDIR /app\nStep 3/8 : COPY .zenml_integration_requirements .\nStep 4/8 : RUN pip install --default-timeout=60 --no-cache-dir  -r .zenml_integration_requirements\nStep 5/8 : ENV ZENML_ENABLE_REPO_INIT_WARNINGS=False\nStep 6/8 : ENV ZENML_CONFIG_PATH=/app/.zenconfig\nStep 7/8 : COPY . .\nStep 8/8 : RUN chmod -R a+rw .\nPushing Docker image europe-west1-docker.pkg.dev/zenml-core/test/zenml:simple_pipeline-orchestrator.\nFinished pushing Docker image.\nFinished building Docker image(s).\nRunning pipeline simple_pipeline on stack gcp-demo (caching disabled)\nWaiting for Kubernetes orchestrator pod...\nKubernetes orchestrator pod started.\nWaiting for pod of step step_1 to start...\nStep step_1 has started.\nStep step_1 has finished in 1.357s.\nPod of step step_1 completed.\nWaiting for pod of step simple_step_two to start...\nStep step_2 has started.\nHello World!\nStep step_2 has finished in 3.136s.\nPod of step step_2 completed.\nOrchestration pod completed.\nDashboard URL: http://34.148.132.191/workspaces/default/pipelines/cec118d1-d90a-44ec-8bd7-d978f726b7aa/runs\n```\n````\n{% endcode %}\n\n</details>\n\n<details>\n\n<summary>VertexAI Orchestrator, GCS Artifact Store, Google Artifact Registry and GCP Image Builder with single-instance GCP Service Connectors</summary>\n\nThis is an example of an end-to-end workflow involving Service Connectors that use multiple single-instance GCP Service Connectors, each giving access to a resource for a Stack Component. A complete ZenML Stack is registered and composed of the following Stack Components, all connected through its individual Service Connector:\n\n* a [VertexAI Orchestrator](../../component-guide"}
{"input": "/orchestrators/vertex.md) connected to the GCP project\n* a [GCS Artifact Store](../../component-guide/artifact-stores/gcp.md) connected to a GCS bucket\n* a [GCP Container Registry](../../component-guide/container-registries/gcp.md) connected to a GCR container registry\n* a [Google Cloud Image Builder](../../component-guide/image-builders/gcp.md) connected to the GCP project\n\nAs a last step, a simple pipeline is run on the resulting Stack.\n\n1.  Configure the local GCP CLI with valid user account credentials with a wide range of permissions (i.e. by running `gcloud auth application-default login`) and install ZenML integration prerequisites:\n\n    ```sh\n    zenml integration install -y gcp\n    ```\n\n    ```sh\n    gcloud auth application-default login\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nCredentials saved to file: [/home/stefan/.config/gcloud/application_default_credentials.json]\n\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\n\nQuota project \"zenml-core\" was added to ADC which can be used by Google client libraries for billing\nand quota. Note that some services may still bill the project owning the resource.\n```\n````\n{% endcode %}\n\n2.  Make sure the GCP Service Connector Type is available\n\n    ```sh\n    zenml service-connector list-types --type gcp\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS    \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 GCP Service Connector \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit        \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 impersonation   \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n3.  Register an individual single-instance GCP Service Connector using auto-configuration for each of the resources that will be needed for the Stack Components: a GCS bucket, a GCR registry, and generic GCP access for the VertexAI orchestrator and another one for the GCP Cloud Builder:\n\n    ```sh\n    zenml service-connector register gcs-zenml-bucket-sl --type gcp --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl --auto-configure\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nSuccessfully registered service connector `gcs-zenml-bucket-sl` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector register gcr-zenml-core --type gcp --resource-type docker-registry --auto-configure\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nSuccessfully registered service connector `gcr-zenml-core` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE       \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry    \u2502 gcr.io/zenml-core                               \u2503\n\u2503                       \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                       \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                       \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                       \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                       \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                       \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io"}
{"input": "          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector register vertex-ai-zenml-core --type gcp --resource-type gcp-generic --auto-configure\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nSuccessfully registered service connector `vertex-ai-zenml-core` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector register gcp-cloud-builder-zenml-core --type gcp --resource-type gcp-generic --auto-configure\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nSuccessfully registered service connector `gcp-cloud-builder-zenml-core` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n**NOTE**: from this point forward, we don't need the local GCP CLI credentials or the local GCP CLI at all. The steps that follow can be run on any machine regardless of whether it has been configured and authorized to access the GCP project.\n\nIn the end, the service connector list should look like this:\n\n```sh\nzenml service-connector list\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME                         \u2502 ID                                   \u2502 TYPE"}
{"input": "   \u2502 RESOURCE TYPES     \u2502 RESOURCE NAME        \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcs-zenml-bucket-sl          \u2502 405034fe-5e6e-4d29-ba62-8ae025381d98 \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udce6 gcs-bucket      \u2502 gs://zenml-bucket-sl \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcr-zenml-core               \u2502 9fddfaba-6d46-4806-ad96-9dcabef74639 \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udc33 docker-registry \u2502 gcr.io/zenml-core    \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 vertex-ai-zenml-core         \u2502 f97671b9-8c73-412b-bf5e-4b7c48596f5f \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic     \u2502 zenml-core           \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 gcp-cloud-builder-zenml-core \u2502 648c1016-76e4-4498-8de7-808fd20f057b \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic     \u2502 zenml-core           \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b"}
{"input": "\n```\n````\n{% endcode %}\n\n4.  register and connect a GCS Artifact Store Stack Component to the GCS bucket:\n\n    ```sh\n    zenml artifact-store register gcs-zenml-bucket-sl --flavor gcp --path=gs://zenml-bucket-sl\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered artifact_store `gcs-zenml-bucket-sl`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml artifact-store connect gcs-zenml-bucket-sl --connector gcs-zenml-bucket-sl\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully connected artifact store `gcs-zenml-bucket-sl` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME      \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 405034fe-5e6e-4d29-ba62-8ae025381d98 \u2502 gcs-zenml-bucket-sl \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n5.  register and connect a Google Cloud Image Builder Stack Component to the target GCP project:\n\n    ```sh\n    zenml image-builder register gcp-zenml-core --flavor gcp\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully registered image_builder `gcp-zenml-core`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml image-builder connect gcp-zenml-core --"}
{"input": "connector gcp-cloud-builder-zenml-core \n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully connected image builder `gcp-zenml-core` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME               \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 648c1016-76e4-4498-8de7-808fd20f057b \u2502 gcp-cloud-builder-zenml-core \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n6.  register and connect a Vertex AI Orchestrator Stack Component to the target GCP project\n\n    **NOTE**: If we do not specify a workload service account, the Vertex AI Pipelines Orchestrator uses the Compute Engine default service account in the target project to run pipelines. You must grant this account the Vertex AI Service Agent role, otherwise the pipelines will fail. More information on other configurations possible for the Vertex AI Orchestrator can be found [here](../../component-guide/orchestrators/vertex.md#how-to-use-it).\n\n    ```sh\n    zenml orchestrator register vertex-ai-zenml-core --flavor=vertex --location=europe-west1 --synchronous=true\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully registered orchestrator `vertex-ai-zenml-core`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml orchestrator connect vertex-ai-zenml-core --connector vertex-ai-zenml-core\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' ("}
{"input": "repository)\nSuccessfully connected orchestrator `vertex-ai-zenml-core` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME       \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 f97671b9-8c73-412b-bf5e-4b7c48596f5f \u2502 vertex-ai-zenml-core \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n7.  Register and connect a GCP Container Registry Stack Component to a GCR container registry:\n\n    ```sh\n    zenml container-registry register gcr-zenml-core --flavor gcp --uri=gcr.io/zenml-core \n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully registered container_registry `gcr-zenml-core`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml container-registry connect gcr-zenml-core --connector gcr-zenml-core\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully connected container registry `gcr-zenml-core` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 9fddf"}
{"input": "aba-6d46-4806-ad96-9dcabef74639 \u2502 gcr-zenml-core \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udc33 docker-registry \u2502 gcr.io/zenml-core \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n8.  Combine all Stack Components together into a Stack and set it as active:\n\n    ```sh\n    zenml stack register gcp-demo -a gcs-zenml-bucket-sl -o vertex-ai-zenml-core -c gcr-zenml-core -i gcp-zenml-core --set\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nStack 'gcp-demo' successfully registered!\nActive repository stack set to:'gcp-demo'\n```\n````\n{% endcode %}\n\n9.  Finally, run a simple pipeline to prove that everything works as expected. We'll use the simplest pipelines possible for this example:\n\n    ```python\n    from zenml import pipeline, step\n\n\n    @step\n    def step_1() -> str:\n        \"\"\"Returns the `world` string.\"\"\"\n        return \"world\"\n\n\n    @step(enable_cache=False)\n    def step_2(input_one: str, input_two: str) -> None:\n        \"\"\"Combines the two strings at its input and prints them.\"\"\"\n        combined_str = f\"{input_one} {input_two}\"\n        print(combined_str)\n\n\n    @pipeline\n    def my_pipeline():\n        output_step_one = step_1()\n        step_2(input_one=\"hello\", input_two=output_step_one)\n\n\n    if __name__ == \"__main__\":\n        my_pipeline()\n    ```\n\n    Saving that to a `run.py` file and running it gives us:\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n$ python run.py \nBuilding Docker image(s) for pipeline simple_pipeline.\nBuilding Docker image gcr.io/zenml-core/zenml:simple_pipeline-orchestrator.\n- Including integration requirements: gcsfs, google-cloud-aiplatform>=1.11.0, google-cloud-build>=3.11.0, google-cloud-container>=2.21.0, google-cloud-functions>=1.8.3, google-cloud-scheduler>=2.7.3, google-cloud-secret-manager, google-cloud-storage>=2.9.0, kfp==1.8.16, shapely<2.0\nUsing Cloud Build to build image gcr.io/zenml-core/zenml:s"}
{"input": "imple_pipeline-orchestrator\nNo .dockerignore found, including all files inside build context.\nUploading build context to gs://zenml-bucket-sl/cloud-build-contexts/5dda6dbb60e036398bee4974cfe3eb768a138b2e.tar.gz.\nBuild context located in bucket zenml-bucket-sl and object path cloud-build-contexts/5dda6dbb60e036398bee4974cfe3eb768a138b2e.tar.gz\nUsing Cloud Builder image gcr.io/cloud-builders/docker to run the steps in the build. Container will be attached to network using option --network=cloudbuild.\nRunning Cloud Build to build the Docker image. Cloud Build logs: https://console.cloud.google.com/cloud-build/builds/068e77a1-4e6f-427a-bf94-49c52270af7a?project=20219041791\nThe Docker image has been built successfully. More information can be found in the Cloud Build logs: https://console.cloud.google.com/cloud-build/builds/068e77a1-4e6f-427a-bf94-49c52270af7a?project=20219041791.\nFinished building Docker image(s).\nRunning pipeline simple_pipeline on stack gcp-demo (caching disabled)\nThe attribute pipeline_root has not been set in the orchestrator configuration. One has been generated automatically based on the path of the GCPArtifactStore artifact store in the stack used to execute the pipeline. The generated pipeline_root is gs://zenml-bucket-sl/vertex_pipeline_root/simple_pipeline/simple_pipeline_default_6e72f3e1.\n/home/stefan/aspyre/src/zenml/.venv/lib/python3.8/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n    warnings.warn(\nWriting Vertex workflow definition to /home/stefan/.config/zenml/vertex/8a0b53ee-644a-4fbe-8e91-d4d6ddf79ae8/pipelines/simple_pipeline_default_6e72f3e1.json.\nNo schedule detected. Creating one-off vertex job...\nSubmitting pipeline job with job_id simple-pipeline-default-6e72f3e1 to Vertex AI Pipelines service.\nThe Vertex AI Pipelines job workload will be executed using the connectors-vertex-ai-workload@zenml-core.iam.gserviceaccount.com service account.\nCreating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nPipelineJob created. Resource name: projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1"}
{"input": "\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1\nTo use this PipelineJob in another session:\nINFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\npipeline_job = aiplatform.PipelineJob.get('projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1')\nINFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1')\nView Pipeline Job:\nhttps://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/simple-pipeline-default-6e72f3e1?project=20219041791\nINFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\nhttps://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/simple-pipeline-default-6e72f3e1?project=20219041791\nView the Vertex AI Pipelines job at https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/simple-pipeline-default-6e72f3e1?project=20219041791\nWaiting for the Vertex AI Pipelines job to finish...\nPipelineJob projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1 current state:\nPipelineState.PIPELINE_STATE_RUNNING\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n...\nPipelineJob run completed. Resource name: projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1\nDashboard URL: https://34.148.132.191/workspaces/default/pipelines/17cac6b5-3071-45fa-a2ef-cda4a7965039/runs\n```\n````\n{% endcode %}\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Configuring HyperAI Connectors to connect ZenML to HyperAI instances.\n---\n\n# HyperAI Service Connector\n\nThe ZenML HyperAI Service Connector allows authenticating with a HyperAI instance for deployment of pipeline runs. This connector provides pre-authenticated Paramiko SSH clients to Stack Components that are linked to it.\n\n```shell\n$ zenml service-connector list-types --type hyperai\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503           NAME            \u2502 TYPE       \u2502 RESOURCE TYPES     \u2502 AUTH METHODS \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 HyperAI Service Connector \u2502 \ud83e\udd16 hyperai \u2502 \ud83e\udd16 hyperai-instance \u2502 rsa-key      \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                           \u2502            \u2502                    \u2502 dsa-key      \u2502       \u2502        \u2503\n\u2503                           \u2502            \u2502                    \u2502 ecdsa-key    \u2502       \u2502        \u2503\n\u2503                           \u2502            \u2502                    \u2502 ed25519-key  \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n## Prerequisites\nThe HyperAI Service Connector is part of the HyperAI integration. It is necessary to install the integration in order to use this Service Connector:\n\n* `zenml integration install hyperai` installs the HyperAI integration\n\n## Resource Types\nThe HyperAI Service Connector supports HyperAI instances.\n\n## Authentication Methods\nZenML creates an SSH connection to the HyperAI instance in the background when using this Service Connector. It then provides these connections to stack components requiring them, such as the HyperAI Orchestrator. Multiple authentication methods are supported:\n\n1. RSA key based authentication.\n2. DSA (DSS) key based authentication.\n3. ECDSA key based authentication.\n4. ED25519 key based authentication.\n\n{% hint style=\"warning\" %}\nSSH private keys configured in the connector will be distributed to all clients that use them to run pipelines with the HyperAI orchestrator. SSH keys are long-lived credentials that give unrestricted access to HyperAI instances.\n{% endhint %}\n\nWhen configuring the Service Connector, it is required to provide at least one hostname via `hostnames` and the `username` with which to login. Optionally, it is possible to provide an `ssh_passphrase` if"}
{"input": " applicable. This way, it is possible to use the HyperAI service connector in multiple ways:\n\n1. Create one service connector per HyperAI instance with different SSH keys.\n2. Configure a reused SSH key just once for multiple HyperAI instances, then select the individual instance when creating the HyperAI orchestrator component.\n\n## Auto-configuration\n\n{% hint style=\"info\" %}\nThis Service Connector does not support auto-discovery and extraction of authentication credentials from HyperAI instances. If this feature is useful to you or your organization, please let us know by messaging us in [Slack](https://zenml.io/slack) or [creating an issue on GitHub](https://github.com/zenml-io/zenml/issues).\n{% endhint %}\n\n## Stack Components use\n\nThe HyperAI Service Connector can be used by the HyperAI Orchestrator to deploy pipeline runs to HyperAI instances.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Configuring AWS Service Connectors to connect ZenML to AWS resources like S3\n  buckets, EKS Kubernetes clusters and ECR container registries.\n---\n\n# AWS Service Connector\n\nThe ZenML AWS Service Connector facilitates the authentication and access to managed AWS services and resources. These encompass a range of resources, including S3 buckets, ECR container repositories, and EKS clusters. The connector provides support for various authentication methods, including explicit long-lived AWS secret keys, IAM roles, short-lived STS tokens, and implicit authentication.\n\nTo ensure heightened security measures, this connector also enables [the generation of temporary STS security tokens that are scoped down to the minimum permissions necessary](best-security-practices.md#generating-temporary-and-down-scoped-credentials) for accessing the intended resource. Furthermore, it includes [automatic configuration and detection of credentials locally configured through the AWS CLI](service-connectors-guide.md#auto-configuration).\n\nThis connector serves as a general means of accessing any AWS service by issuing pre-authenticated boto3 sessions. Additionally, the connector can handle specialized authentication for S3, Docker, and Kubernetes Python clients. It also allows for the configuration of local Docker and Kubernetes CLIs.\n\n```shell\n$ zenml service-connector list-types --type aws\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS     \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AWS Service Connector \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic        \u2502 implicit         \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key       \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token        \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 iam-role         \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 session-token    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 federation-token \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n{% hint style=\"info\" %}\nThis service connector will not be able to work if [Multi"}
{"input": "-Factor Authentication (MFA)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_mfa\\_enable\\_cliapi.html) is enabled on the role used by the AWS CLI. When MFA is enabled, the AWS CLI generates temporary credentials that are valid for a limited time. These temporary credentials cannot be used by the ZenML AWS Service Connector, as it requires long-lived credentials to authenticate and access AWS resources.\n\nTo use the AWS Service Connector with ZenML, you will need to use a different AWS CLI profile that does not have MFA enabled. You can do this by setting the `AWS_PROFILE` environment variable to the name of the profile you want to use before running the ZenML CLI commands.\n{% endhint %}\n\n## Prerequisites\n\nThe AWS Service Connector is part of the AWS ZenML integration. You can either install the entire integration or use a PyPI extra to install it independently of the integration:\n\n* `pip install \"zenml[connectors-aws]\"` installs only prerequisites for the AWS Service Connector Type\n* `zenml integration install aws` installs the entire AWS ZenML integration\n\nIt is not required to [install and set up the AWS CLI on your local machine](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) to use the AWS Service Connector to link Stack Components to AWS resources and services. However, it is recommended to do so if you are looking for a quick setup that includes using the auto-configuration Service Connector features.\n\n{% hint style=\"info\" %}\nThe auto-configuration examples in this page rely on the AWS CLI being installed and already configured with valid credentials of one type or another. If you want to avoid installing the AWS CLI, we recommend using the interactive mode of the ZenML CLI to register Service Connectors:\n\n```\nzenml service-connector register -i --type aws\n```\n{% endhint %}\n\n## Resource Types\n\n### Generic AWS resource\n\nThis resource type allows consumers to use the AWS Service Connector to connect to any AWS service or resource. When used by connector clients, they are provided a generic Python boto3 session instance pre-configured with AWS credentials. This session can then be used to create boto3 clients for any particular AWS service.\n\nThis generic AWS resource type is meant to be used with Stack Components that are not represented by other, more specific resource types, like S3 buckets, Kubernetes clusters, or Docker registries. It should be accompanied by a matching set of AWS permissions that allow access to the set of remote resources required by the client(s).\n\nThe resource name represents the AWS region that the connector is authorized to access.\n\n### S3 bucket\n\nAllows users to connect to S3 buckets. When used by connector consumers, they are provided a pre-configured boto3 S3 client instance.\n\nThe configured credentials must have at least the following [AWS IAM permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access"}
{"input": "\\_policies.html) associated with [the ARNs of S3 buckets ](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-arn-format.html)that the connector will be allowed to access (e.g. `arn:aws:s3:::*` and `arn:aws:s3:::*/*` represent all the available S3 buckets).\n\n* `s3:ListBucket`\n* `s3:GetObject`\n* `s3:PutObject`\n* `s3:DeleteObject`\n* `s3:ListAllMyBuckets`\n\n{% hint style=\"info\" %}\nIf you are using the [AWS IAM role](aws-service-connector.md#aws-iam-role), [Session Token](aws-service-connector.md#aws-session-token), or [Federation Token](aws-service-connector.md#aws-federation-token) authentication methods, you don't have to worry too much about restricting the permissions of the AWS credentials that you use to access the AWS cloud resources. These authentication methods already support [automatically generating temporary tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) with permissions down-scoped to the minimum required to access the target resource.\n{% endhint %}\n\nIf set, the resource name must identify an S3 bucket using one of the following formats:\n\n* S3 bucket URI (canonical resource name): `s3://{bucket-name}`\n* S3 bucket ARN: `arn:aws:s3:::{bucket-name}`\n* S3 bucket name: `{bucket-name}`\n\n### EKS Kubernetes cluster\n\nAllows users to access an EKS cluster as a standard Kubernetes cluster resource. When used by Stack Components, they are provided a pre-authenticated Python Kubernetes client instance.\n\nThe configured credentials must have at least the following [AWS IAM permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access\\_policies.html) associated with the [ARNs of EKS clusters](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html) that the connector will be allowed to access (e.g. `arn:aws:eks:{region_id}:{project_id}:cluster/*` represents all the EKS clusters available in the target AWS region).\n\n* `eks:ListClusters`\n* `eks:DescribeCluster`\n\n{% hint style=\"info\" %}\nIf you are using the [AWS IAM role](aws-service-connector.md#aws-iam-role), [Session Token](aws-service-connector.md#aws-session-token) or [Federation Token](aws-service-connector.md#aws-federation-token) authentication methods, you don't have to worry too much about restricting the permissions of the AWS credentials that you use to access the AWS cloud resources. These authentication methods already support [automatically generating temporary tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) with permissions down-scoped to the minimum required"}
{"input": " to access the target resource.\n{% endhint %}\n\nIn addition to the above permissions, if the credentials are not associated with the same IAM user or role that created the EKS cluster, the IAM principal must be manually added to the EKS cluster's `aws-auth` ConfigMap, otherwise the Kubernetes client will not be allowed to access the cluster's resources. This makes it more challenging to use [the AWS Implicit](aws-service-connector.md#implicit-authentication) and [AWS Federation Token](aws-service-connector.md#aws-federation-token) authentication methods for this resource. For more information, [see this documentation](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html).\n\nIf set, the resource name must identify an EKS cluster using one of the following formats:\n\n* EKS cluster name (canonical resource name): `{cluster-name}`\n* EKS cluster ARN: `arn:aws:eks:{region}:{account-id}:cluster/{cluster-name}`\n\nEKS cluster names are region scoped. The connector can only be used to access EKS clusters in the AWS region that it is configured to use.\n\n### ECR container registry\n\nAllows Stack Components to access one or more ECR repositories as a standard Docker registry resource. When used by Stack Components, they are provided a pre-authenticated python-docker client instance.\n\nThe configured credentials must have at least the following [AWS IAM permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access\\_policies.html) associated with the [ARNs of one or more ECR repositories](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html) that the connector will be allowed to access (e.g. `arn:aws:ecr:{region}:{account}:repository/*` represents all the ECR repositories available in the target AWS region).\n\n* `ecr:DescribeRegistry`\n* `ecr:DescribeRepositories`\n* `ecr:ListRepositories`\n* `ecr:BatchGetImage`\n* `ecr:DescribeImages`\n* `ecr:BatchCheckLayerAvailability`\n* `ecr:GetDownloadUrlForLayer`\n* `ecr:InitiateLayerUpload`\n* `ecr:UploadLayerPart`\n* `ecr:CompleteLayerUpload`\n* `ecr:PutImage`\n* `ecr:GetAuthorizationToken`\n\n{% hint style=\"info\" %}\nIf you are using the [AWS IAM role](aws-service-connector.md#aws-iam-role), [Session Token](aws-service-connector.md#aws-session-token), or [Federation Token](aws-service-connector.md#aws-federation-token) authentication methods, you don't have to worry too much about restricting the permissions of the AWS credentials that you use to access the AWS cloud resources. These authentication methods already support [automatically generating temporary tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) with permissions"}
{"input": " down-scoped to the minimum required to access the target resource.\n{% endhint %}\n\nThis resource type is not scoped to a single ECR repository. Instead, a connector configured with this resource type will grant access to all the ECR repositories that the credentials are allowed to access under the configured AWS region (i.e. all repositories under the Docker registry URL `https://{account-id}.dkr.ecr.{region}.amazonaws.com`).\n\nThe resource name associated with this resource type uniquely identifies an ECR registry using one of the following formats (the repository name is ignored, only the registry URL/ARN is used):\n\n* ECR repository URI (canonical resource name):\n\n`[https://]{account}.dkr.ecr.{region}.amazonaws.com[/{repository-name}]`\n\n* ECR repository ARN :\n\n`arn:aws:ecr:{region}:{account-id}:repository[/{repository-name}]`\n\nECR repository names are region scoped. The connector can only be used to access ECR repositories in the AWS region that it is configured to use.\n\n## Authentication Methods\n\n### Implicit authentication\n\n[Implicit authentication](best-security-practices.md#implicit-authentication) to AWS services using environment variables, local configuration files or IAM roles.\n\n{% hint style=\"warning\" %}\nThis method may constitute a security risk, because it can give users access to the same cloud resources and services that the ZenML Server itself is configured to access. For this reason, all implicit authentication methods are disabled by default and need to be explicitly enabled by setting the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable or the helm chart `enableImplicitAuthMethods` configuration option to `true` in the ZenML deployment.\n{% endhint %}\n\nThis authentication method doesn't require any credentials to be explicitly configured. It automatically discovers and uses credentials from one of the following sources:\n\n* environment variables (AWS\\_ACCESS\\_KEY\\_ID, AWS\\_SECRET\\_ACCESS\\_KEY, AWS\\_SESSION\\_TOKEN, AWS\\_DEFAULT\\_REGION)\n* local configuration files [set up through the AWS CLI ](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)(\\~/aws/credentials, \\~/.aws/config)\n* IAM roles for Amazon EC2, ECS, EKS, Lambda, etc. Only works when running the ZenML server on an AWS resource with an IAM role attached to it.\n\nThis is the quickest and easiest way to authenticate to AWS services. However, the results depend on how ZenML is deployed and the environment where it is used and is thus not fully reproducible:\n\n* when used with the default local ZenML deployment or a local ZenML server, the credentials are the same as those used by the AWS CLI or extracted from local environment variables\n* when connected to a ZenML server, this method only works if the ZenML server is deployed in AWS and will use the IAM role attached to the AWS resource where the Zen"}
{"input": "ML server is running (e.g. an EKS cluster). The IAM role permissions may need to be adjusted to allow listing and accessing/describing the AWS resources that the connector is configured to access.\n\nAn IAM role may optionally be specified to be assumed by the connector on top of the implicit credentials. This is only possible when the implicit credentials have permissions to assume the target IAM role. Configuring an IAM role has all the advantages of the [AWS IAM Role](aws-service-connector.md#aws-iam-role) authentication method plus the added benefit of not requiring any explicit credentials to be configured and stored:\n\n* the connector will [generate temporary STS tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) upon request by [calling the AssumeRole STS API](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_assumerole).\n* allows implementing [a two layer authentication scheme](best-security-practices.md#impersonating-accounts-and-assuming-roles) that keeps the set of permissions associated with implicit credentials down to the bare minimum and grants permissions to the privilege-bearing IAM role instead.\n* one or more optional [IAM session policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access\\_policies.html#policies\\_session) may also be configured to further restrict the permissions of the generated STS tokens. If not specified, IAM session policies are automatically configured for the generated STS tokens [to restrict them to the minimum set of permissions required to access the target resource](best-security-practices.md#generating-temporary-and-down-scoped-credentials). Refer to the documentation for each supported Resource Type for the complete list of AWS permissions automatically granted to the generated STS tokens.\n* the default expiration period for generated STS tokens is 1 hour with a minimum of 15 minutes up to the maximum session duration setting configured for the IAM role (default is 1 hour). If you need longer-lived tokens, you can configure the IAM role to use a higher maximum expiration value (up to 12 hours) or use the AWS Federation Token or AWS Session Token authentication methods.\n\nNote that the discovered credentials inherit the full set of permissions of the local AWS client configuration, environment variables, or remote AWS IAM role. Depending on the extent of those permissions, this authentication instead method might not be recommended for production use, as it can lead to accidental privilege escalation. It is recommended to also configure an IAM role when using the implicit authentication method, or to use the [AWS IAM Role](aws-service-connector.md#aws-iam-role), [AWS Session Token](aws-service-connector.md#aws-session-token), or [AWS Federation Token](aws-service-connector.md#aws-federation-token) authentication methods instead to limit the validity and/or permissions of the credentials being issued to connector clients.\n\n{% hint style=\"info\" %}\nIf you need to access"}
{"input": " an EKS Kubernetes cluster with this authentication method, please be advised that the EKS cluster's `aws-auth` ConfigMap may need to be manually configured to allow authentication with the implicit IAM user or role picked up by the Service Connector. For more information, [see this documentation](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html).\n{% endhint %}\n\nAn AWS region is required and the connector may only be used to access AWS resources in the specified region.\n\n<details>\n\n<summary>Example configuration</summary>\n\nThe following assumes the local AWS CLI has a `connectors` AWS CLI profile already configured with credentials:\n\n```sh\nAWS_PROFILE=connectors zenml service-connector register aws-implicit --type aws --auth-method implicit --region=us-east-1\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-implicit'...\nSuccessfully registered service connector `aws-implicit` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2503                       \u2502 s3://zenml-public-datasets                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNo credentials are stored with the Service Connector:\n\n```sh\nzenml service-connector describe aws-implicit \n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-implicit' of type 'aws' with id 'e3853748-34a0-4d78-8006-00422ad32884' is owned by user 'default' and is 'private'.\n                        "}
{"input": " 'aws-implicit' aws Service Connector Details                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 9a810521-ef41-4e45-bb48-8569c5943dc6                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-implicit                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 implicit                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 18:08:37.969928                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 18:08:37.969930                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\ufffd"}
{"input": "\ufffd\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n     Configuration      \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region   \u2502 us-east-1 \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nVerifying access to resources (note the `AWS_PROFILE` environment points to the same AWS CLI profile used during registration, but may yield different results with a different profile, which is why this method is not suitable for reproducible results):\n\n```sh\nAWS_PROFILE=connectors zenml service-connector verify aws-implicit --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Verifying service connector 'aws-implicit'...\nService connector 'aws-implicit' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles                         \u2503\n\u2503               \u2502 s3://zenml-demos                      \u2503\n\u2503               \u2502 s3://zenml-generative-chat            \u2503\n\u2503               \u2502 s3://zenml-public-datasets            \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector verify aws-implicit --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Verifying service connector 'aws-implicit'...\nService connector 'aws-implicit' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://sagemaker-studio-907999144431-m11qlsdyqr8 \u2503\n\u2503               \u2502 s3://sagemaker-studio-d8a14tvjsmb             "}
{"input": " \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nDepending on the environment, clients are issued either temporary STS tokens or long-lived credentials, which is a reason why this method isn't well suited for production:\n\n```sh\nAWS_PROFILE=zenml zenml service-connector describe aws-implicit --resource-type s3-bucket --resource-id zenfiles --client\n```\n\n{% code title=\"Example Command Output\" %}\n```\nINFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\nService connector 'aws-implicit (s3-bucket | s3://zenfiles client)' of type 'aws' with id 'e3853748-34a0-4d78-8006-00422ad32884' is owned by user 'default' and is 'private'.\n    'aws-implicit (s3-bucket | s3://zenfiles client)' aws Service     \n                          Connector Details                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 9a810521-ef41-4e45-bb48-8569c5943dc6            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-implicit (s3-bucket | s3://zenfiles client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 sts-token                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 s3://zenfiles                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 59m57s                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2528\n\u2503 OWNER            \u2502 default                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 18:13:34.146659                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 18:13:34.146664                      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe aws-implicit --resource-type s3-bucket --resource-id s3://sagemaker-studio-d8a14tvjsmb --client\n```\n\n{% code title=\"Example Command Output\" %}\n```\nINFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\nService connector 'aws-implicit (s3-bucket | s3://sagemaker-studio-d8a14tvjsmb client)' of type 'aws' with id 'e3853748-34a0-4d78-8006-00422ad32884' is owned by user 'default' and is 'private'.\n    'aws-implicit (s3-bucket | s3://sagemaker-studio-d8a14tvjsmb client)' aws Service     \n                                    Connector Details                                     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 9a810521-ef41-4e45-bb48-8569c5943dc6                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-implicit (s3-bucket | s3://sagemaker-studio-d8a14tvjsmb client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 secret-key                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 s3://sagemaker-studio-d8a14tvjsmb                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 18:12:42.066053                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 18:12:42.066055                                          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### AWS Secret Key\n\n[Long-lived AWS credentials](best-security-practices.md#long-lived-credentials-api-keys-account-keys) consisting of an AWS access key ID and secret access key associated with an AWS IAM user or AWS account root user (not recommended).\n\nThis method is preferred during development and testing due to its simplicity and ease of use. It is not recommended as a direct authentication method for production use cases because the clients have direct access to long-lived credentials and are granted the full set of permissions of the IAM user or AWS account root user associated with the credentials. For production, it is recommended to use [the AWS IAM Role](aws-service-connector.md#aws-iam-role), [AWS Session Token](aws-service-connector.md#aws-session-token), or [AWS Federation Token](aws-service-connector.md#aws-federation-token) authentication method instead.\n\nAn AWS region is required and the connector may only be used to access AWS resources in the specified region.\n\nIf you already have the local AWS CLI set up with these credentials, they will be automatically picked up when auto-configuration is used (see the example below).\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nThe following assumes the local AWS CLI has a `connectors` AWS CLI profile configured with an AWS Secret Key. We need to force the ZenML CLI to use the Secret Key authentication by passing the `--auth-method secret-key` option, otherwise it would automatically use [the AWS Session Token authentication method](aws-service-connector.md#aws-session-token) as an extra precaution:\n\n```sh\nAWS_PROFILE=connectors zenml service-connector register aws-secret-key --type aws --auth-method secret-key --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-secret-key'...\nSuccessfully registered service connector `aws-secret-key` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\ufffd"}
{"input": "\ufffd\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe AWS Secret Key was lifted up from the local host:\n\n```sh\nzenml service-connector describe aws-secret-key\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-secret-key' of type 'aws' with id 'a1b07c5a-13af-4571-8e63-57a809c85790' is owned by user 'default' and is 'private'.\n                        'aws-secret-key' aws Service Connector Details                        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 37c97fa0-fa47-4d55-9970-e2aa6e1b50cf                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-secret-key                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 secret-key                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ufffd"}
{"input": "\ufffd\ufffd s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 b889efe1-0e23-4e2d-afc3-bdd785ee2d80                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:23:39.982950                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:23:39.982952                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### AWS STS Token\n\nUses [temporary STS tokens](best-security-practices.md#short-lived-credentials) explicitly configured by"}
{"input": " the user or auto-configured from a local environment.\n\nThis method has the major limitation that the user must regularly generate new tokens and update the connector configuration as STS tokens expire. On the other hand, this method is ideal in cases where the connector only needs to be used for a short period of time, such as sharing access temporarily with someone else in your team.\n\nUsing other authentication methods like [IAM role](aws-service-connector.md#aws-iam-role), [Session Token](aws-service-connector.md#aws-session-token), or [Federation Token](aws-service-connector.md#aws-federation-token) will automatically generate and refresh STS tokens for clients upon request.\n\nAn AWS region is required and the connector may only be used to access AWS resources in the specified region.\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nFetching STS tokens from the local AWS CLI is possible if the AWS CLI is already configured with valid credentials. In our example, the `connectors` AWS CLI profile is configured with an IAM user Secret Key. We need to force the ZenML CLI to use the STS token authentication by passing the `--auth-method sts-token` option, otherwise it would automatically use [the session token authentication method](aws-service-connector.md#aws-session-token):\n\n```sh\nAWS_PROFILE=connectors zenml service-connector register aws-sts-token --type aws --auto-configure --auth-method sts-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-sts-token'...\nSuccessfully registered service connector `aws-sts-token` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector configuration shows that the connector is configured with an STS token:\n\n```sh\nzenml service-connector describe aws-sts-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-sts-token' of type 'aws' with id '63e14350-6719-4255-b3f5-0539c8f7c303' is owned by user 'default' and is 'private'.\n                        'aws-sts-token' aws Service Connector Details                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 a05ef4ef-92cb-46b2-8a3a-a48535adccaf                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-sts-token                                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 sts-token                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 bffd79c7-6d76-483b-9001-e9dda4e865ae                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 11h58m24s                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:25:40.278681                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:25:40.278684                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNote the temporary nature of the Service Connector. It will become unusable in 12 hours:\n\n```sh\nzenml service-connector list --name aws-sts-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME          \u2502 ID                                   \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-sts-token \u2502 a05ef4ef-92cb-46b2-8a3a-a48535adccaf \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic        \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502 11h57m51s  \u2502        \u2503\n\u2503        \u2502               \u2502                                      \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502               \u2502                                      \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502               \u2502                                      \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### AWS IAM Role\n\nGenerates [temporary STS credentials](best-security-practices.md#impersonating-accounts-and-assuming-roles) by assuming an AWS IAM role.\n\nThis authentication method still requires credentials to be explicitly configured. If your ZenML server is running in AWS and you're looking for an alternative that uses implicit credentials while at the same time benefits from all the security advantages of assuming an IAM role, you should [use the implicit authentication method with a configured IAM role](aws-service-connector.md#implicit-authentication) instead.\n\nThe connector needs to be configured with the IAM role to be assumed accompanied by an AWS secret key associated with an IAM user or an STS token associated with another IAM role. The IAM user or IAM role must have permission to assume the target IAM role. The connector will [generate temporary STS tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) upon request by [calling the AssumeRole STS API](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_assumerole).\n\n[The best practice implemented with this authentication scheme](best-security-practices.md#impersonating-accounts-and-assuming-roles) is to keep the set of permissions associated with the primary IAM user or IAM role down to the bare minimum and grant permissions to the privilege-bearing IAM role instead.\n\nAn AWS region is required and the connector may only be used to access AWS resources in the specified region.\n\nOne or more optional [IAM session policies](https://"}
{"input": "docs.aws.amazon.com/IAM/latest/UserGuide/access\\_policies.html#policies\\_session) may also be configured to further restrict the permissions of the generated STS tokens. If not specified, IAM session policies are automatically configured for the generated STS tokens [to restrict them to the minimum set of permissions required to access the target resource](best-security-practices.md#generating-temporary-and-down-scoped-credentials). Refer to the documentation for each supported Resource Type for the complete list of AWS permissions automatically granted to the generated STS tokens.\n\nThe default expiration period for generated STS tokens is 1 hour with a minimum of 15 minutes up to the maximum session duration setting configured for the IAM role (default is 1 hour). If you need longer-lived tokens, you can configure the IAM role to use a higher maximum expiration value (up to 12 hours) or use the AWS Federation Token or AWS Session Token authentication methods.\n\nFor more information on IAM roles and the AssumeRole AWS API, see [the official AWS documentation on the subject](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_assumerole).\n\nFor more information about the difference between this method and the AWS Federation Token authentication method, [consult this AWS documentation page](https://aws.amazon.com/blogs/security/understanding-the-api-options-for-securely-delegating-access-to-your-aws-account/).\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nThe following assumes the local AWS CLI has a `zenml` AWS CLI profile already configured with an AWS Secret Key and an IAM role to be assumed:\n\n```sh\nAWS_PROFILE=zenml zenml service-connector register aws-iam-role --type aws --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-iam-role'...\nSuccessfully registered service connector `aws-iam-role` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                            "}
{"input": " \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector configuration shows an IAM role and long-lived credentials:\n\n```sh\nzenml service-connector describe aws-iam-role\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-iam-role' of type 'aws' with id '8e499202-57fd-478e-9d2f-323d76d8d211' is owned by user 'default' and is 'private'.\n                         'aws-iam-role' aws Service Connector Details                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 2b99de14-6241-4194-9608-b9d478e1bcfc                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-iam-role                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 iam-role                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 87795fdd-b70e-4895-b0dd-8bca5fd4d10e                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 3600s                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:28:31.679843                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:28:31.679848                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n                                          Configuration                                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 role_arn              \u2502 arn:aws:iam::715803424590:role/OrganizationAccountRestrictedAccessRole \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]                                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nHowever, clients receive temporary STS tokens instead of the AWS Secret Key configured in the connector (note the authentication method, expiration time, and credentials):\n\n```sh\nzenml service-connector describe aws-iam-role --resource-type s3-b"}
{"input": "ucket --resource-id zenfiles --client\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-iam-role (s3-bucket | s3://zenfiles client)' of type 'aws' with id '8e499202-57fd-478e-9d2f-323d76d8d211' is owned by user 'default' and is 'private'.\n    'aws-iam-role (s3-bucket | s3://zenfiles client)' aws Service     \n                          Connector Details                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 2b99de14-6241-4194-9608-b9d478e1bcfc            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-iam-role (s3-bucket | s3://zenfiles client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 sts-token                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 s3://zenfiles                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 59m56s                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:30:51.462445                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:30:51.462449                      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### AWS Session Token\n\nGenerates [temporary session STS tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) for IAM users.\n\nThe connector needs to be configured with an AWS secret key associated with an IAM user or AWS account root user (not recommended). The connector will [generate temporary STS tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials) upon request by calling [the GetSessionToken STS API](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_getsessiontoken).\n\nThe STS tokens have an expiration period longer than those issued through the [AWS IAM Role authentication method](aws-service-connector.md#aws-iam-role) and are more suitable for long-running processes that cannot automatically re-generate credentials upon expiration.\n\nAn AWS region is required and the connector may only be used to access AWS resources in the specified region.\n\nThe default expiration period for generated STS tokens is 12 hours with a minimum of 15 minutes and a maximum of 36 hours. Temporary credentials obtained by using the AWS account root user credentials (not recommended) have a maximum duration of 1 hour.\n\nAs a precaution, when long-lived credentials (i.e. AWS Secret Keys) are detected on your environment by the Service Connector during auto-configuration, this authentication method is automatically chosen instead of the AWS [Secret Key authentication method](aws-service-connector.md#aws-secret-key) alternative.\n\nGenerated STS tokens inherit the full set of permissions of the IAM user or AWS account root user that is calling the"}
{"input": " GetSessionToken API. Depending on your security needs, this may not be suitable for production use, as it can lead to accidental privilege escalation. Instead, it is recommended to use the AWS Federation Token or [AWS IAM Role authentication](aws-service-connector.md#aws-iam-role) methods to restrict the permissions of the generated STS tokens.\n\nFor more information on session tokens and the GetSessionToken AWS API, see [the official AWS documentation on the subject](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_getsessiontoken).\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nThe following assumes the local AWS CLI has a `connectors` AWS CLI profile already configured with an AWS Secret Key:\n\n```sh\nAWS_PROFILE=connectors zenml service-connector register aws-session-token --type aws --auth-method session-token --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-session-token'...\nSuccessfully registered service connector `aws-session-token` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector configuration shows long-lived credentials were lifted from the local environment and the AWS Session Token authentication method was configured:\n\n```sh\nzenml service-connector describe aws-session-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-session-token' of type 'aws' with id '3ae3e595-5cbc-446e-be"}
{"input": "64-e54e854e0e3f' is owned by user 'default' and is 'private'.\n                      'aws-session-token' aws Service Connector Details                       \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 c0f8e857-47f9-418b-a60f-c3b03023da54                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-session-token                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 session-token                                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 16f35107-87ef-4a86-bbae-caa4a918fc15                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 43200s                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:31:54.971869                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:31:54.971871                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nHowever, clients receive temporary STS tokens instead of the AWS Secret Key configured in the connector (note the authentication method, expiration time, and credentials):\n\n```sh\nzenml service-connector describe aws-session-token --resource-type s3-bucket --resource-id zenfiles --client\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-session-token (s3-bucket | s3://zenfiles client)' of type 'aws' with id '3ae3e595-5cbc-446e-be64-e54e854e0e3f' is owned by user 'default' and is 'private'.\n    'aws-session-token (s3-bucket | s3://zenfiles client)' aws Service     \n                             Connector Details                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 c0f8e857-47f9-418b-a60f-c3b03023da54                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-session-token (s3-bucket | s3://zenfiles client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503"}
{"input": " AUTH METHOD      \u2502 sts-token                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 s3://zenfiles                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 11h59m56s                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:35:24.090861                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:35:24.090863                           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### AWS Federation Token\n\nGenerates [temporary STS tokens](best-security-practices.md"}
{"input": "#generating-temporary-and-down-scoped-credentials) for federated users by [impersonating another user](best-security-practices.md#impersonating-accounts-and-assuming-roles).\n\nThe connector needs to be configured with an AWS secret key associated with an IAM user or AWS account root user (not recommended). The IAM user must have permission to call [the GetFederationToken STS API](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_getfederationtoken) (i.e. allow the `sts:GetFederationToken` action on the `*` IAM resource). The connector will generate temporary STS tokens upon request by calling the GetFederationToken STS API.\n\nThese STS tokens have an expiration period longer than those issued through [the AWS IAM Role authentication method](aws-service-connector.md#aws-iam-role) and are more suitable for long-running processes that cannot automatically re-generate credentials upon expiration.\n\nAn AWS region is required and the connector may only be used to access AWS resources in the specified region.\n\nOne or more optional [IAM session policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access\\_policies.html#policies\\_session) may also be configured to further restrict the permissions of the generated STS tokens. If not specified, IAM session policies are automatically configured for the generated STS tokens [to restrict them to the minimum set of permissions required to access the target resource](best-security-practices.md#generating-temporary-and-down-scoped-credentials). Refer to the documentation for each supported Resource Type for the complete list of AWS permissions automatically granted to the generated STS tokens.\n\n{% hint style=\"warning\" %}\nIf this authentication method is used with [the generic AWS resource type](aws-service-connector.md#generic-aws-resource), a session policy MUST be explicitly specified, otherwise, the generated STS tokens will not have any permissions.\n{% endhint %}\n\nThe default expiration period for generated STS tokens is 12 hours with a minimum of 15 minutes and a maximum of 36 hours. Temporary credentials obtained by using the AWS account root user credentials (not recommended) have a maximum duration of 1 hour.\n\n{% hint style=\"info\" %}\nIf you need to access an EKS Kubernetes cluster with this authentication method, please be advised that the EKS cluster's `aws-auth` ConfigMap may need to be manually configured to allow authentication with the federated user. For more information, [see this documentation](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html).\n{% endhint %}\n\nFor more information on user federation tokens, session policies, and the GetFederationToken AWS API, see [the official AWS documentation on the subject](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_temp\\_request.html#api\\_getfederationtoken).\n\nFor more information about"}
{"input": " the difference between this method and [the AWS IAM Role authentication method](aws-service-connector.md#aws-iam-role), [consult this AWS documentation page](https://aws.amazon.com/blogs/security/understanding-the-api-options-for-securely-delegating-access-to-your-aws-account/).\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nThe following assumes the local AWS CLI has a `connectors` AWS CLI profile already configured with an AWS Secret Key:\n\n```sh\nAWS_PROFILE=connectors zenml service-connector register aws-federation-token --type aws --auth-method federation-token --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2838 Registering service connector 'aws-federation-token'...\nSuccessfully registered service connector `aws-federation-token` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector configuration shows long-lived credentials have been picked up from the local AWS CLI configuration:\n\n```sh\nzenml service-connector describe aws-federation-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-federation-token' of type 'aws' with id '868b17d4-b950-4d89-a6c4-12e520e66610' is owned by user 'default' and is 'private'.\n                     'aws-federation-token' aws Service Connector Details                     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 e28c403e-8503-4cce-9226-8a7cd7934763                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-federation-token                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 federation-token                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 958b840d-2a27-4f6b-808b-c94830babd99                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 43200s                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:36:28.619751                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:36:28.619753                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nHowever, clients receive temporary STS tokens instead of the AWS Secret Key configured in the connector (note the authentication method, expiration time, and credentials):\n\n```sh\nzenml service-connector describe aws-federation-token --resource-type s3-bucket --resource-id zenfiles --client\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-federation-token (s3-bucket | s3://zenfiles client)' of type 'aws' with id '868b17d4-b950-4d89-a6c4-12e520e66610' is owned by user 'default' and is 'private'.\n    'aws-federation-token (s3-bucket | s3://zenfiles client)' aws Service     \n                              Connector Details                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 e28c403e-8503-4cce-9226-8a7cd7934763                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-federation-token (s3-bucket | s3://zenfiles client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 sts-token                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 s3://zenfiles                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 11h59m56s                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:38:29.406986                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:38:29.406991                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Auto-configuration\n\nThe AWS Service Connector allows [auto-discovering and fetching credentials](service-connectors-guide.md#auto-configuration) and configuration set up [by the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) during registration. The default AWS CLI profile is used unless the AWS"}
{"input": "\\_PROFILE environment points to a different profile.\n\n<details>\n\n<summary>Auto-configuration example</summary>\n\nThe following is an example of lifting AWS credentials granting access to the same set of AWS resources and services that the local AWS CLI is allowed to access. In this case, [the IAM role authentication method](aws-service-connector.md#aws-iam-role) was automatically detected:\n\n```sh\nAWS_PROFILE=zenml zenml service-connector register aws-auto --type aws --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2839 Registering service connector 'aws-auto'...\nSuccessfully registered service connector `aws-auto` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenbytes-bucket                         \u2503\n\u2503                       \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector configuration shows how credentials have automatically been fetched from the local AWS CLI configuration:\n\n```sh\nzenml service-connector describe aws-auto\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-auto' of type 'aws' with id '9f3139fd-4726-421a-bc07-312d83f0c89e' is owned by user 'default' and is 'private'.\n                           'aws-auto' aws Service Connector Details                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 9cdc926e-55d7-49f0-838e-db5ac34bb7dc                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-auto                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 iam-role                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 a137151e-1778-4f50-b64b-7cf6c1f715f5                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 3600s                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 19:39:11.958426                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 19:39:11.958428                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n                                          Configuration                                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 role_arn              \u2502 arn:aws:iam::715803424590:role/OrganizationAccountRestrictedAccessRole \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]                                                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Local client provisioning\n\nThe local AWS CLI, Kubernetes `kubectl` CLI and the Docker CLI can be [configured with credentials extracted from or generated by a compatible AWS Service Connector](service-connectors-guide.md#configure-local-clients). Please note that unlike the configuration made possible through the AWS CLI, the Kubernetes and Docker credentials issued by the AWS Service Connector have a short lifetime and will need to be regularly refreshed. This is a byproduct of implementing a high-security profile.\n\n{% hint style=\"info\" %}\nConfiguring the local AWS CLI with credentials issued by the AWS Service Connector results in a local AWS CLI configuration profile being created with the name inferred from the first digits of the Service Connector UUID in the form -\\<uuid\\[:8]>. For example, a Service Connector with UUID `9f3139fd-4726-421a-bc07-312d83f0c89e` will result in a local AWS CLI configuration profile named `zenml-9f3139fd`.\n{% endhint %}\n\n<details>\n\n<summary>Local CLI configuration examples</summary>\n\nThe following shows an example of configuring the local Kubernetes CLI to access an EKS cluster reachable through an AWS Service Connector:\n\n```sh\nzenml service-connector list --name aws-session-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME              \u2502 ID                                   \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-session-token \u2502 c0f8e857-47f9-418b-a60f-c3b03023da54 \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic        \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2503        \u2502                   \u2502                                      \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                   \u2502                                      \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                   \u2502                                      \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThis checks the Kubernetes clusters that the AWS Service Connector has access to:\n\n```sh\nzenml service-connector verify aws-session-token --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-session-token' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nRunning the login CLI command will configure the local `kubectl` CLI"}
{"input": " to access the Kubernetes cluster:\n\n```sh\nzenml service-connector login aws-session-token --resource-type kubernetes-cluster --resource-id zenhacks-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2807 Attempting to configure local client using service connector 'aws-session-token'...\nCluster \"arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster\" set.\nContext \"arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster\" modified.\nUpdated local kubeconfig with the cluster details. The current kubectl context was set to 'arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster'.\nThe 'aws-session-token' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.\n```\n{% endcode %}\n\nThe following can be used to check that the local `kubectl` CLI is correctly configured:\n\n```sh\nkubectl cluster-info\n```\n\n{% code title=\"Example Command Output\" %}\n```\nKubernetes control plane is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com\nCoreDNS is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```\n{% endcode %}\n\nA similar process is possible with ECR container registries:\n\n```sh\nzenml service-connector verify aws-session-token --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-session-token' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector login aws-session-token --resource-type docker-registry \n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u280f Attempting to configure local client using service connector 'aws-session-token'...\nWARNING! Your password will be"}
{"input": " stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'aws-session-token' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n\nThe following can be used to check that the local Docker client is correctly configured:\n\n```sh\ndocker pull 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server\n```\n\n{% code title=\"Example Command Output\" %}\n```\nUsing default tag: latest\nlatest: Pulling from zenml-server\ne9995326b091: Pull complete \nf3d7f077cdde: Pull complete \n0db71afa16f3: Pull complete \n6f0b5905c60c: Pull complete \n9d2154d50fd1: Pull complete \nd072bba1f611: Pull complete \n20e776588361: Pull complete \n3ce69736a885: Pull complete \nc9c0554c8e6a: Pull complete \nbacdcd847a66: Pull complete \n482033770844: Pull complete \nDigest: sha256:bf2cc3895e70dfa1ee1cd90bbfa599fa4cd8df837e27184bac1ce1cc239ecd3f\nStatus: Downloaded newer image for 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest\n715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest\n```\n{% endcode %}\n\nIt is also possible to update the local AWS CLI configuration with credentials extracted from the AWS Service Connector:\n\n```sh\nzenml service-connector login aws-session-token --resource-type aws-generic\n```\n\n{% code title=\"Example Command Output\" %}\n```\nConfigured local AWS SDK profile 'zenml-c0f8e857'.\nThe 'aws-session-token' AWS Service Connector connector was used to successfully configure the local Generic AWS resource client/SDK.\n```\n{% endcode %}\n\nA new profile is created in the local AWS CLI configuration holding the credentials. It can be used to access AWS resources and services, e.g.:\n\n```sh\naws --profile zenml-c0f8e857 s3 ls\n```\n\n</details>\n\n## Stack Components use\n\nThe [S3 Artifact Store Stack Component](../../component-guide/artifact-stores/s3.md) can be connected to a remote AWS S3 bucket through an AWS Service Connector.\n\nThe AWS Service Connector can also be used with any Orchestrator or Model Deployer stack component flavor that relies on Kubernetes clusters to manage workloads. This allows EKS Kubernetes container workloads to be managed without the need to configure and maintain explicit AWS"}
{"input": " or Kubernetes `kubectl` configuration contexts and credentials in the target environment and in the Stack Component.\n\nSimilarly, Container Registry Stack Components can be connected to an ECR Container Registry through an AWS Service Connector. This allows container images to be built and published to ECR container registries without the need to configure explicit AWS credentials in the target environment or the Stack Component.\n\n## End-to-end examples\n\n<details>\n\n<summary>EKS Kubernetes Orchestrator, S3 Artifact Store and ECR Container Registry with a multi-type AWS Service Connector</summary>\n\nThis is an example of an end-to-end workflow involving Service Connectors that use a single multi-type AWS Service Connector to give access to multiple resources for multiple Stack Components. A complete ZenML Stack is registered and composed of the following Stack Components, all connected through the same Service Connector:\n\n* a [Kubernetes Orchestrator](../../component-guide/orchestrators/kubernetes.md) connected to an EKS Kubernetes cluster\n* an [S3 Artifact Store](../../component-guide/artifact-stores/s3.md) connected to an S3 bucket\n* an [ECR Container Registry](../../component-guide/container-registries/aws.md) stack component connected to an ECR container registry\n* a local [Image Builder](../../component-guide/image-builders/local.md)\n\nAs a last step, a simple pipeline is run on the resulting Stack.\n\n1.  [Configure the local AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html) with valid IAM user account credentials with a wide range of permissions (i.e. by running `aws configure`) and install ZenML integration prerequisites:\n\n    ```sh\n    zenml integration install -y aws s3\n    ```\n\n    ```sh\n    aws configure --profile connectors\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n```\n````\n{% endcode %}\n\n2.  Make sure the AWS Service Connector Type is available\n\n    ```sh\n    zenml service-connector list-types --type aws\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS     \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AWS Service Connector \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic        \u2502 implicit         \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key       \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token        \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 iam-role         \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 session-token    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 federation-token \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n3.  Register a multi-type AWS Service Connector using auto-configuration\n\n    ```sh\n    AWS_PROFILE=connectors zenml service-connector register aws-demo-multi --type aws --auto-configure\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n\u283c Registering service connector 'aws-demo-multi'...\nSuccessfully registered service connector `aws-demo-multi` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% end"}
{"input": "code %}\n\n```\n**NOTE**: from this point forward, we don't need the local AWS CLI credentials or the local AWS CLI at all. The steps that follow can be run on any machine regardless of whether it has been configured and authorized to access the AWS platform or not.\n```\n\n4\\. find out which S3 buckets, ECR registries, and EKS Kubernetes clusters we can gain access to. We'll use this information to configure the Stack Components in our minimal AWS stack: an S3 Artifact Store, a Kubernetes Orchestrator, and an ECR Container Registry.\n\n````\n```sh\nzenml service-connector list-resources --resource-type s3-bucket\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nThe following 's3-bucket' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME      \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bf073e06-28ce-4a4a-8100-32e7cb99dced \u2502 aws-demo-multi      \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles                         \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 s3://zenml-demos                      \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 s3://zenml-generative-chat            \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector list-resources --resource-type kubernetes-cluster\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nThe following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bf073e06-28ce-4a4a-8100-32e7cb99dced \u2502 aws-demo-multi        \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml service-connector list-resources --resource-type docker-registry\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nThe following 'docker-registry' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME     \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bf073e06-28ce-4a4a-8100-32e7cb99dced \u2502 aws-demo-multi     \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n5.  register and connect an S3"}
{"input": " Artifact Store Stack Component to an S3 bucket:\n\n    ```sh\n    zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully registered artifact_store `s3-zenfiles`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml artifact-store connect s3-zenfiles --connector aws-demo-multi\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully connected artifact store `s3-zenfiles` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bf073e06-28ce-4a4a-8100-32e7cb99dced \u2502 aws-demo-multi \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n6.  register and connect a Kubernetes Orchestrator Stack Component to an EKS cluster:\n\n    ```sh\n    zenml orchestrator register eks-zenml-zenhacks --flavor kubernetes --synchronous=true --kubernetes_namespace=zenml-workloads\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully registered orchestrator `eks-zenml-zenhacks`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml orchestrator connect eks-zenml-zenhacks --connector aws-demo-multi\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository"}
{"input": ")\nSuccessfully connected orchestrator `eks-zenml-zenhacks` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bf073e06-28ce-4a4a-8100-32e7cb99dced \u2502 aws-demo-multi \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n7.  Register and connect an EC GCP Container Registry Stack Component to an ECR container registry:\n\n    ```sh\n    zenml container-registry register ecr-us-east-1 --flavor aws --uri=715803424590.dkr.ecr.us-east-1.amazonaws.com\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully registered container_registry `ecr-us-east-1`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml container-registry connect ecr-us-east-1 --connector aws-demo-multi\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (repository)\nRunning with active stack: 'default' (repository)\nSuccessfully connected container registry `ecr-us-east-1` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bf073e06-28ce-4a4a-8100-32e7cb99dced \u2502 aws-demo-multi \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n8.  Combine all Stack Components together into a Stack and set it as active (also throw in a local Image Builder for completion):\n\n    ```sh\n    zenml image-builder register local --flavor local\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered image_builder `local`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml stack register aws-demo -a s3-zenfiles -o eks-zenml-zenhacks -c ecr-us-east-1 -i local --set\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```text\nConnected to the ZenML server: 'https://stefan.develaws.zenml.io'\nRunning with active workspace: 'default' (repository)\nStack 'aws-demo' successfully registered!\nActive repository stack set to:'aws-demo'\n```\n````\n{% endcode %}\n\n9.  Finally, run a simple pipeline to prove that everything works as expected. We'll use the simplest pipelines possible for this example:\n\n    ```python\n    from zenml import pipeline, step\n\n\n    @step\n    def step_1() -> str:\n        \"\"\"Returns the `world` string.\"\"\"\n        return \"world\"\n\n\n    @step(enable_cache=False)\n    def step_2(input_one: str, input_two: str) -> None:\n        \"\"\"Combines the two strings at its input and prints them.\"\"\"\n        combined_str = f\"{input_one} {input_two}\"\n        print(combined_str)\n\n\n    @pipeline\n    def my_pipeline():\n        output_step_one = step_1()\n        step_2(input_one=\"hello\", input_two=output_step_one)\n\n\n    if __name__ == \"__main__\":\n        my_pipeline()\n    ```\n\n    Saving that to a `run.py` file and running it gives us:\n\n{% code title=\"Example Command Output\" %}\n````\n```text\n$ python run.py \nBuilding Docker image(s) for pipeline simple_pipeline.\n"}
{"input": "Building Docker image 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml:simple_pipeline-orchestrator.\n- Including user-defined requirements: boto3==1.26.76\n- Including integration requirements: boto3, kubernetes==18.20.0, s3fs>2022.3.0,<=2023.4.0, sagemaker==2.117.0\nNo .dockerignore found, including all files inside build context.\nStep 1/10 : FROM zenmldocker/zenml:0.39.1-py3.8\nStep 2/10 : WORKDIR /app\nStep 3/10 : COPY .zenml_user_requirements .\nStep 4/10 : RUN pip install --default-timeout=60 --no-cache-dir  -r .zenml_user_requirements\nStep 5/10 : COPY .zenml_integration_requirements .\nStep 6/10 : RUN pip install --default-timeout=60 --no-cache-dir  -r .zenml_integration_requirements\nStep 7/10 : ENV ZENML_ENABLE_REPO_INIT_WARNINGS=False\nStep 8/10 : ENV ZENML_CONFIG_PATH=/app/.zenconfig\nStep 9/10 : COPY . .\nStep 10/10 : RUN chmod -R a+rw .\nAmazon ECR requires you to create a repository before you can push an image to it. ZenML is trying to push the image 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml:simple_pipeline-orchestrator but could only detect the following repositories: []. We will try to push anyway, but in case it fails you need to create a repository named zenml.\nPushing Docker image 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml:simple_pipeline-orchestrator.\nFinished pushing Docker image.\nFinished building Docker image(s).\nRunning pipeline simple_pipeline on stack aws-demo (caching disabled)\nWaiting for Kubernetes orchestrator pod...\nKubernetes orchestrator pod started.\nWaiting for pod of step step_1 to start...\nStep step_1 has started.\nStep step_1 has finished in 0.390s.\nPod of step step_1 completed.\nWaiting for pod of step step_2 to start...\nStep step_2 has started.\nHello World!\nStep step_2 has finished in 2.364s.\nPod of step step_2 completed.\nOrchestration pod completed.\nDashboard URL: https://stefan.develaws.zenml.io/workspaces/default/pipelines/be5adfe9-45af-4709-a8eb-9522c01640ce/runs\n```\n````\n{% endcode %}\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0"}
{"input": "a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Connect your ZenML deployment to a cloud provider and other infrastructure\n  services and resources.\n---\n\n# \ud83d\udd0c Connect services (AWS, GCP, Azure, K8s etc)\n\nA production-grade MLOps platform involves interactions between a diverse combination of third-party libraries and external services sourced from various different vendors. One of the most daunting hurdles in building and operating an MLOps platform composed of multiple components is configuring and maintaining uninterrupted and secured access to the infrastructure resources and services that it consumes.\n\nIn layman's terms, your pipeline code needs to \"connect\" to a handful of different services to run successfully and do what it's designed to do. For example, it might need to connect to a private AWS S3 bucket to read and store artifacts, a Kubernetes cluster to execute steps with Kubeflow or Tekton, and a private GCR container registry to build and store container images. ZenML makes this possible by allowing you to configure authentication information and credentials embedded directly into your Stack Components, but this doesn't scale well when you have more than a few Stack Components and has many other disadvantages related to usability and security.\n\nGaining access to infrastructure resources and services requires knowledge about the different authentication and authorization mechanisms and involves configuring and maintaining valid credentials. It gets even more complicated when these different services need to access each other. For instance, the Kubernetes container running your pipeline step needs access to the S3 bucket to store artifacts or needs to access a cloud service like AWS SageMaker, VertexAI, or AzureML to run a CPU/GPU intensive task like training a model.\n\nThe challenge comes from _setting up and implementing proper authentication and authorization_ with the best security practices in mind, while at the same time _keeping this complexity away from the day-to-day routines_ of coding and running pipelines.\n\nThe hard-to-swallow truth is there is no single standard that unifies all authentication and authorization-related matters or a single, well-defined set of security best practices that you can follow. However, with ZenML you get the next best thing, an abstraction that keeps the complexity of authentication and authorization away from your code and makes it easier to tackle them: _<mark style=\"color:blue;\">the ZenML Service Connectors</mark>_.\n\n<figure><img src=\"../../.gitbook/assets/ConnectorsDiagram.png\" alt=\"\"><figcaption><p>Service Connectors abstract away complexity and implement security best practices</p></figcaption></figure>\n\n## A representative use-case\n\nThe range of features covered by Service Connectors is extensive and going through the entire [Service Connector Guide](service-connectors-guide.md) can be overwhelming. If all you want is to get a quick overview of how Service Connectors work and what they can do for you, this section is for you.\n\nThis is a representative example of how you would use a Service Connector to connect ZenML to a cloud service. This example uses [the AWS Service Connector](aws-service"}
{"input": "-connector.md) to connect ZenML to an AWS S3 bucket and then link [an S3 Artifact Store Stack Component](../../component-guide/artifact-stores/s3.md) to it.\n\nSome details about the current alternatives to using Service Connectors and their drawbacks are provided below. Feel free to skip them if you are already familiar with them or just want to get to the good part.\n\n<details>\n\n<summary>Alternatives to Service Connectors</summary>\n\nThere are quicker alternatives to using a Service Connector to link an S3 Artifact Store to a private AWS S3 bucket. Let's lay them out first and then explain why using a Service Connector is the better option:\n\n1.  the authentication information can be embedded directly into the Stack Component, although this is not recommended for security reasons:\n\n    ```shell\n    zenml artifact-store register s3 --flavor s3 --path=s3://BUCKET_NAME --key=AWS_ACCESS_KEY --secret=AWS_SECRET_KEY\n    ```\n2.  [a ZenML secret](../../getting-started/deploying-zenml/secret-management.md) can hold the AWS credentials and then be referenced in the S3 Artifact Store configuration attributes:\n\n    ```shell\n    zenml secret create aws --aws_access_key_id=AWS_ACCESS_KEY --aws_secret_access_key=AWS_SECRET_KEY\n    zenml artifact-store register s3 --flavor s3 --path=s3://BUCKET_NAME --key='{{aws.aws_access_key_id}}' --secret='{{aws.aws_secret_access_key}}'\n    ```\n3.  an even better version is to reference the secret itself in the S3 Artifact Store configuration:\n\n    ```shell\n    zenml secret create aws --aws_access_key_id=AWS_ACCESS_KEY --aws_secret_access_key=AWS_SECRET_KEY\n    zenml artifact-store register s3 --flavor s3 --path=s3://BUCKET_NAME --authentication_secret=aws\n    ```\n\nAll these options work, but they have many drawbacks:\n\n* first of all, not all Stack Components support referencing secrets in their configuration attributes, so this is not a universal solution.\n* some Stack Components, like those linked to Kubernetes clusters, rely on credentials being set up on the machine where the pipeline is running, which makes pipelines less portable and more difficult to set up. In other cases, you also need to install and set up cloud-specific SDKs and CLIs to be able to use the Stack Component.\n* people configuring and using Stack Components linked to cloud resources need to be given access to cloud credentials, or even provision the credentials themselves, which requires access to the cloud provider platform and knowledge about how to do it.\n* in many cases, you can only configure long-lived credentials directly in Stack Components. This is a security risk because they can inadvertently grant access to key resources and services to a malicious party if they are compromised. Implementing a process that rotates credentials regularly is a complex task that requires"}
{"input": " a lot of effort and maintenance.\n* Stack Components don't implement any kind of verification regarding the validity and permission of configured credentials. If the credentials are invalid or if they lack the proper permissions to access the remote resource or service, you will only find this out later, when running a pipeline will fail at runtime.\n* ultimately, given that different Stack Component flavors rely on the same type of resource or cloud provider, it is not good design to duplicate the logic that handles authentication and authorization in each Stack Component implementation.\n\nThese drawbacks are addressed by Service Connectors.\n\n</details>\n\nWithout Service Connectors, credentials are stored directly in the Stack Component configuration or ZenML Secret and are directly used in the runtime environment. The Stack Component implementation is directly responsible for validating credentials, authenticating and connecting to the infrastructure service. This is illustrated in the following diagram:\n\n![Authentication without Service Connectors](../../.gitbook/assets/authentication\\_without\\_connectors.png)\n\nWhen Service Connectors are involved in the authentication and authorization process, they can act as brokers. The credentials validation and authentication process takes place on the ZenML server. In most cases, the main credentials never have to leave the ZenML server as the Service Connector automatically converts them into short-lived credentials with a reduced set of privileges and issues these credentials to clients. Furthermore, multiple Stack Components of different flavors can use the same Service Connector to access different types or resources with the same credentials:\n\n![Authentication with Service Connectors](../../.gitbook/assets/authentication\\_with\\_connectors.png)\n\nIn working with Service Connectors, the first step is usually _<mark style=\"color:purple;\">finding out what types of resources you can connect ZenML to</mark>_. Maybe you have already planned out the infrastructure options for your MLOps platform and are looking to find out whether ZenML can accommodate them. Or perhaps you want to use a particular Stack Component flavor in your Stack and are wondering whether you can use a Service Connector to connect it to external resources.\n\nListing the available Service Connector Types will give you a good idea of what you can do with Service Connectors:\n\n```sh\nzenml service-connector list-types\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             NAME             \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 AUTH METHODS     \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Kubernetes Service Connector \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 password         \u2502 \u2705    \u2502"}
{"input": " \u2705     \u2503\n\u2503                              \u2502               \u2502                       \u2502 token            \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   Docker Service Connector   \u2502 \ud83d\udc33 docker     \u2502 \ud83d\udc33 docker-registry    \u2502 password         \u2502 \u2705    \u2502 \u2705     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    AWS Service Connector     \u2502 \ud83d\udd36 aws        \u2502 \ud83d\udd36 aws-generic        \u2502 implicit         \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key       \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token        \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 iam-role         \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 session-token    \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 federation-token \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    GCP Service Connector     \u2502 \ud83d\udd35 gcp        \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit         \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account     \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account  \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token     \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 impersonation    \u2502       \u2502        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  HyperAI Service Connector   \u2502 \ud83e\udd16 hyperai    \u2502 \ud83e\udd16 hyperai-instance   \u2502 rsa-key          \u2502 \u2705   \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502                       \u2502 dsa-key          \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 ecdsa-key        \u2502       \u2502        \u2503\n\u2503                              \u2502               \u2502                       \u2502 ed25519-key      \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nService Connector Types are also displayed in the dashboard during the configuration of a new Service Connector:\n\nThe cloud provider of choice for our example is AWS and we're looking to hook up an S3 bucket to an S3 Artifact Store Stack Component. We'll use the AWS Service Connector Type.\n\n<details>\n\n<summary>Interactive structured docs with Service Connector Types</summary>\n\nA lot more is hidden behind a Service Connector Type than a name and a simple list of resource types. Before using a Service Connector Type to configure a Service Connector, you probably need to understand what it is, what it can offer and what are the supported authentication methods and their requirements. All this can be accessed on-site directly through the CLI or in the dashboard. Some examples are included here.\n\nShowing information about the AWS Service Connector Type:\n\n```sh\nzenml service-connector describe-type aws\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                \ud83d\udd36 AWS Service Connector (connector type: aws)                \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 secret-key                                                                \n \u2022 \ud83d\udd12 sts-token                                                                 \n \u2022 \ud83d\udd12 iam-role                                                                  \n \u2022 \ud83d\udd12 session-token                                                             \n \u2022 \ud83d\udd12 federation-token                                                          \n                                                                                \nResource types:                                                                 \n                                                                                \n \u2022 \ud83d\udd36 aws-generic                                                               \n \u2022 \ud83d\udce6 s3-bucket                                                                 \n \u2022 \ud83c\udf00 kubernetes-cluster                                                        \n \u2022 \ud83d\udc33 docker-registry                                                           \n                                                                                \nSupports auto-configuration: True                                               \n                                                                                \nAvailable locally: True                                                         \n                                                                                \nAvailable remotely: True                                                        \n                                                                                \nThe ZenML AWS Service Connector facilitates the authentication and access to    \nmanaged AWS services and resources. These encompass a range of resources,       \nincluding S3 buckets, ECR repositories, and EKS clusters. The connector provides\nsupport for various authentication methods, including explicit long-lived AWS   \nsecret keys, IAM roles, short-lived STS tokens and implicit authentication.     \n                                                                                \nTo ensure heightened security measures, this connector also enables the         \ngeneration of temporary STS security tokens that are scoped down to the minimum \npermissions necessary for accessing the intended resource. Furthermore, it      \nincludes automatic configuration and detection of credentials locally configured\nthrough the AWS CLI.                                                            \n                                                                                \nThis connector serves as a general means of accessing any AWS service by issuing\npre-authenticated boto3"}
{"input": " sessions to clients. Additionally, the connector can    \nhandle specialized authentication for S3, Docker and Kubernetes Python clients. \nIt also allows for the configuration of local Docker and Kubernetes CLIs.       \n                                                                                \nThe AWS Service Connector is part of the AWS ZenML integration. You can either  \ninstall the entire integration or use a pypi extra to install it independently  \nof the integration:                                                             \n                                                                                \n \u2022 pip install \"zenml[connectors-aws]\" installs only prerequisites for the AWS    \n   Service Connector Type                                                       \n \u2022 zenml integration install aws installs the entire AWS ZenML integration      \n                                                                                \nIt is not required to install and set up the AWS CLI on your local machine to   \nuse the AWS Service Connector to link Stack Components to AWS resources and     \nservices. However, it is recommended to do so if you are looking for a quick    \nsetup that includes using the auto-configuration Service Connector features.    \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\nDashboard equivalent:\n\n<img src=\"../../.gitbook/assets/aws-service-connector-type.png\" alt=\"AWS Service Connector Type Details\" data-size=\"original\">\n\nFetching details about the S3 bucket resource type:\n\n```sh\nzenml service-connector describe-type aws --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                 \ud83d\udce6 AWS S3 bucket (resource type: s3-bucket)                  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nAuthentication methods: implicit, secret-key, sts-token, iam-role,              \nsession-token, federation-token                                                 \n                                                                                \nSupports resource instances: True                                               \n                                                                                \nAuthentication methods:                                                         \n                                                                                \n \u2022 \ud83d\udd12 implicit                                                                  \n \u2022 \ud83d\udd12 secret-key                                                                \n \u2022 \ud83d\udd12 sts-token                                                                 \n \u2022 \ud83d\udd12 iam-role                                                                  \n \u2022 \ud83d\udd12 session-token                                                             \n \u2022 \ud83d\udd12 federation-token                                                          \n                                                                                \nAllows users to connect to S3 buckets. When used by Stack Components, they are  \nprovided a pre-configured boto3 S3 client instance.                             \n                                                                                \nThe configured credentials must have at least the following AWS IAM permissions \nassociated with the ARNs of S3 buckets that the connector will be allowed to    \naccess (e.g. arn:aws:s3:::* and arn:aws:s3:::*/* represent all the available S3 \nbuckets).                                                                       \n                                                                                \n \u2022 s3:ListBucket                                                                \n \u2022 s3:GetObject                                                                 \n \u2022 s3:PutObject                                                                 \n \u2022 s3:DeleteObject                                                              "}
{"input": "\n \u2022 s3:ListAllMyBuckets                                                          \n                                                                                \nIf set, the resource name must identify an S3 bucket using one of the following \nformats:                                                                        \n                                                                                \n \u2022 S3 bucket URI (canonical resource name): s3://{bucket-name}                  \n \u2022 S3 bucket ARN: arn:aws:s3:::{bucket-name}                                    \n \u2022 S3 bucket name: {bucket-name}                                                \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\nDashboard equivalent:\n\n<img src=\"../../.gitbook/assets/.gitbook/assets/aws-s3-bucket-resource-type.png\" alt=\"AWS Service Connector Type Details\" data-size=\"original\">\n\nDisplaying information about the AWS Session Token authentication method:\n\n```sh\nzenml service-connector describe-type aws --auth-method session-token\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551              \ud83d\udd12 AWS Session Token (auth method: session-token)               \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                                \nSupports issuing temporary credentials: True                                    \n                                                                                \nGenerates temporary session STS tokens for IAM users. The connector needs to be \nconfigured with an AWS secret key associated with an IAM user or AWS account    \nroot user (not recommended). The connector will generate temporary STS tokens   \nupon request by calling the GetSessionToken STS API.                            \n                                                                                \nThese STS tokens have an expiration period longer that those issued through the \nAWS IAM Role authentication method and are more suitable for long-running       \nprocesses that cannot automatically re-generate credentials upon expiration.    \n                                                                                \nAn AWS region is required and the connector may only be used to access AWS      \nresources in the specified region.                                              \n                                                                                \nThe default expiration period for generated STS tokens is 12 hours with a       \nminimum of 15 minutes and a maximum of 36 hours. Temporary credentials obtained \nby using the AWS account root user credentials (not recommended) have a maximum \nduration of 1 hour.                                                             \n                                                                                \nAs a precaution, when long-lived credentials (i.e. AWS Secret Keys) are detected\non your environment by the Service Connector during auto-configuration, this    \nauthentication method is automatically chosen instead of the AWS Secret Key     \nauthentication method alternative.                                              \n                                                                                \nGenerated STS tokens inherit the full set of permissions of the IAM user or AWS \naccount root user that is calling the GetSessionToken API. Depending on your    \nsecurity needs, this may not be suitable for production use, as it can lead to  \naccidental privilege escalation. Instead, it"}
{"input": " is recommended to use the AWS      \nFederation Token or AWS IAM Role authentication methods to restrict the         \npermissions of the generated STS tokens.                                        \n                                                                                \nFor more information on session tokens and the GetSessionToken AWS API, see: the\nofficial AWS documentation on the subject.                                      \n                                                                                \nAttributes:                                                                     \n                                                                                \n \u2022 aws_access_key_id {string, secret, required}: AWS Access Key ID              \n \u2022 aws_secret_access_key {string, secret, required}: AWS Secret Access Key      \n \u2022 region {string, required}: AWS Region                                        \n \u2022 endpoint_url {string, optional}: AWS Endpoint URL                            \n                                                                                \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n{% endcode %}\n\nDashboard equivalent:\n\n<img src=\"../../.gitbook/assets/.gitbook/assets/aws-session-token-auth-method.png\" alt=\"AWS Service Connector Type Details\" data-size=\"original\">\n\n</details>\n\nNot all Stack Components support being linked to a Service Connector. This is indicated in the flavor description of each Stack Component. Our example uses the S3 Artifact Store, which does support it:\n\n```sh\n$ zenml artifact-store flavor describe s3\nConfiguration class: S3ArtifactStoreConfig\n\n[...]\n\nThis flavor supports connecting to external resources with a Service Connector. It requires a 's3-bucket' resource. You can get a list of all available connectors and the compatible resources that they can \naccess by running:\n\n'zenml service-connector list-resources --resource-type s3-bucket'\nIf no compatible Service Connectors are yet registered, you can register a new one by running:\n\n'zenml service-connector register -i'\n```\n\nThe second step is _<mark style=\"color:purple;\">registering a Service Connector</mark>_ that effectively enables ZenML to authenticate to and access one or more remote resources. This step is best handled by someone with some infrastructure knowledge, but there are sane defaults and auto-detection mechanisms built into most Service Connectors that can make this a walk in the park even for the uninitiated. For our simple example, we're registering an AWS Service Connector with AWS credentials _automatically lifted up from your local host_, giving ZenML access to the same resources that you can access from your local machine through the AWS CLI.\n\nThis step assumes the AWS CLI is already installed and set up with credentials on your machine (e.g. by running `aws configure`).\n\n```sh\nzenml service-connector register aws-s3 --type aws --auto-configure --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u283c Registering service connector 'aws-s3'...\nSuccessfully registered service connector `aws-s3` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES                        \ufffd"}
{"input": "\ufffd\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://aws-ia-mwaa-715803424590         \u2503\n\u2503               \u2502 s3://zenbytes-bucket                  \u2503\n\u2503               \u2502 s3://zenfiles                         \u2503\n\u2503               \u2502 s3://zenml-demos                      \u2503\n\u2503               \u2502 s3://zenml-generative-chat            \u2503\n\u2503               \u2502 s3://zenml-public-datasets            \u2503\n\u2503               \u2502 s3://zenml-public-swagger-spec        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe CLI validates and shows all S3 buckets that can be accessed with the auto-discovered credentials.\n\n{% hint style=\"info\" %}\nThe ZenML CLI provides an interactive way of registering Service Connectors. Just use the `-i` command line argument and follow the interactive guide:\n\n```\nzenml service-connector register -i\n```\n{% endhint %}\n\n<details>\n\n<summary>What happens during auto-configuration</summary>\n\nA quick glance into the Service Connector configuration that was automatically detected gives a better idea of what happened:\n\n```sh\nzenml service-connector describe aws-s3\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-s3' of type 'aws' with id '96a92154-4ec7-4722-bc18-21eeeadb8a4f' is owned by user 'default' and is 'private'.\n          'aws-s3' aws Service Connector Details           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 96a92154-4ec7-4722-bc18-21eeeadb8a4f \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-s3                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 session-token                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 a8c6d0ff-456a-4b25-8557-f0d7e3c12c5f \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 43200s                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-15 18:45:17.822337           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-15 18:45:17.822341           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe AWS Service Connector discovered and lifted the AWS Secret Key that was configured on the local machine and securely stored it in the [Secrets Store](../../getting-started/deploying-zenml/secret-management.md).\n\nMoreover, the following security best practice is automatically enforced by the AWS connector: the AWS Secret Key will be kept hidden on the ZenML Server and the clients will never use it directly to gain access to any AWS resources. Instead, the AWS Service Connector will generate short-lived security tokens and distribute those to clients. It will also take"}
{"input": " care of issuing new tokens when those expire. This is identifiable from the `session-token` authentication method and the session duration configuration attributes.\n\nOne way to confirm this is to ask ZenML to show us the exact configuration that a Service Connector client would see, but this requires us to pick an S3 bucket for which temporary credentials can be generated:\n\n```sh\nzenml service-connector describe aws-s3 --resource-id s3://zenfiles\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'aws-s3 (s3-bucket | s3://zenfiles client)' of type 'aws' with id '96a92154-4ec7-4722-bc18-21eeeadb8a4f' is owned by user 'default' and is 'private'.\n    'aws-s3 (s3-bucket | s3://zenfiles client)' aws Service     \n                       Connector Details                        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 96a92154-4ec7-4722-bc18-21eeeadb8a4f      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-s3 (s3-bucket | s3://zenfiles client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 sts-token                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udce6 s3-bucket                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 s3://zenfiles                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 11h59m56s                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                   \u2503"}
{"input": "\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-15 18:56:33.880081                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-15 18:56:33.880082                \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs can be seen, this configuration is of a temporary STS AWS token that will expire in 12 hours. The AWS Secret Key is not visible on the client side.\n\n</details>\n\nThe next step in this journey is _<mark style=\"color:purple;\">configuring and connecting one (or more) Stack Components to a remote resource</mark>_ via the Service Connector registered in the previous step. This is as easy as saying \"_I want this S3 Artifact Store to use the `s3://my-bucket` S3 bucket_\" and doesn't require any knowledge whatsoever about the authentication mechanisms or even the provenance of those resources. The following example creates an S3 Artifact store and connects it to an S3 bucket with the earlier connector:\n\n```sh\nzenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles\nzenml artifact-store connect s3-zenfiles --connector aws-s3\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles\nSuccessfully registered artifact_store `s3-zenfiles`.\n\n$ zenml artifact-store connect s3-zenfiles --connector aws-s"}
{"input": "3\nSuccessfully connected artifact store `s3-zenfiles` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 96a92154-4ec7-4722-bc18-21eeeadb8a4f \u2502 aws-s3         \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n{% hint style=\"info\" %}\nThe ZenML CLI provides an even easier and more interactive way of connecting a stack component to an external resource. Just pass the `-i` command line argument and follow the interactive guide:\n\n```\nzenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles\nzenml artifact-store connect s3-zenfiles -i\n```\n{% endhint %}\n\nThe S3 Artifact Store Stack Component we just connected to the infrastructure is now ready to be used in a stack to run a pipeline:\n\n```sh\nzenml stack register s3-zenfiles -o default -a s3-zenfiles --set\n```\n\nA simple pipeline could look like this:\n\n```python\nfrom zenml import step, pipeline\n\n@step\ndef simple_step_one() -> str:\n    \"\"\"Simple step one.\"\"\"\n    return \"Hello World!\"\n\n\n@step\ndef simple_step_two(msg: str) -> None:\n    \"\"\"Simple step two.\"\"\"\n    print(msg)\n\n\n@pipeline\ndef simple_pipeline() -> None:\n    \"\"\"Define single step pipeline.\"\"\"\n    message = simple_step_one()\n    simple_step_two(msg=message)\n\n\nif __name__ == \"__main__\":\n    simple_pipeline()\n```\n\nSave this as `run.py` and run it with the following command:\n\n```sh\npython run.py\n```\n\n{% code title=\"Example Command Output\" %}\n```\nRunning pipeline simple_pipeline on stack s3-zenfiles (caching enabled)\nStep simple_step_one has started.\nStep simple_step_one has finished in 1.065s.\nStep simple_step_two has started.\nHello World!\nStep simple_step"}
{"input": "_two has finished in 5.681s.\nPipeline run simple_pipeline-2023_06_15-19_29_42_159831 has finished in 12.522s.\nDashboard URL: http://127.0.0.1:8237/workspaces/default/pipelines/8267b0bc-9cbd-42ac-9b56-4d18275bdbb4/runs\n```\n{% endcode %}\n\nThis example is just a simple demonstration of how to use Service Connectors to connect ZenML Stack Components to your infrastructure. The range of features and possibilities is much larger. ZenML ships with built-in Service Connectors able to connect and authenticate to AWS, GCP, and Azure and offers many different authentication methods and security best practices. Follow the resources below for more information.\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1fa84\">\ud83e\ude84</span> <mark style=\"color:purple;\"><strong>The complete guide to Service Connectors</strong></mark></td><td>Everything you need to know to unlock the power of Service Connectors in your project.</td><td></td><td><a href=\"./service-connectors-guide.md\">./service-connectors-guide.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"2705\">\u2705</span> <mark style=\"color:purple;\"><strong>Security Best Practices</strong></mark></td><td>Best practices concerning the various authentication methods implemented by Service Connectors.</td><td></td><td><a href=\"./best-security-practices.md\">./best-security-practices.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f40b\">\ud83d\udc0b</span> <mark style=\"color:purple;\"><strong>Docker Service Connector</strong></mark></td><td>Use the Docker Service Connector to connect ZenML to a generic Docker container registry.</td><td></td><td><a href=\"./docker-service-connector.md\">./docker-service-connector.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f300\">\ud83c\udf00</span> <mark style=\"color:purple;\"><strong>Kubernetes Service Connector</strong></mark></td><td>Use the Kubernetes Service Connector to connect ZenML to a generic Kubernetes cluster.</td><td></td><td><a href=\"./kubernetes-service-connector.md\">./kubernetes-service-connector.md</a></td></tr><tr><td><"}
{"input": "span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f536\">\ud83d\udd36</span> <mark style=\"color:purple;\"><strong>AWS Service Connector</strong></mark></td><td>Use the AWS Service Connector to connect ZenML to AWS cloud resources.</td><td></td><td><a href=\"./aws-service-connector.md\">./aws-service-connector.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f535\">\ud83d\udd35</span> <mark style=\"color:purple;\"><strong>GCP Service Connector</strong></mark></td><td>Use the GCP Service Connector to connect ZenML to GCP cloud resources.</td><td></td><td><a href=\"./gcp-service-connector.md\">./gcp-service-connector.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag=\"emoji\" data-code=\"1f170\">\ud83c\udd70\ufe0f</span> <mark style=\"color:purple;\"><strong>Azure Service Connector</strong></mark></td><td>Use the Azure Service Connector to connect ZenML to Azure cloud resources.</td><td></td><td><a href=\"./azure-service-connector.md\">./azure-service-connector.md</a></td></tr></tbody></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Best practices concerning the various authentication methods implemented by\n  Service Connectors.\n---\n\n# Security best practices\n\nService Connector Types, especially those targeted at cloud providers, offer a plethora of authentication methods matching those supported by remote cloud platforms. While there is no single authentication standard that unifies this process, there are some patterns that are easily identifiable and can be used as guidelines when deciding which authentication method to use to configure a Service Connector.\n\nThis section explores some of those patterns and gives some advice regarding which authentication methods are best suited for your needs.\n\n{% hint style=\"info\" %}\nThis section may require some general knowledge about authentication and authorization to be properly understood. We tried to keep it simple and limit ourselves to talking about high-level concepts, but some areas may get a bit too technical.\n{% endhint %}\n\n## Username and password\n\n{% hint style=\"danger\" %}\nThe key takeaway is this: you should avoid using your primary account password as authentication credentials as much as possible. If there are alternative authentication methods that you can use or other types of credentials (e.g. session tokens, API keys, API tokens), you should always try to use those instead.\n\nUltimately, if you have no choice, be cognizant of the third parties you share your passwords with. If possible, they should never leave the premises of your local host or development environment.\n{% endhint %}\n\nThis is the typical authentication method that uses a username or account name plus the associated password. While this is the de facto method used to log in with web consoles and local CLIs, this is the least secure of all authentication methods and _never_ something you want to share with other members of your team or organization or use to authenticate automated workloads.\n\nIn fact, cloud platforms don't even allow using user account passwords directly as a credential when authenticating to the cloud platform APIs. There is always a process in place that allows exchanging the account/password credential for [another form of long-lived credential](best-security-practices.md#long-lived-credentials-api-keys-account-keys).\n\nEven when passwords are mentioned as credentials, some services (e.g. DockerHub) also allow using an API access key in place of the user account password.\n\n## Implicit authentication\n\n{% hint style=\"info\" %}\nThe key takeaway here is that implicit authentication gives you immediate access to some cloud resources and requires no configuration, but it may take some extra effort to expand the range of resources that you're initially allowed to access with it. This is not an authentication method you want to use if you're interested in portability and enabling others to reproduce your results.\n{% endhint %}\n\n{% hint style=\"warning\" %}\nThis method may constitute a security risk, because it can give users access to the same cloud resources and services that the ZenML Server itself is configured to access. For this reason, all implicit authentication methods are disabled by default and need to be explicitly enabled by setting the `ZENML_ENABLE"}
{"input": "_IMPLICIT_AUTH_METHODS` environment variable or the helm chart `enableImplicitAuthMethods` configuration option to `true` in the ZenML deployment.\n{% endhint %}\n\nImplicit authentication is just a fancy way of saying that the Service Connector will use locally stored credentials, configuration files, environment variables, and basically any form of authentication available in the environment where it is running, either locally or in the cloud.\n\nMost cloud providers and their associated Service Connector Types include some form of implicit authentication that is able to automatically discover and use the following forms of authentication in the environment where they are running:\n\n* configuration and credentials set up and stored locally through the cloud platform CLI\n* configuration and credentials passed as environment variables\n* some form of implicit authentication attached to the workload environment itself. This is only available in virtual environments that are already running inside the same cloud where other resources are available for use. This is called differently depending on the cloud provider in question, but they are essentially the same thing:\n  * in AWS, if you're running on Amazon EC2, ECS, EKS, Lambda, or some other form of AWS cloud workload, credentials can be loaded directly from _the instance metadata service._ This [uses the IAM role attached to your workload](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) to authenticate to other AWS services without the need to configure explicit credentials.\n  * in GCP, a similar _metadata service_ allows accessing other GCP cloud resources via [the service account attached to the GCP workload](https://cloud.google.com/docs/authentication/application-default-credentials#attached-sa) (e.g. GCP VMs or GKE clusters).\n  * in Azure, the [Azure Managed Identity](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) services can be used to gain access to other Azure services without requiring explicit credentials\n\nThere are a few caveats that you should be aware of when choosing an implicit authentication method. It may seem like the easiest way out, but it carries with it some implications that may impact portability and usability later down the road:\n\n* when used with a local ZenML deployment, like the default deployment, or [a local ZenML server started with `zenml up`](../../user-guide/production-guide/README.md), the implicit authentication method will use the configuration files and credentials or environment variables set up _on your local machine_. These will not be available to anyone else outside your local environment and will also not be accessible to workloads running in other environments on your local host. This includes for example local K3D Kubernetes clusters and local Docker containers.\n* when used with a remote ZenML server, the implicit authentication method only works if your ZenML server is deployed in the same cloud as the one supported by the Service Connector Type that you are using. For instance, if you're"}
{"input": " using the AWS Service Connector Type, then the ZenML server must also be deployed in AWS (e.g. in an EKS Kubernetes cluster). You may also need to manually adjust the cloud configuration of the remote cloud workload where the ZenML server is running to allow access to resources (e.g. add permissions to the AWS IAM role attached to the EC2 or EKS node, add roles to the GCP service account attached to the GKE cluster nodes).\n\n<details>\n\n<summary>GCP implicit authentication method example</summary>\n\nThe following is an example of using the GCP Service Connector's implicit authentication method to gain immediate access to all the GCP resources that the ZenML server also has access to. Note that this is only possible because the ZenML server is also deployed in GCP, in a GKE cluster, and the cluster is attached to a GCP service account with permissions to access the project resources:\n\n```sh\nzenml service-connector register gcp-implicit --type gcp --auth-method implicit --project_id=zenml-core\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nSuccessfully registered service connector `gcp-implicit` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 gs://annotation-gcp-store                       \u2503\n\u2503                       \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                       \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                       \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                       \u2502 gs://zenml-datasets                             \u2503\n\u2503                       \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2503                       \u2502 gs://zenml-project-time-series-bucket           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 gcr.io/zenml-core                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n</details>\n\n### Long-lived credentials (API keys, account keys)\n\n{% hint style=\"success\" %}\nThis is the magic formula of authentication methods. When paired with another ability, such as [automatically generating short-lived API tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials), or [impersonating accounts or assuming roles](best-security-practices.md#impersonating-accounts-and-assuming-roles), this is the ideal authentication mechanism to use, particularly when using ZenML in production and when sharing results with other members of your ZenML team.\n{% endhint %}\n\nAs a general best practice, but implemented particularly well for cloud platforms, account passwords are never directly used as a credential when authenticating to the cloud platform APIs. There is always a process in place that exchanges the account/password credential for another type of long-lived credential:\n\n* AWS uses the [`aws configure` CLI command](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)\n* GCP offers [the `gcloud auth application-default login` CLI commands](https://cloud.google.com/docs/authentication/provide-credentials-adc#how\\_to\\_provide\\_credentials\\_to\\_adc)\n* Azure provides [the `az login` CLI command](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli)\n\nNone of your original login information is stored on your local machine or used to access workloads. Instead, an API key, account key or some other form of intermediate credential is generated and stored on the local host and used to authenticate to remote cloud service APIs.\n\n{% hint style=\"info\" %}\nWhen using auto-configuration with Service Connector registration, this is usually the type of credentials automatically identified and extracted from your local machine.\n{% endhint %}\n\nDifferent cloud providers use different names for these types of long-lived credentials, but they usually represent the same concept, with minor variations regarding the identity information and level of permissions attached to them:\n\n* AWS has [Account Access Keys](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html) and [IAM User Access Keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id\\_credentials\\_access-keys.html)\n* GCP has [User Account Credentials](https://cloud.google.com/docs/authentication#user-accounts) and [Service Account Credentials](https://cloud.google.com/docs/authentication#service-accounts)\n\nGenerally speaking, a differentiation is being made between the following two classes of credentials:\n\n* _user credentials_: credentials representing a human user and usually directly tied to a user account identity. These credentials are usually associated with a broad spectrum of permissions and it is therefore not recommended to share them or make them available outside the confines of your local host.\n* _service credentials:_ credentials used with automated processes and programmatic access"}
{"input": ", where humans are not directly involved. These credentials are not directly tied to a user account identity, but some other form of accounting like a service account or an IAM user devised to be used by non-human actors. It is also usually possible to restrict the range of permissions associated with this class of credentials, which makes them better candidates for sharing them with a larger audience.\n\nZenML cloud provider Service Connectors can use both classes of credentials, but you should aim to use _service credentials_ as often as possible instead of _user credentials_, especially in production environments. Attaching automated workloads like ML pipelines to service accounts instead of user accounts acts as an extra layer of protection for your user identity and facilitates enforcing another security best practice called [_\"the least-privilege principle\"_](https://en.wikipedia.org/wiki/Principle\\_of\\_least\\_privilege)_:_ granting each actor only the minimum level of permissions required to function correctly.\n\nUsing long-lived credentials on their own still isn't ideal, because if leaked, they pose a security risk, even when they have limited permissions attached. The good news is that ZenML Service Connectors include additional mechanisms that, when used in combination with long-lived credentials, make it even safer to share long-lived credentials with other ZenML users and automated workloads:\n\n* automatically [generating temporary credentials](best-security-practices.md#generating-temporary-and-down-scoped-credentials) from long-lived credentials and even downgrading their permission scope to enforce the least-privilege principle\n* implementing [authentication schemes that impersonate accounts and assume roles](best-security-practices.md#impersonating-accounts-and-assuming-roles)\n\n### Generating temporary and down-scoped credentials\n\nMost [authentication methods that utilize long-lived credentials](best-security-practices.md#long-lived-credentials-api-keys-account-keys) also implement additional mechanisms that help reduce the accidental credentials exposure and risk of security incidents even further, making them ideal for production.\n\n_**Issuing temporary credentials**_: this authentication strategy keeps long-lived credentials safely stored on the ZenML server and away from the eyes of actual API clients and people that need to authenticate to the remote resources. Instead, clients are issued API tokens that have a limited lifetime and expire after a given amount of time. The Service Connector is able to generate these API tokens from long-lived credentials on a need-to-have basis. For example, the AWS Service Connector's \"Session Token\", \"Federation Token\" and \"IAM Role\" authentication methods and basically all authentication methods supported by the GCP Service Connector support this feature.\n\n<details>\n\n<summary>AWS temporary credentials example</summary>\n\nThe following example shows the difference between the long-lived AWS credentials configured for an AWS Service Connector and kept on the ZenML server and the temporary Kubernetes API token credentials that the client receives and uses to access the resource.\n\nFirst, showing the long-lived AWS credentials configured for the AWS Service Connector:\n\n```sh\nzenml service-connector describe"}
{"input": " eks-zenhacks-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nService connector 'eks-zenhacks-cluster' of type 'aws' with id 'be53166a-b39c-4e39-8e31-84658e50eec4' is owned by user 'default' and is 'private'.\n   'eks-zenhacks-cluster' aws Service Connector Details    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 be53166a-b39c-4e39-8e31-84658e50eec4 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 eks-zenhacks-cluster                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 session-token                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83c\udf00 kubernetes-cluster                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 zenhacks-cluster                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 fa42ab38-3c93-4765-a4c6-9ce0b548a86c \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 43200s                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-16 10:15:26.393769           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-16 10:15:26.393772           \u2503"}
{"input": "\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThen, showing the temporary credentials that are issued to clients.  Note the expiration time on the Kubernetes API token:\n\n```sh\nzenml service-connector describe eks-zenhacks-cluster --client\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nService connector 'eks-zenhacks-cluster (kubernetes-cluster | zenhacks-cluster client)' of type 'kubernetes' with id 'be53166a-b39c-4e39-8e31-84658e50eec4' is owned by user 'default' and is 'private'.\n 'eks-zenhacks-cluster (kubernetes-cluster | zenhacks-cluster client)' kubernetes Service \n                                    Connector Details                                     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 be53166a-b39c-4e39-8e31-84658e50eec4                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 eks-zenhacks-cluster (kubernetes-cluster | zenhacks-cluster client) \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83c\udf00 kubernetes                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 token                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83c\udf00 kubernetes-cluster                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 11h59m57s                                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-16 10:17:46.931091                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-16 10:17:46.931094                                          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n                                           Configuration                                            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 server                \u2502 https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 insecure              \u2502 False                                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 cluster_name          \u2502 arn:aws:eks:us-east-1:715803424590:cluster/zen"}
{"input": "hacks-cluster              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 token                 \u2502 [HIDDEN]                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 certificate_authority \u2502 [HIDDEN]                                                                 \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n_**Issuing downscoped credentials**_: in addition to the above, some authentication methods also support restricting the generated temporary API tokens to the minimum set of permissions required to access the target resource or set of resources. This is currently available for the AWS Service Connector's \"Federation Token\" and \"IAM Role\" authentication methods.\n\n<details>\n\n<summary>AWS down-scoped credentials example</summary>\n\nIt's not easy to showcase this without using some ZenML Python Client code, but here is an example that proves that the AWS client token issued to an S3 client can only access the S3 bucket resource it was issued for, even if the originating AWS Service Connector is able to access multiple S3 buckets with the corresponding long-lived credentials:\n\n```sh\nzenml service-connector register aws-federation-multi --type aws --auth-method=federation-token --auto-configure \n```\n\n{% code title=\"Example Command Output\" %}\n```text\nSuccessfully registered service connector `aws-federation-multi` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://aws-ia-mwaa-715803424590                \u2503\n\u2503                       \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2503                       \u2502 s3://zenml-public-datasets                   \u2503\n\u2503                       \u2502 s3://zenml-public-swagger-spec               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster"}
{"input": "                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe next part involves running some ZenML Python code to showcase that the downscoped credentials issued to a client are indeed restricted to the S3 bucket that the client asked to access:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\n\n# Get a Service Connector client for a particular S3 bucket\nconnector_client = client.get_service_connector_client(\n    name_id_or_prefix=\"aws-federation-multi\",\n    resource_type=\"s3-bucket\",\n    resource_id=\"s3://zenfiles\"\n)\n\n# Get the S3 boto3 python client pre-configured and pre-authenticated\n# from the Service Connector client\ns3_client = connector_client.connect()\n\n# Verify access to the chosen S3 bucket using the temporary token that\n# was issued to the client.\ns3_client.head_bucket(Bucket=\"zenfiles\")\n\n# Try to access another S3 bucket that the original AWS long-lived credentials can access.\n# An error will be thrown indicating that the bucket is not accessible.\ns3_client.head_bucket(Bucket=\"zenml-demos\")\n```\n\n{% code title=\"Example Output\" %}\n```text\n>>> from zenml.client import Client\n>>> \n>>> client = Client()\nUnable to find ZenML repository in your current working directory (/home/stefan/aspyre/src/zenml) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run zenml init.\nRunning without an active repository root.\n>>> \n>>> # Get a Service Connector client for a particular S3 bucket\n>>> connector_client = client.get_service_connector_client(\n...     name_id_or_prefix=\"aws-federation-multi\",\n...     resource_type=\"s3-bucket\",\n...     resource_id=\"s3://zenfiles\"\n... )\n>>> \n>>> # Get the S3 boto3 python client pre-configured and pre-authenticated\n>>> # from the Service Connector client\n>>> s3_client = connector_client.connect()\n>>> \n>>> # Verify access to the chosen S3 bucket using the temporary token that\n>>> # was issued to the client.\n>>> s3_client.head_bucket(Bucket=\"zenfiles\")\n{'ResponseMetadata': {'RequestId': '62YRYW5XJ1VYPCJ0', 'HostId': 'YNBXcGUMSOh90AsTgPW6/Ra89mqzf"}
{"input": "N/arQq/FMcJzYCK98cFx53+9LLfAKzZaLhwaiJTm+s3mnU=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'YNBXcGUMSOh90AsTgPW6/Ra89mqzfN/arQq/FMcJzYCK98cFx53+9LLfAKzZaLhwaiJTm+s3mnU=', 'x-amz-request-id': '62YRYW5XJ1VYPCJ0', 'date': 'Fri, 16 Jun 2023 11:04:20 GMT', 'x-amz-bucket-region': 'us-east-1', 'x-amz-access-point-alias': 'false', 'content-type': 'application/xml', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n>>> \n>>> # Try to access another S3 bucket that the original AWS long-lived credentials can access.\n>>> # An error will be thrown indicating that the bucket is not accessible.\n>>> s3_client.head_bucket(Bucket=\"zenml-demos\")\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 <stdin>:1 in <module>                                                                            \u2502\n\u2502                                                                                                  \u2502\n\u2502 /home/stefan/aspyre/src/zenml/.venv/lib/python3.8/site-packages/botocore/client.py:508 in        \u2502\n\u2502 _api_call                                                                                        \u2502\n\u2502                                                                                                  \u2502\n\u2502    505 \u2502   \u2502   \u2502   \u2502   \u2502   f\"{py_operation_name}() only accepts keyword arguments.\"              \u2502\n\u2502    506 \u2502   \u2502   \u2502   \u2502   )                                                                         \u2502\n\u2502    507 \u2502   \u2502   \u2502   # The \"self\" in this scope is referring to the BaseClient.                    \u2502\n\u2502 \u2771  508 \u2502   \u2502   \u2502   return self._make_api_call(operation_name, kwargs)                            \u2502\n\u2502    509 \u2502   \u2502                                                                                     \u2502\n\u2502    510 \u2502   \u2502   _api_call.__name__ = str(py_operation_name)                                       \u2502\n\u2502    511                                                                                           \u2502\n\u2502                                                                                                  \u2502\n\u2502 /home/stefan/aspyre/src/zenml/.venv/lib/python3.8/site-packages/botocore/client.py:915 in        \u2502\n\u2502 _make_api_call                                                                                   \u2502\n\u2502                                                                                                  \u2502\n\u2502    912 \u2502   \u2502   if http.status_code >= 300:                                                       \u2502\n\u2502    913 \u2502   \u2502   \u2502   error_code = parsed_response.get(\"Error\", {}).get(\"Code\")                     \u2502\n\u2502    914 \u2502   \u2502   \u2502   error_class = self.exceptions.from_code(error_code)                           \u2502\n\u2502 \ufffd"}
{"input": "\ufffd  915 \u2502   \u2502   \u2502   raise error_class(parsed_response, operation_name)                            \u2502\n\u2502    916 \u2502   \u2502   else:                                                                             \u2502\n\u2502    917 \u2502   \u2502   \u2502   return parsed_response                                                        \u2502\n\u2502    918                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nClientError: An error occurred (403) when calling the HeadBucket operation: Forbidden\n```\n{% endcode %}\n\n</details>\n\n### Impersonating accounts and assuming roles\n\n{% hint style=\"success\" %}\nThese types of authentication methods require more work to set up because multiple permission-bearing accounts and roles need to be provisioned in advance depending on the target audience. On the other hand, they also provide the most flexibility and control. Despite their operational cost, if you are a platform engineer and have the infrastructure know-how necessary to understand and set up the authentication resources, this is for you.\n{% endhint %}\n\nThese authentication methods deliver another way of [configuring long-lived credentials](best-security-practices.md#long-lived-credentials-api-keys-account-keys) in your Service Connectors without exposing them to clients. They are especially useful as an alternative to cloud provider Service Connectors authentication methods that do not support [automatically downscoping the permissions of issued temporary tokens](best-security-practices.md#generating-temporary-and-down-scoped-credentials).\n\nThe processes of account impersonation and role assumption are very similar and can be summarized as follows:\n\n* you configure a Service Connector with long-lived credentials associated with a primary user account or primary service account (preferable). As a best practice, it is common to attach a reduced set of permissions or even no permissions to these credentials other than those that allow the account impersonation or role assumption operation. This makes it more difficult to do any damage if the primary credentials are accidentally leaked.\n* in addition to the primary account and its long-lived credentials, you also need to provision one or more secondary access entities in the cloud platform bearing the effective permissions that will be needed to access the target resource(s):\n  * one or more IAM roles (to be assumed)\n  * one or more service accounts (to be impersonated)\n* the Service Connector configuration also needs to contain the name of a target IAM role to be assumed or a service account to be impersonated.\n* upon request, the Service Connector will exchange the long-lived credentials associated with the primary account for short-lived API tokens that only have the permissions associated with the target IAM role or service account. These temporary credentials are issued to clients and used to access the target resource, while the long-lived credentials are kept safe and never have to leave the ZenML server boundary.\n\n<details>\n\n<summary>GCP account impersonation example</summary>\n\nFor this example, we have the following set up in GCP:\n\n* a primary `empty-connectors@zenml-core.iam.gserviceaccount"}
{"input": ".com` GCP service account with no permissions whatsoever aside from the \"Service Account Token Creator\" role that allows it to impersonate the secondary service account below. We also generate a service account key for this account.\n* a secondary `zenml-bucket-sl@zenml-core.iam.gserviceaccount.com` GCP service account that only has permissions to access the `zenml-bucket-sl` GCS bucket\n\nFirst, let's show that the `empty-connectors` service account has no permissions to access any GCS buckets or any other resources for that matter. We'll register a regular GCP Service Connector that uses the service account key (long-lived credentials) directly:\n\n```sh\nzenml service-connector register gcp-empty-sa --type gcp --auth-method service-account --service_account_json=@empty-connectors@zenml-core.json --project_id=zenml-core\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nExpanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.\nSuccessfully registered service connector `gcp-empty-sa` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd35 gcp-generic     \u2502 zenml-core                                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 gcs-bucket     \u2502 \ud83d\udca5 error: connector authorization failure: failed to list GCS buckets: 403 GET               \u2503\n\u2503                       \u2502 https://storage.googleapis.com/storage/v1/b?project=zenml-core&projection=noAcl&prettyPrint= \u2503\n\u2503                       \u2502 false: empty-connectors@zenml-core.iam.gserviceaccount.com does not have                     \u2503\n\u2503                       \u2502 storage.buckets.list access to the Google Cloud project. Permission 'storage.buckets.list'   \u2503\n\u2503                       \u2502 denied on resource (or it may not exist).                                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 \ud83d\udca5 error: connector authorization failure: Failed to list GKE clusters: 403 Required         \u2503\n\u2503                       \u2502 \"container.clusters.list\" permission(s) for \"projects"}
{"input": "/20219041791\". [request_id:             \u2503\n\u2503                       \u2502 \"0xcb7086235111968a\"                                                                         \u2503\n\u2503                       \u2502 ]                                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 gcr.io/zenml-core                                                                            \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNext, we'll register a GCP Service Connector that actually uses account impersonation to access the `zenml-bucket-sl` GCS bucket and verify that it can actually access the bucket:\n\n```sh\nzenml service-connector register gcp-impersonate-sa --type gcp --auth-method impersonation --service_account_json=@empty-connectors@zenml-core.json  --project_id=zenml-core --target_principal=zenml-bucket-sl@zenml-core.iam.gserviceaccount.com --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nExpanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.\nSuccessfully registered service connector `gcp-impersonate-sa` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### Short-lived credentials\n\n{% hint style=\"info\" %}\nThis category of authentication methods uses temporary credentials explicitly configured in the Service Connector or generated by the Service Connector during auto-configuration. Of all available authentication methods, this is probably the least useful and you will likely never have to use it because it is terribly impractical: when short-lived credentials expire, Service Connectors become unusable and need to either be manually updated or replaced.\n\nOn the other hand, this authentication method is ideal if you're looking to grant someone else in your team temporary access to some resources without exposing your long-lived credentials.\n{% endhint %}\n\nA previous section described how [temporary credentials can be automatically generated from other,"}
{"input": " long-lived credentials](best-security-practices.md#generating-temporary-and-down-scoped-credentials) by most cloud provider Service Connectors. It only stands to reason that temporary credentials can also be generated manually by external means such as cloud provider CLIs and used directly to configure Service Connectors, or automatically generated during Service Connector auto-configuration.\n\nThis may be used as a way to grant an external party temporary access to some resources and have the Service Connector automatically become unusable (i.e. expire) after some time. Your long-lived credentials are kept safe, while the Service Connector only stores a short-lived credential.\n\n<details>\n\n<summary>AWS short-lived credentials auto-configuration example</summary>\n\nThe following is an example of using Service Connector auto-configuration to automatically generate a short-lived token from long-lived credentials configured for the local cloud provider CLI (AWS in this case):\n\n```sh\nAWS_PROFILE=connectors zenml service-connector register aws-sts-token --type aws --auto-configure --auth-method sts-token\n```\n\n{% code title=\"Example Command Output\" %}\n```text\n\u2838 Registering service connector 'aws-sts-token'...\nSuccessfully registered service connector `aws-sts-token` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503    \ud83d\udd36 aws-generic     \u2502 us-east-1                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503     \ud83d\udce6 s3-bucket      \u2502 s3://zenfiles                                \u2503\n\u2503                       \u2502 s3://zenml-demos                             \u2503\n\u2503                       \u2502 s3://zenml-generative-chat                   \u2503\n\u2503                       \u2502 s3://zenml-public-datasets                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector is now configured with a short-lived token that will expire after some time. You can verify this by inspecting the Service Connector:\n\n```sh\nzenml service-"}
{"input": "connector describe aws-sts-token \n```\n\n{% code title=\"Example Command Output\" %}\n```text\nService connector 'aws-sts-token' of type 'aws' with id '63e14350-6719-4255-b3f5-0539c8f7c303' is owned by user 'default' and is 'private'.\n                        'aws-sts-token' aws Service Connector Details                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 e316bcb3-6659-467b-81e5-5ec25bfd36b0                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 aws-sts-token                                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83d\udd36 aws                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 sts-token                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83d\udd36 aws-generic, \ud83d\udce6 s3-bucket, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 971318c9-8db9-4297-967d-80cda070a121                                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 11h58m17s                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                      \u2503\n"}
{"input": "\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-19 17:58:42.999323                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-19 17:58:42.999324                                              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n            Configuration            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 region                \u2502 us-east-1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_access_key_id     \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_secret_access_key \u2502 [HIDDEN]  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aws_session_token     \u2502 [HIDDEN]  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNote the temporary nature of the Service Connector. It will become unusable in 12 hours:\n\n```sh\nzenml service-connector list --name aws-sts-token \n```\n\n{% code title=\"Example Command Output\" %}\n```text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME          \u2502 ID                              \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 aws-sts-token \u2502 e316bcb3-6659-467b-81e5-5ec25bf \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic"}
{"input": "        \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502 11h57m12s  \u2502        \u2503\n\u2503        \u2502               \u2502 d36b0                           \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502               \u2502                                 \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502               \u2502                                 \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Configuring Kubernetes Service Connectors to connect ZenML to Kubernetes clusters.\n---\n\n# Kubernetes Service Connector\n\nThe ZenML Kubernetes service connector facilitates authenticating and connecting to a Kubernetes cluster. The connector can be used to access to any generic Kubernetes cluster by providing pre-authenticated Kubernetes python clients to Stack Components that are linked to it and also allows configuring the local Kubernetes CLI (i.e. `kubectl`).\n\n## Prerequisites\n\nThe Kubernetes Service Connector is part of the Kubernetes ZenML integration. You can either install the entire integration or use a pypi extra to install it independently of the integration:\n\n* `pip install \"zenml[connectors-kubernetes]\"` installs only prerequisites for the Kubernetes Service Connector Type\n* `zenml integration install kubernetes` installs the entire Kubernetes ZenML integration\n\nA local Kubernetes CLI (i.e. `kubectl` ) and setting up local `kubectl` configuration contexts is not required to access Kubernetes clusters in your Stack Components through the Kubernetes Service Connector.\n\n```shell\n$ zenml service-connector list-types --type kubernetes\n```\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             NAME             \u2502 TYPE          \u2502 RESOURCE TYPES        \u2502 AUTH METHODS \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Kubernetes Service Connector \u2502 \ud83c\udf00 kubernetes \u2502 \ud83c\udf00 kubernetes-cluster \u2502 password     \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                              \u2502               \u2502                       \u2502 token        \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n## Resource Types\n\nThe Kubernetes Service Connector only supports authenticating to and granting access to a generic Kubernetes cluster. This type of resource is identified by the `kubernetes-cluster` Resource Type.\n\nThe resource name is a user-friendly cluster name configured during registration.\n\n## Authentication Methods\n\nTwo authentication methods are supported:\n\n1. username and password. This is not recommended for production purposes.\n2. authentication token with or without client certificates.\n\nFor Kubernetes clusters that use neither username and password nor authentication tokens, such as local K3D clusters, the authentication token method can be used with an empty token. \n\n{% hint style=\"warning\" %}\nThis Service Connector does not support generating short-lived credentials from the credentials configured in the"}
{"input": " Service Connector. In effect, this means that the configured credentials will be distributed directly to clients and used to authenticate to the target Kubernetes API. It is recommended therefore to use API tokens accompanied by client certificates if possible.\n{% endhint %}\n\n## Auto-configuration\n\nThe Kubernetes Service Connector allows fetching credentials from the local Kubernetes CLI (i.e. `kubectl`) during registration. The current Kubernetes kubectl configuration context is used for this purpose. The following is an example of lifting Kubernetes credentials granting access to a GKE cluster:\n\n```sh\nzenml service-connector register kube-auto --type kubernetes --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```text\nSuccessfully registered service connector `kube-auto` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 35.185.95.223  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe kube-auto \n```\n\n{% code title=\"Example Command Output\" %}\n```text\nService connector 'kube-auto' of type 'kubernetes' with id '4315e8eb-fcbd-4938-a4d7-a9218ab372a1' is owned by user 'default' and is 'private'.\n     'kube-auto' kubernetes Service Connector Details      \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 4315e8eb-fcbd-4938-a4d7-a9218ab372a1 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 kube-auto                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83c\udf00 kubernetes                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 token                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83c\udf00 kubernetes-cluster                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 35.175.95.223                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\n\u2503 SECRET ID        \u2502 a833e86d-b845-4584-9656-4b041335e299 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-05-16 21:45:33.224740           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-05-16 21:45:33.224743           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n                  Configuration                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY              \u2502 VALUE                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 server                \u2502 https://35.175.95.223 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 insecure              \u2502 False                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 cluster_name          \u2502 35.175.95.223         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 token                 \u2502 [HIDDEN]              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 certificate_authority \u2502 [HIDDEN]              \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n{% hint style=\"info\" %}\nCredentials auto-discovered and lifted through the Kubernetes Service Connector might have a limited lifetime, especially if the target Kubernetes cluster is managed through a 3rd party authentication provider such a GCP or AWS. Using short-lived credentials with your Service Connectors could lead to loss of connectivity and other unexpected errors in your pipeline.\n{% end"}
{"input": "hint %}\n\n## Local client provisioning\n\nThis Service Connector allows configuring the local Kubernetes client (i.e. `kubectl`) with credentials:\n\n```sh\nzenml service-connector login kube-auto \n```\n\n{% code title=\"Example Command Output\" %}\n```text\n\u2826 Attempting to configure local client using service connector 'kube-auto'...\nCluster \"35.185.95.223\" set.\n\u2807 Attempting to configure local client using service connector 'kube-auto'...\n\u280f Attempting to configure local client using service connector 'kube-auto'...\nUpdated local kubeconfig with the cluster details. The current kubectl context was set to '35.185.95.223'.\nThe 'kube-auto' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.\n```\n{% endcode %}\n\n## Stack Components use\n\nThe Kubernetes Service Connector can be used in Orchestrator and Model Deployer stack component flavors that rely on Kubernetes clusters to manage their workloads. This allows Kubernetes container workloads to be managed without the need to configure and maintain explicit Kubernetes `kubectl` configuration contexts and credentials in the target environment and in the Stack Component.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Configuring Azure Service Connectors to connect ZenML to Azure resources such\n  as Blob storage buckets, AKS Kubernetes clusters, and ACR container\n  registries.\n---\n\n# Azure Service Connector\n\nThe ZenML Azure Service Connector facilitates the authentication and access to managed Azure services and resources. These encompass a range of resources, including blob storage containers, ACR repositories, and AKS clusters.\n\nThis connector also supports [automatic configuration and detection of credentials](service-connectors-guide.md#auto-configuration) locally configured through the Azure CLI.\n\nThis connector serves as a general means of accessing any Azure service by issuing credentials to clients. Additionally, the connector can handle specialized authentication for Azure blob storage, Docker and Kubernetes Python clients. It also allows for the configuration of local Docker and Kubernetes CLIs.\n\n```shell\n$ zenml service-connector list-types --type azure\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503          NAME           \u2502 TYPE     \u2502 RESOURCE TYPES        \u2502 AUTH METHODS      \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Azure Service Connector \u2502 \ud83c\udde6 azure \u2502 \ud83c\udde6 azure-generic      \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                         \u2502          \u2502 \ud83d\udce6 blob-container     \u2502 service-principal \u2502       \u2502        \u2503\n\u2503                         \u2502          \u2502 \ud83c\udf00 kubernetes-cluster \u2502 access-token      \u2502       \u2502        \u2503\n\u2503                         \u2502          \u2502 \ud83d\udc33 docker-registry    \u2502                   \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\n## Prerequisites\n\nThe Azure Service Connector is part of the Azure ZenML integration. You can either install the entire integration or use a pypi extra to install it independently of the integration:\n\n* `pip install \"zenml[connectors-azure]\"` installs only prerequisites for the Azure Service Connector Type\n* `zenml integration install azure` installs the entire Azure ZenML integration\n\nIt is not required to [install and set up the Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli) on your local machine to use the Azure Service Connector to link Stack Components to Azure resources and services. However,"}
{"input": " it is recommended to do so if you are looking for a quick setup that includes using the auto-configuration Service Connector features.\n\n{% hint style=\"info\" %}\nThe auto-configuration option is limited to using temporary access tokens that don't work with Azure blob storage resources. To unlock the full power of the Azure Service Connector it is therefore recommended that you [configure and use an Azure service principal and its credentials](https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication-on-premises-apps?tabs=azure-portal).\n{% endhint %}\n\n## Resource Types\n\n### Generic Azure resource\n\nThis resource type allows Stack Components to use the Azure Service Connector to connect to any Azure service or resource. When used by Stack Components, they are provided generic azure-identity credentials that can be used to create Azure python clients for any particular Azure service.\n\nThis generic Azure resource type is meant to be used with Stack Components that are not represented by other, more specific resource type, like Azure blob storage containers, Kubernetes clusters or Docker registries. It should be accompanied by a matching set of Azure permissions that allow access to the set of remote resources required by the Stack Components.\n\nThe resource name represents the name of the Azure subscription that the connector is authorized to access.\n\n### Azure blob storage container\n\nAllows users to connect to Azure Blob containers. When used by Stack Components, they are provided a pre-configured Azure Blob Storage client.\n\nThe configured credentials must have at least the following Azure IAM permissions associated with the blob storage account or containers that the connector that the connector will be allowed to access:\n\n* allow read and write access to blobs (e.g. the `Storage Blob Data Contributor` role)\n* allow listing the storage accounts (e.g. the `Reader and Data Access` role). This is only required if a storage account is not configured in the connector.\n* allow listing the containers in a storage account (e.g. the `Reader and Data Access` role)\n\nIf set, the resource name must identify an Azure blob storage container using one of the following formats:\n\n* Azure blob container URI (canonical resource name): `{az|abfs}://{container-name}`\n* Azure blob container name: `{container-name}`\n\nIf a storage account is configured in the connector, only blob storage containers in that storage account will be accessible. Otherwise, if a resource group is configured in the connector, only blob storage containers in storage accounts in that resource group will be accessible. Finally, if neither a storage account nor a resource group is configured in the connector, all blob storage containers in all accessible storage accounts will be accessible.\n\n{% hint style=\"warning\" %}\nThe only Azure authentication method that works with Azure blob storage resources is the service principal authentication method.\n{% endhint %}\n\n### AKS Kubernetes cluster\n\nAllows Stack Components to access an AKS cluster as a standard Kubernetes cluster resource. When used by Stack Components, they are provided a pre-authenticated python-kubernetes client instance.\n\nThe"}
{"input": " configured credentials must have at least the following Azure IAM permissions associated with the AKS clusters that the connector will be allowed to access:\n\n* allow listing the AKS clusters and fetching their credentials (e.g. the `Azure Kubernetes Service Cluster Admin Role` role)\n\nIf set, the resource name must identify an EKS cluster using one of the following formats:\n\n* resource group scoped AKS cluster name (canonical): `[{resource-group}/]{cluster-name}`\n* AKS cluster name: `{cluster-name}`\n\nGiven that the AKS cluster name is unique within a resource group, the resource group name may be included in the resource name to avoid ambiguity. If a resource group is configured in the connector, the resource group name in the resource name must match the configured resource group. If no resource group is configured in the connector and a resource group name is not included in the resource name, the connector will attempt to find the AKS cluster in any resource group.\n\nIf a resource group is configured in the connector, only AKS clusters in that resource group will be accessible.\n\n### ACR container registry\n\nAllows Stack Components to access one or more ACR registries as a standard Docker registry resource. When used by Stack Components, they are provided a pre-authenticated python-docker client instance.\n\nThe configured credentials must have at least the following Azure IAM permissions associated with the ACR registries that the connector will be allowed to access:\n\n* allow access to pull and push images (e.g. the `AcrPull` and `AcrPush` roles)\n* allow access to list registries (e.g. the `Contributor` role)\n\nIf set, the resource name must identify an ACR registry using one of the following formats:\n\n* ACR registry URI (canonical resource name): `[https://]{registry-name}.azurecr.io`\n* ACR registry name: `{registry-name}`\n\nIf a resource group is configured in the connector, only ACR registries in that resource group will be accessible.\n\nIf an authentication method other than the Azure service principal is used for authentication, the admin account must be enabled for the registry, otherwise, clients will not be able to authenticate to the registry. See the official Azure [documentation on the admin account](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication#admin-account) for more information.\n\n## Authentication Methods\n\n### Implicit authentication\n\n[Implicit authentication](best-security-practices.md#implicit-authentication) to Azure services using environment variables, local configuration files, workload or managed identities.\n\n{% hint style=\"warning\" %}\nThis method may constitute a security risk, because it can give users access to the same cloud resources and services that the ZenML Server itself is configured to access. For this reason, all implicit authentication methods are disabled by default and need to be explicitly enabled by setting the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable or the helm chart `enableImplicitAuthMethods`"}
{"input": " configuration option to `true` in the ZenML deployment.\n{% endhint %}\n\nThis authentication method doesn't require any credentials to be explicitly configured. It automatically discovers and uses credentials from one of the following sources:\n\n* [environment variables](https://learn.microsoft.com/en-us/python/api/overview/azure/identity-readme?view=azure-python#environment-variables)\n* workload identity - if the application is deployed to an Azure Kubernetes Service with Managed Identity enabled. This option can only be used when running the ZenML server on an AKS cluster.\n* managed identity - if the application is deployed to an Azure host with Managed Identity enabled. This option can only be used when running the ZenML client or server on an Azure host.\n* Azure CLI - if a user has signed in via [the Azure CLI `az login` command](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli).\n\nThis is the quickest and easiest way to authenticate to Azure services. However, the results depend on how ZenML is deployed and the environment where it is used and is thus not fully reproducible:\n\n* when used with the default local ZenML deployment or a local ZenML server, the credentials are the same as those used by the Azure CLI or extracted from local environment variables.\n* when connected to a ZenML server, this method only works if the ZenML server is deployed in Azure and will use the workload identity attached to the Azure resource where the ZenML server is running (e.g. an AKS cluster). The permissions of the managed identity may need to be adjusted to allows listing and accessing/describing the Azure resources that the connector is configured to access.\n\nNote that the discovered credentials inherit the full set of permissions of the local Azure CLI configuration, environment variables or remote Azure managed identity. Depending on the extent of those permissions, this authentication method might not be recommended for production use, as it can lead to accidental privilege escalation. Instead, it is recommended to use the Azure service principal authentication method to limit the validity and/or permissions of the credentials being issued to connector clients.\n\n<details>\n\n<summary>Example configuration</summary>\n\nThe following assumes the local Azure CLI has already been configured with user account credentials by running the `az login` command:\n\n```sh\nzenml service-connector register azure-implicit --type azure --auth-method implicit --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2819 Registering service connector 'azure-implicit'...\nSuccessfully registered service connector `azure-implicit` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n"}
{"input": "\u2503   \ud83c\udde6 azure-generic    \u2502 ZenML Subscription                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udce6 blob-container   \u2502 az://demo-zenmlartifactstore                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 demozenmlcontainerregistry.azurecr.io         \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNo credentials are stored with the Service Connector:\n\n```sh\nzenml service-connector describe azure-implicit\n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'azure-implicit' of type 'azure' with id 'ad645002-0cd4-4d4f-ae20-499ce888a00a' is owned by user 'default' and is 'private'.\n                          'azure-implicit' azure Service Connector Details                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 ad645002-0cd4-4d4f-ae20-499ce888a00a                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 azure-implicit                                                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83c\udde6  azure                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 implicit                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83c\udde6  azure-generic, \ud83d\udce6 blob-container, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502                                                                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-05 09:47:42.415949                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-05 09:47:42.415954                                                     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### Azure Service Principal\n\nAzure service principal credentials consists of an Azure client ID and client secret. These credentials are used to authenticate clients to Azure services.\n\nFor this authentication method, the Azure Service Connector requires [an Azure service principal to be created](https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication-on-premises-apps?tabs=azure-portal) and a client secret to be generated.\n\n<details>\n\n<summary>Example configuration</summary>\n\nThe following assumes an Azure service principal was configured with a client secret and has permissions to access an Azure blob storage container, an AKS Kubernetes cluster and an ACR container registry. The service principal client ID, tenant ID and client secret are then used to configure the Azure Service Connector.\n\n```sh\nzenml service-connector register azure-service-principal --type azure --auth-method service-principal --tenant_id=a79f3633-8f45-4a74-a42e-68871c17b7fb --client_id=8926254a-8c3f-430a-a2fd-bdab234d491e --client_secret=AzureSuperSecret\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2819 Registering service connector 'azure-service-principal"}
{"input": "'...\nSuccessfully registered service connector `azure-service-principal` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83c\udde6 azure-generic    \u2502 ZenML Subscription                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udce6 blob-container   \u2502 az://demo-zenmlartifactstore                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 demozenmlcontainerregistry.azurecr.io         \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe Service Connector configuration shows that the connector is configured with service principal credentials:\n\n```sh\nzenml service-connector describe azure-service-principal\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 273d2812-2643-4446-82e6-6098b8ccdaa4                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 azure-service-principal                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83c\udde6  azure                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 service-principal                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83c\udde6  azure-generic, \ud83d\udce6 blob-container, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33"}
{"input": " docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SECRET ID        \u2502 50d9f230-c4ea-400e-b2d7-6b52ba2a6f90                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 N/A                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-20 19:16:26.802374                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-20 19:16:26.802378                                                     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n                     Configuration                      \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY      \u2502 VALUE                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 tenant_id     \u2502 a79ff333-8f45-4a74-a42e-68871c17b7fb \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 client_id     \u2502 8926254a-8c3f-430a-a2fd-bdab234d491e \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 client_secret \u2502 [HIDDEN]                             \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n### Azure Access Token\n\nUses [temporary Azure access tokens](best-security-practices.md#short-lived-credentials) explicitly configured by the user or auto-configured from a local environment.\n\nThis method has the major limitation that the user must regularly generate new tokens and update the connector configuration as API tokens expire. On the other hand, this method is ideal in cases where the connector only needs to be used for a short period of time, such as sharing access temporarily with someone else in your team.\n\nThis is the authentication method used during auto-configuration, if you have [the local Azure CLI set up with credentials](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli). The connector will generate an access token from the Azure CLI credentials and store it in the connector configuration.\n\n{% hint style=\"warning\" %}\nGiven that Azure access tokens are scoped to a particular Azure resource and the access token generated during auto-configuration is scoped to the Azure Management API, this method does not work with Azure blob storage resources. You should use [the Azure service principal authentication method](azure-service-connector.md#azure-service-principal) for blob storage resources instead.\n{% endhint %}\n\n<details>\n\n<summary>Example auto-configuration</summary>\n\nFetching Azure session tokens from the local Azure CLI is possible if the Azure CLI is already configured with valid credentials (i.e. by running `az login`):\n\n```sh\nzenml service-connector register azure-session-token --type azure --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2819 Registering service connector 'azure-session-token'...\nconnector authorization failure: the 'access-token' authentication method is not supported for blob storage resources\nSuccessfully registered service connector `azure-session-token` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                                                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83c\udde6 azure-generic    \u2502 ZenML Subscription                                                                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udce6 blob-container   \u2502 \ud83d\udca5 error: connector authorization failure: the 'access-token' authentication method is not supported for blob storage resources \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster                                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 demozenmlcontainerregistry.azurecr.io                                                                                           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector describe azure-session-token \n```\n\n{% code title=\"Example Command Output\" %}\n```\nService connector 'azure-session-token' of type 'azure' with id '94d64103-9902-4aa5-8ce4-877061af89af' is owned by user 'default' and is 'private'.\n                        'azure-session-token' azure Service Connector Details                        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY         \u2502 VALUE                                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ID               \u2502 94d64103-9902-4aa5-8ce4-877061af89af                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME             \u2502 azure-session-token                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE             \u2502 \ud83c\udde6 azure                                                                       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AUTH METHOD      \u2502 access-token                                                                   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE TYPES   \u2502 \ud83c\udde6 azure-generic, \ud83d\udce6 blob-container, \ud83c\udf00 kubernetes-cluster, \ud83d\udc33 docker-registry \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RESOURCE NAME    \u2502 <multiple>                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n"}
{"input": "\u2503 SECRET ID        \u2502 b34f2e95-ae16-43b6-8ab6-f0ee33dbcbd8                                           \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SESSION DURATION \u2502 N/A                                                                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 EXPIRES IN       \u2502 42m25s                                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 OWNER            \u2502 default                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 WORKSPACE        \u2502 default                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SHARED           \u2502 \u2796                                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT       \u2502 2023-06-05 10:03:32.646351                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT       \u2502 2023-06-05 10:03:32.646352                                                     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n     Configuration     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 PROPERTY \u2502 VALUE    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 token    \u2502 [HIDDEN] \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nNote the temporary nature of the Service Connector. It will expire and become unusable in approximately 1 hour:\n\n```sh\nzenml service-connector list --name azure-session-token \n```\n\n{% code title=\"Example Command Output\" %}\n```\nCould not import GCP service connector: No module named 'google.api_core'.\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME                \u2502 ID                                   \u2502 TYPE     \u2502"}
{"input": " RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 azure-session-token \u2502 94d64103-9902-4aa5-8ce4-877061af89af \u2502 \ud83c\udde6 azure \u2502 \ud83c\udde6 azure-generic      \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502 40m58s     \u2502        \u2503\n\u2503        \u2502                     \u2502                                      \u2502          \u2502 \ud83d\udce6 blob-container     \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                     \u2502                                      \u2502          \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                     \u2502                                      \u2502          \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n</details>\n\n## Auto-configuration\n\nThe Azure Service Connector allows [auto-discovering and fetching credentials](service-connectors-guide.md#auto-configuration) and [configuration set up by the Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli) on your local host.\n\n{% hint style=\"warning\" %}\nThe Azure service connector auto-configuration comes with two limitations:\n\n1. it can only pick up temporary Azure access tokens and therefore cannot be used for long-term authentication scenarios\n2. it doesn't support authenticating to the Azure blob storage service. [The Azure service principal authentication method](azure-service-connector.md#azure-service-principal) can be used instead.\n{% endhint %}\n\nFor an auto-configuration example, please refer to the [section about Azure access tokens](azure-service-connector.md#azure-access-token).\n\n## Local client provisioning\n\nThe local Azure CLI, Kubernetes `kubectl` CLI and the Docker CLI can be [configured with credentials extracted from or generated by a compatible Azure Service Connector](service-connectors-guide.md#configure-local-clients).\n\n{% hint style=\"info\" %}\nNote that the Azure local CLI can only be configured with credentials issued by the Azure Service Connector if the connector is configured with the [service principal authentication method](azure-service-connector.md#"}
{"input": "azure-service-principal).\n{% endhint %}\n\n<details>\n\n<summary>Local CLI configuration examples</summary>\n\nThe following shows an example of configuring the local Kubernetes CLI to access an AKS cluster reachable through an Azure Service Connector:\n\n```sh\nzenml service-connector list --name azure-service-principal\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 NAME                    \u2502 ID                                   \u2502 TYPE     \u2502 RESOURCE TYPES        \u2502 RESOURCE NAME \u2502 SHARED \u2502 OWNER   \u2502 EXPIRES IN \u2502 LABELS \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 azure-service-principal \u2502 3df920bc-120c-488a-b7fc-0e79bc8b021a \u2502 \ud83c\udde6 azure \u2502 \ud83c\udde6 azure-generic      \u2502 <multiple>    \u2502 \u2796     \u2502 default \u2502            \u2502        \u2503\n\u2503        \u2502                         \u2502                                      \u2502          \u2502 \ud83d\udce6 blob-container     \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                         \u2502                                      \u2502          \u2502 \ud83c\udf00 kubernetes-cluster \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2503        \u2502                         \u2502                                      \u2502          \u2502 \ud83d\udc33 docker-registry    \u2502               \u2502        \u2502         \u2502            \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe verify CLI command can be used to list all Kubernetes clusters accessible through the Azure Service Connector:\n\n```sh\nzenml service-connector verify azure-service-principal --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2819 Verifying service connector 'azure-service-principal'...\nService connector 'azure-service-principal' is"}
{"input": " correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nThe login CLI command can be used to configure the local Kubernetes CLI to access a Kubernetes cluster reachable through an Azure Service Connector:\n\n```sh\nzenml service-connector login azure-service-principal --resource-type kubernetes-cluster --resource-id demo-zenml-demos/demo-zenml-terraform-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2819 Attempting to configure local client using service connector 'azure-service-principal'...\nUpdated local kubeconfig with the cluster details. The current kubectl context was set to 'demo-zenml-terraform-cluster'.\nThe 'azure-service-principal' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.\n```\n{% endcode %}\n\nThe local Kubernetes CLI can now be used to interact with the Kubernetes cluster:\n\n```sh\nkubectl cluster-info\n```\n\n{% code title=\"Example Command Output\" %}\n```\nKubernetes control plane is running at https://demo-43c5776f7.hcp.westeurope.azmk8s.io:443\nCoreDNS is running at https://demo-43c5776f7.hcp.westeurope.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://demo-43c5776f7.hcp.westeurope.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n```\n{% endcode %}\n\nA similar process is possible with ACR container registries:\n\n```sh\nzenml service-connector verify azure-service-principal --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2826 Verifying service connector 'azure-service-principal'...\nService connector 'azure-service-principal' is correctly configured with valid credentials and has access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503  "}
{"input": " RESOURCE TYPE    \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 demozenmlcontainerregistry.azurecr.io \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n```sh\nzenml service-connector login azure-service-principal --resource-type docker-registry --resource-id demozenmlcontainerregistry.azurecr.io\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u2839 Attempting to configure local client using service connector 'azure-service-principal'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'azure-service-principal' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n\nThe local Docker CLI can now be used to interact with the container registry:\n\n```sh\ndocker push demozenmlcontainerregistry.azurecr.io/zenml:example_pipeline\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe push refers to repository [demozenmlcontainerregistry.azurecr.io/zenml]\nd4aef4f5ed86: Pushed \n2d69a4ce1784: Pushed \n204066eca765: Pushed \n2da74ab7b0c1: Pushed \n75c35abda1d1: Layer already exists \n415ff8f0f676: Layer already exists \nc14cb5b1ec91: Layer already exists \na1d005f5264e: Layer already exists \n3a3fd880aca3: Layer already exists \n149a9c50e18e: Layer already exists \n1f6d3424b922: Layer already exists \n8402c959ae6f: Layer already exists \n419599cb5288: Layer already exists \n8553b91047da: Layer already exists \nconnectors: digest: sha256:a4cfb18a5cef5b2201759a42dd9fe8eb2f833b788e9d8a6ebde194765b42fe46 size: 3256\n```\n{% endcode %}\n\nIt is also possible to update the local Azure CLI configuration with credentials extracted from the Azure Service Connector:\n\n```sh\nzenml service-connector login azure-service-principal --resource-type azure-generic\n```\n\n{% code title=\"Example Command Output\" %}\n```\nUpdated the local Azure CLI configuration with the connector's service principal credentials.\nThe 'azure-service-principal' Azure"}
{"input": " Service Connector connector was used to successfully configure the local Generic Azure resource client/SDK.\n```\n{% endcode %}\n\n</details>\n\n## Stack Components use\n\nThe [Azure Artifact Store Stack Component](../../component-guide/artifact-stores/azure.md) can be connected to a remote Azure blob storage container through an Azure Service Connector.\n\nThe Azure Service Connector can also be used with any Orchestrator or Model Deployer stack component flavor that relies on a Kubernetes clusters to manage workloads. This allows AKS Kubernetes container workloads to be managed without the need to configure and maintain explicit Azure or Kubernetes `kubectl` configuration contexts and credentials in the target environment or in the Stack Component itself.\n\nSimilarly, Container Registry Stack Components can be connected to a ACR Container Registry through an Azure Service Connector. This allows container images to be built and published to private ACR container registries without the need to configure explicit Azure credentials in the target environment or the Stack Component.\n\n## End-to-end examples\n\n<details>\n\n<summary>AKS Kubernetes Orchestrator, Azure Blob Storage Artifact Store and ACR Container Registry with a multi-type Azure Service Connector</summary>\n\nThis is an example of an end-to-end workflow involving Service Connectors that uses a single multi-type Azure Service Connector to give access to multiple resources for multiple Stack Components. A complete ZenML Stack is registered composed of the following Stack Components, all connected through the same Service Connector:\n\n* a [Kubernetes Orchestrator](../../component-guide/orchestrators/kubernetes.md) connected to an AKS Kubernetes cluster\n* a [Azure Blob Storage Artifact Store](../../component-guide/artifact-stores/azure.md) connected to an Azure blob storage container\n* an [Azure Container Registry](../../component-guide/container-registries/azure.md) connected to an ACR container registry\n* a local [Image Builder](../../component-guide/image-builders/local.md)\n\nAs a last step, a simple pipeline is run on the resulting Stack.\n\nThis example needs to use a remote ZenML Server that is reachable from Azure.\n\n1.  Configure an Azure service principal with a client secret and give it permissions to access an Azure blob storage container, an AKS Kubernetes cluster and an ACR container registry. Also make sure you have the Azure ZenML integration installed:\n\n    ```sh\n    zenml integration install -y azure\n    ```\n2.  Make sure the Azure Service Connector Type is available\n\n    ```sh\n    zenml service-connector list-types --type azure\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503          NAME           \u2502 TYPE     \u2502 RESOURCE TYPES        \u2502 AUTH"}
{"input": " METHODS      \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Azure Service Connector \u2502 \ud83c\udde6 azure \u2502 \ud83c\udde6 azure-generic      \u2502 implicit          \u2502 \u2705    \u2502 \u2705     \u2503\n\u2503                         \u2502          \u2502 \ud83d\udce6 blob-container     \u2502 service-principal \u2502       \u2502        \u2503\n\u2503                         \u2502          \u2502 \ud83c\udf00 kubernetes-cluster \u2502 access-token      \u2502       \u2502        \u2503\n\u2503                         \u2502          \u2502 \ud83d\udc33 docker-registry    \u2502                   \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n3.  Register a multi-type Azure Service Connector using the Azure service principal credentials set up at the first step. Note the resources that it has access to:\n\n    ```sh\n    zenml service-connector register azure-service-principal --type azure --auth-method service-principal --tenant_id=a79ff3633-8f45-4a74-a42e-68871c17b7fb --client_id=8926254a-8c3f-430a-a2fd-bdab234fd491e --client_secret=AzureSuperSecret\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```\n\u2838 Registering service connector 'azure-service-principal'...\nSuccessfully registered service connector `azure-service-principal` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83c\udde6 azure-generic    \u2502 ZenML Subscription                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udce6 blob-container   \u2502 az://demo-zenmlartifactstore                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503  \ud83d\udc33 docker-registry   \u2502 demozenmlcontainerregistry.azurecr.io         \u2503\n"}
{"input": "\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n4.  register and connect an Azure Blob Storage Artifact Store Stack Component to an Azure blob container:\n\n    ```sh\n    zenml artifact-store register azure-demo --flavor azure --path=az://demo-zenmlartifactstore\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```\nSuccessfully registered artifact_store `azure-demo`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml artifact-store connect azure-demo --connector azure-service-principal\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```\nSuccessfully connected artifact store `azure-demo` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME          \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE     \u2502 RESOURCE NAMES               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 f2316191-d20b-4348-a68b-f5e347862196 \u2502 azure-service-principal \u2502 \ud83c\udde6 azure       \u2502 \ud83d\udce6 blob-container \u2502 az://demo-zenmlartifactstore \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n5.  register and connect a Kubernetes Orchestrator Stack Component to an AKS cluster:\n\n    ```sh\n    zenml orchestrator register aks-demo-cluster --flavor kubernetes --synchronous=true --kubernetes_namespace=zenml-workloads\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```\nSuccessfully registered orchestrator `aks-demo-cluster`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml orchestrator connect aks-demo-cluster --connector azure-service-principal\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```\nSuccessfully connected orchestrator `aks-demo-cluster` to the following resources:\n\u250f"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME          \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 f2316191-d20b-4348-a68b-f5e347862196 \u2502 azure-service-principal \u2502 \ud83c\udde6 azure       \u2502 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n6.  Register and connect an Azure Container Registry Stack Component to an ACR container registry:\n\n    ```sh\n    zenml container-registry register acr-demo-registry --flavor azure --uri=demozenmlcontainerregistry.azurecr.io\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```\nSuccessfully registered container_registry `acr-demo-registry`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml container-registry connect acr-demo-registry --connector azure-service-principal\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```\nSuccessfully connected container registry `acr-demo-registry` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME          \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 f2316191-d20"}
{"input": "b-4348-a68b-f5e347862196 \u2502 azure-service-principal \u2502 \ud83c\udde6 azure       \u2502 \ud83d\udc33 docker-registry \u2502 demozenmlcontainerregistry.azurecr.io \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n````\n{% endcode %}\n\n7.  Combine all Stack Components together into a Stack and set it as active (also throw in a local Image Builder for completion):\n\n    ```sh\n    zenml image-builder register local --flavor local\n    ```\n\n{% code title=\"Example Command Output\" %}\n````\n```\nRunning with active workspace: 'default' (global)\nRunning with active stack: 'default' (global)\nSuccessfully registered image_builder `local`.\n```\n````\n{% endcode %}\n\n````\n```sh\nzenml stack register gcp-demo -a azure-demo -o aks-demo-cluster -c acr-demo-registry -i local --set\n```\n\n````\n\n{% code title=\"Example Command Output\" %}\n````\n```\nStack 'gcp-demo' successfully registered!\nActive repository stack set to:'gcp-demo'\n```\n````\n{% endcode %}\n\n8.  Finally, run a simple pipeline to prove that everything works as expected. We'll use the simplest pipelines possible for this example:\n\n    ```python\n    from zenml import pipeline, step\n\n\n    @step\n    def step_1() -> str:\n        \"\"\"Returns the `world` string.\"\"\"\n        return \"world\"\n\n\n    @step(enable_cache=False)\n    def step_2(input_one: str, input_two: str) -> None:\n        \"\"\"Combines the two strings at its input and prints them.\"\"\"\n        combined_str = f\"{input_one} {input_two}\"\n        print(combined_str)\n\n\n    @pipeline\n    def my_pipeline():\n        output_step_one = step_1()\n        step_2(input_one=\"hello\", input_two=output_step_one)\n\n\n    if __name__ == \"__main__\":\n        my_pipeline()\n    ```\n\n    Saving that to a `run.py` file and running it gives us:\n\n{% code title=\"Example Command Output\" %}\n````\n```\n$ python run.py\nBuilding Docker image(s) for pipeline simple_pipeline.\nBuilding Docker image demozenmlcontainerregistry.azurecr.io/zenml:simple_pipeline-orchestrator.\n- Including integration requirements: adlfs==2021.10.0, azure-identity==1.10.0, azure-keyvault-keys, azure-keyvault-secrets, azure-mgmt-containerservice>=20."}
{"input": "0.0, azureml-core==1.48.0, kubernetes, kubernetes==18.20.0\nNo .dockerignore found, including all files inside build context.\nStep 1/10 : FROM zenmldocker/zenml:0.40.0-py3.8\nStep 2/10 : WORKDIR /app\nStep 3/10 : COPY .zenml_user_requirements .\nStep 4/10 : RUN pip install --default-timeout=60 --no-cache-dir  -r .zenml_user_requirements\nStep 5/10 : COPY .zenml_integration_requirements .\nStep 6/10 : RUN pip install --default-timeout=60 --no-cache-dir  -r .zenml_integration_requirements\nStep 7/10 : ENV ZENML_ENABLE_REPO_INIT_WARNINGS=False\nStep 8/10 : ENV ZENML_CONFIG_PATH=/app/.zenconfig\nStep 9/10 : COPY . .\nStep 10/10 : RUN chmod -R a+rw .\nPushing Docker image demozenmlcontainerregistry.azurecr.io/zenml:simple_pipeline-orchestrator.\nFinished pushing Docker image.\nFinished building Docker image(s).\nRunning pipeline simple_pipeline on stack gcp-demo (caching disabled)\nWaiting for Kubernetes orchestrator pod...\nKubernetes orchestrator pod started.\nWaiting for pod of step simple_step_one to start...\nStep simple_step_one has started.\nINFO:azure.identity._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nINFO:azure.identity._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nINFO:azure.identity._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nINFO:azure.identity.aio._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nStep simple_step_one has finished in 0.396s.\nPod of step simple_step_one completed.\nWaiting for pod of step simple_step_two to start...\nStep simple_step_two has started.\nINFO:azure.identity._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nINFO:azure.identity._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nINFO:azure.identity.aio._internal.get_token_mixin:ClientSecretCredential.get_token succeeded\nHello World!\nStep simple_step_two has finished in 3.203s.\nPod of step simple_step_two completed.\nOrchestration pod completed.\nDashboard URL: https://zenml.stefan.20.23.46.143.nip.io/workspaces/default/pipelines/98c41e2a-1ab0-4ec9-8375-6ea1ab473686/runs\n```\n````\n{% endcode %}\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee"}
{"input": "424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Ensuring your pipelines or steps run on GPU-backed hardware.\n---\n\n# Specify cloud resources\n\nThere are several reasons why you may want to scale your machine learning pipelines to the cloud, such as utilizing more powerful hardware or distributing tasks across multiple nodes. In order to achieve this with ZenML you'll need to run your steps on GPU-backed hardware using `ResourceSettings` to allocate greater resources on an orchestrator node and/or make some adjustments to the container environment.\n\n## Specify resource requirements for steps\n\nSome steps of your machine learning pipeline might be more resource-intensive and require special hardware to execute. In such cases, you can specify the required resources for steps as follows:\n\n```python\nfrom zenml.config import ResourceSettings\nfrom zenml import step\n\n@step(settings={\"resources\": ResourceSettings(cpu_count=8, gpu_count=2, memory=\"8GB\")})\ndef training_step(...) -> ...:\n    # train a model\n```\n\nIf the underlying [orchestrator](../../component-guide/orchestrators/orchestrators.md) in your stack then supports specifying resources, this setting will attempt to secure these resources. Some orchestrators (like the [Skypilot orchestrator](../../component-guide/orchestrators/skypilot-vm.md)) do not support `ResourceSettings` directly, but rather use their `Orchestrator` specific settings to achieve the same effect:\n\n```python\nfrom zenml import step\nfrom zenml.integrations.skypilot.flavors.skypilot_orchestrator_aws_vm_flavor import SkypilotAWSOrchestratorSettings\n\nskypilot_settings = SkypilotAWSOrchestratorSettings(\n    cpus=\"2\",\n    memory=\"16\",\n    accelerators=\"V100:2\",\n)\n\n\n@step(settings={\"orchestrator.vm_aws\": skypilot_settings)\ndef training_step(...) -> ...:\n    # train a model\n```\n\nPlease refer to the source code and documentation of each orchestrator to find out which orchestrator supports specifying resources in what way.\n\n{% hint style=\"info\" %}\nIf you're using an orchestrator which does not support this feature or its underlying infrastructure does not cover your requirements, you can also take a look at [step operators](../../component-guide/step-operators/step-operators.md) which allow you to execute individual steps of your pipeline in environments independent of your orchestrator.\n{% endhint %}\n\n### Ensure your container is CUDA-enabled\n\nTo run steps or pipelines on GPUs, it's crucial to have the necessary CUDA tools installed in the environment. This section will guide you on how to configure your environment to utilize GPU capabilities effectively.\n\n{% hint style=\"warning\" %}\nNote that these configuration changes are **required** for the GPU hardware to be properly utilized. If you don't update the settings, your steps might run, but they will not see any boost in performance from the custom hardware.\n{% endhint %"}
{"input": "}\n\nAll steps running on GPU-backed hardware will be executed within a containerized environment, whether you're using the local Docker orchestrator or a cloud instance of Kubeflow. Therefore, you need to make two amendments to your Docker settings for the relevant steps:\n\n#### 1. **Specify a CUDA-enabled parent image in your `DockerSettings`**\n\nFor complete details, refer to the [containerization page](../customize-docker-builds/README.md) that explains how to do this. As an example, if you want to use the latest CUDA-enabled official PyTorch image for your entire pipeline run, you can include the following code:\n\n```python\nfrom zenml import pipeline\nfrom zenml.config import DockerSettings\n\ndocker_settings = DockerSettings(parent_image=\"pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime\")\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\nFor TensorFlow, you might use the `tensorflow/tensorflow:latest-gpu` image, as detailed in the [official TensorFlow documentation](https://www.tensorflow.org/install/docker#gpu\\_support) or their [DockerHub overview](https://hub.docker.com/r/tensorflow/tensorflow).\n\n#### 2. **Add ZenML as an explicit pip requirement**\n\nZenML requires that ZenML itself be installed for the containers running your pipelines and steps. Therefore, you need to explicitly state that ZenML should be installed. There are several ways to specify this, but as an example, you can update the code from above as follows:\n\n```python\nfrom zenml.config import DockerSettings\nfrom zenml import pipeline\n\ndocker_settings = DockerSettings(\n    parent_image=\"pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime\",\n    requirements=[\"zenml==0.39.1\", \"torchvision\"]\n)\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\nAdding these two extra settings options will ensure that CUDA is enabled for the specific steps that require GPU acceleration. Be cautious when choosing the image to avoid confusion when switching between local and remote environments. For example, you might have one version of PyTorch installed locally with a particular CUDA version, but when you switch to your remote stack or environment, you might be forced to use a different CUDA version.\n\nThe core cloud operators offer prebuilt Docker images that fit with their hardware. You can find more information on them here:\n\n* [AWS](https://github.com/aws/deep-learning-containers/blob/master/available\\_images.md)\n* [GCP](https://cloud.google.com/deep-learning-vm/docs/images)\n* [Azure](https://learn.microsoft.com/en-us/azure/machine-learning/concept-prebuilt-docker-images-inference)\n\nNot all of these images are available on DockerHub, so ensure that the\norchestrator environment your"}
{"input": " pipeline runs in has sufficient permissions to\npull images from registries if you are using one of those.\n\n### Reset the CUDA cache in between steps\n\nYour use case will determine whether this is necessary or makes sense to do, but\nwe have seen that resetting the CUDA cache in between steps can help avoid issues\nwith the GPU cache. This is particularly necessary if your training jobs are\npushing the boundaries of the GPU cache. Doing so is simple; just use a helper\nfunction to reset the cache at the beginning of any GPU-enabled steps. For\nexample, something as simple as this might suffice:\n\n```python\nimport gc\nimport torch\n\ndef cleanup_memory() -> None:\n    while gc.collect():\n        torch.cuda.empty_cache()\n```\n\nYou can then call this function at the beginning of your GPU-enabled steps:\n\n```python\nfrom zenml import step\n\n@step\ndef training_step(...):\n    cleanup_memory()\n    # train a model\n```\n\nNote that resetting the memory cache will potentially affect others using the\nsame GPU, so use this judiciously.\n\n## Train across multiple GPUs\n\nZenML supports training your models with multiple GPUs on a single node. This is\nuseful if you have a large dataset and want to train your model in parallel. The\nmost important thing that you'll have to handle is preventing multiple ZenML\ninstances from being spawned as you split the work among multiple GPUs.\n\nIn practice this will probably involve:\n\n- creating a script / Python function that contains the logic of training your\n  model (with the specification that this should run in parallel across multiple\n  GPUs)\n- calling that script / external function from within the step, possibly with\n  some wrapper or helper code to dynamically configure or update the external\n  script function\n\nWe're aware that this is not the most elegant solution and we're at work to\nimplement a better option with some inbuilt support for this task. If this is\nsomething you're struggling with and need support getting the step code working,\nplease do [connect with us on Slack](https://zenml.io/slack) and we'll do our best\nto help you out.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Run distributed training with Hugging Face's Accelerate library in ZenML pipelines.\n---\n\n# Distributed training with \ud83e\udd17 Accelerate\n\nThere are several reasons why you might want to scale your machine learning pipelines to utilize distributed training, such as leveraging multiple GPUs or training across multiple nodes. ZenML now integrates with [Hugging Face's Accelerate library](https://github.com/huggingface/accelerate) to make this process seamless and efficient.\n\n## Use \ud83e\udd17 Accelerate in your steps\n\nSome steps in your machine learning pipeline, particularly training steps, can benefit from distributed execution. You can now use the `run_with_accelerate` function to enable this:\n\n```python\nfrom zenml import step, pipeline\nfrom zenml.integrations.huggingface.steps import run_with_accelerate\n\n@step\ndef training_step():\n    # your training code here\n    ...\n\n@pipeline\ndef training_pipeline(num_processes: int):\n    run_with_accelerate(training_step, num_processes=num_processes)()\n```\n\nThe `run_with_accelerate` function wraps your step, enabling it to run with Accelerate's distributed training capabilities. It accepts various arguments that correspond to Accelerate CLI options.\n\n{% hint style=\"info\" %}\nFor a complete list of available arguments and more details, refer to the [Accelerate CLI documentation](https://huggingface.co/docs/accelerate/en/package_reference/cli#accelerate-launch).\n{% endhint %}\n\n### Configuration\n\nThe `run_with_accelerate` function accepts various arguments to configure your distributed training environment. Some common arguments include:\n\n- `num_processes`: The number of processes to use for distributed training.\n- `cpu`: Whether to force training on CPU.\n- `multi_gpu`: Whether to launch distributed GPU training.\n- `mixed_precision`: Mixed precision training mode ('no', 'fp16', or 'bf16').\n\n### Important Usage Notes\n\n1. The `run_with_accelerate` function cannot be used directly on steps using the '@' syntax. Use it within your pipeline definition instead.\n\n2. Steps defined inside the entrypoint script cannot be used with `run_with_accelerate`. Move your step code to another file and import it.\n\n3. Accelerated steps do not support positional arguments. Use keyword arguments when calling your steps.\n\n4. If `run_with_accelerate` is misused, it will raise a `RuntimeError` with a helpful message explaining the correct usage.\n\n{% hint style=\"info\" %}\nTo see a full example where Accelerate is used within a ZenML pipeline, check out our <a href=\"https://github.com/zenml-io/zenml-projects/blob/main/llm-lora-finetuning/README.md\">llm-lora-finetuning</a> project which leverages the distributed training functionalities while finetuning an LLM.\n{% endhint %}\n\n## Ensure your container is Accelerate-ready\n\nTo run steps with Accelerate, it"}
{"input": "'s crucial to have the necessary dependencies installed in the environment. This section will guide you on how to configure your environment to utilize Accelerate effectively.\n\n{% hint style=\"warning\" %}\nNote that these configuration changes are **required** for Accelerate to function properly. If you don't update the settings, your steps might run, but they will not leverage distributed training capabilities.\n{% endhint %}\n\nAll steps using Accelerate will be executed within a containerized environment. Therefore, you need to make two amendments to your Docker settings for the relevant steps:\n\n### 1. Specify a CUDA-enabled parent image in your `DockerSettings`\n\nFor complete details, refer to the [containerization page](../customize-docker-builds/README.md). Here's an example using a CUDA-enabled PyTorch image:\n\n```python\nfrom zenml import pipeline\nfrom zenml.config import DockerSettings\n\ndocker_settings = DockerSettings(parent_image=\"pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime\")\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n### 2. Add Accelerate as explicit pip requirements\n\nEnsure that Accelerate is installed in your container:\n\n```python\nfrom zenml.config import DockerSettings\nfrom zenml import pipeline\n\ndocker_settings = DockerSettings(\n    parent_image=\"pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime\",\n    requirements=[\"accelerate\", \"torchvision\"]\n)\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    ...\n```\n\n## Train across multiple GPUs\n\nZenML's Accelerate integration supports training your models with multiple GPUs on a single node or across multiple nodes. This is particularly useful for large datasets or complex models that benefit from parallelization.\n\nIn practice, using Accelerate with multiple GPUs involves:\n\n- Wrapping your training step with the `run_with_accelerate` function in your pipeline definition\n- Configuring the appropriate Accelerate arguments (e.g., `num_processes`, `multi_gpu`)\n- Ensuring your training code is compatible with distributed training (Accelerate handles most of this automatically)\n\n{% hint style=\"info\" %}\nIf you're new to distributed training or encountering issues, please [connect with us on Slack](https://zenml.io/slack) and we'll be happy to assist you.\n{% endhint %}\n\nBy leveraging the Accelerate integration in ZenML, you can easily scale your training processes and make the most of your available hardware resources, all while maintaining the structure and benefits of your ZenML pipelines.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>"}
{"input": "---\ndescription: How to set the logging verbosity in ZenML.\n---\n\n# Set logging verbosity\n\nBy default, ZenML sets the logging verbosity to `INFO`. If you wish to change this, you can do so by setting the following environment variable:\n\n```bash\nexport ZENML_LOGGING_VERBOSITY=INFO\n```\n\nChoose from `INFO`, `WARN`, `ERROR`, `CRITICAL`, `DEBUG`. This will set the logs\nto whichever level you suggest.\n\nNote that setting this on the [client environment](../configure-python-environments/README.md#client-environment-or-the-runner-environment) (e.g. your local machine which runs the pipeline) will **not automatically set the same logging verbosity for remote pipeline runs**. That means setting this variable locally with only effect pipelines that run locally.\n\nIf you wish to control for [remote pipeline runs](../../user-guide/production-guide/cloud-orchestration.md), you can set the `ZENML_LOGGING_VERBOSITY` environment variable in your pipeline runs environment as follows:\n\n```python\ndocker_settings = DockerSettings(environment={\"ZENML_LOGGING_VERBOSITY\": \"DEBUG\"})\n\n# Either add it to the decorator\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline() -> None:\n    my_step()\n\n# Or configure the pipelines options\nmy_pipeline = my_pipeline.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: How to disable rich traceback output in ZenML.\n---\n\n# Disable `rich` traceback output\n\nBy default, ZenML uses the [`rich`](https://rich.readthedocs.io/en/stable/traceback.html) library to display rich traceback output. This is especially useful when debugging your pipelines. However, if you wish to disable this feature, you can do so by setting the following environment variable:\n\n```bash\nexport ZENML_ENABLE_RICH_TRACEBACK=false\n```\n\nThis will ensure that you see only the plain text traceback output.\n\nNote that setting this on the [client environment](../configure-python-environments/README.md#client-environment-or-the-runner-environment) (e.g. your local machine which runs the pipeline) will **not automatically disable rich tracebacks on remote pipeline runs**. That means setting this variable locally with only effect pipelines that run locally.\n\nIf you wish to disable it also for [remote pipeline runs](../../user-guide/production-guide/cloud-orchestration.md), you can set the `ZENML_ENABLE_RICH_TRACEBACK` environment variable in your pipeline runs environment as follows:\n\n```python\ndocker_settings = DockerSettings(environment={\"ZENML_ENABLE_RICH_TRACEBACK\": \"false\"})\n\n# Either add it to the decorator\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline() -> None:\n    my_step()\n\n# Or configure the pipelines options\nmy_pipeline = my_pipeline.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Viewing logs on the dashboard\n\nBy default, ZenML uses a logging handler to capture the logs that occur during the execution of a step. Users are free to use the default python logging module or print statements, and ZenML's logging handler will catch these logs and store them.\n\n```python\nimport logging\n\nfrom zenml import step\n\n@step \ndef my_step() -> None:\n    logging.warning(\"`Hello`\")  # You can use the regular `logging` module.\n    print(\"World.\")  # You can utilize `print` statements as well. \n```\n\nThese logs are stored within the respective artifact store of your stack. This means that you can only view these logs in the dashboard\n*if the deployed ZenML server has direct access to the underlying artifact store*. There are two cases in which this will be true:\n\n* In case of a local ZenML server (via `zenml up`), both local and remote artifact stores may be accessible, depending on configuration of the client.\n* In case of a deployed ZenML server, logs for runs on a [local artifact store](../../component-guide/artifact-stores/local.md) will not be accessible. Logs\nfor runs using a [remote artifact store](../../user-guide/production-guide/remote-storage.md) **may be** accessible, if the artifact store has been configured\nwith a [service connector](../auth-management/service-connectors-guide.md). Please read [this chapter](../../user-guide/production-guide/remote-storage.md) of\nthe production guide to learn how to configure a remote artifact store with a service connector.\n\nIf configured correctly, the logs are displayed in the dashboard as follows:\n\n![Displaying step logs on the dashboard](../../.gitbook/assets/zenml\\_step\\_logs.png)\n\n{% hint style=\"warning\" %}\nIf you do not want to store the logs for your pipeline (for example due to performance reduction or storage limits),\nyou can follow [these instructions](./enable-or-disable-logs-storing.md).\n{% endhint %}\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: How to disable colorful logging in ZenML.\n---\n\n# Disable colorful logging\n\nBy default, ZenML uses colorful logging to make it easier to read logs. However, if you wish to disable this feature, you can do so by setting the following environment variable:\n\n```bash\nZENML_LOGGING_COLORS_DISABLED=true\n```\n\nNote that setting this on the [client environment](../configure-python-environments/README.md#client-environment-or-the-runner-environment) (e.g. your local machine which runs the pipeline) will automatically disable colorful logging on remote pipeline runs. If you wish to only disable it locally, but turn on for remote pipeline runs, you can set the `ZENML_LOGGING_COLORS_DISABLED` environment variable in your pipeline runs environment as follows:\n\n```python\ndocker_settings = DockerSettings(environment={\"ZENML_LOGGING_COLORS_DISABLED\": \"false\"})\n\n# Either add it to the decorator\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline() -> None:\n    my_step()\n\n# Or configure the pipelines options\nmy_pipeline = my_pipeline.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Configuring ZenML's default logging behavior\n---\n\n# \ud83c\udf32 Control logging\n\nZenML produces various kinds of logs:\n\n* The [ZenML Server](../../getting-started/deploying-zenml/README.md) produces server logs (like any FastAPI server).\n* The [Client or Runner](../configure-python-environments/README.md#client-environment-or-the-runner-environment) environment produces logs, for example after running a pipeline. These are steps that are typically before, after, and during the creation of a pipeline run.\n* The [Execution environment](../configure-python-environments/README.md#execution-environments) (on the orchestrator level) produces logs when it executes each step of a pipeline. These are logs that are typically written in your steps using the python `logging` module.\n\nThis section talks about how users can control logging behavior in these various environments.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Enable or disable logs storing\n\nBy default, ZenML uses a logging handler to capture the logs that occur during the execution of a step. Users are free to use the default python logging module or print statements, and ZenML's logging handler will catch these logs and store them. \n\n```python\nimport logging\n\nfrom zenml import step\n\n@step \ndef my_step() -> None:\n    logging.warning(\"`Hello`\")  # You can use the regular `logging` module.\n    print(\"World.\")  # You can utilize `print` statements as well. \n```\n\nThese logs are stored within the respective artifact store of your stack. You can display the logs in the dashboard as follows:\n\n![Displaying step logs on the dashboard](../../.gitbook/assets/zenml\\_step\\_logs.png)\n\n{% hint style=\"warning\" %}\nNote that if you are not connected to a cloud artifact store with a service connector configured then you will not\nbe able to view your logs in the dashboard. Read more [here](./view-logs-on-the-dasbhoard.md).\n{% endhint %}\n\nIf you do not want to store the logs in your artifact store, you can:\n\n1.  Disable it by using the `enable_step_logs` parameter either with your `@pipeline` or `@step` decorator:\n\n    ```python\n    from zenml import pipeline, step\n\n    @step(enable_step_logs=False)  # disables logging for this step\n    def my_step() -> None:\n        ...\n\n    @pipeline(enable_step_logs=False)  # disables logging for the entire pipeline\n    def my_pipeline():\n        ...\n    ```\n2. Disable it by using the environmental variable `ZENML_DISABLE_STEP_LOGS_STORAGE` and setting it to `true`. This environmental variable takes precedence over the parameters mentioned above. Note this environmental variable needs to be set on the [execution environment](../configure-python-environments/README.md#execution-environments), i.e., on the orchestrator level:\n\n```python\ndocker_settings = DockerSettings(environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": \"true\"})\n\n# Either add it to the decorator\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline() -> None:\n    my_step()\n\n# Or configure the pipelines options\nmy_pipeline = my_pipeline.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Configuring ZenML's default behavior\n---\n\n# Configuring ZenML\n\nThere are various ways to adapt how ZenML behaves in certain situations. This guide walks users through how to configure certain aspects of ZenML.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Using settings to configure runtime configuration.\n---\n\n# Stack component specific configuration\n\n{% embed url=\"https://www.youtube.com/embed/AdwW6DlCWFE\" %}\nStack Component Config vs Settings in ZenML\n{% endembed %}\n\nPart of the configuration of a pipeline are its `Settings`. These allow you to configure runtime configurations for stack components and pipelines. Concretely, they allow you to configure:\n\n* The [resources](../training-with-gpus/training-with-gpus.md#specify-resource-requirements-for-steps) required for a step\n* Configuring the [containerization](../customize-docker-builds/README.md) process of a pipeline (e.g. What requirements get installed in the Docker image)\n* Stack component-specific configuration, e.g., if you have an experiment tracker passing in the name of the experiment at runtime\n\nYou will learn about all of the above in more detail later, but for now, let's try to understand that all of this configuration flows through one central concept called `BaseSettings`. (From here on, we use `settings` and `BaseSettings` as analogous in this guide).\n\n## Types of settings\n\nSettings are categorized into two types:\n\n* **General settings** that can be used on all ZenML pipelines. Examples of these are:\n  * [`DockerSettings`](../customize-docker-builds/README.md) to specify Docker settings.\n  * [`ResourceSettings`](../training-with-gpus/training-with-gpus.md) to specify resource settings.\n* **Stack-component-specific settings**: These can be used to supply runtime configurations to certain stack components (key= \\<COMPONENT\\_CATEGORY>.\\<COMPONENT\\_FLAVOR>). Settings for components not in the active stack will be ignored. Examples of these are:\n  * [`SkypilotAWSOrchestratorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-skypilot_aws/#zenml.integrations.skypilot_aws.flavors.skypilot_orchestrator_aws_vm_flavor.SkypilotAWSOrchestratorSettings) to specify Skypilot settings (works for `SkypilotGCPOrchestratorSettings` and `SkypilotAzureOrchestratorSettings` as well).\n  * [`KubeflowOrchestratorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-kubeflow/#zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor.KubeflowOrchestratorSettings) to specify Kubeflow settings.\n  * [`MLflowExperimentTrackerSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-mlflow/#zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor.MLFlowExperimentTrackerSettings) to specify MLflow settings.\n "}
{"input": " * [`WandbExperimentTrackerSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-wandb/#zenml.integrations.wandb.flavors.wandb_experiment_tracker_flavor.WandbExperimentTrackerSettings) to specify W\\&B settings.\n  * [`WhylogsDataValidatorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-whylogs/#zenml.integrations.whylogs.flavors.whylogs_data_validator_flavor.WhylogsDataValidatorSettings) to specify Whylogs settings.\n  * [`SagemakerStepOperatorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-aws/#zenml.integrations.aws.flavors.sagemaker_step_operator_flavor.SagemakerStepOperatorSettings) to specify AWS Sagemaker step operator settings.\n  * [`VertexStepOperatorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-gcp/#zenml.integrations.gcp.flavors.vertex_step_operator_flavor.VertexStepOperatorSettings) to specify GCP Vertex step operator settings.\n  * [`AzureMLStepOperatorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-azure/#zenml.integrations.azure.flavors.azureml_step_operator_flavor.AzureMLStepOperatorSettings) to specify AzureML step operator settings.\n\n### Difference between stack component settings at registration-time vs real-time\n\nFor stack-component-specific settings, you might be wondering what the difference is between these and the configuration passed in while doing `zenml stack-component register <NAME> --config1=configvalue --config2=configvalue`, etc. The answer is that the configuration passed in at registration time is static and fixed throughout all pipeline runs, while the settings can change.\n\nA good example of this is the [`MLflow Experiment Tracker`](../../component-guide/experiment-trackers/mlflow.md), where configuration which remains static such as the `tracking_url` is sent through at registration time, while runtime configuration such as the `experiment_name` (which might change every pipeline run) is sent through as runtime settings.\n\nEven though settings can be overridden at runtime, you can also specify _default_ values for settings while configuring a stack component. For example, you could set a default value for the `nested` setting of your MLflow experiment tracker: `zenml experiment-tracker register <NAME> --flavor=mlflow --nested=True`\n\nThis means that all pipelines that run using this experiment tracker use nested MLflow runs unless overridden by specifying settings for the pipeline at runtime.\n\n### Using the right key for Stack-component-specific settings\n\nWhen specifying stack-component-specific settings, a key needs to be passed. This key should always correspond to the pattern: \\<COMPONENT\\_CATEGORY>.\\<COMPONENT\\_FLAVOR>\n\nFor example, the [SagemakerStepOperator](../../component-guide/"}
{"input": "step-operators/sagemaker.md) supports passing in [`estimator_args`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.flavors.sagemaker\\_step\\_operator\\_flavor.SagemakerStepOperatorSettings). The way to specify this would be to use the key `step_operator.sagemaker`\n\n```python\n@step(step_operator=\"nameofstepoperator\", settings= {\"step_operator.sagemaker\": {\"estimator_args\": {\"instance_type\": \"m7g.medium\"}}})\ndef my_step():\n  ...\n\n# Using the class\n@step(step_operator=\"nameofstepoperator\", settings= {\"step_operator.sagemaker\": SagemakerStepOperatorSettings(instance_type=\"m7g.medium\")})\ndef my_step():\n  ...\n```\n\nor in YAML:\n\n```yaml\nsteps:\n  my_step:\n    step_operator: \"nameofstepoperator\"\n    settings:\n      step_operator.sagemaker:\n        estimator_args:\n          instance_type: m7g.medium\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# What can be configured\n\nHere is an example of a sample YAML file, with the most important configuration highlighted. For brevity,\nwe have removed all possible keys. To view a sample file with all possible keys, refer to\n[this page](./autogenerate-a-template-yaml-file.md).\n\n```yaml\n# Build ID (i.e. which Docker image to use)\nbuild: dcd6fafb-c200-4e85-8328-428bef98d804\n\n# Enable flags (boolean flags that control behavior)\nenable_artifact_metadata: True\nenable_artifact_visualization: False\nenable_cache: False\nenable_step_logs: True\n\n# Extra dictionary to pass in arbitrary values\nextra: \n  any_param: 1\n  another_random_key: \"some_string\"\n\n# Specify the \"ZenML Model\"\nmodel:\n  name: \"classification_model\"\n  version: production\n\n  audience: \"Data scientists\"\n  description: \"This classifies hotdogs and not hotdogs\"\n  ethics: \"No ethical implications\"\n  license: \"Apache 2.0\"\n  limitations: \"Only works for hotdogs\"\n  tags: [\"sklearn\", \"hotdog\", \"classification\"]\n\n# Parameters of the pipeline \nparameters: \n  dataset_name: \"another_dataset\"\n\n# Name of the run\nrun_name: \"my_great_run\"\n\n# Schedule, if supported on the orchestrator\nschedule:\n  catchup: true\n  cron_expression: \"* * * * *\"\n\n# Real-time settings for Docker and resources\nsettings:\n  # Controls Docker building\n  docker:\n    apt_packages: [\"curl\"]\n    copy_files: True\n    dockerfile: \"Dockerfile\"\n    dockerignore: \".dockerignore\"\n    environment:\n      ZENML_LOGGING_VERBOSITY: DEBUG\n    parent_image: \"zenml-io/zenml-cuda\"\n    requirements: [\"torch\"]\n    skip_build: False\n  \n  # Control resources for the entire pipeline\n  resources:\n    cpu_count: 2\n    gpu_count: 1\n    memory: \"4Gb\"\n  \n# Per step configuration\nsteps:\n  # Top-level key should be the name of the step invocation ID\n  train_model:\n    # Parameters of the step\n    parameters:\n      data_source: \"best_dataset\"\n\n    # Step-only configuration\n    experiment_tracker: \"mlflow_production\"\n    step_operator: \"vertex_gpu\"\n    outputs: {}\n    failure_hook_source: {}\n    success_hook_source: {}\n\n    # Same as pipeline level configuration, if specified overrides for this step\n    enable_artifact_metadata: True\n    enable_artifact_visualization: True\n    enable_cache: False\n    enable_step_logs: True\n\n    # Same as pipeline level configuration, if specified overrides for this step\n    extra: {}\n\n    # Same as pipeline level configuration, if specified overrides for this step\n    model"}
{"input": ": {}\n      \n    # Same as pipeline level configuration, if specified overrides for this step\n    settings:\n      docker: {}\n      resources: {}\n\n      # Stack component specific settings\n      step_operator.sagemaker:\n        estimator_args:\n          instance_type: m7g.medium\n```\n\n## Deep-dive\n\n### `enable_XXX` parameters\n\nThese are boolean flags for various configurations:\n\n* `enable_artifact_metadata`: Whether to [associate metadata with artifacts or not](../handle-data-artifacts/handle-custom-data-types.md#optional-which-metadata-to-extract-for-the-artifact).\n* `enable_artifact_visualization`: Whether to [attach visualizations of artifacts](../visualize-artifacts/README.md).\n* `enable_cache`: Utilize [caching](../build-pipelines/control-caching-behavior.md) or not.\n* `enable_step_logs`: Enable tracking [step logs](../control-logging/enable-or-disable-logs-storing.md).\n\n```yaml\nenable_artifact_metadata: True\nenable_artifact_visualization: True\nenable_cache: True\nenable_step_logs: True\n```\n\n### `build` ID\n\nThe UUID of the [`build`](../customize-docker-builds/README.md) to use for this pipeline. If specified, Docker image building is skipped for remote orchestrators, and the Docker image specified in this build is used.\n\n```yaml\nbuild: <INSERT-BUILD-ID-HERE>\n```\n\n### Configuring the `model`\n\nSpecifies the ZenML [Model](../../user-guide/starter-guide/track-ml-models.md) to use for this pipeline.\n\n```yaml\nmodel:\n  name: \"ModelName\"\n  version: \"production\"\n  description: An example model\n  tags: [\"classifier\"]\n```\n\n### Pipeline and step `parameters`\n\nA dictionary of JSON-serializable [parameters](../build-pipelines/use-pipeline-step-parameters.md) specified at the pipeline or step level. For example:\n\n```yaml\nparameters:\n    gamma: 0.01\n\nsteps:\n    trainer:\n        parameters:\n            gamma: 0.001\n```\n\nCorresponds to:\n\n```python\nfrom zenml import step, pipeline\n\n@step\ndef trainer(gamma: float):\n    # Use gamma as normal\n    print(gamma)\n\n@pipeline\ndef my_pipeline(gamma: float):\n    # use gamma or pass it into the step\n    print(0.01)\n    trainer(gamma=gamma)\n```\n\nImportant note, in the above case, the value of the step would be the one defined in the `steps` key (i.e. 0.001). So the YAML config always takes precedence over pipeline parameters that are passed down to steps in code. Read [this section for more details](configuration-hierarchy.md).\n\nNormally, parameters defined at the pipeline level are used in multiple steps, and then no step-level configuration is defined.\n\n{%"}
{"input": " hint style=\"info\" %}\nNote that `parameters` are different from `artifacts`. Parameters are JSON-serializable values that are passed in the runtime configuration of a pipeline. Artifacts are inputs and outputs of a step, and need not always be JSON-serializable ([materializers](../handle-data-artifacts/handle-custom-data-types.md) handle their persistence in the [artifact store](../../component-guide/artifact-stores/artifact-stores.md)).\n{% endhint %}\n\n### Setting the `run_name`\n\nTo change the name for a run, pass `run_name` as a parameter. This can be a dynamic value as well.&#x20;\n\n```python\nrun_name: <INSERT_RUN_NAME_HERE>  \n```\n\n{% hint style=\"warning\" %}\nYou will not be able to run with the same run\\_name twice. Do not set this statically when running on a schedule. Try to include some auto-incrementation or timestamp to the name.\n{% endhint %}\n\n### Stack Component Runtime settings\n\nSettings are special runtime configurations of a pipeline or a step that require a [dedicated section](runtime-configuration.md). In short, they define a bunch of execution configuration such as Docker building and resource settings.\n\n### Docker Settings\n\nDocker Settings can be passed in directly as objects, or a dictionary representation of the object. For example, the Docker configuration can be set in configuration files as follows:\n\n```yaml\nsettings:\n  docker:\n    requirements:\n      - pandas\n    \n```\n\n{% hint style=\"info\" %}\nFind a complete list of all Docker Settings [here](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-config/#zenml.config.docker\\_settings.DockerSettings). To learn more about pipeline containerization consult our documentation on this [here](../customize-docker-builds/README.md).\n{% endhint %}\n\n### Resource Settings\n\nSome stacks allow setting the resource settings using these settings.\n\n```yaml\nresources:\n  cpu_count: 2\n  gpu_count: 1\n  memory: \"4Gb\"\n```\n\nNote that this may not work for all types of stack components. To learn which components support this,\nplease refer to the specific orchestrator docs.\n\n### `failure_hook_source` and `success_hook_source`\n\nThe `source` of the [failure and success hooks](../build-pipelines/use-failure-success-hooks.md) can be specified.\n\n### Step-specific configuration\n\nA lot of pipeline-level configuration can also be applied at a step level (as we have already seen with the `enable_cache` flag). However, there is some configuration that is step-specific, meaning it cannot be applied at a pipeline level, but only at a step level.\n\n* `experiment_tracker`: Name of the [experiment\\_tracker](../../component-guide/experiment-trackers/experiment-trackers.md) to enable for this step. This experiment\\_tracker should be defined in the active stack with the same name.\n"}
{"input": "* `step_operator`: Name of the [step\\_operator](../../component-guide/step-operators/step-operators.md) to enable for this step. This step\\_operator should be defined in the active stack with the same name.\n* `outputs`: This is configuration of the output artifacts of this step. This is further keyed by output name (by default, step outputs [are named `output`](../handle-data-artifacts/return-multiple-outputs-from-a-step.md)). The most interesting configuration here is the `materializer_source`, which is the UDF path of the materializer in code to use for this output (e.g. `materializers.some_data.materializer.materializer_class`). Read more about this source path [here](../handle-data-artifacts/handle-custom-data-types.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: ZenML makes it easy to configure and run a pipeline with configuration files.\n---\n\n# \ud83d\udcc3 Use configuration files\n\nZenML pipelines can be configured at runtime with a simple YAML file that can help you set [parameters](../build-pipelines/use-pipeline-step-parameters.md), control [caching behavior](../build-pipelines/control-caching-behavior.md) or even configure different stack components.\n\nLearn more about the different options in the following sections:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>What can be configured</td><td></td><td></td><td><a href=\"what-can-be-configured.md\">what-can-be-configured.md</a></td></tr><tr><td>Configuration hierarchy</td><td></td><td></td><td><a href=\"configuration-hierarchy.md\">configuration-hierarchy.md</a></td></tr><tr><td>Autogenerate a template yaml file</td><td></td><td></td><td><a href=\"autogenerate-a-template-yaml-file.md\">autogenerate-a-template-yaml-file.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "# Find out which configuration was used for a run\n\nSometimes you might want to extract the used configuration from a pipeline that has already run. You can do this simply by loading the pipeline run and accessing its `config` attribute.\n\n<pre class=\"language-python\"><code class=\"lang-python\">from zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"&#x3C;PIPELINE_RUN_NAME>\")\n\n<strong>configuration = pipeline_run.config\n</strong></code></pre>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Specify a configuration file\n---\n\n# \ud83d\udcc3 Use configuration files\n\n{% hint style=\"info\" %}\nAll configuration that can be specified in a YAML file can also be specified in code itself.\nHowever, it is best practice to use a YAML file to separate config from code.\n{% endhint %}\n\nYou can use the `with_options(config_path=<PATH_TO_CONFIG>)` pattern to apply your\nconfiguration to a pipeline. Here is a minimal example of using a file based configuration yaml.\n\n```yaml\nenable_cache: False\n\n# Configure the pipeline parameters\nparameters:\n  dataset_name: \"best_dataset\"  \n  \nsteps:\n  load_data:  # Use the step name here\n    enable_cache: False  # same as @step(enable_cache=False)\n```\n\n```python\nfrom zenml import step, pipeline\n\n@step\ndef load_data(dataset_name: str) -> dict:\n    ...\n\n@pipeline  # This function combines steps together \ndef simple_ml_pipeline(dataset_name: str):\n    load_data(dataset_name)\n    \nif __name__==\"__main__\":\n    simple_ml_pipeline.with_options(config_path=<INSERT_PATH_TO_CONFIG_YAML>)()\n```\n\nThe above would run the `simple_ml_pipeline` with cache disabled for `load_data` and the parameter\n`dataset_name` set to `best_dataset`.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: >-\n  When things can be configured on the pipeline and step level, the step\n  configuration overrides the pipeline.\n---\n\n# Configuration hierarchy\n\nThere are a few general rules when it comes to settings and configurations that are applied in multiple places. Generally the following is true:\n\n* Configurations in code override configurations made inside of the yaml file\n* Configurations at the step level override those made at the pipeline level\n* In case of attributes the dictionaries are merged\n\n```python\nfrom zenml import pipeline, step\nfrom zenml.config import ResourceSettings\n\n\n@step\ndef load_data(parameter: int) -> dict:\n    ...\n\n@step(settings={\"resources\": ResourceSettings(gpu_count=1, memory=\"2GB\")})\ndef train_model(data: dict) -> None:\n    ...\n\n\n@pipeline(settings={\"resources\": ResourceSettings(cpu_count=2, memory=\"1GB\")}) \ndef simple_ml_pipeline(parameter: int):\n    ...\n    \n# ZenMl merges the two configurations and uses the step configuration to override \n# values defined on the pipeline level\n\ntrain_model.configuration.settings[\"resources\"]\n# -> cpu_count: 2, gpu_count=1, memory=\"2GB\"\n\nsimple_ml_pipeline.configuration.settings[\"resources\"]\n# -> cpu_count: 2, memory=\"1GB\"\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  To help you figure out what you can put in your configuration file, simply\n  autogenerate a template.\n---\n\n# Autogenerate a template yaml file\n\nIf you want to generate a template yaml file of your specific pipeline, you can do so by using the `.write_run_configuration_template()` method. This will generate a yaml file with all options commented out. This way you can pick and choose the settings that are relevant to you.\n\n```python\nfrom zenml import pipeline\n...\n\n@pipeline(enable_cache=True) # set cache behavior at step level\ndef simple_ml_pipeline(parameter: int):\n    dataset = load_data(parameter=parameter)\n    train_model(dataset)\n\nsimple_ml_pipeline.write_run_configuration_template(path=\"<Insert_path_here>\")\n```\n\n<details>\n\n<summary>An example of a generated YAML configuration template</summary>\n\n```yaml\nbuild: Union[PipelineBuildBase, UUID, NoneType]\nenable_artifact_metadata: Optional[bool]\nenable_artifact_visualization: Optional[bool]\nenable_cache: Optional[bool]\nenable_step_logs: Optional[bool]\nextra: Mapping[str, Any]\nmodel:\n  audience: Optional[str]\n  description: Optional[str]\n  ethics: Optional[str]\n  license: Optional[str]\n  limitations: Optional[str]\n  name: str\n  save_models_to_registry: bool\n  suppress_class_validation_warnings: bool\n  tags: Optional[List[str]]\n  trade_offs: Optional[str]\n  use_cases: Optional[str]\n  version: Union[ModelStages, int, str, NoneType]\n  was_created_in_this_run: bool\nparameters: Optional[Mapping[str, Any]]\nrun_name: Optional[str]\nschedule:\n  catchup: bool\n  cron_expression: Optional[str]\n  end_time: Optional[datetime]\n  interval_second: Optional[timedelta]\n  name: Optional[str]\n  run_once_start_time: Optional[datetime]\n  start_time: Optional[datetime]\nsettings:\n  docker:\n    apt_packages: List[str]\n    build_context_root: Optional[str]\n    build_options: Mapping[str, Any]\n    copy_files: bool\n    copy_global_config: bool\n    dockerfile: Optional[str]\n    dockerignore: Optional[str]\n    environment: Mapping[str, Any]\n    install_stack_requirements: bool\n    parent_image: Optional[str]\n    python_package_installer: PythonPackageInstaller\n    replicate_local_python_environment: Union[List[str], PythonEnvironmentExportMethod,\n     NoneType]\n    required_integrations: List[str]\n    requirements: Union[NoneType, str, List[str]]\n    skip_build: bool\n    prevent_build_reuse: bool\n    allow_including_files_in_images: bool\n    allow_download_from_code_repository: bool\n    allow_download_from_artifact_store: bool\n    target_repository: str\n    user: Optional[str]\n  resources:\n    cpu_count: Optional[PositiveFloat]\n    gpu_count:"}
{"input": " Optional[NonNegativeInt]\n    memory: Optional[ConstrainedStrValue]\nsteps:\n  load_data:\n    enable_artifact_metadata: Optional[bool]\n    enable_artifact_visualization: Optional[bool]\n    enable_cache: Optional[bool]\n    enable_step_logs: Optional[bool]\n    experiment_tracker: Optional[str]\n    extra: Mapping[str, Any]\n    failure_hook_source:\n      attribute: Optional[str]\n      module: str\n      type: SourceType\n    model:\n      audience: Optional[str]\n      description: Optional[str]\n      ethics: Optional[str]\n      license: Optional[str]\n      limitations: Optional[str]\n      name: str\n      save_models_to_registry: bool\n      suppress_class_validation_warnings: bool\n      tags: Optional[List[str]]\n      trade_offs: Optional[str]\n      use_cases: Optional[str]\n      version: Union[ModelStages, int, str, NoneType]\n      was_created_in_this_run: bool\n    name: Optional[str]\n    outputs:\n      output:\n        default_materializer_source:\n          attribute: Optional[str]\n          module: str\n          type: SourceType\n      materializer_source: Optional[Tuple[Source, ...]]\n    parameters: {}\n    settings:\n      docker:\n        apt_packages: List[str]\n        build_context_root: Optional[str]\n        build_options: Mapping[str, Any]\n        copy_files: bool\n        copy_global_config: bool\n        dockerfile: Optional[str]\n        dockerignore: Optional[str]\n        environment: Mapping[str, Any]\n        install_stack_requirements: bool\n        parent_image: Optional[str]\n        python_package_installer: PythonPackageInstaller\n        replicate_local_python_environment: Union[List[str], PythonEnvironmentExportMethod,\n         NoneType]\n        required_integrations: List[str]\n        requirements: Union[NoneType, str, List[str]]\n        skip_build: bool\n        prevent_build_reuse: bool\n        allow_including_files_in_images: bool\n        allow_download_from_code_repository: bool\n        allow_download_from_artifact_store: bool\n        target_repository: str\n        user: Optional[str]\n      resources:\n        cpu_count: Optional[PositiveFloat]\n        gpu_count: Optional[NonNegativeInt]\n        memory: Optional[ConstrainedStrValue]\n    step_operator: Optional[str]\n    success_hook_source:\n      attribute: Optional[str]\n      module: str\n      type: SourceType\n  train_model:\n    enable_artifact_metadata: Optional[bool]\n    enable_artifact_visualization: Optional[bool]\n    enable_cache: Optional[bool]\n    enable_step_logs: Optional[bool]\n    experiment_tracker: Optional[str]\n    extra: Mapping[str, Any]\n    failure_hook_source:\n      attribute: Optional[str]\n      module: str\n      type: SourceType\n    model:\n      audience: Optional[str]\n      description: Optional[str]\n      ethics: Optional[str]\n      license: Optional"}
{"input": "[str]\n      limitations: Optional[str]\n      name: str\n      save_models_to_registry: bool\n      suppress_class_validation_warnings: bool\n      tags: Optional[List[str]]\n      trade_offs: Optional[str]\n      use_cases: Optional[str]\n      version: Union[ModelStages, int, str, NoneType]\n      was_created_in_this_run: bool\n    name: Optional[str]\n    outputs: {}\n    parameters: {}\n    settings:\n      docker:\n        apt_packages: List[str]\n        build_context_root: Optional[str]\n        build_options: Mapping[str, Any]\n        copy_files: bool\n        copy_global_config: bool\n        dockerfile: Optional[str]\n        dockerignore: Optional[str]\n        environment: Mapping[str, Any]\n        install_stack_requirements: bool\n        parent_image: Optional[str]\n        python_package_installer: PythonPackageInstaller\n        replicate_local_python_environment: Union[List[str], PythonEnvironmentExportMethod,\n         NoneType]\n        required_integrations: List[str]\n        requirements: Union[NoneType, str, List[str]]\n        skip_build: bool\n        prevent_build_reuse: bool\n        allow_including_files_in_images: bool\n        allow_download_from_code_repository: bool\n        allow_download_from_artifact_store: bool\n        target_repository: str\n        user: Optional[str]\n      resources:\n        cpu_count: Optional[PositiveFloat]\n        gpu_count: Optional[NonNegativeInt]\n        memory: Optional[ConstrainedStrValue]\n    step_operator: Optional[str]\n    success_hook_source:\n      attribute: Optional[str]\n      module: str\n      type: SourceType\n\n```\n\n</details>\n\n{% hint style=\"info\" %}\nWhen you want to configure your pipeline with a certain stack in mind, you can do so as well:\\\n\\`...write\\_run\\_configuration\\_template(stack=\\<Insert\\_stack\\_here>)\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Creating and running templates\n---\n\n{% hint style=\"warning\" %}\nThis is a [ZenML Pro](https://zenml.io/pro) only feature. Please \n[sign up here](https://cloud.zenml.io) get access.\n{% endhint %}\n\nRun templates allow you to use the dashboard or our Client/REST API to run a pipeline with updated configuration\nwhich allows you to iterate quickly with minimal friction. \n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Create a run template</td><td></td><td></td><td><a href=\"create-a-run-template.md\">create-a-run-template.md</a></td></tr><tr><td>Run a template</td><td></td><td></td><td><a href=\"run-a-template.md\">run-a-template.md</a></td></tr></tbody></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Create run templates for running pipelines from the server\n---\n\n# Create a run template from a previous pipeline run\n\n{% hint style=\"warning\" %}\nThis is a [ZenML Pro](https://zenml.io/pro) only feature. Please \n[sign up here](https://cloud.zenml.io) get access.\n\nThe creation of a run template from a pipeline run **only** \nworks for runs that were executed on a remote stack (i.e. at least a remote \norchestrator, artifact store, and container registry).\n{% endhint %}\n\n<!-- ## Create a template from the dashboard -->\n\n## Create a template in code\n\n```python\nfrom zenml.client import Client\n\nrun = Client().get_pipeline_run(<RUN_NAME_OR_ID>)\nClient().create_run_template(\n    name=<NAME>,\n    deployment_id=run.deployment_id\n)\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Run a template\n---\n\n# Run a template\n\n{% hint style=\"warning\" %}\nThis is a [ZenML Pro](https://zenml.io/pro) only feature. Please \n[sign up here](https://cloud.zenml.io) get access.\n{% endhint %}\n\n<!-- ## Run a template from the dashboard -->\n\n## Run a template in code\n\n```python\nfrom zenml.client import Client\n\ntemplate = Client().get_run_template(<NAME_OR_ID>)\nconfig = template.config_template\n# Optionally modify the config here\n\nClient().trigger_pipeline(\n    template_id=template.id,\n    run_configuration=config,\n)\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: How to handle issues with conflicting dependencies\n---\n\n# Handling dependencies\n\nThis page documents a some of the common issues that arise when using ZenML with other libraries.\n\nWhen using ZenML with other libraries, you may encounter issues with conflicting dependencies. ZenML aims to be stack- and integration-agnostic, allowing you to run your pipelines using the tools that make sense for your problems. With this flexibility comes the possibility of dependency conflicts.\n\nZenML allows you to install dependencies required by integrations through the `zenml integration install ...` command. This is a convenient way to install dependencies for a specific integration, but it can also lead to dependency conflicts if you are using other libraries in your environment. An easy way to see if the ZenML requirements are still met (after installing any extra dependencies required by your work) by running `zenml integration list` and checking that your desired integrations still bear the green tick symbol denoting that all requirements are met.\n\n## Suggestions for Resolving Dependency Conflicts\n\n### Use a tool like `pip-compile` for reproducibility\n\nConsider using a tool like `pip-compile` (available through [the `pip-tools`\npackage](https://pip-tools.readthedocs.io/)) to compile your dependencies into a\nstatic `requirements.txt` file that can be used across environments. (If you are\nusing [`uv`](https://github.com/astral-sh/uv), you might want to use `uv pip compile` as an alternative.)\n\nFor a practical example and explanation of using `pip-compile` to address exactly this need, see [our 'gitflow' repository and workflow](https://github.com/zenml-io/zenml-gitflow#-software-requirements-management) to learn more.\n\n### Use `pip check` to discover dependency conflicts\n\nRunning [`pip check`](https://pip.pypa.io/en/stable/cli/pip\\_check/) will verify that your environment's dependencies are compatible with one another. If not, you will see a list of the conflicts. This may or may not be a problem or something that will prevent you from moving forward with your specific use case, but it is certainly worth being aware of whether this is the case.\n\n### Well-known dependency resolution issues\n\nSome of ZenML's integrations come with strict dependency and package version\nrequirements. We try to keep these dependency requirements ranges as wide as\npossible for the integrations developed by ZenML, but it is not always possible\nto make this work completely smoothly. Here is one of the known issues:\n\n* `click`: ZenML currently requires `click~=8.0.3` for its CLI. This is on account of another dependency of ZenML. Using versions of `click` in your own project that are greater than 8.0.3 may cause unanticipated behaviors.\n\n### Manually bypassing ZenML's integration installation\n\nIt is possible to skip ZenML's integration installation process and"}
{"input": " install dependencies manually. This is not recommended, but it is possible and can be run at your own risk.\n\n{% hint style=\"info\" %}\nNote that the `zenml integration install ...` command runs a `pip install ...` under the hood as part of its implementation, taking the dependencies listed in the integration object and installing them. For example, `zenml integration install gcp` will run `pip install \"kfp==1.8.16\" \"gcsfs\" \"google-cloud-secret-manager\" ...` and so on, since they are [specified in the integration definition](https://github.com/zenml-io/zenml/blob/ec2283473e5e0c5a2f1b7868875539a83e617f8c/src/zenml/integrations/gcp/\\_\\_init\\_\\_.py#L45).\n{% endhint %}\n\nTo do this, you will need to install the dependencies for the integration you\nwant to use manually. You can find the dependencies for the integrations by\nrunning the following:\n\n```bash\n# to have the requirements exported to a file\nzenml integration export-requirements --output-file integration-requirements.txt INTEGRATION_NAME\n\n# to have the requirements printed to the console\nzenml integration export-requirements INTEGRATION_NAME\n```\n\nYou can then amend and tweak those requirements as you see fit. Note that if you\nare using a remote orchestrator, you would then have to place the updated\nversions for the dependencies in a `DockerSettings` object (described in detail\n[here](../customize-docker-builds/docker-settings-on-a-pipeline.md))\nwhich will then make sure everything is working as you need.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Navigating multiple development environments.\n---\n\n# \ud83d\udc0d Configure python environments\n\nZenML deployments often involve multiple environments. This guide helps you manage dependencies and configurations across these environments.\n\nHere is a visual overview of the different environments:\n\n<figure><img src=\"../../.gitbook/assets/SystemArchitecture.png\" alt=\"\"><figcaption><p>Left box is the client environment, middle is the zenml server environment, and the right most contains the build environments</p></figcaption></figure>\n\n## Client Environment (or the Runner environment)\n\nThe client environment (sometimes known as the runner environment) is where the ZenML pipelines are _compiled_, i.e., where you call the pipeline function (typically in a `run.py` script). There are different types of client environments:\n\n* A local development environment\n* A CI runner in production.\n* A [ZenML Pro](https://zenml.io/pro) runner.\n* A `runner` image orchestrated by the ZenML server to start pipelines.\n\nIn all the environments, you should use your preferred package manager (e.g., `pip` or `poetry`) to manage dependencies. Ensure you install the ZenML package and any required [integrations](../../component-guide/README.md).\n\nThe client environment typically follows these key steps when starting a pipeline:\n\n1. Compiling an intermediate pipeline representation via the `@pipeline` function.\n2. Creating or triggering [pipeline and step build environments](../../component-guide/image-builders/image-builders.md) if running remotely.\n3. Triggering a run in the [orchestrator](../../component-guide/orchestrators/orchestrators.md).\n\nPlease note that the `@pipeline` function in your code is **only ever called** in this environment. Therefore, any computational logic that is executed in the pipeline function needs to be relevant to this so-called _compile time_, rather than at _execution_ time, which happens later.\n\n## ZenML Server Environment\n\nThe ZenML server environment is a FastAPI application managing pipelines and metadata. It includes the ZenML Dashboard and is accessed when you [deploy ZenML](../../getting-started/deploying-zenml/README.md). To manage dependencies, install them during [ZenML deployment](../../getting-started/deploying-zenml/README.md), but only if you have custom integrations, as most are built-in.\n\nSee also [here](./configure-the-server-environment.md) for more on [configuring the server environment](./configure-the-server-environment.md).\n\n## Execution Environments\n\nWhen running locally, there is no real concept of an `execution` environment as the client, server, and execution environment are all the same. However, when running a pipeline remotely, ZenML needs to transfer your code and environment over to the remote [orchestrator](../../component-guide/orchestrators/orchestrators.md). In order to achieve this, ZenML builds Docker images known as `"}
{"input": "execution environments`.\n\nZenML handles the Docker image configuration, creation, and pushing, starting with a [base image](https://hub.docker.com/r/zenmldocker/zenml) containing ZenML and Python, then adding pipeline dependencies. To manage the Docker image configuration, follow the steps in the [containerize your pipeline](../../how-to/customize-docker-builds/README.md) guide, including specifying additional pip dependencies, using a custom parent image, and customizing the build process.\n\n## Image Builder Environment\n\nBy default, execution environments are created locally in the [client environment](#client-environment-or-the-runner-environment) using the local Docker client. However, this requires Docker installation and permissions. ZenML offers [image builders](../../component-guide/image-builders/image-builders.md), a special [stack component](../../component-guide/README.md), allowing users to build and push Docker images in a different specialized _image builder environment_.\n\nNote that even if you don't configure an image builder in your stack, ZenML still uses the [local image builder](../../component-guide/image-builders/local.md) to retain consistency across all builds. In this case, the image builder environment is the same as the client environment.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to configure the server environment\n---\n\n# Configure the server environment\n\nThe ZenML server environment is configured using environment variables. You will\nneed to set these before deploying your server instance. Please refer to [the full list of environment\nvariables](../../reference/environment-variables.md) available to you\n[here](../../reference/environment-variables.md).\n\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Understand how ZenML stores your data under-the-hood.\n---\n\n# How ZenML stores data\n\nZenML seamlessly integrates data versioning and lineage into its core functionality. When a pipeline is executed, each run generates automatically tracked and managed artifacts. One can easily view the entire lineage of how artifacts are created and interact with them. The dashboard is also a way to interact with the artifacts produced by different pipeline runs. ZenML's artifact management, caching, lineage tracking, and visualization capabilities can help gain valuable insights, streamline the experimentation process, and ensure the reproducibility and reliability of machine learning workflows.\n\n## Artifact Creation and Caching\n\nEach time a ZenML pipeline runs, the system first checks if there have been any changes in the inputs, outputs, parameters, or configuration of the pipeline steps. Each step in a run gets a new directory in the artifact store:\n\n![Artifact Stores Under the Hood](../../.gitbook/assets/zenml_artifact_store_underthehood_1.png)\n\nSuppose a step is new or has been modified. In that case, ZenML creates a new directory structure in the [Artifact Store](../../component-guide/artifact-stores/artifact-stores.md) with a unique ID and stores the data using the appropriate materializers in this directory.\n\n![Artifact Stores Under the Hood](../../.gitbook/assets/zenml_artifact_store_underthehood_2.png)\n\nOn the other hand, if the step remains unchanged, ZenML intelligently decides whether to cache the step or not. By caching steps that have not been modified, ZenML can save [valuable time and computational resources](../../user-guide/starter-guide/cache-previous-executions.md), allowing you to focus on experimenting with different configurations and improving your machine-learning models without the need to rerun unchanged parts of your pipeline.\n\nWith ZenML, you can easily trace an artifact back to its origins and understand the exact sequence of executions that led to its creation, such as a trained model. This feature enables you to gain insights into the entire lineage of your artifacts, providing a clear understanding of how your data has been processed and transformed throughout your machine-learning pipelines. With ZenML, you can ensure the reproducibility of your results, and identify potential issues or bottlenecks in your pipelines. This level of transparency and traceability is essential for maintaining the reliability and trustworthiness of machine learning projects, especially when working in a team or across different environments.\n\nFor more details on how to adjust the names or versions assigned to your artifacts, assign tags to them, or adjust other artifact properties, see the [documentation on artifact versioning and configuration](../../user-guide/starter-guide/manage-artifacts.md).\n\nBy tracking the lineage of artifacts across environments and stacks, ZenML enables ML engineers to reproduce results and understand the exact steps taken to create a model. This is crucial for ensuring the reliability and reproducibility of machine learning models, especially when working in a team or"}
{"input": " across different environments.\n\n## Saving and Loading Artifacts with Materializers\n\n[Materializers](handle-custom-data-types.md) play a crucial role in ZenML's artifact management system. They are responsible for handling the serialization and deserialization of artifacts, ensuring that data is consistently stored and retrieved from the [artifact store](../../component-guide/artifact-stores/artifact-stores.md). Each materializer stores data flowing through a pipeline in one or more files within a unique directory in the artifact store:\n\n![Visualizing artifacts](../../.gitbook/assets/zenml_artifact_store_underthehood_3.png)\n\nMaterializers are designed to be extensible and customizable, allowing you to define your own serialization and deserialization logic for specific data types or storage systems. By default, ZenML provides built-in materializers for common data types and uses `cloudpickle` to pickle objects where there is no default materializer. If you want direct control over how objects are serialized, you can easily create custom materializers by extending the `BaseMaterializer` class and implementing the required methods for your specific use case. Read more about materializers [here](handle-custom-data-types.md).\n\n{% hint style=\"warning\" %}\nZenML provides a built-in [CloudpickleMaterializer](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-materializers/#zenml.materializers.cloudpickle\\_materializer.CloudpickleMaterializer) that can handle any object by saving it with [cloudpickle](https://github.com/cloudpipe/cloudpickle). However, this is not production-ready because the resulting artifacts cannot be loaded when running with a different Python version. In such cases, you should consider building a [custom Materializer](handle-custom-data-types.md#custom-materializers) to save your objects in a more robust and efficient format.\n\nMoreover, using the `CloudpickleMaterializer` could allow users to upload of any kind of object. This could be exploited to upload a malicious file, which could execute arbitrary code on the vulnerable system.\n{% endhint %}\n\nWhen a pipeline runs, ZenML uses the appropriate materializers to save and load artifacts using the ZenML `fileio` system (built to work across multiple artifact stores). This not only simplifies the process of working with different data formats and storage systems but also enables artifact caching and lineage tracking. You can see an example of a default materializer (the `numpy` materializer) in action [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/numpy\\_materializer.py).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Skip materialization of artifacts.\n---\n\n# Unmaterialized artifacts\n\nA ZenML pipeline is built in a data-centric way. The outputs and inputs of steps define how steps are connected and the order in which they are executed. Each step should be considered as its very own process that reads and writes its inputs and outputs from and to the [artifact store](../../component-guide/artifact-stores/artifact-stores.md). This is where **materializers** come into play.\n\nA materializer dictates how a given artifact can be written to and retrieved from the artifact store and also contains all serialization and deserialization logic. Whenever you pass artifacts as outputs from one pipeline step to other steps as inputs, the corresponding materializer for the respective data type defines how this artifact is first serialized and written to the artifact store, and then deserialized and read in the next step. Read more about this [here](handle-custom-data-types.md).\n\nHowever, there are instances where you might **not** want to materialize an artifact in a step, but rather use a reference to it instead.\nThis is where skipping materialization comes in.\n\n{% hint style=\"warning\" %}\nSkipping materialization might have unintended consequences for downstream tasks that rely on materialized artifacts. Only skip materialization if there is no other way to do what you want to do.\n{% endhint %}\n\n## How to skip materialization\n\nWhile materializers should in most cases be used to control how artifacts are returned and consumed from pipeline steps, you might sometimes need to have a completely unmaterialized artifact in a step, e.g., if you need to know the exact path to where your artifact is stored.\n\nAn unmaterialized artifact is a [`zenml.materializers.UnmaterializedArtifact`](https://sdkdocs.zenml.io/latest/core_code_docs/core-artifacts/#zenml.artifacts.unmaterialized_artifact). Among others, it has a property `uri` that points to the unique path in the artifact store where the artifact is persisted. One can use an unmaterialized artifact by specifying `UnmaterializedArtifact` as the type in the step:\n\n```python\nfrom zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact\nfrom zenml import step\n\n@step\ndef my_step(my_artifact: UnmaterializedArtifact):  # rather than pd.DataFrame\n    pass\n```\n\n## Code Example\n\nThe following shows an example of how unmaterialized artifacts can be used in the steps of a pipeline. The pipeline we define will look like this:\n\n```shell\ns1 -> s3 \ns2 -> s4\n```\n\n`s1` and `s2` produce identical artifacts, however `s3` consumes materialized artifacts while `s4` consumes unmaterialized artifacts. `s4` can now use the `dict_.uri` and `list_.uri` paths directly rather than their materialized counterparts.\n\n```python\nfrom typing_extensions import An"}
{"input": "notated  # or `from typing import Annotated on Python 3.9+\nfrom typing import Dict, List, Tuple\n\nfrom zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact\nfrom zenml import pipeline, step\n\n\n@step\ndef step_1() -> Tuple[\n    Annotated[Dict[str, str], \"dict_\"],\n    Annotated[List[str], \"list_\"],\n]:\n    return {\"some\": \"data\"}, []\n\n\n@step\ndef step_2() -> Tuple[\n    Annotated[Dict[str, str], \"dict_\"],\n    Annotated[List[str], \"list_\"],\n]:\n    return {\"some\": \"data\"}, []\n\n\n@step\ndef step_3(dict_: Dict, list_: List) -> None:\n    assert isinstance(dict_, dict)\n    assert isinstance(list_, list)\n\n\n@step\ndef step_4(\n        dict_: UnmaterializedArtifact,\n        list_: UnmaterializedArtifact,\n) -> None:\n    print(dict_.uri)\n    print(list_.uri)\n\n\n@pipeline\ndef example_pipeline():\n    step_3(*step_1())\n    step_4(*step_2())\n\n\nexample_pipeline()\n```\n\nYou can see another example of using an `UnmaterializedArtifact` when triggering a [pipeline from another](../trigger-pipelines/trigger-a-pipeline-from-another.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Use Annotated to return multiple outputs from a step and name them for easy retrieval and dashboard display.\n---\n\n# Return multiple outputs from a step\n\nYou can use the `Annotated` type to return multiple outputs from a step and give each output a name. Naming your step outputs will help you retrieve the specific artifact later and also improves the readability of your pipeline's dashboard.\n\n```python\nfrom typing import Annotated, Tuple\n\nimport pandas as pd\nfrom zenml import step\n\n\n@step\ndef clean_data(\n    data: pd.DataFrame,\n) -> Tuple[\n    Annotated[pd.DataFrame, \"x_train\"],\n    Annotated[pd.DataFrame, \"x_test\"],\n    Annotated[pd.Series, \"y_train\"],\n    Annotated[pd.Series, \"y_test\"],\n]:\n    from sklearn.model_selection import train_test_split\n\n    x = data.drop(\"target\", axis=1)\n    y = data[\"target\"]\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    return x_train, x_test, y_train, y_test\n```\n\nIn this code, the `clean_data` step takes a pandas DataFrame as input and returns a tuple of four elements: `x_train`, `x_test`, `y_train`, and `y_test`. Each element in the tuple is annotated with a specific name using the `Annotated` type.\n\nInside the step, we split the input data into features (`x`) and target (`y`), and then use `train_test_split` from scikit-learn to split the data into training and testing sets. The resulting DataFrames and Series are returned as a tuple, with each element annotated with its respective name.\n\nBy using `Annotated`, we can easily identify and retrieve specific artifacts later in the pipeline. Additionally, the names will be displayed on the pipeline's dashboard, making it more readable and understandable.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Use tags to organize tags in ZenML.\n---\n\n# Organizing data with tags\n\nOrganizing and categorizing your machine learning artifacts and models can\nstreamline your workflow and enhance discoverability. ZenML enables the use of\ntags as a flexible tool to classify and filter your ML assets. In this guide,\nwe'll demonstrate how to assign tags to both artifacts and models within the\nZenML ecosystem.\n\n![Tags are visible in the ZenML Dashboard](../../.gitbook/assets/tags-in-dashboard.png)\n\n## Assigning tags to artifacts\n\nIf you want to tag the artifact versions of a step or pipeline that is executed\nrepeatedly, you can use the `tags` property of `ArtifactConfig` to assign an arbitrary number of tags to the created artifacts:\n\n{% tabs %}\n{% tab title=\"Python SDK\" %}\n\n```python\nfrom zenml import step, ArtifactConfig\n\n@step\ndef training_data_loader() -> (\n    Annotated[pd.DataFrame, ArtifactConfig(tags=[\"sklearn\", \"pre-training\"])]\n):\n    ...\n```\n\n{% endtab %}\n{% tab title=\"CLI\" %}\nYou can use the `zenml artifacts` CLI to add tags:\n\n```shell\n# Tag the artifact\nzenml artifacts update iris_dataset -t sklearn\n\n# Tag the artifact version\nzenml artifacts versions update iris_dataset raw_2023 -t sklearn\n```\n\n{% endtab %}\n{% endtabs %}\n\nThis will assign tags `sklearn` and `pre-training` to all artifacts created by\nthis step, which can later be used to filter and organize these artifacts.\n\nNote that [ZenML Pro](https://zenml.io/pro) users can tag artifacts directly in the cloud dashboard.\n\n## Assigning tags to models\n\nJust like artifacts, you can also tag your models to organize them semantically. Here's how to use tags with models in the ZenML Python SDK and CLI (or in the [ZenML Pro Dashboard directly](https://zenml.io/pro)).\n\nWhen creating a model using the `Model` object, you can specify tags as key-value pairs that will be attached to the model upon creation:\n\n```python\nfrom zenml.models import Model\n\n# Define tags to be added to the model\ntags = [\"experiment\", \"v1\", \"classification-task\"]\n\n# Create a model with tags\nmodel = Model(\n    name=\"iris_classifier\",\n    version=\"1.0.0\",\n    tags=tags,\n)\n\n# Use this tagged model in your steps and pipelines as needed\n@pipeline(model=model)\ndef my_pipeline(...):\n    ...\n```\n\nYou can also assign tags when creating or updating models with the Python SDK:\n\n```python\nfrom zenml.models import Model\nfrom zenml.client import Client\n\n# Create or register a new model with tags\nClient().create_model(\n    name=\"iris_logistic_regression\",\n    tags=[\"classification\", \"iris-dataset\"],\n)\n\n# Create or register a new"}
{"input": " model version also with tags\nClient().create_model_version(\n    model_name_or_id=\"iris_logistic_regression\",\n    name=\"2\",\n    tags=[\"version-1\", \"experiment-42\"],\n)\n```\n\nTo add tags to existing models and their versions using the ZenML CLI, you can use the following commands:\n\n```shell\n# Tag an existing model\nzenml model update iris_logistic_regression --tag \"classification\"\n\n# Tag a specific model version\nzenml model version update iris_logistic_regression 2 --tag \"experiment3\"\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Not all artifacts need to come through the step interface from direct upstream\n  steps.\n---\n\n# Get arbitrary artifacts in a step\n\nAs described in [the metadata guide](../track-metrics-metadata/logging-metadata.md), the metadata can be fetched with the client, and this is how you would use it to fetch it within a step. This allows you to fetch artifacts from other upstream steps or even completely different pipelines.\n\n```python\nfrom zenml.client import Client\nfrom zenml import step\n\n@step\ndef my_step():\n    client = Client()\n    # Directly fetch an artifact\n    output = client.get_artifact_version(\"my_dataset\", \"my_version\")\n    output.run_metadata[\"accuracy\"].value\n```\n\nThis is one of the ways you can access artifacts that have already been created\nand stored in the artifact store. This can be useful when you want to use\nartifacts from other pipelines or steps that are not directly upstream.\n\n## See Also\n\n- [Managing artifacts](../../user-guide/starter-guide/manage-artifacts.md) -\n  learn about the `ExternalArtifact` type and how to pass artifacts between steps.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "# Load artifacts into memory\n\nOften ZenML pipeline steps consume artifacts produced by one another directly in the pipeline code, but there are scenarios where you need to pull external data into your steps. Such external data could be artifacts produced by non-ZenML codes. For those cases, it is advised to use [ExternalArtifact](../../user-guide/starter-guide/manage-artifacts.md#consuming-external-artifacts-within-a-pipeline), but what if we plan to exchange data created with other ZenML pipelines?\n\nZenML pipelines are first compiled and only executed at some later point. During the compilation phase, all function calls are executed, and this data is fixed as step input parameters. Given all this, the late materialization of dynamic objects, like data artifacts, is crucial. Without late materialization, it would not be possible to pass not-yet-existing artifacts as step inputs, or their metadata, which is often the case in a multi-pipeline setting.\n\nWe identify two major use cases for exchanging artifacts between pipelines:\n\n* You semantically group your data products using ZenML Models\n* You prefer to use [ZenML Client](../../reference/python-client.md#client-methods) to bring all the pieces together\n\n{% hint style=\"warning\" %}\nWe recommend using models to group and access artifacts across pipelines. Find out how to load an artifact from a ZenML Model [here](../use-the-model-control-plane/load-artifacts-from-model.md).\n{% endhint %}\n\n## Use client methods to exchange artifacts\n\nIf you don't yet use the Model Control Plane, you can still exchange data between pipelines with late materialization. Let's rework the `do_predictions` pipeline code as follows:\n\n```python\nfrom typing import Annotated\nfrom zenml import step, pipeline\nfrom zenml.client import Client\nimport pandas as pd\nfrom sklearn.base import ClassifierMixin\n\n\n@step\ndef predict(\n    model1: ClassifierMixin,\n    model2: ClassifierMixin,\n    model1_metric: float,\n    model2_metric: float,\n    data: pd.DataFrame,\n) -> Annotated[pd.Series, \"predictions\"]:\n    # compare which model performs better on the fly\n    if model1_metric < model2_metric:\n        predictions = pd.Series(model1.predict(data))\n    else:\n        predictions = pd.Series(model2.predict(data))\n    return predictions\n\n@step\ndef load_data() -> pd.DataFrame:\n    # load inference data\n    ...\n\n@pipeline\ndef do_predictions():\n    # get specific artifact version\n    model_42 = Client().get_artifact_version(\"trained_model\", version=\"42\")\n    metric_42 = model_42.run_metadata[\"MSE\"].value\n\n    # get latest artifact version\n    model_latest = Client().get_artifact_version(\"trained_model\")\n    metric_latest = model_latest.run_metadata[\"MSE\"].value\n\n    inference_data = load_data()\n    predict(\n        model1=model_42,\n        model"}
{"input": "2=model_latest,\n        model1_metric=metric_42,\n        model2_metric=metric_latest,\n        data=inference_data,\n    )\n\nif __name__ == \"__main__\":\n    do_predictions()\n```\n\nHere, we enriched the `predict` step logic with a metric comparison by MSE metric, so predictions are done on the best possible model. We also added a `load_data` step to load the inference data.\n\nAs before, calls like `Client().get_artifact_version(\"trained_model\", version=\"42\")` or `model_latest.run_metadata[\"MSE\"].value` are not evaluating the actual objects behind them at pipeline compilation time. Rather, they do so only at the point of step execution. By doing so, we ensure that the latest version is actually the latest at the moment and not just the latest at the point of pipeline compilation.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Step outputs in ZenML are stored in the artifact store. This enables caching,\n  lineage and auditability. Using type annotations helps with transparency, \n  passing data between steps, and serializing/deserializing the data.\n---\n\n# \ud83d\uddc4\ufe0f Handle Data/Artifacts\n\nFor best results, use type annotations for your outputs. This is good coding practice for transparency, helps ZenML handle passing data between steps, and also enables ZenML to serialize and deserialize (referred to as 'materialize' in ZenML) the data.\n\n```python\n@step\ndef load_data(parameter: int) -> Dict[str, Any]:\n\n    # do something with the parameter here\n\n    training_data = [[1, 2], [3, 4], [5, 6]]\n    labels = [0, 1, 0]\n    return {'features': training_data, 'labels': labels}\n\n@step\ndef train_model(data: Dict[str, Any]) -> None:\n    total_features = sum(map(sum, data['features']))\n    total_labels = sum(data['labels'])\n    \n    # Train some model here\n    \n    print(f\"Trained model using {len(data['features'])} data points. \"\n          f\"Feature sum is {total_features}, label sum is {total_labels}\")\n\n\n@pipeline  \ndef simple_ml_pipeline(parameter: int):\n    dataset = load_data(parameter=parameter)  # Get the output \n    train_model(dataset)  # Pipe the previous step output into the downstream step\n```\n\nIn this code, we define two steps: `load_data` and `train_model`. The `load_data` step takes an integer parameter and returns a dictionary containing training data and labels. The `train_model` step receives the dictionary from `load_data`, extracts the features and labels, and trains a model (not shown here).\n\nFinally, we define a pipeline `simple_ml_pipeline` that chains the `load_data`\nand `train_model` steps together. The output from `load_data` is passed as input\nto `train_model`, demonstrating how data flows between steps in a ZenML\npipeline.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Learn how to delete artifacts.\n---\n\n# Delete an artifact\n\nThere is currently no way to delete an artifact directly, because it may lead to\na broken state of the ZenML database (dangling references to pipeline runs that produce artifacts).\n\nHowever, it is possible to delete artifacts that are no longer referenced by any pipeline runs:\n\n```shell\nzenml artifact prune\n```\n\nBy default, this method deletes artifacts physically from the underlying [artifact store](../../component-guide/artifact-stores/artifact-stores.md)\nAND also the entry in the database. You can control this behavior by using the\n`--only-artifact` and `--only-metadata` flags.\n\nYou might find that some artifacts throw errors when you try to prune them,\nlikely because they were stored locally and no longer exist. If you wish to\ncontinue pruning and to ignore these errors, please add the `--ignore-errors`\nflag. Warning messages will still be output to the terminal during this process.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Configuring ZenML to display data visualizations in the dashboard.\n---\n\n# Visualize artifacts\n\nZenML automatically saves visualizations of many common data types and allows you to view these visualizations in the ZenML dashboard:\n\n![ZenML Artifact Visualizations](../../.gitbook/assets/artifact_visualization_dashboard.png)\n\nAlternatively, any of these visualizations can also be displayed in Jupyter notebooks using the `artifact.visualize()` method:\n\n![output.visualize() Output](../../.gitbook/assets/artifact_visualization_evidently.png)\n\nCurrently, the following visualization types are supported:\n\n* **HTML:** Embedded HTML visualizations such as data validation reports,\n* **Image:** Visualizations of image data such as Pillow images or certain numeric numpy arrays,\n* **CSV:** Tables, such as the pandas DataFrame `.describe()` output,\n* **Markdown:** Markdown strings or pages.\n\n## Giving the ZenML Server Access to Visualizations\n\nIn order for the visualizations to show up on the dashboard, the following must be true:\n\n### Configuring a Service Connector\n\nVisualizations are usually stored alongside the artifact, in the [artifact store](../../component-guide/artifact-stores/artifact-stores.md). Therefore, if a user would like to see the visualization displayed on the ZenML dashboard, they must give access to the server to connect to the artifact store.\n\nThe [service connector](../auth-management/) documentation goes deeper into the concept of service connectors and how they can be configured to give the server permission to access the artifact store. For a concrete example, see the [AWS S3](../../component-guide/artifact-stores/s3.md) artifact store documentation.\n\n{% hint style=\"info\" %}\nWhen using the default/local artifact store with a deployed ZenML, the server naturally does not have access to your local files. In this case, the visualizations are also not displayed on the dashboard.\n\nPlease use a service connector enabled and remote artifact store alongside a deployed ZenML to view visualizations.\n{% endhint %}\n\n### Configuring Artifact Stores\n\nIf all visualizations of a certain pipeline run are not showing up in the dashboard, it might be that your ZenML server does not have the required dependencies or permissions to access that artifact store. See the [custom artifact store docs page](../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores) for more information.\n\n## Creating Custom Visualizations\n\nThere are two ways how you can add custom visualizations to the dashboard:\n\n* If you are already handling HTML, Markdown, or CSV data in one of your steps, you can have them visualized in just a few lines of code by casting them to a [special class](visualize-artifacts.md#visualization-via-special-return-types) inside your step.\n* If you want to automatically extract visualizations for all artifacts of a certain data type, you can define type-specific visualization logic by [building a custom"}
{"input": " materializer](visualize-artifacts.md#visualization-via-materializers).\n* If you want to create any other custom visualizations, you can [create a custom return type class with corresponding materializer](visualize-artifacts.md#visualization-via-custom-return-type-and-materializer) and build and return this custom return type from one of your steps.\n\n### Visualization via Special Return Types\n\nIf you already have HTML, Markdown, or CSV data available as a string inside your step, you can simply cast them to one of the following types and return them from your step:\n\n* `zenml.types.HTMLString` for strings in HTML format, e.g., `\"<h1>Header</h1>Some text\"`,\n* `zenml.types.MarkdownString` for strings in Markdown format, e.g., `\"# Header\\nSome text\"`,\n* `zenml.types.CSVString` for strings in CSV format, e.g., `\"a,b,c\\n1,2,3\"`.\n\n#### Example:\n\n```python\nfrom zenml.types import CSVString\n\n@step\ndef my_step() -> CSVString:\n    some_csv = \"a,b,c\\n1,2,3\"\n    return CSVString(some_csv)\n```\n\nThis would create the following visualization in the dashboard:\n\n![CSV Visualization Example](../../.gitbook/assets/artifact\\_visualization\\_csv.png)\n\n### Visualization via Materializers\n\nIf you want to automatically extract visualizations for all artifacts of a certain data type, you can do so by overriding the `save_visualizations()` method of the corresponding materializer. See the [materializer docs page](handle-custom-data-types.md#optional-how-to-visualize-the-artifact) for more information on how to create custom materializers that do this.\n\n### Visualization via Custom Return Type and Materializer\n\nBy combining the ideas behind the above two visualization approaches, you can visualize virtually anything you want inside your ZenML dashboard in three simple steps:\n\n1. Create a **custom class** that will hold the data that you want to visualize.\n2. [Build a custom **materializer**](handle-custom-data-types.md#custom-materializers) for this custom class with the visualization logic implemented in the `save_visualizations()` method.\n3. Return your custom class from any of your ZenML steps.\n\n#### Example: Facets Data Skew Visualization\n\nAs an example, have a look at the models, materializers, and steps of the [Facets Integration](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-facets), which can be used to visualize the data skew between multiple Pandas DataFrames:\n\n![Facets Visualization](../../.gitbook/assets/facets-visualization.png)\n\n**1. Custom Class** The [FacetsComparison](https://sdkdocs.zenml.io/0.42.0/integration\\_code\\_docs/integrations-facets/#zenml.int"}
{"input": "egrations.facets.models.FacetsComparison) is the custom class that holds the data required for the visualization.\n\n```python\nclass FacetsComparison(BaseModel):\n    datasets: List[Dict[str, Union[str, pd.DataFrame]]]\n```\n\n**2. Materializer** The [FacetsMaterializer](https://sdkdocs.zenml.io/0.42.0/integration\\_code\\_docs/integrations-facets/#zenml.integrations.facets.materializers.facets\\_materializer.FacetsMaterializer) is a custom materializer that only handles this custom class and contains the corresponding visualization logic.\n\n```python\nclass FacetsMaterializer(BaseMaterializer):\n\n    ASSOCIATED_TYPES = (FacetsComparison,)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS\n\n    def save_visualizations(\n        self, data: FacetsComparison\n    ) -> Dict[str, VisualizationType]:\n        html = ...  # Create a visualization for the custom type \n        visualization_path = os.path.join(self.uri, VISUALIZATION_FILENAME)\n        with fileio.open(visualization_path, \"w\") as f:\n            f.write(html)\n        return {visualization_path: VisualizationType.HTML}\n```\n\n**3. Step** There are three different steps in the `facets` integration that can be used to create `FacetsComparison`s for different sets of inputs. E.g., the `facets_visualization_step` below takes two DataFrames as inputs and builds a `FacetsComparison` object out of them:\n\n```python\n@step\ndef facets_visualization_step(\n    reference: pd.DataFrame, comparison: pd.DataFrame\n) -> FacetsComparison:  # Return the custom type from your step\n    return FacetsComparison(\n        datasets=[\n            {\"name\": \"reference\", \"table\": reference},\n            {\"name\": \"comparison\", \"table\": comparison},\n        ]\n    )\n```\n\n{% hint style=\"info\" %}\nThis is what happens now under the hood when you add the `facets_visualization_step` into your pipeline:\n\n1. The step creates and returns a `FacetsComparison`.\n2. When the step finishes, ZenML will search for a materializer class that can handle this type, finds the `FacetsMaterializer`, and calls the `save_visualizations()` method which creates the visualization and saves it into your artifact store as an HTML file.\n3. When you open your dashboard and click on the artifact inside the run DAG, the visualization HTML file is loaded from the artifact store and displayed.\n{% endhint %}\n\n## Disabling Visualizations\n\nIf you would like to disable artifact visualization altogether, you can set `enable_artifact_visualization` at either pipeline or step level:\n\n```python\n@step(enable_artifact_visualization=False)\ndef my_step():\n    ...\n\n@pipeline(enable_artifact_visualization=False)\ndef my_pipeline():\n    ...\n```\n\n<figure><img src"}
{"input": "=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Using materializers to pass custom data types through steps.\n---\n\n# Handle custom data types\n\nA ZenML pipeline is built in a data-centric way. The outputs and inputs of steps define how steps are connected and the order in which they are executed. Each step should be considered as its very own process that reads and writes its inputs and outputs from and to the [artifact store](../../component-guide/artifact-stores/artifact-stores.md). This is where **materializers** come into play.\n\nA materializer dictates how a given artifact can be written to and retrieved from the artifact store and also contains all serialization and deserialization logic. Whenever you pass artifacts as outputs from one pipeline step to other steps as inputs, the corresponding materializer for the respective data type defines how this artifact is first serialized and written to the artifact store, and then deserialized and read in the next step.\n\n## Built-In Materializers\n\nZenML already includes built-in materializers for many common data types. These are always enabled and are used in the background without requiring any user interaction / activation:\n\n<table data-full-width=\"true\"><thead><tr><th>Materializer</th><th>Handled Data Types</th><th>Storage Format</th></tr></thead><tbody><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer\">BuiltInMaterializer</a></td><td><code>bool</code>, <code>float</code>, <code>int</code>, <code>str</code>, <code>None</code></td><td><code>.json</code></td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.built_in_materializer.BytesMaterializer\">BytesInMaterializer</a></td><td><code>bytes</code></td><td><code>.txt</code></td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.built_in_materializer.BuiltInContainerMaterializer\">BuiltInContainerMaterializer</a></td><td><code>dict</code>, <code>list</code>, <code>set</code>, <code>tuple</code></td><td>Directory</td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer\">NumpyMaterializer</a></td><td><code>np.ndarray</code></td><td><code>.npy</code></td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers"}
{"input": "/#zenml.materializers.pandas_materializer.PandasMaterializer\">PandasMaterializer</a></td><td><code>pd.DataFrame</code>, <code>pd.Series</code></td><td><code>.csv</code> (or <code>.gzip</code> if <code>parquet</code> is installed)</td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.pydantic_materializer.PydanticMaterializer\">PydanticMaterializer</a></td><td><code>pydantic.BaseModel</code></td><td><code>.json</code></td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.service_materializer.ServiceMaterializer\">ServiceMaterializer</a></td><td><code>zenml.services.service.BaseService</code></td><td><code>.json</code></td></tr><tr><td><a href=\"https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers/#zenml.materializers.structured_string_materializer.StructuredStringMaterializer\">StructuredStringMaterializer</a></td><td><code>zenml.types.CSVString</code>, <code>zenml.types.HTMLString</code>, <code>zenml.types.MarkdownString</code></td><td><code>.csv</code> / <code>.html</code> / <code>.md</code> (depending on type)</td></tr></tbody></table>\n\n{% hint style=\"warning\" %}\nZenML provides a built-in [CloudpickleMaterializer](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-materializers/#zenml.materializers.cloudpickle\\_materializer.CloudpickleMaterializer) that can handle any object by saving it with [cloudpickle](https://github.com/cloudpipe/cloudpickle). However, this is not production-ready because the resulting artifacts cannot be loaded when running with a different Python version. In such cases, you should consider building a [custom Materializer](handle-custom-data-types.md#custom-materializers) to save your objects in a more robust and efficient format.\n\nMoreover, using the `CloudpickleMaterializer` could allow users to upload of any kind of object. This could be exploited to upload a malicious file, which could execute arbitrary code on the vulnerable system.\n{% endhint %}\n\n## Integration Materializers\n\nIn addition to the built-in materializers, ZenML also provides several integration-specific materializers that can be activated by installing the respective [integration](../../component-guide/README.md):\n\n<table data-full-width=\"true\"><thead><tr><th width=\"199.5\">Integration</th><th width=\"271\">Materializer</th><th width=\"390\">Handled Data Types</th"}
{"input": "><th>Storage Format</th></tr></thead><tbody><tr><td>bentoml</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-bentoml/#zenml.integrations.bentoml.materializers.bentoml_bento_materializer.BentoMaterializer\">BentoMaterializer</a></td><td><code>bentoml.Bento</code></td><td><code>.bento</code></td></tr><tr><td>deepchecks</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-deepchecks/#zenml.integrations.deepchecks.materializers.deepchecks_results_materializer.DeepchecksResultMaterializer\">DeepchecksResultMateriailzer</a></td><td><code>deepchecks.CheckResult</code>, <code>deepchecks.SuiteResult</code></td><td><code>.json</code></td></tr><tr><td>evidently</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-evidently/#zenml.integrations.evidently.materializers.evidently_profile_materializer.EvidentlyProfileMaterializer\">EvidentlyProfileMaterializer</a></td><td><code>evidently.Profile</code></td><td><code>.json</code></td></tr><tr><td>great_expectations</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-great_expectations/#zenml.integrations.great_expectations.materializers.ge_materializer.GreatExpectationsMaterializer\">GreatExpectationsMaterializer</a></td><td><code>great_expectations.ExpectationSuite</code>, <code>great_expectations.CheckpointResult</code></td><td><code>.json</code></td></tr><tr><td>huggingface</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-huggingface/#zenml.integrations.huggingface.materializers.huggingface_datasets_materializer.HFDatasetMaterializer\">HFDatasetMaterializer</a></td><td><code>datasets.Dataset</code>, <code>datasets.DatasetDict</code></td><td>Directory</td></tr><tr><td>huggingface</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-huggingface/#zenml.integrations.huggingface.materializers.huggingface_pt_model_materializer.HFPTModelMaterializer\">HFPTModelMaterializer</a></td><td><code>transformers.PreTrainedModel</code></td><td>Directory</td></tr><tr><"}
{"input": "td>huggingface</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-huggingface/#zenml.integrations.huggingface.materializers.huggingface_tf_model_materializer.HFTFModelMaterializer\">HFTFModelMaterializer</a></td><td><code>transformers.TFPreTrainedModel</code></td><td>Directory</td></tr><tr><td>huggingface</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-huggingface/#zenml.integrations.huggingface.materializers.huggingface_tokenizer_materializer.HFTokenizerMaterializer\">HFTokenizerMaterializer</a></td><td><code>transformers.PreTrainedTokenizerBase</code></td><td>Directory</td></tr><tr><td>lightgbm</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-lightgbm/#zenml.integrations.lightgbm.materializers.lightgbm_booster_materializer.LightGBMBoosterMaterializer\">LightGBMBoosterMaterializer</a></td><td><code>lgbm.Booster</code></td><td><code>.txt</code></td></tr><tr><td>lightgbm</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-lightgbm/#zenml.integrations.lightgbm.materializers.lightgbm_dataset_materializer.LightGBMDatasetMaterializer\">LightGBMDatasetMaterializer</a></td><td><code>lgbm.Dataset</code></td><td><code>.binary</code></td></tr><tr><td>neural_prophet</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-neural_prophet/#zenml.integrations.neural_prophet.materializers.neural_prophet_materializer.NeuralProphetMaterializer\">NeuralProphetMaterializer</a></td><td><code>NeuralProphet</code></td><td><code>.pt</code></td></tr><tr><td>pillow</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-pillow/#zenml.integrations.pillow.materializers.pillow_image_materializer.PillowImageMaterializer\">PillowImageMaterializer</a></td><td><code>Pillow.Image</code></td><td><code>.PNG</code></td></tr><tr><td>polars</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-polars/#zenml.integrations.p"}
{"input": "olars.materializers.dataframe_materializer.PolarsMaterializer\">PolarsMaterializer</a></td><td><code>pl.DataFrame</code>, <code>pl.Series</code></td><td><code>.parquet</code></td></tr><tr><td>pycaret</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-pycaret/#zenml.integrations.pycaret.materializers.model_materializer.PyCaretMaterializer\">PyCaretMaterializer</a></td><td>Any <code>sklearn</code>, <code>xgboost</code>, <code>lightgbm</code> or <code>catboost</code> model</td><td><code>.pkl</code></td></tr><tr><td>pytorch</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-pytorch/#zenml.integrations.pytorch.materializers.pytorch_dataloader_materializer.PyTorchDataLoaderMaterializer\">PyTorchDataLoaderMaterializer</a></td><td><code>torch.Dataset</code>, <code>torch.DataLoader</code></td><td><code>.pt</code></td></tr><tr><td>pytorch</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-pytorch/#zenml.integrations.pytorch.materializers.pytorch_module_materializer.PyTorchModuleMaterializer\">PyTorchModuleMaterializer</a></td><td><code>torch.Module</code></td><td><code>.pt</code></td></tr><tr><td>scipy</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-scipy/#zenml.integrations.scipy.materializers.sparse_materializer.SparseMaterializer\">SparseMaterializer</a></td><td><code>scipy.spmatrix</code></td><td><code>.npz</code></td></tr><tr><td>spark</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-spark/#zenml.integrations.spark.materializers.spark_dataframe_materializer.SparkDataFrameMaterializer\">SparkDataFrameMaterializer</a></td><td><code>pyspark.DataFrame</code></td><td><code>.parquet</code></td></tr><tr><td>spark</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-spark/#zenml.integrations.spark.materializers.spark_model_materializer.SparkModelMaterializer\">SparkModelMaterializer</a></td><td><code>pyspark.Transformer</"}
{"input": "code></td><td><code>pyspark.Estimator</code></td></tr><tr><td>tensorflow</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-tensorflow/#zenml.integrations.tensorflow.materializers.keras_materializer.KerasMaterializer\">KerasMaterializer</a></td><td><code>tf.keras.Model</code></td><td>Directory</td></tr><tr><td>tensorflow</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-tensorflow/#zenml.integrations.tensorflow.materializers.tf_dataset_materializer.TensorflowDatasetMaterializer\">TensorflowDatasetMaterializer</a></td><td><code>tf.Dataset</code></td><td>Directory</td></tr><tr><td>whylogs</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-whylogs/#zenml.integrations.whylogs.materializers.whylogs_materializer.WhylogsMaterializer\">WhylogsMaterializer</a></td><td><code>whylogs.DatasetProfileView</code></td><td><code>.pb</code></td></tr><tr><td>xgboost</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-xgboost/#zenml.integrations.xgboost.materializers.xgboost_booster_materializer.XgboostBoosterMaterializer\">XgboostBoosterMaterializer</a></td><td><code>xgb.Booster</code></td><td><code>.json</code></td></tr><tr><td>xgboost</td><td><a href=\"https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-xgboost/#zenml.integrations.xgboost.materializers.xgboost_dmatrix_materializer.XgboostDMatrixMaterializer\">XgboostDMatrixMaterializer</a></td><td><code>xgb.DMatrix</code></td><td><code>.binary</code></td></tr></tbody></table>\n\n{% hint style=\"warning\" %}\nIf you are running pipelines with a Docker-based [orchestrator](../../component-guide/orchestrators/orchestrators.md), you need to specify the corresponding integration as `required_integrations` in the `DockerSettings` of your pipeline in order to have the integration materializer available inside your Docker container. See the [pipeline configuration documentation](../use-configuration-files/runtime-configuration.md) for more information.\n{% endhint %}\n\n## Custom materializers\n\n### Configuring a step/pipeline to use a custom materializer\n\n#### Defining which step uses what materializer\n\nZenML automatically detects if your materializer is"}
{"input": " imported in your source code and registers them for the corresponding data type (defined in `ASSOCIATED_TYPES`). Therefore, just having a custom materializer definition in your code is enough to enable the respective data type to be used in your pipelines.\n\nHowever, it is best practice to explicitly define which materializer to use for a specific step and not rely on the `ASSOCIATED_TYPES` to make that connection:\n\n```python\nclass MyObj:\n    ...\n\nclass MyMaterializer(BaseMaterializer):\n    \"\"\"Materializer to read data to and from MyObj.\"\"\"\n\n    ASSOCIATED_TYPES = (MyObj)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA\n\n    # Read below to learn how to implement this materializer\n\n# You can define it at the decorator level\n@step(output_materializers=MyMaterializer)\ndef my_first_step() -> MyObj:\n    return 1\n\n# No need to explicitly specify materializer here:\n# it is coupled with Artifact Version generated by\n# `my_first_step` already.\ndef my_second_step(a: MyObj):\n    print(a)\n\n# or you can use the `configure()` method of the step. E.g.:\nmy_first_step.configure(output_materializers=MyMaterializer)\n```\n\nWhen there are multiple outputs, a dictionary of type `{<OUTPUT_NAME>: <MATERIALIZER_CLASS>}` can be supplied to the decorator or the `.configure(...)` method:\n\n```python\nclass MyObj1:\n    ...\n\nclass MyObj2:\n    ...\n\nclass MyMaterializer1(BaseMaterializer):\n    \"\"\"Materializer to read data to and from MyObj1.\"\"\"\n\n    ASSOCIATED_TYPES = (MyObj1)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA\n\nclass MyMaterializer2(BaseMaterializer):\n    \"\"\"Materializer to read data to and from MyObj2.\"\"\"\n\n    ASSOCIATED_TYPES = (MyObj2)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA\n\n# This is where we connect the objects to the materializer\n@step(output_materializers={\"1\": MyMaterializer1, \"2\": MyMaterializer2})\ndef my_first_step() -> Tuple[Annotated[MyObj1, \"1\"], Annotated[MyObj2, \"2\"]]:\n    return 1\n```\n\nAlso, as briefly outlined in the [configuration docs](../use-configuration-files/what-can-be-configured.md) section, which materializer to use for the output of what step can also be configured within YAML config files.\n\nFor each output of your steps, you can define custom materializers to handle the loading and saving. You can configure them like this in the config:\n\n```yaml\n...\nsteps:\n  <STEP_NAME>:\n    ...\n    outputs:\n      <OUTPUT_NAME>:\n        materializer_source: run.MyMaterializer\n```\n\nCheck out [this page](../../user-guide/starter-guide/manage-artifacts"}
{"input": ".md) for information on your step output names and how to customize them.\n\n#### Defining a materializer globally\n\nSometimes, you would like to configure ZenML to use a custom materializer globally for all pipelines, and override the default materializers that come built-in with ZenML. A good example of this would be to build a materializer for a `pandas.DataFrame` to handle the reading and writing of that dataframe in a different way than the default mechanism.\n\nAn easy way to do that is to use the internal materializer registry of ZenML and override its behavior:\n\n```python\n# Entrypoint file where we run pipelines (i.e. run.py)\n\nfrom zenml.materializers.materializer_registry import materializer_registry\n\n# Create a new materializer\nclass FastPandasMaterializer(BaseMaterializer):\n    ...\n\n# Register the FastPandasMaterializer for pandas dataframes objects\nmaterializer_registry.register_and_overwrite_type(key=pd.DataFrame, type_=FastPandasMaterializer)\n\n# Run your pipelines: They will now all use the custom materializer\n```\n\n### Developing a custom materializer\n\nNow that we know how to configure a pipeline to use a custom materializer, let us briefly discuss how materializers in general are implemented.\n\n#### Base implementation\n\nIn the following, you can see the implementation of the abstract base class `BaseMaterializer`, which defines the interface of all materializers:\n\n```python\nclass BaseMaterializer(metaclass=BaseMaterializerMeta):\n    \"\"\"Base Materializer to realize artifact data.\"\"\"\n\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.BASE\n    ASSOCIATED_TYPES = ()\n\n    def __init__(\n        self, uri: str, artifact_store: Optional[BaseArtifactStore] = None\n    ):\n        \"\"\"Initializes a materializer with the given URI.\n\n        Args:\n            uri: The URI where the artifact data will be stored.\n            artifact_store: The artifact store used to store this artifact.\n        \"\"\"\n        self.uri = uri\n        self._artifact_store = artifact_store\n\n    def load(self, data_type: Type[Any]) -> Any:\n        \"\"\"Write logic here to load the data of an artifact.\n\n        Args:\n            data_type: The type of data that the artifact should be loaded as.\n\n        Returns:\n            The data of the artifact.\n        \"\"\"\n        # read from a location inside self.uri\n        # \n        # Example:\n        # data_path = os.path.join(self.uri, \"abc.json\")\n        # with self.artifact_store.open(filepath, \"r\") as fid:\n        #     return json.load(fid)\n        ...\n\n    def save(self, data: Any) -> None:\n        \"\"\"Write logic here to save the data of an artifact.\n\n        Args:\n            data: The data of the artifact to save.\n        \"\"\"\n        # write `data` into self.uri\n        # \n        # Example:\n        # data_path = os.path.join(self.uri,"}
{"input": " \"abc.json\")\n        # with self.artifact_store.open(filepath, \"w\") as fid:\n        #     json.dump(data,fid)\n        ...\n\n    def save_visualizations(self, data: Any) -> Dict[str, VisualizationType]:\n        \"\"\"Save visualizations of the given data.\n\n        Args:\n            data: The data of the artifact to visualize.\n\n        Returns:\n            A dictionary of visualization URIs and their types.\n        \"\"\"\n        # Optionally, define some visualizations for your artifact\n        #\n        # E.g.:\n        # visualization_uri = os.path.join(self.uri, \"visualization.html\")\n        # with self.artifact_store.open(visualization_uri, \"w\") as f:\n        #     f.write(\"<html><body>data</body></html>\")\n\n        # visualization_uri_2 = os.path.join(self.uri, \"visualization.png\")\n        # data.save_as_png(visualization_uri_2)\n\n        # return {\n        #     visualization_uri: ArtifactVisualizationType.HTML,\n        #     visualization_uri_2: ArtifactVisualizationType.IMAGE\n        # }\n        ...\n\n    def extract_metadata(self, data: Any) -> Dict[str, \"MetadataType\"]:\n        \"\"\"Extract metadata from the given data.\n\n        This metadata will be tracked and displayed alongside the artifact.\n\n        Args:\n            data: The data to extract metadata from.\n\n        Returns:\n            A dictionary of metadata.\n        \"\"\"\n        # Optionally, extract some metadata from `data` for ZenML to store.\n        #\n        # Example:\n        # return {\n        #     \"some_attribute_i_want_to_track\": self.some_attribute,\n        #     \"pi\": 3.14,\n        # }\n        ...\n```\n\n#### Handled data types\n\nEach materializer has an `ASSOCIATED_TYPES` attribute that contains a list of data types that this materializer can handle. ZenML uses this information to call the right materializer at the right time. I.e., if a ZenML step returns a `pd.DataFrame`, ZenML will try to find any materializer that has `pd.DataFrame` in its `ASSOCIATED_TYPES`. List the data type of your custom object here to link the materializer to that data type.\n\n#### The type of the generated artifact\n\nEach materializer also has an `ASSOCIATED_ARTIFACT_TYPE` attribute, which defines what `zenml.enums.ArtifactType` is assigned to this data.\n\nIn most cases, you should choose either `ArtifactType.DATA` or `ArtifactType.MODEL` here. If you are unsure, just use `ArtifactType.DATA`. The exact choice is not too important, as the artifact type is only used as a tag in some of ZenML's visualizations.\n\n#### Target location to store the artifact\n\nEach materializer has a `uri` attribute, which is automatically created by ZenML whenever you run a pipeline and points to the directory of a file system where the respective artifact is stored (some location in"}
{"input": " the artifact store).\n\n#### Storing and retrieving the artifact\n\nThe `load()` and `save()` methods define the serialization and deserialization of artifacts.\n\n* `load()` defines how data is read from the artifact store and deserialized,\n* `save()` defines how data is serialized and saved to the artifact store.\n\nYou will need to override these methods according to how you plan to serialize your objects. E.g., if you have custom PyTorch classes as `ASSOCIATED_TYPES`, then you might want to use `torch.save()` and `torch.load()` here.\n\n#### (Optional) How to Visualize the Artifact\n\nOptionally, you can override the `save_visualizations()` method to automatically save visualizations for all artifacts saved by your materializer. These visualizations are then shown next to your artifacts in the dashboard:\n\n![Evidently Artifact Visualization Example](../../.gitbook/assets/artifact_visualization_dashboard.png)\n\nCurrently, artifacts can be visualized either as CSV table, embedded HTML, image or Markdown. For more information, see [zenml.enums.VisualizationType](https://github.com/zenml-io/zenml/blob/main/src/zenml/enums.py).\n\nTo create visualizations, you need to:\n\n1. Compute the visualizations based on the artifact\n2. Save all visualizations to paths inside `self.uri`\n3. Return a dictionary mapping visualization paths to visualization types.\n\nAs an example, check out the implementation of the [zenml.materializers.NumpyMaterializer](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/numpy\\_materializer.py) that use matplotlib to automatically save or plot certain arrays.\n\nRead more about visualizations [here](../visualize-artifacts/creating-custom-visualizations.md).\n\n#### (Optional) Which Metadata to Extract for the Artifact\n\nOptionally, you can override the `extract_metadata()` method to track custom metadata for all artifacts saved by your materializer. Anything you extract here will be displayed in the dashboard next to your artifacts.\n\nTo extract metadata, define and return a dictionary of values you want to track. The only requirement is that all your values are built-in types ( like `str`, `int`, `list`, `dict`, ...) or among the special types defined in [zenml.metadata.metadata\\_types](https://github.com/zenml-io/zenml/blob/main/src/zenml/metadata/metadata\\_types.py) that are displayed in a dedicated way in the dashboard. See [zenml.metadata.metadata\\_types.MetadataType](https://github.com/zenml-io/zenml/blob/main/src/zenml/metadata/metadata\\_types.py) for more details.\n\nBy default, this method will only extract the storage size of an artifact, but you can override it to track anything you wish. E.g., the [zenml.materializers.NumpyMaterializer](https://github.com/zenml-io/zenml"}
{"input": "/blob/main/src/zenml/materializers/numpy\\_materializer.py) overrides this method to track the `shape`, `dtype`, and some statistical properties of each `np.ndarray` that it saves.\n\n{% hint style=\"info\" %}\nIf you would like to disable artifact visualization altogether, you can set `enable_artifact_visualization` at either pipeline or step level via `@pipeline(enable_artifact_visualization=False)` or `@step(enable_artifact_visualization=False)`.\n{% endhint %}\n\n#### (Optional) Which Metadata to Extract for the Artifact\n\nOptionally, you can override the `extract_metadata()` method to track custom metadata for all artifacts saved by your materializer. Anything you extract here will be displayed in the dashboard next to your artifacts.\n\nTo extract metadata, define and return a dictionary of values you want to track. The only requirement is that all your values are built-in types ( like `str`, `int`, `list`, `dict`, ...) or among the special types defined in [src.zenml.metadata.metadata\\_types](https://github.com/zenml-io/zenml/blob/main/src/zenml/metadata/metadata\\_types.py) that are displayed in a dedicated way in the dashboard. See [src.zenml.metadata.metadata\\_types.MetadataType](https://github.com/zenml-io/zenml/blob/main/src/zenml/metadata/metadata\\_types.py) for more details.\n\nBy default, this method will only extract the storage size of an artifact, but you can overwrite it to track anything you wish. E.g., the `zenml.materializers.NumpyMaterializer` overwrites this method to track the `shape`, `dtype`, and some statistical properties of each `np.ndarray` that it saves.\n\n{% hint style=\"info\" %}\nIf you would like to disable artifact metadata extraction altogether, you can set `enable_artifact_metadata` at either pipeline or step level via `@pipeline(enable_artifact_metadata=False)` or `@step(enable_artifact_metadata=False)`.\n{% endhint %}\n\n## Skipping materialization\n\nYou can learn more about skipping materialization [here](unmaterialized-artifacts.md).\n\n## Interaction with custom artifact stores\n\nWhen creating a custom artifact store, you may encounter a situation where the default materializers do not function properly. Specifically, the `self.artifact_store.open` method used in these materializers may not be compatible with your custom store due to not being implemented properly.\n\nIn this case, you can create a modified version of the failing materializer by copying it and modifying it to copy the artifact to a local path, then opening it from there. For example, consider the following implementation of a custom [PandasMaterializer](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/pandas\\_materializer.py) that works with a custom artifact store. In this implementation, we copy the artifact to a local path because"}
{"input": " we want to use the `pandas.read_csv` method to read it. If we were to use the `self.artifact_store.open` method instead, we would not need to make this copy.\n\n{% hint style=\"warning\" %}\nIt is worth noting that copying the artifact to a local path may not always be necessary and can potentially be a performance bottleneck.\n{% endhint %}\n\n<details>\n\n<summary>Pandas Materializer code example</summary>\n\n```python\nimport os\nfrom typing import Any, ClassVar, Dict, Optional, Tuple, Type, Union\n\nimport pandas as pd\n\nfrom zenml.artifact_stores.base_artifact_store import BaseArtifactStore\nfrom zenml.enums import ArtifactType, VisualizationType\nfrom zenml.logger import get_logger\nfrom zenml.materializers.base_materializer import BaseMaterializer\nfrom zenml.metadata.metadata_types import DType, MetadataType\n\nlogger = get_logger(__name__)\n\nPARQUET_FILENAME = \"df.parquet.gzip\"\nCOMPRESSION_TYPE = \"gzip\"\n\nCSV_FILENAME = \"df.csv\"\n\n\nclass PandasMaterializer(BaseMaterializer):\n    \"\"\"Materializer to read data to and from pandas.\"\"\"\n\n    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (\n        pd.DataFrame,\n        pd.Series,\n    )\n    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.DATA\n\n    def __init__(\n        self, uri: str, artifact_store: Optional[BaseArtifactStore] = None\n    ):\n        \"\"\"Define `self.data_path`.\n\n        Args:\n            uri: The URI where the artifact data is stored.\n            artifact_store: The artifact store where the artifact data is stored.\n        \"\"\"\n        super().__init__(uri, artifact_store)\n        try:\n            import pyarrow  # type: ignore # noqa\n\n            self.pyarrow_exists = True\n        except ImportError:\n            self.pyarrow_exists = False\n            logger.warning(\n                \"By default, the `PandasMaterializer` stores data as a \"\n                \"`.csv` file. If you want to store data more efficiently, \"\n                \"you can install `pyarrow` by running \"\n                \"'`pip install pyarrow`'. This will allow `PandasMaterializer` \"\n                \"to automatically store the data as a `.parquet` file instead.\"\n            )\n        finally:\n            self.parquet_path = os.path.join(self.uri, PARQUET_FILENAME)\n            self.csv_path = os.path.join(self.uri, CSV_FILENAME)\n\n    def load(self, data_type: Type[Any]) -> Union[pd.DataFrame, pd.Series]:\n        \"\"\"Reads `pd.DataFrame` or `pd.Series` from a `.parquet` or `.csv` file.\n\n        Args:\n            data_type: The type of the data to read.\n\n        Raises:\n            ImportError: If pyarrow or fastparquet is not installed.\n\n        Returns:\n            The pandas dataframe or"}
{"input": " series.\n        \"\"\"\n        if self.artifact_store.exists(self.parquet_path):\n            if self.pyarrow_exists:\n                with self.artifact_store.open(\n                    self.parquet_path, mode=\"rb\"\n                ) as f:\n                    df = pd.read_parquet(f)\n            else:\n                raise ImportError(\n                    \"You have an old version of a `PandasMaterializer` \"\n                    \"data artifact stored in the artifact store \"\n                    \"as a `.parquet` file, which requires `pyarrow` \"\n                    \"for reading, You can install `pyarrow` by running \"\n                    \"'`pip install pyarrow fastparquet`'.\"\n                )\n        else:\n            with self.artifact_store.open(self.csv_path, mode=\"rb\") as f:\n                df = pd.read_csv(f, index_col=0, parse_dates=True)\n\n        # validate the type of the data.\n        def is_dataframe_or_series(\n            df: Union[pd.DataFrame, pd.Series],\n        ) -> Union[pd.DataFrame, pd.Series]:\n            \"\"\"Checks if the data is a `pd.DataFrame` or `pd.Series`.\n\n            Args:\n                df: The data to check.\n\n            Returns:\n                The data if it is a `pd.DataFrame` or `pd.Series`.\n            \"\"\"\n            if issubclass(data_type, pd.Series):\n                # Taking the first column if it is a series as the assumption\n                # is that there will only be one\n                assert len(df.columns) == 1\n                df = df[df.columns[0]]\n                return df\n            else:\n                return df\n\n        return is_dataframe_or_series(df)\n\n    def save(self, df: Union[pd.DataFrame, pd.Series]) -> None:\n        \"\"\"Writes a pandas dataframe or series to the specified filename.\n\n        Args:\n            df: The pandas dataframe or series to write.\n        \"\"\"\n        if isinstance(df, pd.Series):\n            df = df.to_frame(name=\"series\")\n\n        if self.pyarrow_exists:\n            with self.artifact_store.open(self.parquet_path, mode=\"wb\") as f:\n                df.to_parquet(f, compression=COMPRESSION_TYPE)\n        else:\n            with self.artifact_store.open(self.csv_path, mode=\"wb\") as f:\n                df.to_csv(f, index=True)\n\n```\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n\n## Code example\n\nLet's see how materialization works with a basic example. Let's say you have a custom class called `MyObject` that flows between two steps in a pipeline:\n\n```python\nimport logging\nfrom zenml import step, pipeline\n\n\nclass MyObj:\n    def __init__(self, name: str):\n        self.name ="}
{"input": " name\n\n\n@step\ndef my_first_step() -> MyObj:\n    \"\"\"Step that returns an object of type MyObj.\"\"\"\n    return MyObj(\"my_object\")\n\n\n@step\ndef my_second_step(my_obj: MyObj) -> None:\n    \"\"\"Step that logs the input object and returns nothing.\"\"\"\n    logging.info(\n        f\"The following object was passed to this step: `{my_obj.name}`\"\n    )\n\n\n@pipeline\ndef first_pipeline():\n    output_1 = my_first_step()\n    my_second_step(output_1)\n\n\nfirst_pipeline()\n```\n\nRunning the above without a custom materializer will work but print the following warning:\n\n`No materializer is registered for type MyObj, so the default Pickle materializer was used. Pickle is not production ready and should only be used for prototyping as the artifacts cannot be loaded when running with a different Python version. Please consider implementing a custom materializer for type MyObj`\n\nTo get rid of this warning and make our pipeline more robust, we will subclass the `BaseMaterializer` class, listing `MyObj` in `ASSOCIATED_TYPES`, and overwriting `load()` and `save()`:\n\n```python\nimport os\nfrom typing import Type\n\nfrom zenml.enums import ArtifactType\nfrom zenml.materializers.base_materializer import BaseMaterializer\n\n\nclass MyMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (MyObj,)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA\n\n    def load(self, data_type: Type[MyObj]) -> MyObj:\n        \"\"\"Read from artifact store.\"\"\"\n        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'r') as f:\n            name = f.read()\n        return MyObj(name=name)\n\n    def save(self, my_obj: MyObj) -> None:\n        \"\"\"Write to artifact store.\"\"\"\n        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'w') as f:\n            f.write(my_obj.name)\n```\n\n{% hint style=\"info\" %}\nPro-tip: Use the `self.artifact_store` property to ensure your materialization logic works across artifact stores (local and remote like S3 buckets).\n{% endhint %}\n\nNow, ZenML can use this materializer to handle the outputs and inputs of your customs object. Edit the pipeline as follows to see this in action:\n\n```python\nmy_first_step.configure(output_materializers=MyMaterializer)\nfirst_pipeline()\n```\n\n{% hint style=\"info\" %}\nDue to the typing of the inputs and outputs and the `ASSOCIATED_TYPES` attribute of the materializer, you won't necessarily have to add `.configure(output_materializers=MyMaterializer)` to the step. It should automatically be detected. It doesn't hurt to be explicit though.\n{% endhint %}\n\nThis will now work as expected and yield the following output:\n\n```shell\nCreating run for pipeline: `"}
{"input": "first_pipeline`\nCache enabled for pipeline `first_pipeline`\nUsing stack `default` to run pipeline `first_pipeline`...\nStep `my_first_step` has started.\nStep `my_first_step` has finished in 0.081s.\nStep `my_second_step` has started.\nThe following object was passed to this step: `my_object`\nStep `my_second_step` has finished in 0.048s.\nPipeline run `first_pipeline-22_Apr_22-10_58_51_135729` has finished in 0.153s.\n```\n\n<details>\n\n<summary>Code Example for Materializing Custom Objects</summary>\n\n```python\nimport logging\nimport os\nfrom typing import Type\n\nfrom zenml import step, pipeline\n\nfrom zenml.enums import ArtifactType\nfrom zenml.materializers.base_materializer import BaseMaterializer\n\n\nclass MyObj:\n    def __init__(self, name: str):\n        self.name = name\n\n\nclass MyMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (MyObj,)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA\n\n    def load(self, data_type: Type[MyObj]) -> MyObj:\n        \"\"\"Read from artifact store.\"\"\"\n        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'r') as f:\n            name = f.read()\n        return MyObj(name=name)\n\n    def save(self, my_obj: MyObj) -> None:\n        \"\"\"Write to artifact store.\"\"\"\n        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'w') as f:\n            f.write(my_obj.name)\n\n\n@step\ndef my_first_step() -> MyObj:\n    \"\"\"Step that returns an object of type MyObj.\"\"\"\n    return MyObj(\"my_object\")\n\n\nmy_first_step.configure(output_materializers=MyMaterializer)\n\n\n@step\ndef my_second_step(my_obj: MyObj) -> None:\n    \"\"\"Step that log the input object and returns nothing.\"\"\"\n    logging.info(\n        f\"The following object was passed to this step: `{my_obj.name}`\"\n    )\n\n\n@pipeline\ndef first_pipeline():\n    output_1 = my_first_step()\n    my_second_step(output_1)\n\n\nif __name__ == \"__main__\":\n    first_pipeline()\n```\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Structuring an MLOps project\n---\n\n# Passing artifacts between pipelines\n\nAn MLOps project can often be broken down into many different pipelines. For example:\n\n* A feature engineering pipeline that prepares raw data into a format ready to get trained.\n* A training pipeline that takes input data from a feature engineering pipeline and trains a models on it.\n* An inference pipeline that runs batch predictions on the trained model and often takes pre-processing from the training pipeline.\n* A deployment pipeline that deploys a trained model into a production endpoint.\n\nThe lines between these pipelines can often get blurry: Some use cases call for these pipelines to be merged into one big pipeline. Others go further and break the pipeline down into even smaller chunks. Ultimately, the decision of how to structure your pipelines depends on the use case and requirements of the project.\n\nNo matter how you design these pipelines, one thing stays consistent: you will often need to transfer or share information (in particular artifacts, models, and metadata) between pipelines. Here are some common patterns that you can use to help facilitate such an exchange:\n\n## Pattern 1: Artifact exchange between pipelines through `Client`\n\nLet's say we have a feature engineering pipeline and a training pipeline. The feature engineering pipeline is like a factory, pumping out many different datasets. Only a few of these datasets should be selected to be sent to the training pipeline to train an actual model.\n\n<figure><img src=\"../../.gitbook/assets/artifact_exchange.png\" alt=\"\"><figcaption><p>A simple artifact exchange between two pipelines</p></figcaption></figure>\n\nIn this scenario, the [ZenML Client](../../reference/python-client.md#client-methods) can be used to facilitate such an exchange:\n\n```python\nfrom zenml import pipeline\nfrom zenml.client import Client\n\n@pipeline\ndef feature_engineering_pipeline():\n    dataset = load_data()\n    # This returns artifacts called \"iris_training_dataset\" and \"iris_testing_dataset\"\n    train_data, test_data = prepare_data()\n\n@pipeline\ndef training_pipeline():\n    client = Client()\n    # Fetch by name alone - uses the latest version of this artifact\n    train_data = client.get_artifact_version(name=\"iris_training_dataset\")\n    # For test, we want a particular version\n    test_data = client.get_artifact_version(name=\"iris_testing_dataset\", version=\"raw_2023\")\n\n    # We can now send these directly into ZenML steps\n    sklearn_classifier = model_trainer(train_data)\n    model_evaluator(model, sklearn_classifier)\n```\n\n{% hint style=\"info\" %}\nNote that in the above example, the `train_data` and `test_data` artifacts are not [materialized](artifact-versioning.md) in memory in the `@pipeline` function, but rather the `train_data` and `test_data` objects are simply references to where this data is stored in the artifact store. Therefore, one cannot use any logic regarding the nature of this data"}
{"input": " itself during compilation time (i.e. in the `@pipeline` function).\n{% endhint %}\n\n## Pattern 2: Artifact exchange between pipelines through a `Model`\n\nWhile passing around artifacts with IDs or names is very useful, it is often desirable to have the ZenML Model be the point of reference instead.\n\nFor example, let's say we have a training pipeline called `train_and_promote` and an inference pipeline called `do_predictions`. The training pipeline produces many different model artifacts, all of which are collected within a [ZenML Model](../../user-guide/starter-guide/track-ml-models.md). Each time the `train_and_promote` pipeline runs, it creates a new `iris_classifier`. However, it only promotes the model to `production` if a certain accuracy threshold is met. The promotion can be also be done manually with human intervention, or it can be automated through setting a particular threshold.\n\nOn the other side, the `do_predictions` pipeline simply picks up the latest promoted model and runs batch inference on it. It need not know of the IDs or names of any of the artifacts produced by the training pipeline's many runs. This way these two pipelines can independently be run, but can rely on each other's output.\n\n<figure><img src=\"../../.gitbook/assets/mcp_pipeline_overview.png\" alt=\"\"><figcaption><p>A simple artifact exchange between pipelines through the Model Control Plane.</p></figcaption></figure>\n\nIn code, this is very simple. Once the [pipelines are configured to use a particular model](../../user-guide/starter-guide/track-ml-models.md#configuring-a-model-in-a-pipeline), we can use `get_step_context` to fetch the configured model within a step directly. Assuming there is a `predict` step in the `do_predictions` pipeline, we can fetch the `production` model like so:\n\n```python\nfrom zenml import step, get_step_context\n\n# IMPORTANT: Cache needs to be disabled to avoid unexpected behavior\n@step(enable_cache=False)\ndef predict(\n    data: pd.DataFrame,\n) -> Annotated[pd.Series, \"predictions\"]:\n    # model name and version are derived from pipeline context\n    model = get_step_context().model\n\n    # Fetch the model directly from the model control plane\n    model = model.get_model_artifact(\"trained_model\")\n\n    # Make predictions\n    predictions = pd.Series(model.predict(data))\n    return predictions\n```\n\nHowever, this approach has the downside that if the step is cached, then it could lead to unexpected results. You could simply disable the cache in the above step or the corresponding pipeline. However, one other way of achieving this would be to resolve the artifact at the pipeline level:\n\n```python\nfrom typing_extensions import Annotated\nfrom zenml import get_pipeline_context, pipeline, Model\nfrom zenml.enums import ModelStages\nimport pandas as pd\nfrom sklearn.base import ClassifierMixin\n\n\n"}
{"input": "@step\ndef predict(\n    model: ClassifierMixin,\n    data: pd.DataFrame,\n) -> Annotated[pd.Series, \"predictions\"]:\n    predictions = pd.Series(model.predict(data))\n    return predictions\n\n@pipeline(\n    model=Model(\n        name=\"iris_classifier\",\n        # Using the production stage\n        version=ModelStages.PRODUCTION,\n    ),\n)\ndef do_predictions():\n    # model name and version are derived from pipeline context\n    model = get_pipeline_context().model\n    inference_data = load_data()\n    predict(\n        # Here, we load in the `trained_model` from a trainer step\n        model=model.get_model_artifact(\"trained_model\"),  \n        data=inference_data,\n    )\n\n\nif __name__ == \"__main__\":\n    do_predictions()\n```\n\nUltimately, both approaches are fine. You should decide which one to use based on your own preferences.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Steps and pipelines can be parameterized just like any other python function\n  that you are familiar with.\n---\n\n# Use pipeline/step parameters\n\n## Parameters for your steps\n\nWhen calling a step in a pipeline, the inputs provided to the step function can either be an **artifact** or a **parameter**. An artifact represents the output of another step that was executed as part of the same pipeline and serves as a means to share data between steps. Parameters, on the other hand, are values provided explicitly when invoking a step. They are not dependent on the output of other steps and allow you to parameterize the behavior of your steps.\n\n{% hint style=\"warning\" %}\nIn order to allow the configuration of your steps using a configuration file, only values that can be serialized to JSON using Pydantic can be passed as parameters. If you want to pass other non-JSON-serializable objects such as NumPy arrays to your steps, use [External Artifacts](../../user-guide/starter-guide/manage-artifacts.md#consuming-external-artifacts-within-a-pipeline) instead.\n{% endhint %}\n\n```python\nfrom zenml import step, pipeline\n\n@step\ndef my_step(input_1: int, input_2: int) -> None:\n    pass\n\n\n@pipeline\ndef my_pipeline():\n    int_artifact = some_other_step()\n    # We supply the value of `input_1` as an artifact and\n    # `input_2` as a parameter\n    my_step(input_1=int_artifact, input_2=42)\n    # We could also call the step with two artifacts or two\n    # parameters instead:\n    # my_step(input_1=int_artifact, input_2=int_artifact)\n    # my_step(input_1=1, input_2=2)\n```\n\nParameters of steps and pipelines can also be passed in using YAML configuration files. The following configuration file and Python code can work together and give you the flexibility to update configuration only in YAML file, once needed:\n\n```yaml\n# config.yaml\n\n# these are parameters of the pipeline\nparameters:\n  environment: production\n\nsteps:\n  my_step:\n    # these are parameters of the step `my_step`\n    parameters:\n      input_2: 42\n```\n\n```python\nfrom zenml import step, pipeline\n@step\ndef my_step(input_1: int, input_2: int) -> None:\n    ...\n\n# input `environment` will come from the configuration file,\n# and it is evaluated to `production`\n@pipeline\ndef my_pipeline(environment: str):\n    ...\n\nif __name__==\"__main__\":\n    my_pipeline.with_options(config_paths=\"config.yaml\")()\n```\n\n{% hint style=\"warning\" %}\nThere might be conflicting settings for step or pipeline inputs, while working with YAML configuration files. Such situations happen when you define a step or a"}
{"input": " pipeline parameter in the configuration file and override it from the code later on. Don't worry - once it happens you will be informed with details and instructions how to fix. Example of such a conflict:\n\n```yaml\n# config.yaml\nparameters:\n    some_param: 24\n\nsteps:\n  my_step:\n    parameters:\n      input_2: 42\n```\n\n```python\n# run.py\nfrom zenml import step, pipeline\n\n@step\ndef my_step(input_1: int, input_2: int) -> None:\n    pass\n\n@pipeline\ndef my_pipeline(some_param: int):\n    # here an error will be raised since `input_2` is\n    # `42` in config, but `43` was provided in the code\n    my_step(input_1=42, input_2=43)\n\nif __name__==\"__main__\":\n    # here an error will be raised since `some_param` is\n    # `24` in config, but `23` was provided in the code\n    my_pipeline(23)\n```\n{% endhint %}\n\n**Parameters and caching**\n\nWhen an input is passed as a parameter, the step will only be cached if all parameter values are exactly the same as for previous executions of the step.\n\n**Artifacts and caching**\n\nWhen an artifact is used as a step function input, the step will only be cached if all the artifacts are exactly the same as for previous executions of the step. This means that if any of the upstream steps that produce the input artifacts for a step were not cached, the step itself will always be executed.\n\n***\n\n### See Also:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Use configuration files to set parameters</td><td></td><td></td><td><a href=\"use-pipeline-step-parameters.md\">use-pipeline-step-parameters.md</a></td></tr><tr><td>How caching works and how to control it</td><td></td><td></td><td><a href=\"control-caching-behavior.md\">control-caching-behavior.md</a></td></tr></tbody></table>\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Learn how to set, pause and stop a schedule for pipelines.\n---\n\n# Schedule a pipeline\n\n{% hint style=\"info\" %}\nSchedules don't work for all orchestrators. Here is a list of all supported orchestrators.\n{% endhint %}\n\n| Orchestrator                                                                   | Scheduling Support |\n|--------------------------------------------------------------------------------|--------------------|\n| [LocalOrchestrator](../../component-guide/orchestrators/local.md)              | \u26d4\ufe0f                 |\n| [LocalDockerOrchestrator](../../component-guide/orchestrators/local-docker.md) | \u26d4\ufe0f                 |\n| [KubernetesOrchestrator](../../component-guide/orchestrators/kubernetes.md)    | \u2705                  |\n| [KubeflowOrchestrator](../../component-guide/orchestrators/kubeflow.md)        | \u2705                  |\n| [VertexOrchestrator](../../component-guide/orchestrators/vertex.md)            | \u2705                  |\n| [TektonOrchestrator](../../component-guide/orchestrators/tekton.md)            | \u26d4\ufe0f                 |\n| [AirflowOrchestrator](../../component-guide/orchestrators/airflow.md)          | \u2705                  |\n| [AzureMLOrchestrator](../../component-guide/orchestrators/azureml.md)          | \u2705                  |\n\n### Set a schedule\n\n```python\nfrom zenml.config.schedule import Schedule\nfrom zenml import pipeline\nfrom datetime import datetime\n\n@pipeline()\ndef my_pipeline(...):\n    ...\n\n# Use cron expressions\nschedule = Schedule(cron_expression=\"5 14 * * 3\")\n# or alternatively use human-readable notations\nschedule = Schedule(start_time=datetime.now(), interval_second=1800)\n\nmy_pipeline = my_pipeline.with_options(schedule=schedule)\nmy_pipeline()\n```\n\n{% hint style=\"info\" %}\nCheck out our [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-config/#zenml.config.schedule.Schedule) to learn more about the different scheduling options.\n{% endhint %}\n\n### Pause/Stop a schedule\n\nThe way pipelines are scheduled depends on the orchestrator you are using. For example, if you are using Kubeflow, you can use the Kubeflow UI to stop or pause a scheduled run. However, the exact steps for stopping or pausing a scheduled run may vary depending on the orchestrator you are using. We recommend consulting the documentation for your orchestrator to learn the current method for stopping or pausing a scheduled run.\n\n{% hint style=\"warning\" %}\nNote that ZenML only gets involved to schedule a run, but maintaining the lifecycle of the schedule (as explained above) is the responsibility of the user. If you run a pipeline containing a schedule two times, two scheduled pipelines (with different/unique names) will be created.\n{% endhint %}\n\n\n***\n\n### See"}
{"input": " Also:\n\n<table data-view=\"cards\">\n    <thead>\n    <tr>\n        <th></th>\n        <th></th>\n        <th></th>\n        <th data-hidden data-card-target data-type=\"content-ref\"></th>\n    </tr>\n    </thead>\n    <tbody>\n    <tr>\n        <td>Schedules rely on remote orchestrators, learn about those here</td>\n        <td></td>\n        <td></td>\n        <td><a href=\"../../component-guide/orchestrators/orchestrators.md\">orchestrators.md</a></td>\n    </tr>\n    </tbody>\n</table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Step outputs are stored in your artifact store. Annotate and name them to make\n  more explicit.\n---\n\n# Step output typing and annotation\n\n## Type annotations\n\nYour functions will work as ZenML steps even if you don't provide any type annotations for their inputs and outputs. However, adding type annotations to your step functions gives you lots of additional benefits:\n\n* **Type validation of your step inputs**: ZenML makes sure that your step functions receive an object of the correct type from the upstream steps in your pipeline.\n* **Better serialization**: Without type annotations, ZenML uses [Cloudpickle](https://github.com/cloudpipe/cloudpickle) to serialize your step outputs. When provided with type annotations, ZenML can choose a [materializer](../../getting-started/core-concepts.md#materializers) that is best suited for the output. In case none of the builtin materializers work, you can even [write a custom materializer](../handle-data-artifacts/handle-custom-data-types.md).\n\n{% hint style=\"warning\" %}\nZenML provides a built-in [CloudpickleMaterializer](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-materializers/#zenml.materializers.cloudpickle\\_materializer.CloudpickleMaterializer) that can handle any object by saving it with [cloudpickle](https://github.com/cloudpipe/cloudpickle). However, this is not production-ready because the resulting artifacts cannot be loaded when running with a different Python version. In such cases, you should consider building a [custom Materializer](../handle-data-artifacts/handle-custom-data-types.md#custom-materializers) to save your objects in a more robust and efficient format.\n\nMoreover, using the `CloudpickleMaterializer` could allow users to upload of any kind of object. This could be exploited to upload a malicious file, which could execute arbitrary code on the vulnerable system.\n{% endhint %}\n\n```python\nfrom typing import Tuple\nfrom zenml import step\n\n@step\ndef square_root(number: int) -> float:\n    return number ** 0.5\n\n# To define a step with multiple outputs, use a `Tuple` type annotation\n@step\ndef divide(a: int, b: int) -> Tuple[int, int]:\n    return a // b, a % b\n```\n\nIf you want to make sure you get all the benefits of type annotating your steps, you can set the environment variable `ZENML_ENFORCE_TYPE_ANNOTATIONS` to `True`. ZenML will then raise an exception in case one of the steps you're trying to run is missing a type annotation.\n\n### Tuple vs multiple outputs\n\nIt is impossible for ZenML to detect whether you want your step to have a single output artifact of type `Tuple` or multiple output artifacts just by looking at the type annotation.\n\nWe use the following convention to differentiate between the two: When the `return` statement is followed"}
{"input": " by a tuple literal (e.g. `return 1, 2` or `return (value_1, value_2)`) we treat it as a step with multiple outputs. All other cases are treated as a step with a single output of type `Tuple`.\n\n```python\nfrom zenml import step\nfrom typing_extensions import Annotated\nfrom typing import Tuple\n\n# Single output artifact\n@step\ndef my_step() -> Tuple[int, int]:\n    output_value = (0, 1)\n    return output_value\n\n# Single output artifact with variable length\n@step\ndef my_step(condition) -> Tuple[int, ...]:\n    if condition:\n        output_value = (0, 1)\n    else:\n        output_value = (0, 1, 2)\n\n    return output_value\n\n# Single output artifact using the `Annotated` annotation\n@step\ndef my_step() -> Annotated[Tuple[int, ...], \"my_output\"]:\n    return 0, 1\n\n\n# Multiple output artifacts\n@step\ndef my_step() -> Tuple[int, int]:\n    return 0, 1\n\n\n# Not allowed: Variable length tuple annotation when using\n# multiple output artifacts\n@step\ndef my_step() -> Tuple[int, ...]:\n    return 0, 1\n```\n\n## Step output names\n\nBy default, ZenML uses the output name `output` for single output steps and `output_0, output_1, ...` for steps with multiple outputs. These output names are used to display your outputs in the dashboard and [fetch them after your pipeline is finished](fetching-pipelines.md).\n\nIf you want to use custom output names for your steps, use the `Annotated` type annotation:\n\n```python\nfrom typing_extensions import Annotated  # or `from typing import Annotated on Python 3.9+\nfrom typing import Tuple\nfrom zenml import step\n\n@step\ndef square_root(number: int) -> Annotated[float, \"custom_output_name\"]:\n    return number ** 0.5\n\n@step\ndef divide(a: int, b: int) -> Tuple[\n    Annotated[int, \"quotient\"],\n    Annotated[int, \"remainder\"]\n]:\n    return a // b, a % b\n```\n\n{% hint style=\"info\" %}\nIf you do not give your outputs custom names, the created artifacts will be named `{pipeline_name}::{step_name}::output` or `{pipeline_name}::{step_name}::output_{i}` in the dashboard. See the [documentation on artifact versioning and configuration](../../user-guide/starter-guide/manage-artifacts.md) for more information.\n{% endhint %}\n\n\n***\n\n### See Also:\n\n\n<table data-view=\"cards\">\n    <thead>\n    <tr>\n        <th></th>\n        <th></th>\n        <th></th>\n        <th data-hidden data-card-target data"}
{"input": "-type=\"content-ref\"></th>\n    </tr>\n    </thead>\n    <tbody>\n    <tr>\n        <td>Learn more about output annotation here</td>\n        <td></td>\n        <td></td>\n        <td><a href=\"../handle-data-artifacts/return-multiple-outputs-from-a-step.md\">return-multiple-outputs-from-a-step.md</a></td>\n    </tr>\n    <tr>\n        <td>For custom data types you should check these docs out</td>\n        <td></td>\n        <td></td>\n        <td><a href=\"../handle-data-artifacts/handle-custom-data-types.md\">handle-custom-data-types.md</a></td>\n    </tr>\n    </tbody>\n</table>\n\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: The best way to trigger a pipeline run so that it runs in the background\n---\n\n# Run pipelines asynchronously\n\nBy default your pipelines will run synchronously. This means your terminal will follow along the logs as the pipeline is being built/runs.&#x20;\n\nThis behavior can be changed in multiple ways. Either the orchestrator can be configured to always run asynchronously by setting `synchronous=False`. The other option is to temporarily set this at the pipeline configuration level during runtime.\n\n```python\nfrom zenml import pipeline\n\n@pipeline(settings = {\"orchestrator.<STACK_NAME>\": {\"synchronous\": False}})\ndef my_pipeline():\n  ...\n```\n\nor in a yaml config file:\n\n```yaml\nsettings:\n  orchestrator.<STACK_NAME>:\n    synchronous: false\n```\n\n***\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Learn more about orchestrators here</td><td></td><td></td><td><a href=\"../../component-guide/orchestrators/orchestrators.md\">orchestrators.md</a></td></tr></tbody></table>\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Learn how to delete pipelines.\n---\n\n# Delete a pipeline\n\nIn order to delete a pipeline, you can either use the CLI or the Python SDK:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\nzenml pipeline delete <PIPELINE_NAME>\n```\n{% endtab %}\n{% tab title=\"Python SDK\" %}\n```python\nfrom zenml.client import Client\n\nClient().delete_pipeline(<PIPELINE_NAME>)\n```\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nDeleting a pipeline does not automatically delete any of its associated runs or \nartifacts.\n{% endhint %}\n\n\n## Delete a pipeline run\n\nTo delete a pipeline run, you can use the following CLI command or the client:\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n```shell\nzenml pipeline runs delete <RUN_NAME_OR_ID>\n```\n{% endtab %}\n{% tab title=\"Python SDK\" %}\n```python\nfrom zenml.client import Client\n\nClient().delete_pipeline_run(<RUN_NAME_OR_ID>)\n```\n{% endtab %}\n{% endtabs %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  By default steps in ZenML pipelines are cached whenever code and parameters\n  stay unchanged.\n---\n\n# Control caching behavior\n\n```python\n@step(enable_cache=True) # set cache behavior at step level\ndef load_data(parameter: int) -> dict:\n    ...\n\n@step(enable_cache=False) # settings at step level override pipeline level\ndef train_model(data: dict) -> None:\n    ...\n\n@pipeline(enable_cache=True) # set cache behavior at step level\ndef simple_ml_pipeline(parameter: int):\n    ...\n```\n\n{% hint style=\"info\" %}\nCaching only happens when code and parameters stay the same.\n{% endhint %}\n\nLike many other step and pipeline settings, you can also change this afterward:\n\n```python\n# Same as passing it in the step decorator\nmy_step.configure(enable_cache=...)\n\n# Same as passing it in the pipeline decorator\nmy_pipeline.configure(enable_cache=...)\n```\n\n***\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Find out here how to configure this in a YAML file</td><td></td><td></td><td><a href=\"../use-configuration-files/\">use-configuration-files</a></td></tr></tbody></table>\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: >-\n  Building pipelines is as simple as adding the `@step` and `@pipeline`\n  decorators to your code.\n---\n\n# \u26d3\ufe0f Build a pipeline\n\n```python\n@step  # Just add this decorator\ndef load_data() -> dict:\n    training_data = [[1, 2], [3, 4], [5, 6]]\n    labels = [0, 1, 0]\n    return {'features': training_data, 'labels': labels}\n\n\n@step\ndef train_model(data: dict) -> None:\n    total_features = sum(map(sum, data['features']))\n    total_labels = sum(data['labels'])\n\n    # Train some model here\n\n    print(f\"Trained model using {len(data['features'])} data points. \"\n          f\"Feature sum is {total_features}, label sum is {total_labels}\")\n\n\n@pipeline  # This function combines steps together \ndef simple_ml_pipeline():\n    dataset = load_data()\n    train_model(dataset)\n```\n\nYou can now run this pipeline by simply calling the function:\n\n```python\nsimple_ml_pipeline()\n```\n\nWhen this pipeline is executed, the run of the pipeline gets logged to the ZenML dashboard where you can now go to look\nat its DAG and all the associated metadata. To access the dashboard you need to have a ZenML server either running\nlocally or remotely. See our documentation on this [here](../../getting-started/deploying-zenml/README.md).\n\n<figure><img src=\"../../.gitbook/assets/SimplePipelineDag.png\" alt=\"\"><figcaption><p>DAG representation in the ZenML Dahboard.</p></figcaption></figure>\n\nCheck below for more advanced ways to build and interact with your pipeline.\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Configure pipeline/step parameters</td><td></td><td></td><td><a href=\"use-pipeline-step-parameters.md\">use-pipeline-step-parameters.md</a></td></tr><tr><td>Name and annotate step outputs</td><td></td><td></td><td><a href=\"step-output-typing-and-annotation.md\">step-output-typing-and-annotation.md</a></td></tr><tr><td>Control caching behavior</td><td></td><td></td><td><a href=\"control-caching-behavior.md\">control-caching-behavior.md</a></td></tr><tr><td>Run pipeline from a pipeline</td><td></td><td></td><td><a href=\"trigger-a-pipeline-from-another.md\">trigger-a-pipeline-from-another.md</a></td></tr><tr><td>Control the"}
{"input": " execution order of steps</td><td></td><td></td><td><a href=\"control-execution-order-of-steps.md\">control-execution-order-of-steps.md</a></td></tr><tr><td>Customize the step invocation ids</td><td></td><td></td><td><a href=\"using-a-custom-step-invocation-id.md\">using-a-custom-step-invocation-id.md</a></td></tr><tr><td>Name your pipeline runs</td><td></td><td></td><td><a href=\"name-your-pipeline-and-runs.md\">name-your-pipeline-and-runs.md</a></td></tr><tr><td>Use failure/success hooks</td><td></td><td></td><td><a href=\"use-failure-success-hooks.md\">use-failure-success-hooks.md</a></td></tr><tr><td>Hyperparameter tuning</td><td></td><td></td><td><a href=\"hyper-parameter-tuning.md\">hyper-parameter-tuning.md</a></td></tr><tr><td>Attach metadata to steps</td><td></td><td></td><td><a href=\"../track-metrics-metadata/attach-metadata-to-steps.md\">attach-metadata-to-steps.md</a></td></tr><tr><td>Fetch metadata within steps</td><td></td><td></td><td><a href=\"../track-metrics-metadata/fetch-metadata-within-steps.md\">fetch-metadata-within-steps.md</a></td></tr><tr><td>Fetch metadata during pipeline composition</td><td></td><td></td><td><a href=\"../track-metrics-metadata/fetch-metadata-within-pipeline.md\">fetch-metadata-within-pipeline.md</a></td></tr><tr><td>Enable or disable logs storing</td><td></td><td></td><td><a href=\"../control-logging/enable-or-disable-logs-storing.md\">enable-or-disable-logs-storing.md</a></td></tr><tr><td>Special Metadata Types</td><td></td><td></td><td><a href=\"../track-metrics-metadata/logging-metadata.md\">logging-metadata.md</a></td></tr><tr><td>Access secrets in a step</td><td></td><td></td><td><a href=\"access-secrets-in-a-step.md\">access-secrets-in-a-step.md</a></td></tr></tbody></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Automatically configure your steps to retry if they fail.\n---\n\n# Allow step retry in case of failure\n\nZenML provides a built-in retry mechanism that allows you to configure automatic retries for your steps in case of failures. This can be useful when dealing with intermittent issues or transient errors. A common pattern when trying to run a step on GPU-backed hardware is that the provider will not have enough resources available, so you can set ZenML to handle the retries until the resources free up. You can configure three parameters for step retries:\n\n* **max_retries:** The maximum number of times the step should be retried in case of failure.\n* **delay:** The initial delay in **seconds** before the first retry attempt.\n* **backoff:** The factor by which the delay should be multiplied after each retry attempt.\n\n## Using the @step decorator:\n\nYou can specify the retry configuration directly in the definition of your step as follows:\n\n```python\nfrom zenml.config.retry_config import StepRetryConfig\n\n@step(\n    retry=StepRetryConfig(\n        max_retries=3, \n        delay=10, \n        backoff=2\n    )\n)\ndef my_step() -> None:\n    raise Exception(\"This is a test exception\")\nsteps:\n  my_step:\n    retry:\n      max_retries: 3\n      delay: 10\n      backoff: 2\n```\n\n{% hint style=\"info\" %}\nNote that infinite retries are not supported at the moment. If you set `max_retries` to a very large value or do not specify it at all, ZenML will still enforce an internal maximum number of retries to prevent infinite loops. We recommend setting a reasonable `max_retries` value based on your use case and the expected frequency of transient failures.\n{% endhint %}\n\n***\n\n### See Also:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Failure/Success Hooks</td><td></td><td></td><td><a href=\"use-failure-success-hooks.md\">use-failure-success-hooks.md</a></td></tr><tr><td>Configure pipelines</td><td></td><td></td><td><a href=\"../use-configuration-files/how-to-use-config.md\">./use-configuration-files/how-to-use-config.md</a></td></tr></tbody></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>"}
{"input": "# Control execution order of steps\n\nBy default, ZenML uses the data flowing between steps of your pipeline to determine the order in which steps get executed.\n\nThe following example shows a pipeline in which `step_3` depends on the outputs of `step_1` and `step_2`. This means that ZenML can execute both `step_1` and `step_2` in parallel but needs to wait until both are finished before `step_3` can be started.\n\n```python\nfrom zenml import pipeline\n\n@pipeline\ndef example_pipeline():\n    step_1_output = step_1()\n    step_2_output = step_2()\n    step_3(step_1_output, step_2_output)\n```\n\nIf you have additional constraints on the order in which steps get executed, you can specify non-data dependencies by passing the invocation IDs of steps that should run before your step like this: `my_step(after=\"other_step\")`. If you want to define multiple upstream steps, you can also pass a list for the `after` argument when calling your step: `my_step(after=[\"other_step\", \"other_step_2\"])`.\n\n{% hint style=\"info\" %}\nCheck out the [documentation here](using-a-custom-step-invocation-id.md) to learn about the invocation ID and how to use a custom one for your steps.\n{% endhint %}\n\n```python\nfrom zenml import pipeline\n\n@pipeline\ndef example_pipeline():\n    step_1_output = step_1(after=\"step_2\")\n    step_2_output = step_2()\n    step_3(step_1_output, step_2_output)\n```\n\nThis pipeline is similar to the one explained above, but this time ZenML will make sure to only start `step_1` after `step_2` has finished.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Configuring a pipeline at runtime.\n---\n\n# Runtime configuration of a pipeline run\n\nIt is often the case that there is a need to run a pipeline with a different configuration.\nIn this case, you should in most cases use the [`pipeline.with_options`](../use-configuration-files/README.md) method. You can do this:\n\n1. Either by explicitly configuring options like `with_options(steps=\"trainer\": {\"parameters\": {\"param1\": 1}})`\n2. Or by passing a YAML file using `with_options(config_file=\"path_to_yaml_file\")`.\n\nYou can learn more about these options [here](../use-configuration-files/README.md).\n\nHowever, there is one exception: if you would like to trigger a pipeline from the client\nor another pipeline, you would need to pass the `PipelineRunConfiguration` object.\nLearn more about this [here](../trigger-pipelines/trigger-a-pipeline-from-another.md).\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Using config files</td><td></td><td></td><td><a href=\"../use-configuration-files/README.md\">../use-configuration-files/README.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "# Name your pipeline runs\n\nIn the output logs of a pipeline run you will see the name of the run:\n\n```bash\nPipeline run training_pipeline-2023_05_24-12_41_04_576473 has finished in 3.742s.\n```\n\nThis name is automatically generated based on the current date and time. To change the name for a run, pass `run_name` as a parameter to the `with_options()` method:\n\n```python\ntraining_pipeline = training_pipeline.with_options(\n    run_name=\"custom_pipeline_run_name\"\n)\ntraining_pipeline()\n```\n\nPipeline run names must be unique, so if you plan to run your pipelines multiple times or run them on a schedule, make sure to either compute the run name dynamically or include one of the following placeholders that ZenML will replace:\n\n* `{{date}}` will resolve to the current date, e.g. `2023_02_19`\n* `{{time}}` will resolve to the current time, e.g. `11_07_09_326492`\n\n```python\ntraining_pipeline = training_pipeline.with_options(\n    run_name=f\"custom_pipeline_run_name_{{date}}_{{time}}\"\n)\ntraining_pipeline()\n```\n\nBe sure to include the `f` string prefix to allow for the placeholders to be replaced, as shown in the example above. Without the `f` prefix, the placeholders will not be replaced.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Access secrets in a step\n\n## Fetching secret values in a step\n\nZenML secrets are groupings of **key-value pairs** which are securely stored in the ZenML secrets store. Additionally, a\nsecret always has a **name** that allows you to fetch or reference them in your pipelines and stacks. In order to learn\nmore about how to configure and create secrets, please refer to\nthe [platform guide on secrets](../../getting-started/deploying-zenml/secret-management.md).\n\nYou can access secrets directly from within your steps through the ZenML `Client` API. This allows you to use your\nsecrets for querying APIs from within your step without hard-coding your access keys:\n\n```python\nfrom zenml import step\nfrom zenml.client import Client\n\nfrom somewhere import authenticate_to_some_api\n\n\n@step\ndef secret_loader() -> None:\n    \"\"\"Load the example secret from the server.\"\"\"\n    # Fetch the secret from ZenML.\n    secret = Client().get_secret(\"<SECRET_NAME>\")\n\n    # `secret.secret_values` will contain a dictionary with all key-value\n    # pairs within your secret.\n    authenticate_to_some_api(\n        username=secret.secret_values[\"username\"],\n        password=secret.secret_values[\"password\"],\n    )\n    ...\n```\n\n\n***\n\n### See Also:\n\n\n<table data-view=\"cards\">\n    <thead>\n    <tr>\n        <th></th>\n        <th></th>\n        <th></th>\n        <th data-hidden data-card-target data-type=\"content-ref\"></th>\n    </tr>\n    </thead>\n    <tbody>\n    <tr>\n        <td>Learn how to create and manage secrets</td>\n        <td></td>\n        <td></td>\n        <td><a href=\"../interact-with-secrets.md\">interact-with-secrets.md</a></td>\n    </tr>\n    <tr>\n        <td>Find out more about the secrets backend in ZenML</td>\n        <td></td>\n        <td></td>\n        <td><a href=\"../../getting-started/deploying-zenml/secret-management.md\">secret-management.md</a></td>\n    </tr>\n    </tbody>\n</table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Running failure and success hooks after step execution.\n---\n\n# Use failure/success hooks\n\nHooks are a way to perform an action after a step has completed execution. They can be useful in a variety of scenarios, such as sending notifications, logging, or cleaning up resources after a step has been completed.\n\nA hook executes right after step execution, within the same environment as the step, therefore it has access to all the dependencies that a step has. Currently, there are two sorts of hooks that can be defined: `on_failure` and `on_success` .\n\n* `on_failure`: This hook triggers in the event of a step failing.\n* `on_success`: This hook triggers in the event of a step succeeding.\n\nHere is a short demo for hooks in ZenML:\n\n{% embed url=\"https://www.youtube.com/watch?v=KUW2G3EsqF8\" %}\nFailure and Success Hooks in ZenML Short Demo\n{% endembed %}\n\n## Defining hooks\n\nA hook can be defined as a callback function, and must be accessible within the repository where the pipeline and steps are located.\n\nIn case of failure hooks, you can optionally add a `BaseException` argument to the hook, allowing you to access the concrete Exception that caused your step to fail:\n\n```python\nfrom zenml import step\n\ndef on_failure(exception: BaseException):\n    print(f\"Step failed: {str(exception)}\")\n\n\ndef on_success():\n    print(\"Step succeeded!\")\n\n\n@step(on_failure=on_failure)\ndef my_failing_step() -> int:\n    \"\"\"Returns an integer.\"\"\"\n    raise ValueError(\"Error\")\n\n\n@step(on_success=on_success)\ndef my_successful_step() -> int:\n    \"\"\"Returns an integer.\"\"\"\n    return 1\n```\n\nIn this example, we define two hooks: `on_failure` and `on_success`, which print a message when the step fails or succeeds, respectively. We then use these hooks with two steps, `my_failing_step` and `my_successful_step`. When `my_failing_step` is executed, it raises a `ValueError`, which triggers the `on_failure` hook. Similarly, when `my_successful_step` is executed, it returns an integer successfully, which triggers the on\\_success hook.\n\nA step can also be specified as a local user-defined function path (of the form `mymodule.myfile.my_function`). This is particularly useful when defining the hooks via a [YAML Config](../use-configuration-files/).\n\n## Defining hooks on a pipeline level\n\nIn some cases, there is a need to define a hook on all steps of a given pipeline. Rather than having to define it on all steps individually, you can also specify any hook on the pipeline level.\n\n```python\n@pipeline(on_failure=on_failure, on_success=on_success)\ndef my_pipeline(...):\n    ...\n```\n\n{% hint style=\"info\" %}\nNote, that **step-level"}
{"input": "** defined hooks take **precedence** over **pipeline-level** defined hooks.\n{% endhint %}\n\n<details>\n\n<summary>See it in action with the E2E example</summary>\n\n_To set up the local environment used below, follow the recommendations from the_ [_Project templates_](../setting-up-a-project-repository/using-project-templates.md)_._\n\nIn [`steps/alerts/notify_on.py`](../../../../examples/e2e/steps/alerts/notify_on.py), you will find a step to notify the user about success and a function used to notify the user about step failure using the [Alerter](../../component-guide/alerters/alerters.md) from the active stack.\n\nWe use `@step` for success notification to only notify the user about a fully successful pipeline run and not about every successful step.\n\nIn [`pipelines/training.py`](../../../../examples/e2e/pipelines/training.py), you can find the usage of a notification step and a function. We will attach a `notify_on_failure` function directly to the pipeline definition like this:\n\n```python\nfrom zenml import pipeline\n@pipeline(\n    ...\n    on_failure=notify_on_failure,\n    ...\n)\n```\n\nAt the very end of the training pipeline, we will execute the `notify_on_success` step, but only after all other steps have finished - we control it with `after` statement as follows:\n\n```python\n...\nlast_step_name = \"promote_metric_compare_promoter\"\n\nnotify_on_success(after=[last_step_name])\n...\n```\n\n</details>\n\n## Accessing step information inside a hook\n\nSimilar as for regular ZenML steps, you can use the [StepContext](../track-metrics-metadata/fetch-metadata-within-steps.md) to access information about the current pipeline run or step inside your hook function:\n\n```python\nfrom zenml import step, get_step_context\n\ndef on_failure(exception: BaseException):\n    context = get_step_context()\n    print(context.step_run.name)  # Output will be `my_step`\n    print(context.step_run.config.parameters)  # Print parameters of the step\n    print(type(exception))  # Of type value error\n    print(\"Step failed!\")\n\n\n@step(on_failure=on_failure)\ndef my_step(some_parameter: int = 1)\n    raise ValueError(\"My exception\")\n```\n\n<details>\n\n<summary>See it in action with the E2E example</summary>\n\n_To set up the local environment used below, follow the recommendations from the_ [_Project templates_](../setting-up-a-project-repository/using-project-templates.md)_._\n\nIn [`steps/alerts/notify_on.py`](../../../../examples/e2e/steps/alerts/notify_on.py), you will find a step to notify the user about success and a function used to notify the user about step failure using the [Alerter](../../component-guide/alerters/alerters.md) from the active stack.\n\n"}
{"input": "We use `@step` for success notification to only notify the user about a fully successful pipeline run and not about every successful step.\n\nInside the helper function `build_message()`, you will find an example on how developers can work with [StepContext](../track-metrics-metadata/fetch-metadata-within-steps.md) to form a proper notification:\n\n```python\nfrom zenml import get_step_context\n\ndef build_message(status: str) -> str:\n    \"\"\"Builds a message to post.\n\n    Args:\n        status: Status to be set in text.\n\n    Returns:\n        str: Prepared message.\n    \"\"\"\n    step_context = get_step_context()\n    run_url = get_run_url(step_context.pipeline_run)\n\n    return (\n        f\"Pipeline `{step_context.pipeline.name}` [{str(step_context.pipeline.id)}] {status}!\\n\"\n        f\"Run `{step_context.pipeline_run.name}` [{str(step_context.pipeline_run.id)}]\\n\"\n        f\"URL: {run_url}\"\n    )\n\n@step(enable_cache=False)\ndef notify_on_success() -> None:\n    \"\"\"Notifies user on pipeline success.\"\"\"\n    step_context = get_step_context()\n    if alerter and step_context.pipeline_run.config.extra[\"notify_on_success\"]:\n        alerter.post(message=build_message(status=\"succeeded\"))\n```\n\n</details>\n\n## Linking to the `Alerter` Stack component\n\nA common use case is to use the [Alerter](../../component-guide/alerters/alerters.md) component inside the failure or success hooks to notify relevant people. It is quite easy to do this:\n\n```python\nfrom zenml import get_step_context\nfrom zenml.client import Client\n\ndef on_failure():\n    step_name = get_step_context().step_run.name\n    Client().active_stack.alerter.post(f\"{step_name} just failed!\")\n```\n\nZenML provides standard failure and success hooks that use the alerter you have configured in your stack. Here's an example of how to use them in your pipelines:\n\n```python\nfrom zenml.hooks import alerter_success_hook, alerter_failure_hook\n\n\n@step(on_failure=alerter_failure_hook, on_success=alerter_success_hook)\ndef my_step(...):\n    ...\n```\n\n<details>\n\n<summary>See it in action with the E2E example</summary>\n\n_To set up the local environment used below, follow the recommendations from the_ [_Project templates_](../setting-up-a-project-repository/using-project-templates.md)_._\n\nIn [`steps/alerts/notify_on.py`](../../../../examples/e2e/steps/alerts/notify_on.py), you will find a step to notify the user about success and a function used to notify the user about step failure using the [Alerter](../../component-guide/alerters/alerters.md) from the active stack.\n\nWe use `@step` for success notification to only notify the user about a fully"}
{"input": " successful pipeline run and not about every successful step.\n\nInside this code file, you can find how developers can work with Al component to send notification messages across configured channels:\n\n```python\nfrom zenml.client import Client\nfrom zenml import get_step_context\n\nalerter = Client().active_stack.alerter\n\ndef notify_on_failure() -> None:\n    \"\"\"Notifies user on step failure. Used in Hook.\"\"\"\n    step_context = get_step_context()\n    if alerter and step_context.pipeline_run.config.extra[\"notify_on_failure\"]:\n        alerter.post(message=build_message(status=\"failed\"))\n```\n\nIf the Al component is not present in Stack we suppress notification, but you can also dump it to the log as Error using:\n\n```python\nfrom zenml.client import Client\nfrom zenml.logger import get_logger\nfrom zenml import get_step_context\n\nlogger = get_logger(__name__)\nalerter = Client().active_stack.alerter\n\ndef notify_on_failure() -> None:\n    \"\"\"Notifies user on step failure. Used in Hook.\"\"\"\n    step_context = get_step_context()\n    if step_context.pipeline_run.config.extra[\"notify_on_failure\"]:\n        if alerter:\n            alerter.post(message=build_message(status=\"failed\"))\n        else:\n            logger.error(message=build_message(status=\"failed\"))\n```\n\n</details>\n\n## Using the OpenAI ChatGPT failure hook\n\nThe OpenAI ChatGPT failure hook is a hook that uses the OpenAI integration to generate a possible fix for whatever exception caused the step to fail. It is quite easy to use. (You will need [a valid OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) that has correctly set up billing for this.)\n\n{% hint style=\"warning\" %}\nNote that using this integration will incur charges on your OpenAI account.\n{% endhint %}\n\nFirst, ensure that you have the OpenAI integration installed and have stored your API key within a ZenML secret:\n\n```shell\nzenml integration install openai\nzenml secret create openai --api_key=<YOUR_API_KEY>\n```\n\nThen, you can use the hook in your pipeline:\n\n```python\nfrom zenml.integration.openai.hooks import openai_chatgpt_alerter_failure_hook\nfrom zenml import step\n\n@step(on_failure=openai_chatgpt_alerter_failure_hook)\ndef my_step(...):\n    ...\n```\n\nIf you had set up a Slack alerter as your alerter, for example, then you would see a message like this:\n\n![OpenAI ChatGPT Failure Hook](../../.gitbook/assets/failure\\_alerter.png)\n\nYou can use the suggestions as input that can help you fix whatever is going wrong in your code. If you have GPT-4 enabled for your account, you can use the `openai_gpt4_alerter_failure_hook"}
{"input": "` hook instead (imported from the same module).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Inspecting a finished pipeline run and its outputs.\n---\n\n# Fetching pipelines\n\nOnce a pipeline run has been completed, we can access the corresponding information in code, which enables the following:\n\n* Loading artifacts like models or datasets saved by previous runs\n* Accessing metadata or configurations of previous runs\n* Programmatically inspecting the lineage of pipeline runs and their artifacts\n\nThe hierarchy of pipelines, runs, steps, and artifacts is as follows:\n\n```mermaid\nflowchart LR\n    pipelines -->|1:N| runs\n    runs -->|1:N| steps\n    steps -->|1:N| artifacts\n```\n\nAs you can see from the diagram, there are many layers of 1-to-N relationships.\n\nLet us investigate how to traverse this hierarchy level by level:\n\n## Pipelines\n\n### Get a pipeline via the client\n\nAfter you have run a pipeline at least once, you can also fetch the pipeline via the [`Client.get_pipeline()`](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/#zenml.client.Client.get\\_pipeline) method.\n\n```python\nfrom zenml.client import Client\n\npipeline_model = Client().get_pipeline(\"first_pipeline\")\n```\n\n{% hint style=\"info\" %}\nCheck out the [ZenML Client Documentation](../../reference/python-client.md) for more information on the `Client` class and its purpose.\n{% endhint %}\n\n### Discover and list all pipelines\n\nIf you're not sure which pipeline you need to fetch, you can find a list of all registered pipelines in the ZenML dashboard, or list them programmatically either via the Client or the CLI.\n\n{% tabs %}\n{% tab title=\"Python\" %}\nYou can use the [`Client.list_pipelines()`](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/#zenml.client.Client.list\\_pipelines) method to get a list of all pipelines registered in ZenML:\n\n```python\nfrom zenml.client import Client\n\npipelines = Client().list_pipelines()\n```\n{% endtab %}\n\n{% tab title=\"CLI\" %}\nAlternatively, you can also list pipelines with the following CLI command:\n\n```shell\nzenml pipeline list\n```\n{% endtab %}\n{% endtabs %}\n\n## Runs\n\nEach pipeline can be executed many times, resulting in several **Runs**.\n\n### Get all runs of a pipeline\n\nYou can get a list of all runs of a pipeline using the `runs` property of the pipeline:\n\n```python\nruns = pipeline_model.runs\n```\n\nThe result will be a list of the most recent runs of this pipeline, ordered from newest to oldest.\n\n{% hint style=\"info\" %}\nAlternatively, you can also use the `pipeline_model.get_runs()` method which allows you to specify detailed parameters for filtering or pagination. See the [ZenML SDK Docs](../../reference/python-client.md#list-of-resources) for more information.\n{% endhint %}\n\n### Get"}
{"input": " the last run of a pipeline\n\nTo access the most recent run of a pipeline, you can either use the `last_run` property or access it through the `runs` list:\n\n```\nlast_run = pipeline_model.last_run  # OR: pipeline_model.runs[0]\n```\n\n{% hint style=\"info\" %}\nIf your most recent runs have failed, and you want to find the last run that has succeeded, you can use the `last_successful_run` property instead.\n{% endhint %}\n\n### Get the latest run from a pipeline\n\nCalling a pipeline executes it and then returns the response of the freshly executed run.\n\n```python\nrun = training_pipeline()\n```\n\n{% hint style=\"warning\" %}\nThe run that you get back is the model stored in the ZenML database at the point of the method call. This means the pipeline run is still initializing and no steps have been run. To get the latest state can get a refreshed version from the client:\n\n```python\nfrom zenml.client import Client\n\nClient().get_pipeline_run(run.id) # to get a refreshed version\n```\n{% endhint %}\n\n### Get a run via the client\n\nIf you already know the exact run that you want to fetch (e.g., from looking at the dashboard), you can use the [`Client.get_pipeline_run()`](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/#zenml.client.Client.get\\_pipeline\\_run) method to fetch the run directly without having to query the pipeline first:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"first_pipeline-2023_06_20-16_20_13_274466\")\n```\n\n{% hint style=\"info\" %}\nSimilar to pipelines, you can query runs by either ID, name, or name prefix, and you can also discover runs through the Client or CLI via the [`Client.list_pipeline_runs()`](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/#zenml.client.Client.list\\_pipeline\\_runs) or `zenml pipeline runs list` commands.\n{% endhint %}\n\n### Run information\n\nEach run has a collection of useful information which can help you reproduce your runs. In the following, you can find a list of some of the most useful pipeline run information, but there is much more available. See the [`PipelineRunResponse`](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-models/#zenml.models.v2.core.pipeline\\_run.PipelineRunResponse) definition for a comprehensive list.\n\n#### Status\n\nThe status of a pipeline run. There are five possible states: initialized, failed, completed, running, and cached.\n\n```python\nstatus = run.status\n```\n\n#### Configuration\n\nThe `pipeline_configuration` is an object that contains all configurations of the pipeline and pipeline run, including the [pipeline-level settings](../../user-guide/production"}
{"input": "-guide/configure-pipeline.md), which we will learn more about later:\n\n```python\npipeline_config = run.config\npipeline_settings = run.config.settings\n```\n\n#### Component-Specific metadata\n\nDepending on the stack components you use, you might have additional component-specific metadata associated with your run, such as the URL to the UI of a remote orchestrator. You can access this component-specific metadata via the `run_metadata` attribute:\n\n````python\nrun_metadata = run.run_metadata\n# The following only works for runs on certain remote orchestrators\norchestrator_url = run_metadata[\"orchestrator_url\"].value\n\n## Steps\n\nWithin a given pipeline run you can now further zoom in on individual steps using the `steps` attribute:\n\n```python\n# get all steps of a pipeline for a given run\nsteps = run.steps\n\n# get a specific step by its invocation ID\nstep = run.steps[\"first_step\"]\n````\n\n{% hint style=\"info\" %}\nIf you're only calling each step once inside your pipeline, the **invocation ID** will be the same as the name of your step. For more complex pipelines, check out [this page](../build-pipelines/using-a-custom-step-invocation-id.md) to learn more about the invocation ID.\n{% endhint %}\n\n### Inspect pipeline runs with our VS Code extension\n\n![GIF of our VS code extension, showing some of the uses of the sidebar](../../.gitbook/assets/zenml-extension-shortened.gif)\n\nIf you are using [our VS Code extension](https://marketplace.visualstudio.com/items?itemName=ZenML.zenml-vscode), you can easily view your pipeline runs by opening the sidebar (click on the ZenML icon). You can then click on any particular pipeline run to see its status and some other metadata. If you want to delete a run, you can also do so from the same sidebar view.\n\n### Step information\n\nSimilar to the run, you can use the `step` object to access a variety of useful information:\n\n* The parameters used to run the step via `step.config.parameters`,\n* The step-level settings via `step.config.settings`,\n* Component-specific step metadata, such as the URL of an experiment tracker or model deployer, via `step.run_metadata`\n\nSee the [`StepRunResponse`](https://github.com/zenml-io/zenml/blob/main/src/zenml/models/v2/core/step_run.py) definition for a comprehensive list of available information.\n\n## Artifacts\n\nEach step of a pipeline run can have multiple output and input artifacts that we can inspect via the `outputs` and `inputs` properties.\n\nTo inspect the output artifacts of a step, you can use the `outputs` attribute, which is a dictionary that can be indexed using the name of an output. Alternatively, if your step only has a single output, you can use the `output` property as a shortcut directly:\n\n```python"}
{"input": "\n# The outputs of a step are accessible by name\noutput = step.outputs[\"output_name\"]\n\n# If there is only one output, you can use the `.output` property instead \noutput = step.output\n\n# use the `.load()` method to load the artifact into memory\nmy_pytorch_model = output.load()  \n```\n\nSimilarly, you can use the `inputs` and `input` properties to get the input artifacts of a step instead.\n\n{% hint style=\"info\" %}\nCheck out [this page](../../user-guide/starter-guide/manage-artifacts.md#giving-names-to-your-artifacts) to see what the output names of your steps are and how to customize them.\n{% endhint %}\n\nNote that the output of a step corresponds to a specific artifact version.\n\n### Fetching artifacts directly\n\nIf you'd like to fetch an artifact or an artifact version directly, it is easy to do so with the `Client`:\n\n```python\nfrom zenml.client import Client\n\n# Get artifact\nartifact = Client().get_artifact('iris_dataset')\nartifact.versions  # Contains all the versions of the artifact\noutput = artifact.versions['2022']  # Get version name \"2022\" \n\n# Get artifact version directly:\n\n# Using version name:\noutput = Client().get_artifact_version('iris_dataset', '2022')\n\n# Using UUID\noutput = Client().get_artifact_version('f429f94c-fb15-43b5-961d-dbea287507c5')\nloaded_artifact = output.load()\n```\n\n### Artifact information\n\nRegardless of how one fetches it, each artifact contains a lot of general information about the artifact as well as datatype-specific metadata and visualizations.\n\n#### Metadata\n\nAll output artifacts saved through ZenML will automatically have certain datatype-specific metadata saved with them. NumPy Arrays, for instance, always have their storage size, `shape`, `dtype`, and some statistical properties saved with them. You can access such metadata via the `run_metadata` attribute of an output, e.g.:\n\n```python\noutput_metadata = output.run_metadata\nstorage_size_in_bytes = output_metadata[\"storage_size\"].value\n```\n\nWe will talk more about metadata [in the next section](../../user-guide/starter-guide/manage-artifacts.md#logging-metadata-for-an-artifact).\n\n#### Visualizations\n\nZenML automatically saves visualizations for many common data types. Using the `visualize()` method you can programmatically show these visualizations in Jupyter notebooks:\n\n```python\noutput.visualize()\n```\n\n![output.visualize() Output](../../.gitbook/assets/artifact\\_visualization\\_evidently.png)\n\n{% hint style=\"info\" %}\nIf you're not in a Jupyter notebook, you can simply view the visualizations in the ZenML dashboard by running `zenml up` and clicking on the respective artifact in the pipeline run DAG instead. Check out the [artifact visualization page](../"}
{"input": "visualize-artifacts/README.md) to learn more about how to build and view artifact visualizations in ZenML!\n{% endhint %}\n\n## Fetching information during run execution\n\nWhile most of this document has focused on fetching objects after a pipeline run has been completed, the same logic can also be used within the context of a running pipeline.\n\nThis is often desirable in cases where a pipeline is running continuously over time and decisions have to be made according to older runs.\n\nFor example, this is how we can fetch the last pipeline run of the same pipeline from within a ZenML step:\n\n```python\nfrom zenml import get_step_context\nfrom zenml.client import Client\n\n@step\ndef my_step():\n    # Get the name of the current pipeline run\n    current_run_name = get_step_context().pipeline_run.name\n\n    # Fetch the current pipeline run\n    current_run = Client().get_pipeline_run(current_run_name)\n\n    # Fetch the previous run of the same pipeline \n    previous_run = current_run.pipeline.runs[1]  # index 0 is the current run\n```\n\n{% hint style=\"info\" %}\nAs shown in the example, we can get additional information about the current run using the `StepContext`, which is explained in more detail in the [advanced docs](../track-metrics-metadata/fetch-metadata-within-steps.md).\n{% endhint %}\n\n## Code example\n\nThis section combines all the code from this section into one simple script that you can use to see the concepts discussed above:\n\n<details>\n\n<summary>Code Example of this Section</summary>\n\nPutting it all together, this is how we can load the model trained by the `svc_trainer` step of our example pipeline from the previous sections:\n\n```python\nfrom typing_extensions import Tuple, Annotated\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.svm import SVC\n\nfrom zenml import pipeline, step\nfrom zenml.client import Client\n\n\n@step\ndef training_data_loader() -> Tuple[\n    Annotated[pd.DataFrame, \"X_train\"],\n    Annotated[pd.DataFrame, \"X_test\"],\n    Annotated[pd.Series, \"y_train\"],\n    Annotated[pd.Series, \"y_test\"],\n]:\n    \"\"\"Load the iris dataset as tuple of Pandas DataFrame / Series.\"\"\"\n    iris = load_iris(as_frame=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n\n\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Tuple[\n    Annotated[ClassifierMixin, \"trained"}
{"input": "_model\"],\n    Annotated[float, \"training_acc\"],\n]:\n    \"\"\"Train a sklearn SVC classifier and log to MLflow.\"\"\"\n    model = SVC(gamma=gamma)\n    model.fit(X_train.to_numpy(), y_train.to_numpy())\n    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n    print(f\"Train accuracy: {train_acc}\")\n    return model, train_acc\n\n\n@pipeline\ndef training_pipeline(gamma: float = 0.002):\n    X_train, X_test, y_train, y_test = training_data_loader()\n    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)\n\n\nif __name__ == \"__main__\":\n    # You can run the pipeline and get the run object directly\n    last_run = training_pipeline()\n    print(last_run.id)\n\n    # You can also use the class directly with the `model` object\n    last_run = training_pipeline.model.last_run\n    print(last_run.id)\n\n    # OR you can fetch it after execution is finished:\n    pipeline = Client().get_pipeline(\"training_pipeline\")\n    last_run = pipeline.last_run\n    print(last_run.id)\n\n    # You can now fetch the model\n    trainer_step = last_run.steps[\"svc_trainer\"]\n    model = trainer_step.outputs[\"trained_model\"].load()\n```\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Running a hyperparameter tuning trial with ZenML.\n---\n\n# Hyperparameter tuning\n\n{% hint style=\"warning\" %}\nHyperparameter tuning is not yet a first-class citizen in ZenML, but it is [(high up) on our roadmap of features](https://zenml.featureos.app/p/enable-hyper-parameter-tuning) and will likely receive first-class ZenML support soon. In the meanwhile, the following example shows how hyperparameter tuning can currently be implemented within a ZenML run.\n{% endhint %}\n\nA basic iteration through a number of hyperparameters can be achieved with ZenML by using a simple pipeline like this:\n\n```python\n@pipeline\ndef my_pipeline(step_count: int) -> None:\n    data = load_data_step()\n    after = []\n    for i in range(step_count):\n        train_step(data, learning_rate=i * 0.0001, name=f\"train_step_{i}\")\n        after.append(f\"train_step_{i}\")\n    model = select_model_step(..., after=after)\n```\n\nThis is an implementation of a basic grid search (across a single dimension) that would allow for a different learning rate to be used across the same `train_step`. Once that step has been run for all the different learning rates, the `select_model_step` finds which hyperparameters gave the best results or performance.\n\n<details>\n\n<summary>See it in action with the E2E example</summary>\n\n_To set up the local environment used below, follow the recommendations from the_ [_Project templates_](../setting-up-a-project-repository/using-project-templates.md)_._\n\nIn [`pipelines/training.py`](../../../../examples/e2e/pipelines/training.py), you will find a training pipeline with a `Hyperparameter tuning stage` section. It contains a `for` loop that runs the `hp_tuning_single_search` over the configured model search spaces, followed by the `hp_tuning_select_best_model` being executed after all search steps are completed. As a result, we are getting `best_model_config` to be used to train the best possible model later on.\n\n```python\n...\n########## Hyperparameter tuning stage ##########\nafter = []\nsearch_steps_prefix = \"hp_tuning_search_\"\nfor i, model_search_configuration in enumerate(\n    MetaConfig.model_search_space\n):\n    step_name = f\"{search_steps_prefix}{i}\"\n    hp_tuning_single_search(\n        model_metadata=ExternalArtifact(\n            value=model_search_configuration,\n        ),\n        id=step_name,\n        dataset_trn=dataset_trn,\n        dataset_tst=dataset_tst,\n        target=target,\n    )\n    after.append(step_name)\nbest_model_config = hp_tuning_select_best_model(\n    search_steps_prefix=search_steps_prefix, after=after\n)\n...\n```\n\n</details>\n\nThe main challenge of this implementation is that it is currently not possible to pass a variable number of artifacts into a step"}
{"input": " programmatically, so the `select_model_step` needs to query all artifacts produced by the previous steps via the ZenML Client instead:\n\n```python\nfrom zenml import step, get_step_context\nfrom zenml.client import Client\n\n@step\ndef select_model_step():\n    run_name = get_step_context().pipeline_run.name\n    run = Client().get_pipeline_run(run_name)\n\n    # Fetch all models trained by a 'train_step' before\n    trained_models_by_lr = {}\n    for step_name, step in run.steps.items():\n        if step_name.startswith(\"train_step\"):\n            for output_name, output in step.outputs.items():\n                if output_name == \"<NAME_OF_MODEL_OUTPUT_IN_TRAIN_STEP>\":\n                    model = output.load()\n                    lr = step.config.parameters[\"learning_rate\"]\n                    trained_models_by_lr[lr] = model\n    \n    # Evaluate the models to find the best one\n    for lr, model in trained_models_by_lr.items():\n        ...\n```\n\n<details>\n\n<summary>See it in action with the E2E example</summary>\n\n_To set up the local environment used below, follow the recommendations from the_ [_Project templates_](../setting-up-a-project-repository/using-project-templates.md)_._\n\nIn the `steps/hp_tuning` folder, you will find two step files, which can be used as a starting point for building your own hyperparameter search tailored specifically to your use case:\n\n* [`hp_tuning_single_search(...)`](../../../../examples/e2e/steps/hp_tuning/hp_tuning_single_search.py) is performing a randomized search for the best model hyperparameters in a configured space.\n* [`hp_tuning_select_best_model(...)`](../../../../examples/e2e/steps/hp_tuning/hp_tuning_select_best_model.py) is searching for the best hyperparameters, looping other results of previous random searches to find the best model according to a defined metric.\n\n</details>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Reuse steps between pipelines.\n---\n\n# Compose pipelines\n\nSometimes it can be useful to extract some common functionality into separate functions\nin order to avoid code duplication. To facilitate this, ZenML allows you to compose your pipelines:\n\n```python\nfrom zenml import pipeline\n\n@pipeline\ndef data_loading_pipeline(mode: str):\n    if mode == \"train\":\n        data = training_data_loader_step()\n    else:\n        data = test_data_loader_step()\n    \n    processed_data = preprocessing_step(data)\n    return processed_data\n\n\n@pipeline\ndef training_pipeline():\n    training_data = data_loading_pipeline(mode=\"train\")\n    model = training_step(data=training_data)\n    test_data = data_loading_pipeline(mode=\"test\")\n    evaluation_step(model=model, data=test_data)\n```\n\n{% hint style=\"info\" %}\nHere we are calling one pipeline from within another pipeline, so functionally the `data_loading_pipeline` is functioning as a step within the `training_pipeline`, i.e. the steps of the former are added to the latter. Only the parent pipeline will be visible in the dashboard. In order to actually trigger a pipeline from another, see [here](../trigger-pipelines/trigger-a-pipeline-from-another.md)\n{% endhint %}\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Learn more about orchestrators here</td><td></td><td></td><td><a href=\"../../component-guide/orchestrators/orchestrators.md\">orchestrators.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "# Get past pipeline/step runs\n\nIn order to get past pipeline/step runs, you can use the `get_pipeline` method\nin combination with the `last_run` property or just index into the runs:\n\n<pre class=\"language-python\"><code class=\"lang-python\">from zenml.client import Client\n\nclient = Client()\n<strong>\n</strong><strong># Retrieve a pipeline by its name\n</strong><strong>p = client.get_pipeline(\"mlflow_train_deploy_pipeline\")\n</strong><strong>\n</strong><strong># Get the latest run of this pipeline\n</strong><strong>latest_run = p.last_run\n</strong><strong># Alternatively you can also access runs by index or name\n</strong><strong>first_run = p[0]\n</strong><strong>\n</strong></code></pre>\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Using a custom step invocation ID\n\nWhen calling a ZenML step as part of your pipeline, it gets assigned a unique **invocation ID** that you can use to reference this step invocation when [defining the execution order](control-execution-order-of-steps.md) of your pipeline steps or use it to [fetch information](fetching-pipelines.md) about the invocation after the pipeline has finished running.\n\n```python\nfrom zenml import pipeline, step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef example_pipeline():\n    # When calling a step for the first time inside a pipeline,\n    # the invocation ID will be equal to the step name -> `my_step`.\n    my_step()\n    # When calling the same step again, the suffix `_2`, `_3`, ... will\n    # be appended to the step name to generate a unique invocation ID.\n    # For this call, the invocation ID would be `my_step_2`.\n    my_step()\n    # If you want to use a custom invocation ID when calling a step, you can\n    # do so by passing it like this. If you pass a custom ID, it needs to be\n    # unique for all the step invocations that happen as part of this pipeline.\n    my_step(id=\"my_custom_invocation_id\")\n```\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "---\ndescription: Use Jupyter Notebooks to run remote steps or pipelines\n---\n\n# \ud83d\udcd4 Run remote pipelines from notebooks\n\nZenML steps and pipelines can be defined in a Jupyter notebook and executed remotely. To do so, ZenML will extract the code from your notebook cells and run them as Python modules inside the Docker containers that execute your pipeline steps remotely. For this to work, the notebook cells in which you define your steps need to meet certain conditions.\n\nLearn more about it in the following sections:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td>Define steps in notebook cells</td><td></td><td></td><td><a href=\"define-steps-in-notebook-cells.md\">define-steps-in-notebook-cells.md</a></td></tr></tbody></table>\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "\n# Define steps in notebook cells\n\nIf you want to run ZenML steps defined in notebook cells remotely (either with a remote [orchestrator](../../component-guide/orchestrators/orchestrators.md) or [step operator](../../component-guide/step-operators/step-operators.md)), the cells defining your steps must meet the following conditions:\n- The cell can only contain python code, no Jupyter magic commands or shell commands starting with a `%` or `!`.\n- The cell **must not** call code from other notebook cells. Functions or classes imported from python files are allowed.\n- The cell **must not** rely on imports of previous cells. This means your cell must perform all the imports it needs itself, including ZenML imports like `from zenml import step`.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "# Associate a pipeline with a Model\n\nThe most common use-case for a Model is to associate it with a pipeline.\n\n<pre class=\"language-python\"><code class=\"lang-python\"><strong>from zenml import pipeline\n</strong>from zenml import Model\n\n<strong>@pipeline(\n</strong><strong>    model=Model(\n</strong><strong>        name=\"ClassificationModel\",  # Give your models unique names\n</strong><strong>        tags=[\"MVP\", \"Tabular\"]  # Use tags for future filtering\n</strong><strong>    )\n</strong><strong>)\n</strong>def my_pipeline():\n    ...\n</code></pre>\n\nThis will associate this pipeline with the model specified. In case the model already exists, this will create a new version of that model.\n\nIn case you want to attach the pipeline to an existing model version, specify this as well.\n\n```python\nfrom zenml import pipeline\nfrom zenml import Model\nfrom zenml.enums import ModelStages\n\n@pipeline(\n    model=Model(\n        name=\"ClassificationModel\",  # Give your models unique names\n        tags=[\"MVP\", \"Tabular\"],  # Use tags for future filtering\n        version=ModelStages.LATEST  # Alternatively use a stage: [STAGING, PRODUCTION]]\n    )\n)\ndef my_pipeline():\n    ...\n```\n\nFeel free to also move the Model configuration into your configuration files:\n\n```yaml\n...\n\nmodel:\n  name: text_classifier\n  description: A breast cancer classifier\n  tags: [\"classifier\",\"sgd\"]\n\n...\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Load artifacts from Model\n\nOne of the more common use-cases for a Model is to pass artifacts between pipelines (a pattern we have seen [before](connecting-artifacts-via-a-model.md)). However, when and how to load these artifacts is important to know as well.\n\nAs an example, let's have a look at a two-pipeline project, where the first pipeline is running training logic and the second runs batch inference leveraging trained model artifact(s):\n\n```python\nfrom typing_extensions import Annotated\nfrom zenml import get_pipeline_context, pipeline, Model\nfrom zenml.enums import ModelStages\nimport pandas as pd\nfrom sklearn.base import ClassifierMixin\n\n\n@step\ndef predict(\n    model: ClassifierMixin,\n    data: pd.DataFrame,\n) -> Annotated[pd.Series, \"predictions\"]:\n    predictions = pd.Series(model.predict(data))\n    return predictions\n\n@pipeline(\n    model=Model(\n        name=\"iris_classifier\",\n        # Using the production stage\n        version=ModelStages.PRODUCTION,\n    ),\n)\ndef do_predictions():\n    # model name and version are derived from pipeline context\n    model = get_pipeline_context().model\n    inference_data = load_data()\n    predict(\n        # Here, we load in the `trained_model` from a trainer step\n        model=model.get_model_artifact(\"trained_model\"),  \n        data=inference_data,\n    )\n\n\nif __name__ == \"__main__\":\n    do_predictions()\n```\n\nIn the example above we used `get_pipeline_context().model` property to acquire the model context in which the pipeline is running. During pipeline compilation this context will not yet have been evaluated, because `Production` model version is not a stable version name and another model version can become `Production` before it comes to the actual step execution. The same applies to calls like `model.get_model_artifact(\"trained_model\")`; it will get stored in the step configuration for delayed materialization which will only happen during the step run itself.\n\nIt is also possible to achieve the same using bare `Client` methods reworking the pipeline code as follows:\n\n```python\nfrom zenml.client import Client\n\n@pipeline\ndef do_predictions():\n    # model name and version are directly passed into client method\n    model = Client().get_model_version(\"iris_classifier\", ModelStages.PRODUCTION)\n    inference_data = load_data()\n    predict(\n        # Here, we load in the `trained_model` from a trainer step\n        model=model.get_model_artifact(\"trained_model\"),  \n        data=inference_data,\n    )\n```\n\nIn this case the evaluation of the actual artifact will happen only when the step is actually running.\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd"}
{"input": "-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Model Versions\n\nEach model can have many versions. Model versions are a way for you to track different iterations of your training process, complete with some extra dashboard and API functionality to support the full ML lifecycle.\n\nE.g. Based on your business rules during training, you can associate model version with stages and promote them to production. You have an interface that allows you to link these versions with non-technical artifacts and data, e.g. business data, datasets, or even stages in your process and workflow.\n\nModel versions are created implicitly as you are running your machine learning training, so you don't have to immediately think about this. If you want more control over versions, our API has you covered, with an option to explicitly name your versions.\n\n## Explicitly name your model version\n\nIf you want to explicitly name your model version, you can do so by passing in the `version` argument to the `Model` object. If you don't do this, ZenML will automatically generate a version number for you.\n\n```python\nfrom zenml import Model, step, pipeline\n\nmodel= Model(\n    name=\"my_model\",\n    version=\"1.0.5\"\n)\n\n# The step configuration will take precedence over the pipeline\n@step(model=model)\ndef svc_trainer(...) -> ...:\n    ...\n\n# This configures it for all steps within the pipeline\n@pipeline(model=model)\ndef training_pipeline( ... ):\n    # training happens here\n```\n\nHere we are specifically setting the model configuration for a particular step or for the pipeline as a whole.\n\n{% hint style=\"info\" %}\nPlease note in the above example if the model version exists, it is automatically associated with the pipeline and becomes active in the pipeline context. Therefore, a user should be careful and intentional as to whether you want to create a new pipeline, or fetch an existing one. See below for an example of fetching a model from an existing version/stage.\n{% endhint %}\n\n## Fetching model versions by stage\n\nA common pattern is to assign a special `stage` to a model version, i.e. `production`, `staging`, `development` etc. This marks this version especially, and can be used to fetch it using a particular semantic meaning, disconnected from the concrete model version. A model version can be assigned a particular stage in the dashboard or by executing the following command in the CLI:\n\n```shell\nzenml model version update MODEL_NAME --stage=STAGE\n```\n\nThese stages can then be passed in as a `version` to fetch the right model version at a later point:\n\n\n```python\nfrom zenml import Model, step, pipeline\n\nmodel= Model(\n    name=\"my_model\",\n    version=\"production\"\n)\n\n# The step configuration will take precedence over the pipeline\n@step(model=model)\ndef svc_trainer(...) -> ...:\n    ...\n\n# This configures it for all steps within the pipeline\n@pipeline(model=model)\ndef training_pipeline("}
{"input": " ... ):\n    # training happens here\n```\n\n## Autonumbering of versions\n\nZenML automatically numbers your model versions for you. If you don't specify a version number, or if you pass `None` into the `version` argument of the `Model` object, ZenML will automatically generate a version number (or a new version, if you already have a version) for you. For example if we had a model version `really_good_version` for model `my_model` and we wanted to create a new version of this model, we could do so as follows:\n\n```python\nfrom zenml import Model, step\n\nmodel = Model(\n    name=\"my_model\",\n    version=\"even_better_version\"\n)\n\n@step(model=model)\ndef svc_trainer(...) -> ...:\n    ...\n```\n\nA new model version will be created and ZenML will track that this is the next in the iteration sequence of the models using the `number` property. If `really_good_version` was the 5th version of `my_model`, then `even_better_version` will be the 6th version of `my_model`.\n\n```python\nfrom zenml import Model\n\nearlier_version = Model(\n    name=\"my_model\",\n    version=\"really_good_version\"\n).number # == 5\n\nupdated_version = Model(\n    name=\"my_model\",\n    version=\"even_better_version\"\n).number # == 6\n```\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Learn how to delete models.\n---\n\n# Delete a model\n\nDeleting a model or a specific model version means removing all links between the Model entity\nand artifacts + pipeline runs, and will also delete all metadata associated with that Model.\n\n## Deleting all versions of a model\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n\n```shell\nzenml model delete <MODEL_NAME>\n```\n\n{% endtab %}\n\n{% tab title=\"Python SDK\" %}\n\n```python\nfrom zenml.client import Client\n\nClient().delete_model(<MODEL_NAME>)\n```\n\n{% endtab %}\n\n{% endtabs %}\n\n## Delete a specific version of a model\n\n{% tabs %}\n{% tab title=\"CLI\" %}\n\n```shell\nzenml model version delete <MODEL_VERSION_NAME>\n```\n\n{% endtab %}\n\n{% tab title=\"Python SDK\" %}\n\n```python\nfrom zenml.client import Client\n\nClient().delete_model_version(<MODEL_VERSION_ID>)\n```\n\n{% endtab %}\n\n{% endtabs %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>"}
{"input": "# Load a model in code\n\nThere are a few different ways to load a ZenML Model in code:\n\n## Load the active model in a pipeline\n\nYou can also use the [active model](../../user-guide/starter-guide/track-ml-models.md) to get the model metadata, or the associated artifacts directly as described in the [starter guide](../../user-guide/starter-guide/track-ml-models.md):\n\n```python\nfrom zenml import step, pipeline, get_step_context, pipeline, Model\n\n@pipeline(model=Model(name=\"my_model\"))\ndef my_pipeline():\n    ...\n\n@step\ndef my_step():\n    # Get model from active step context\n    mv = get_step_context().model\n\n    # Get metadata\n    print(mv.run_metadata[\"metadata_key\"].value)\n\n    # Directly fetch an artifact that is attached to the model\n    output = mv.get_artifact(\"my_dataset\", \"my_version\")\n    output.run_metadata[\"accuracy\"].value\n```\n\n## Load any model via the Client\n\nAlternatively, you can use the `Client`:\n\n```python\nfrom zenml import step\nfrom zenml.client import Client\nfrom zenml.enums import ModelStages\n\n@step\ndef model_evaluator_step()\n    ...\n    # Get staging model version \n    try:\n        staging_zenml_model = Client().get_model_version(\n            model_name_or_id=\"<INSERT_MODEL_NAME>\",\n            model_version_name_or_number_or_id=ModelStages.STAGING,\n        )\n    except KeyError:\n        staging_zenml_model = None\n    ...\n```\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# Linking model binaries/data to Models\n\nArtifacts generated during pipeline runs can be linked to models in ZenML. This connecting of artifacts provides lineage tracking and transparency into what data and models are used during training, evaluation, and inference.\n\nThere are a few ways to link artifacts:\n\n## Configuring the Model at a pipeline level\n\nThe easiest way is to configure the `model` parameter on the `@pipeline` decorator or `@step` decorator:\n\n```python\nfrom zenml import Model, pipeline\n\nmodel = Model(\n    name=\"my_model\",\n    version=\"1.0.0\"\n)\n\n@pipeline(model=model)\ndef my_pipeline():\n    ...\n```\n\nThis will automatically link all artifacts from this pipeline run to the specified model configuration.\n\n### Controlling artifact types and linkage\n\nA ZenML model supports linking three types of artifacts:\n\n* `Data artifacts`: These are the default artifacts. If nothing is specified, all artifacts are grouped under this category.\n* `Model artifacts`: If there is a physical model artifact like a `.pkl` file or a model neural network weights file, it should be grouped in this category.\n* `Deployment artifacts`: These artifacts are to do with artifacts related to the endpoints and deployments of the models.\n\nYou can also explicitly specify the linkage on a per-artifact basis by passing a special configuration to the Annotated output:\n\n```python\nfrom zenml import step, ArtifactConfig\nfrom typing import Tuple\nfrom typing_extensions import Annotated\nimport pandas as pd\n\n@step\ndef svc_trainer(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    gamma: float = 0.001,\n) -> Tuple[\n    # This third argument marks this as a Model Artifact\n    Annotated[ClassifierMixin, ArtifactConfig(\"trained_model\", is_model_artifact=True)],\n    # This third argument marks this as a Data Artifact\n    Annotated[str, ArtifactConfig(\"deployment_uri\", is_deployment_artifact=True)],\n]:\n    ...\n```\n\nThe `ArtifactConfig` object allows configuring model linkage directly on the artifact, and you specify whether it's for a model or deployment by using the `is_model_artifact` and `is_deployment_artifact` flags (as shown above) else it will be assumed to be a data artifact.\n\n## Saving intermediate artifacts\n\nIt is often handy to save some of your work half-way: steps like epoch-based training can be running slow, and you don't want to lose any checkpoints along the way if an error occurs. You can use the `save_artifact` utility function to save your data assets as ZenML artifacts. Moreover, if your step has the Model context configured in the `@pipeline` or `@step` decorator it will be automatically linked to it, so you can get easy access to it using the Model Control Plane features.\n\n```python\nfrom zenml import step, Model\nfrom zenml.artifacts.utils import save_artifact\n"}
{"input": "import pandas as pd\nfrom typing_extensions import Annotated\nfrom zenml.artifacts.artifact_config import ArtifactConfig\n\n@step(model=Model(name=\"MyModel\", version=\"1.2.42\"))\ndef trainer(\n    trn_dataset: pd.DataFrame,\n) -> Annotated[\n    ClassifierMixin, ArtifactConfig(\"trained_model\", is_model_artifact=True)\n]:  # this configuration will be applied to `model` output\n    \"\"\"Step running slow training.\"\"\"\n    ...\n\n    for epoch in epochs:\n        checkpoint = model.train(epoch)\n        # this will save each checkpoint in `training_checkpoint` artifact\n        # with distinct version e.g. `1.2.42_0`, `1.2.42_1`, etc.\n        # Checkpoint artifacts will be linked to `MyModel` version `1.2.42`\n        # implicitly.\n        save_artifact(\n            data=checkpoint,\n            name=\"training_checkpoint\",\n            version=f\"1.2.42_{epoch}\",\n        )\n\n    ...\n\n    return model\n```\n\n## Link artifacts explicitly\n\nIf you would like to link an artifact to a model not from the step context or even outside a step, you can use the `link_artifact_to_model` function. All you need is ready to link artifact and the configuration of a model.\n\n```python\nfrom zenml import step, Model, link_artifact_to_model, save_artifact\nfrom zenml.client import Client\n\n\n@step\ndef f_() -> None:\n    # produce new artifact\n    new_artifact = save_artifact(data=\"Hello, World!\", name=\"manual_artifact\")\n    # and link it inside a step\n    link_artifact_to_model(\n        artifact_version_id=new_artifact.id,\n        model=Model(name=\"MyModel\", version=\"0.0.42\"),\n    )\n\n\n# use existing artifact\nexisting_artifact = Client().get_artifact_version(name_id_or_prefix=\"existing_artifact\")\n# and link it even outside a step\nlink_artifact_to_model(\n    artifact_version_id=existing_artifact.id,\n    model=Model(name=\"MyModel\", version=\"0.2.42\"),\n)\n```\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# \ud83e\ude86 Use the Model Control Plane\n\n![Walkthrough of ZenML Model Control Plane (Dashboard available only on ZenML Pro)](../../.gitbook/assets/mcp_walkthrough.gif)\n\nA `Model` is simply an entity that groups pipelines, artifacts, metadata, and other crucial business data into a unified entity. A ZenML Model is a concept that more broadly encapsulates your ML products business logic. You may even think of a ZenML Model as a \"project\" or a \"workspace\"\n\n{% hint style=\"warning\" %}\nPlease note that one of the most common artifacts that is associated with a Model in ZenML is the so-called technical model, which is the actually model file/files that holds the weight and parameters of a machine learning training result. However, this is not the only artifact that is relevant; artifacts such as the training data and the predictions this model produces in production are also linked inside a ZenML Model.\n{% endhint %}\n\nModels are first-class citizens in ZenML and as such viewing and using them is unified and centralized in the ZenML API, client as well as on the [ZenML Pro](https://zenml.io/pro) dashboard.\n\nA Model captures lineage information and more. Within a Model, different Model versions can be staged. For example, you can rely on your predictions at a specific stage, like `Production`, and decide whether the Model version should be promoted based on your business rules during training. Plus, accessing data from other Models and their versions is just as simple.\n\nThe Model Control Plane is how you manage your models through this unified interface. It allows you to combine the logic of your pipelines, artifacts and crucial business data along with the actual 'technical model'.\n\nTo see an end-to-end example, please refer to the [starter guide](../../user-guide/starter-guide/track-ml-models.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Registering models\n\nRegistering models can be done in a number of ways depending on your specific needs. You can explicitly register models using the CLI or the Python SDK, or you can just allow ZenML to implicitly register your models as part of a pipeline run. \n\n{% hint style=\"info\" %}\nIf you are using [ZenML Pro](https://cloud.zenml.io/?utm\\_source=docs\\&utm\\_medium=referral\\_link\\&utm\\_campaign=cloud\\_promotion\\&utm\\_content=signup\\_link/), you already have access to a dashboard interface that allows you to register models.\n{% endhint %}\n\n## Explicit CLI registration\n\nRegistering models using the CLI is as straightforward as the following command:\n\n```bash\nzenml model register iris_logistic_regression --license=... --description=...\n```\n\nYou can view some of the options of what can be passed into this command by running `zenml model register --help` but since you are using the CLI outside a pipeline run the arguments you can pass in are limited to non-runtime items. You can also associate tags with models at this point, for example, using the `--tag` option.\n\n## Explicit dashboard registration\n\n[ZenML Pro](https://zenml.io/pro) can register their models directly from the cloud dashboard interface.\n\n<figure><img src=\"../../.gitbook/assets/mcp_model_register.png\" alt=\"ZenML Pro Register Model.\"><figcaption><p>Register a model on the [ZenML Pro](https://zenml.io/pro) dashboard</p></figcaption></figure>\n\n## Explicit Python SDK registration\n\nYou can register a model using the Python SDK as follows:\n\n```python\nfrom zenml import Model\nfrom zenml.client import Client\n\nClient().create_model(\n    name=\"iris_logistic_regression\",\n    license=\"Copyright (c) ZenML GmbH 2023\",\n    description=\"Logistic regression model trained on the Iris dataset.\",\n    tags=[\"regression\", \"sklearn\", \"iris\"],\n)\n```\n\n## Implicit registration by ZenML\n\nThe most common use case for registering models is to do so implicitly as part of a pipeline run. This is done by specifying a `Model` object as part of the `model` argument of the `@pipeline` decorator.\n\nAs an example, here we have a training pipeline which orchestrates the training of a model object, storing datasets and the model object itself as links within a newly created Model version. This integration is achieved by configuring the pipeline within a Model Context using `Model`. The name is specified, while other fields remain optional for this task.\n\n```python\nfrom zenml import pipeline\nfrom zenml import Model\n\n@pipeline(\n    enable_cache=False,\n    model=Model(\n        name=\"demo\",\n        license=\"Apache\",\n        description=\"Show case Model Control Plane.\",\n    ),\n)\ndef train_and_promote_model():\n    ...\n```\n\nRunning the training"}
{"input": " pipeline creates a new model version, all while maintaining a connection to the artifacts.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Structuring an MLOps project\n---\n\n# Connecting artifacts via a Model\n\nNow that we've learned about managing [artifacts](../../user-guide/starter-guide/manage-artifacts.md) and [models](../../user-guide/starter-guide/track-ml-models.md), we can shift our attention again to the thing that brings them together: [Pipelines](../../user-guide/starter-guide/create-an-ml-pipeline.md). This trifecta together will then inform how we structure our project.\n\n{% hint style=\"info\" %}\nIn order to see the recommended repository structure of a ZenML MLOps project, read the [best practices](../setting-up-a-project-repository/best-practices.md) section.\n{% endhint %}\n\nAn MLOps project can often be broken down into many different pipelines. For example:\n\n* A `feature engineering` pipeline that prepares raw data into a format ready to get trained.\n* A `training pipeline` that takes input data from a feature engineering pipeline and trains a models on it.\n* An `inference pipeline` that runs batch predictions on the trained model and often takes pre-processing from the training pipeline.\n* A deployment pipeline that deploys a trained model into a production endpoint.\n\nThe lines between these pipelines can often get blurry: Some use cases call for these pipelines to be merged into one big pipeline. Others go further and break the pipeline down into even smaller chunks. Ultimately, the decision of how to structure your pipelines depends on the use case and requirements of the project.\n\nNo matter how you design these pipelines, one thing stays consistent: you will often need to transfer or share information (in particular artifacts, models, and metadata) between pipelines. Here are some common patterns that you can use to help facilitate such an exchange:\n\n## Pattern 1: Artifact exchange between pipelines through `Client`\n\nLet's say we have a feature engineering pipeline and a training pipeline. The feature engineering pipeline is like a factory, pumping out many different datasets. Only a few of these datasets should be selected to be sent to the training pipeline to train an actual model.\n\n<figure><img src=\"../../.gitbook/assets/artifact_exchange.png\" alt=\"\"><figcaption><p>A simple artifact exchange between two pipelines</p></figcaption></figure>\n\nIn this scenario, the [ZenML Client](../../reference/python-client.md#client-methods) can be used to facilitate such an exchange:\n\n```python\nfrom zenml import pipeline\nfrom zenml.client import Client\n\n@pipeline\ndef feature_engineering_pipeline():\n    dataset = load_data()\n    # This returns artifacts called \"iris_training_dataset\" and \"iris_testing_dataset\"\n    train_data, test_data = prepare_data()\n\n@pipeline\ndef training_pipeline():\n    client = Client()\n    # Fetch by name alone - uses the latest version of this artifact\n    train_data = client.get_artifact_version(name=\"iris_training_dataset\")\n    # For test, we want a particular"}
{"input": " version\n    test_data = client.get_artifact_version(name=\"iris_testing_dataset\", version=\"raw_2023\")\n\n    # We can now send these directly into ZenML steps\n    sklearn_classifier = model_trainer(train_data)\n    model_evaluator(model, sklearn_classifier)\n```\n\n{% hint style=\"info\" %}\nPlease note, that in the above example, the `train_data` and `test_data` artifacts are not [materialized](../handle-data-artifacts/artifact-versioning.md) in memory in the `@pipeline` function, but rather the `train_data` and `test_data` objects are simply references to where this data is stored in the artifact store. Therefore, one cannot use any logic regarding the nature of this data itself during compilation time (i.e. in the `@pipeline` function).\n{% endhint %}\n\n## Pattern 2: Artifact exchange between pipelines through a `Model`\n\nWhile passing around artifacts with IDs or names is very useful, it is often desirable to have the ZenML Model be the point of reference instead.\n\nFor example, let's say we have a training pipeline called `train_and_promote` and an inference pipeline called `do_predictions`. The training pipeline produces many different model artifacts, all of which are collected within a [ZenML Model](../../user-guide/starter-guide/track-ml-models.md). Each time the `train_and_promote` pipeline runs, it creates a new `iris_classifier`. However, it only promotes the model to `production` if a certain accuracy threshold is met. The promotion can be also be done manually with human intervention, or it can be automated through setting a particular threshold.\n\nOn the other side, the `do_predictions` pipeline simply picks up the latest promoted model and runs batch inference on it. It need not know of the IDs or names of any of the artifacts produced by the training pipeline's many runs. This way these two pipelines can independently be run, but can rely on each other's output.\n\n<figure><img src=\"../../.gitbook/assets/mcp_pipeline_overview.png\" alt=\"\"><figcaption><p>A simple artifact exchange between pipelines through the Model Control Plane.</p></figcaption></figure>\n\nIn code, this is very simple. Once the [pipelines are configured to use a particular model](../../user-guide/starter-guide/track-ml-models.md#configuring-a-model-in-a-pipeline), we can use `get_step_context` to fetch the configured model within a step directly. Assuming there is a `predict` step in the `do_predictions` pipeline, we can fetch the `production` model like so:\n\n```python\nfrom zenml import step, get_step_context\n\n# IMPORTANT: Cache needs to be disabled to avoid unexpected behavior\n@step(enable_cache=False)\ndef predict(\n    data: pd.DataFrame,\n) -> Annotated[pd.Series, \"predictions\"]:\n    # model name and version are derived from pipeline context\n"}
{"input": "    model = get_step_context().model\n\n    # Fetch the model directly from the model control plane\n    model = model.get_model_artifact(\"trained_model\")\n\n    # Make predictions\n    predictions = pd.Series(model.predict(data))\n    return predictions\n```\n\nHowever, this approach has the downside that if the step is cached, then it could lead to unexpected results. You could simply disable the cache in the above step or the corresponding pipeline. However, one other way of achieving this would be to resolve the artifact at the pipeline level:\n\n```python\nfrom typing_extensions import Annotated\nfrom zenml import get_pipeline_context, pipeline, Model\nfrom zenml.enums import ModelStages\nimport pandas as pd\nfrom sklearn.base import ClassifierMixin\n\n\n@step\ndef predict(\n    model: ClassifierMixin,\n    data: pd.DataFrame,\n) -> Annotated[pd.Series, \"predictions\"]:\n    predictions = pd.Series(model.predict(data))\n    return predictions\n\n@pipeline(\n    model=Model(\n        name=\"iris_classifier\",\n        # Using the production stage\n        version=ModelStages.PRODUCTION,\n    ),\n)\ndef do_predictions():\n    # model name and version are derived from pipeline context\n    model = get_pipeline_context().model\n    inference_data = load_data()\n    predict(\n        # Here, we load in the `trained_model` from a trainer step\n        model=model.get_model_artifact(\"trained_model\"),  \n        data=inference_data,\n    )\n\n\nif __name__ == \"__main__\":\n    do_predictions()\n```\n\nUltimately, both approaches are fine. You should decide which one to use based on your own preferences.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# Promote a model\n\n## Stages and Promotion\n\nModel stages are a way to model the progress that different versions takes through various stages in its lifecycle. A ZenML Model version can be promoted to a different stage through the Dashboard, the ZenML CLI or code.\n\nThis is a way to signify the progression of your model version through the ML lifecycle and are an extra layer of metadata to identify the state of a particular model version. Possible options for stages are:\n\n* `staging`: This version is staged for production.\n* `production`: This version is running in a production setting.\n* `latest`: The latest version of the model. This is a virtual stage to retrieve the latest version only - versions cannot be promoted to `latest`.\n* `archived`: This is archived and no longer relevant. This stage occurs when a model moves out of any other stage.\n\nYour own particular business or use case logic will determine which model version you choose to promote, and you can do this in the following ways:\n\n### Promotion via CLI\n\nThis is probably the least common way that you'll use, but it's still possible and perhaps might be useful for some use cases or within a CI system, for example. You simply use the following CLI subcommand:\n\n```bash\nzenml model version update iris_logistic_regression --stage=...\n```\n\n### Promotion via Cloud Dashboard\n\nThis feature is not yet available, but soon you will be able to promote your model versions directly from the ZenML Pro dashboard. \n\n### Promotion via Python SDK\n\nThis is the most common way that you'll use to promote your models. You can see how you would do this here:\n\n```python\nfrom zenml import Model\n\nMODEL_NAME = \"iris_logistic_regression\"\nfrom zenml.enums import ModelStages\n\nmodel = Model(name=MODEL_NAME, version=\"1.2.3\")\nmodel.set_stage(stage=ModelStages.PRODUCTION)\n\n# get latest model and set it as Staging\n# (if there is current Staging version it will get Archived)\nlatest_model = Model(name=MODEL_NAME, version=ModelStages.LATEST)\nlatest_model.set_stage(stage=ModelStages.STAGING)\n```\n\nWithin a pipeline context, you would get the model from the step context but the mechanism for setting the stage is the same.\n\n```python\nfrom zenml import get_step_context, step, pipeline\nfrom zenml.enums import ModelStages\n\n@step\ndef promote_to_staging():\n    model = get_step_context().model\n    model.set_stage(ModelStages.STAGING, force=True)\n\n@pipeline(\n    ...\n)\ndef train_and_promote_model():\n    ...\n    promote_to_staging(after=[\"train_and_evaluate\"])\n```\n\n## Fetching model versions by stage\n\nThe stage can be used to load the right model version, by passing them in as a `version`:\n\n```python\nfrom zenml import Model, step, pipeline\n\nmodel= Model"}
{"input": "(\n    name=\"my_model\",\n    version=\"production\"\n)\n\n# The step configuration will take precedence over the pipeline\n@step(model=model)\ndef svc_trainer(...) -> ...:\n    ...\n\n# This configures it for all steps within the pipeline\n@pipeline(model=model)\ndef training_pipeline( ... ):\n    # training happens here\n```\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Installing ZenML and getting started.\n---\n\n# \ud83e\uddd9 Installation\n\n**ZenML** is a Python package that can be installed directly via `pip`:\n\n```shell\npip install zenml\n```\n\n{% hint style=\"warning\" %}\nNote that ZenML currently supports **Python 3.8, 3.9, 3.10, and 3.11**. Please make sure that you are using a supported Python version.\n{% endhint %}\n\n## Install with the dashboard\n\nZenML comes bundled with a web dashboard that lives inside a [sister repository](https://github.com/zenml-io/zenml-dashboard). In order to get access to the dashboard **locally**, you need to launch the [ZenML Server and Dashboard locally](deploying-zenml/README.md). For this, you need to install the optional dependencies for the ZenML Server:\n\n```shell\npip install \"zenml[server]\"\n```\n\n{% hint style=\"info\" %}\nWe highly encourage you to install ZenML in a virtual environment. At ZenML, We like to use [virtualenvwrapper](https://virtualenvwrapper.readthedocs.io/en/latest/) or [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv) to manage our Python virtual environments.\n{% endhint %}\n\n## Installing onto MacOS with Apple Silicon (M1, M2)\n\nA change in how forking works on Macs running on Apple Silicon means that you\nshould set the following environment variable which will ensure that your\nconnections to the server remain unbroken:\n\n```bash\nexport OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n```\n\nYou can read more about this [here](http://sealiesoftware.com/blog/archive/2017/6/5/Objective-C_and_fork_in_macOS_1013.html). This environment variable is needed if you are working with a local server on your Mac, but if you're just using ZenML as a client / CLI and connecting to a deployed server then you don't need to set it.\n\n## Nightly builds\n\nZenML also publishes nightly builds under the [`zenml-nightly` package name](https://pypi.org/project/zenml-nightly/). These are built from the latest [`develop` branch](https://github.com/zenml-io/zenml/tree/develop) (to which work ready for release is published) and are not guaranteed to be stable. To install the nightly build, run:\n\n```shell\npip install zenml-nightly\n```\n\n## Verifying installations\n\nOnce the installation is completed, you can check whether the installation was successful either through Bash:\n\n```bash\nzenml version\n```\n\nor through Python:\n\n```python\nimport zenml\n\nprint(zenml.__version__)\n```\n\nIf you would like to learn more about the current release, please visit our [PyPi package page"}
{"input": ".](https://pypi.org/project/zenml)\n\n## Running with Docker\n\n`zenml` is also available as a Docker image hosted publicly on [DockerHub](https://hub.docker.com/r/zenmldocker/zenml). Use the following command to get started in a bash environment with `zenml` available:\n\n```shell\ndocker run -it zenmldocker/zenml /bin/bash\n```\n\nIf you would like to run the ZenML server with Docker:\n\n```shell\ndocker run -it -d -p 8080:8080 zenmldocker/zenml-server\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n\n## Deploying the server\n\nThough ZenML can run entirely as a pip package on a local system, complete with the dashboard. You can do this easily:\n\n```shell\npip install \"zenml[server]\"\nzenml up  # opens the dashboard locally \n```\n\nHowever, advanced ZenML features are dependent on a centrally-deployed ZenML server accessible to other MLOps stack components. You can read more about it [here](deploying-zenml/README.md).\n\nFor the deployment of ZenML, you have the option to either [self-host](deploying-zenml/README.md) it or register for a free [ZenML Pro](https://cloud.zenml.io/signup?utm\\_source=docs\\&utm\\_medium=referral\\_link\\&utm\\_campaign=cloud\\_promotion\\&utm\\_content=signup\\_link) account.\n"}
{"input": "---\ndescription: Discovering the core concepts behind ZenML.\n---\n\n# \ud83e\ude84 Core concepts\n\n**ZenML** is an extensible, open-source MLOps framework for creating portable, production-ready **MLOps pipelines**. It's built for data scientists, ML Engineers, and MLOps Developers to collaborate as they develop to production. In order to achieve this goal, ZenML introduces various concepts for different aspects of an ML workflow and we can categorize these concepts under three different threads:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><mark style=\"color:purple;\"><strong>1. Development</strong></mark></td><td>As a developer, how do I design my machine learning workflows?</td><td></td><td><a href=\"core-concepts.md#1-development\">#1-development</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>2. Execution</strong></mark></td><td>While executing, how do my workflows utilize the large landscape of MLOps tooling/infrastructure?</td><td></td><td><a href=\"core-concepts.md#2-execution\">#2-execution</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>3. Management</strong></mark></td><td>How do I establish and maintain a production-grade and efficient solution?</td><td></td><td><a href=\"core-concepts.md#3-management\">#3-management</a></td></tr></tbody></table>\n\n## 1. Development\n\nFirst, let's look at the main concepts which play a role during the development stage of an ML workflow with ZenML.\n\n#### Step\n\nSteps are functions annotated with the `@step` decorator. The easiest one could look like this.\n\n```python\n@step\ndef step_1() -> str:\n    \"\"\"Returns a string.\"\"\"\n    return \"world\"\n```\n\nThese functions can also have inputs and outputs. For ZenML to work properly, these should preferably be typed.\n\n```python\n@step(enable_cache=False)\ndef step_2(input_one: str, input_two: str) -> str:\n    \"\"\"Combines the two strings passed in.\"\"\"\n    combined_str = f\"{input_one} {input_two}\"\n    return combined_str\n```\n\n#### Pipelines\n\nAt its core, ZenML follows a pipeline-based workflow for your projects. A **pipeline** consists of a series of **steps**, organized in any order that makes sense for your use case.\n\n![Representation of a pipeline dag.](../.gitbook/assets/01\\_pipeline.png)\n\nAs seen in the image, a step might use the outputs from a previous step and thus must wait until"}
{"input": " the previous step is completed before starting. This is something you can keep in mind when organizing your steps.\n\nPipelines and steps are defined in code using Python _decorators_ or _classes_. This is where the core business logic and value of your work lives, and you will spend most of your time defining these two things.\n\nEven though pipelines are simple Python functions, you are only allowed to call steps within this function. The inputs for steps called within a pipeline can either be the outputs of previous steps or alternatively, you can pass in values directly (as long as they're JSON-serializable).\n\n```python\n@pipeline\ndef my_pipeline():\n    output_step_one = step_1()\n    step_2(input_one=\"hello\", input_two=output_step_one)\n```\n\nExecuting the Pipeline is as easy as calling the function that you decorated with the `@pipeline` decorator.\n\n```python\nif __name__ == \"__main__\":\n    my_pipeline()\n```\n\n#### Artifacts\n\nArtifacts represent the data that goes through your steps as inputs and outputs and they are automatically tracked and stored by ZenML in the artifact store. They are produced by and circulated among steps whenever your step returns an object or a value. This means the data is not passed between steps in memory. Rather, when the execution of a step is completed they are written to storage, and when a new step gets executed they are loaded from storage.\n\nThe serialization and deserialization logic of artifacts is defined by [Materializers](../how-to/handle-data-artifacts/handle-custom-data-types.md).\n\n#### Models\n\nModels are used to represent the outputs of a training process along with all metadata associated with that output. In other words: models in ZenML are more broadly defined as the weights as well as any associated information. Models are first-class citizens in ZenML and as such viewing and using them is unified and centralized in the ZenML API, client as well as on the [ZenML Pro](https://zenml.io/pro) dashboard.\n\n#### Materializers\n\nMaterializers define how artifacts live in between steps. More precisely, they define how data of a particular type can be serialized/deserialized, so that the steps are able to load the input data and store the output data.\n\nAll materializers use the base abstraction called the `BaseMaterializer` class. While ZenML comes built-in with various implementations of materializers for different datatypes, if you are using a library or a tool that doesn't work with our built-in options, you can write [your own custom materializer](../how-to/handle-data-artifacts/handle-custom-data-types.md) to ensure that your data can be passed from step to step.\n\n#### Parameters & Settings\n\nWhen we think about steps as functions, we know they receive input in the form of artifacts. We also know that they produce output (in the form of artifacts, stored in the artifact store). But steps also take parameters. The parameters that you pass"}
{"input": " into the steps are also (helpfully!) stored by ZenML. This helps freeze the iterations of your experimentation workflow in time, so you can return to them exactly as you run them. On top of the parameters that you provide for your steps, you can also use different `Setting`s to configure runtime configurations for your infrastructure and pipelines.\n\n#### Model and model versions\n\nZenML exposes the concept of a `Model`, which consists of multiple different model versions. A model version represents a unified view of the ML models that are created, tracked, and managed as part of a ZenML project. Model versions link all other entities to a centralized view.\n\n## 2. Execution\n\nOnce you have implemented your workflow by using the concepts described above, you can focus your attention on the execution of the pipeline run.\n\n#### Stacks & Components\n\nWhen you want to execute a pipeline run with ZenML, **Stacks** come into play. A **Stack** is a collection of **stack components**, where each component represents the respective configuration regarding a particular function in your MLOps pipeline such as orchestration systems, artifact repositories, and model deployment platforms.\n\nFor instance, if you take a close look at the default local stack of ZenML, you will see two components that are **required** in every stack in ZenML, namely an _orchestrator_ and an _artifact store_.\n\n![ZenML running code on the Local Stack.](../.gitbook/assets/02\\_pipeline\\_local\\_stack.png)\n\n{% hint style=\"info\" %}\nKeep in mind, that each one of these components is built on top of base abstractions and is completely extensible.\n{% endhint %}\n\n#### Orchestrator\n\nAn **Orchestrator** is a workhorse that coordinates all the steps to run in a pipeline. Since pipelines can be set up with complex combinations of steps with various asynchronous dependencies between them, the orchestrator acts as the component that decides what steps to run and when to run them.\n\nZenML comes with a default _local orchestrator_ designed to run on your local machine. This is useful, especially during the exploration phase of your project. You don't have to rent a cloud instance just to try out basic things.\n\n#### Artifact Store\n\nAn **Artifact Store** is a component that houses all data that pass through the pipeline as inputs and outputs. Each artifact that gets stored in the artifact store is tracked and versioned and this allows for extremely useful features like data caching which speeds up your workflows.\n\nSimilar to the orchestrator, ZenML comes with a default _local artifact store_ designed to run on your local machine. This is useful, especially during the exploration phase of your project. You don't have to set up a cloud storage system to try out basic things.\n\n#### Flavor\n\nZenML provides a dedicated base abstraction for each stack component type. These abstractions are used to develop solutions, called **Flavors**, tailored to specific use cases"}
{"input": "/tools. With ZenML installed, you get access to a variety of built-in and integrated Flavors for each component type, but users can also leverage the base abstractions to create their own custom flavors.\n\n#### Stack Switching\n\nWhen it comes to production-grade solutions, it is rarely enough to just run your workflow locally without including any cloud infrastructure.\n\nThanks to the separation between the pipeline code and the stack in ZenML, you can easily switch your stack independently from your code. For instance, all it would take you to switch from an experimental local stack running on your machine to a remote stack that employs a full-fledged cloud infrastructure is a single CLI command.\n\n## 3. Management\n\nIn order to benefit from the aforementioned core concepts to their fullest extent, it is essential to deploy and manage a production-grade environment that interacts with your ZenML installation.\n\n#### ZenML Server\n\nTo use _stack components_ that are running remotely on a cloud infrastructure, you need to deploy a [**ZenML Server**](../user-guide/production-guide/deploying-zenml.md) so it can communicate with these stack components and run your pipelines. The server is also responsible for managing ZenML business entities like pipelines, steps, models, etc.\n\n![Visualization of the relationship between code and infrastructure.](../.gitbook/assets/04\\_architecture.png)\n\n#### Server Deployment\n\nIn order to benefit from the advantages of using a deployed ZenML server, you can either choose to use the [**ZenML Pro SaaS offering**](zenml-pro/zenml-cloud.md) which provides a control plane for you to create managed instances of ZenML servers, or [deploy it in your self-hosted environment](deploying-zenml/README.md).\n\n#### Metadata Tracking\n\nOn top of the communication with the stack components, the **ZenML Server** also keeps track of all the bits of metadata around a pipeline run. With a ZenML server, you are able to access all of your previous experiments with the associated details. This is extremely helpful in troubleshooting.\n\n#### Secrets\n\nThe **ZenML Server** also acts as a [centralized secrets store](deploying-zenml/secret-management.md) that safely and securely stores sensitive data such as credentials used to access the services that are part of your stack. It can be configured to use a variety of different backends for this purpose, such as the AWS Secrets Manager, GCP Secret Manager, Azure Key Vault, and Hashicorp Vault.\n\nSecrets are sensitive data that you don't want to store in your code or configure alongside your stacks and pipelines. ZenML includes a [centralized secrets store](deploying-zenml/secret-management.md) that you can use to store and access your secrets securely.\n\n#### Collaboration\n\nCollaboration is a crucial aspect of any MLOps team as they often need to bring together individuals with diverse skills and expertise to create a cohesive and effective workflow for machine learning projects"}
{"input": ". A successful MLOps team requires seamless collaboration between data scientists, engineers, and DevOps professionals to develop, train, deploy, and maintain machine learning models.\n\nWith a deployed **ZenML Server**, users have the ability to create their own teams and project structures. They can easily share pipelines, runs, stacks, and other resources, streamlining the workflow and promoting teamwork.\n\n#### Dashboard\n\nThe **ZenML Dashboard** also communicates with **the ZenML Server** to visualize your _pipelines_, _stacks_, and _stack components_. The dashboard serves as a visual interface to showcase collaboration with ZenML. You can invite _users_, and share your stacks with them.\n\nWhen you start working with ZenML, you'll start with a local ZenML setup, and when you want to transition you will need to [deploy ZenML](deploying-zenml/README.md). Don't worry though, there is a one-click way to do it which we'll learn about later.\n\n#### VS Code Extension\n\nZenML also provides a [VS Code extension](https://marketplace.visualstudio.com/items?itemName=ZenML.zenml-vscode) that allows you to interact with your ZenML stacks, runs and server directly from your VS Code editor. If you're working on code in your editor, you can easily switch and inspect the stacks you're using, delete and inspect pipelines as well as even switch stacks.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying ZenML in a Docker container.\n---\n\n# Deploy with Docker\n\nThe ZenML server container image is available at [`zenmldocker/zenml-server`](https://hub.docker.com/r/zenmldocker/zenml/) and can be used to deploy ZenML with a container management or orchestration tool like Docker and docker-compose, or a serverless platform like [Cloud Run](https://cloud.google.com/run), [Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/overview), and more! This guide walks you through the various configuration options that the ZenML server container expects as well as a few deployment use cases.\n\n## Try it out locally first\n\nIf you're just looking for a quick way to deploy the ZenML server using a container, without going through the hassle of interacting with a container management tool like Docker and manually configuring your container, you can use the ZenML CLI to do so. You only need to have Docker installed and running on your machine:\n\n```bash\nzenml up --docker\n```\n\nThis command deploys a ZenML server locally in a Docker container, then connects your client to it. Similar to running plain `zenml up`, the server and the local ZenML client share the same SQLite database.\n\nThe rest of this guide is addressed to advanced users who are looking to manually deploy and manage a containerized ZenML server.\n\n## ZenML server configuration options\n\nIf you're planning on deploying a custom containerized ZenML server yourself, you probably need to configure some settings for it like the **database** it should use, the **default user details,** and more. The ZenML server container image uses sensible defaults, so you can simply start a container without worrying too much about the configuration. However, if you're looking to connect the ZenML server to an external MySQL database or secrets management service, to persist the internal SQLite database, or simply want to control other settings like the default account, you can do so by customizing the container's environment variables.\n\nThe following environment variables can be passed to the container:\n\n*   **ZENML\\_STORE\\_URL**: This URL should point to an SQLite database file _mounted in the container_, or to a MySQL-compatible database service _reachable from the container_. It takes one of these forms:\n\n    ```\n    sqlite:////path/to/zenml.db\n    ```\n\n    or:\n\n    ```\n    mysql://username:password@host:port/database\n    ```\n* **ZENML\\_STORE\\_SSL\\_CA**: This can be set to a custom server CA certificate in use by the MySQL database service. Only valid when `ZENML_STORE_URL` points to a MySQL database that uses SSL-secured connections. The variable can be set either to the path where the certificate file is mounted inside the container or to the certificate contents themselves.\n* **ZENML\\_STORE\\_SSL\\_CERT**: This can be"}
{"input": " set to a client SSL certificate required to connect to the MySQL database service. Only valid when `ZENML_STORE_URL` points to a MySQL database that uses SSL-secured connections and requires client SSL certificates. The variable can be set either to the path where the certificate file is mounted inside the container or to the certificate contents themselves. This variable also requires `ZENML_STORE_SSL_KEY` to be set.\n* **ZENML\\_STORE\\_SSL\\_KEY**: This can be set to a client SSL private key required to connect to the MySQL database service. Only valid when `ZENML_STORE_URL` points to a MySQL database that uses SSL-secured connections and requires client SSL certificates. The variable can be set either to the path where the certificate file is mounted inside the container or to the certificate contents themselves. This variable also requires `ZENML_STORE_SSL_CERT` to be set.\n* **ZENML\\_STORE\\_SSL\\_VERIFY\\_SERVER\\_CERT**: This boolean variable controls whether the SSL certificate in use by the MySQL server is verified. Only valid when `ZENML_STORE_URL` points to a MySQL database that uses SSL-secured connections. Defaults to `False`.\n* **ZENML\\_LOGGING\\_VERBOSITY**: Use this variable to control the verbosity of logs inside the container. It can be set to one of the following values: `NOTSET`, `ERROR`, `WARN`, `INFO` (default), `DEBUG` or `CRITICAL`.\n* **ZENML\\_STORE\\_BACKUP\\_STRATEGY**: This variable controls the database backup strategy used by the ZenML server. See the [Database backup and recovery](deploy-with-docker.md#database-backup-and-recovery) section for more details about this feature and other related environment variables. Defaults to `in-memory`.\n* **ZENML\\_SERVER\\_RATE\\_LIMIT\\_ENABLED**: This variable controls the rate limiting for ZenML API (currently only for the `LOGIN` endpoint). It is disabled by default, so set it to `1` only if you need to enable rate limiting. To determine unique users a `X_FORWARDED_FOR` header or `request.client.host` is used, so before enabling this make sure that your network configuration is associating proper information with your clients in order to avoid disruptions for legitimate requests.\n* **ZENML\\_SERVER\\_LOGIN\\_RATE\\_LIMIT\\_MINUTE**: If rate limiting is enabled, this variable controls how many requests will be allowed to query the login endpoint in a one minute interval. Set it to a desired integer value; defaults to `5`.\n* **ZENML\\_SERVER\\_LOGIN\\_RATE\\_LIMIT\\_DAY**: If rate limiting is enabled, this variable controls how many requests will be allowed to query the login endpoint in an interval of day interval. Set it to a desired integer value; defaults to `1000`.\n\nIf none of the `ZENML_STORE_*"}
{"input": "` variables are set, the container will default to creating and using an SQLite database file stored at `/zenml/.zenconfig/local_stores/default_zen_store/zenml.db` inside the container. The `/zenml/.zenconfig/local_stores` base path where the default SQLite database is located can optionally be overridden by setting the `ZENML_LOCAL_STORES_PATH` environment variable to point to a different path (e.g. a persistent volume or directory that is mounted from the host).\n\n### Secret store environment variables\n\nUnless explicitly disabled or configured otherwise, the ZenML server will use the SQL database as [a secrets store backend](secret-management.md) where secret values are stored. If you want to use an external secrets management service like the AWS Secrets Manager, GCP Secrets Manager, Azure Key Vault, HashiCorp Vault or even your custom Secrets Store back-end implementation instead, you need to configure it explicitly using Docker environment variables. Depending on where you deploy your ZenML server and how your Kubernetes cluster is configured, you will also need to provide the credentials needed to access the secrets management service API.\n\n> **Important:** If you are updating the configuration of your ZenML Server container to use a different secrets store back-end or location, you should follow [the documented secrets migration strategy](secret-management.md#secrets-migration-strategy) to minimize downtime and to ensure that existing secrets are also properly migrated.\n\n{% tabs %}\n{% tab title=\"undefined\" %}\nThe SQL database is used as the default secret store location. You only need to configure these options if you want to change the default behavior.\n\nIt is particularly recommended to enable encryption at rest for the SQL database if you plan on using it as a secrets store backend. You'll have to configure the secret key used to encrypt the secret values. If not set, encryption will not be used and passwords will be stored unencrypted in the database.\n\n* **ZENML\\_SECRETS\\_STORE\\_TYPE:** Set this to `sql` in order to explicitly set this type of secret store.\n*   **ZENML\\_SECRETS\\_STORE\\_ENCRYPTION\\_KEY**: the secret key used to encrypt all secrets stored in the SQL secrets store. It is recommended to set this to a random string with a length of at least 32 characters, e.g.:\n\n    ```python\n    from secrets import token_hex\n    token_hex(32)\n    ```\n\n    or:\n\n    ```shell\n    openssl rand -hex 32\n    ```\n\n> **Important:** If you configure encryption for your SQL database secrets store, you should keep the `ZENML_SECRETS_STORE_ENCRYPTION_KEY` value somewhere safe and secure, as it will always be required by the ZenML server to decrypt the secrets in the database. If you lose the encryption key, you will not be able to decrypt the secrets in the database and will have to reset them.\n{% endtab %}\n\n"}
{"input": "{% tab title=\"AWS\" %}\nThese configuration options are only relevant if you're using the AWS Secrets Manager as the secrets store backend.\n\n* **ZENML\\_SECRETS\\_STORE\\_TYPE:** Set this to `aws` in order to set this type of secret store.\n\nThe AWS Secrets Store uses the ZenML AWS Service Connector under the hood to authenticate with the AWS Secrets Manager API. This means that you can use any of the [authentication methods supported by the AWS Service Connector](../../how-to/auth-management/aws-service-connector.md#authentication-methods) to authenticate with the AWS Secrets Manager API.\n\nThe minimum set of permissions that must be attached to the implicit or configured AWS credentials are: `secretsmanager:CreateSecret`, `secretsmanager:GetSecretValue`, `secretsmanager:DescribeSecret`, `secretsmanager:PutSecretValue`, `secretsmanager:TagResource` and `secretsmanager:DeleteSecret` and they must be associated with secrets that have a name starting with `zenml/` in the target region and account. The following IAM policy example can be used as a starting point:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ZenMLSecretsStore\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:PutSecretValue\",\n                \"secretsmanager:TagResource\",\n                \"secretsmanager:DeleteSecret\"\n            ],\n            \"Resource\": \"arn:aws:secretsmanager:<AWS-region>:<AWS-account-id>:secret:zenml/*\"\n        }\n    ]\n}\n```\n\nThe following configuration options are supported:\n\n* **ZENML\\_SECRETS\\_STORE\\_AUTH\\_METHOD**: The AWS Service Connector authentication method to use (e.g. `secret-key` or `iam-role`).\n* **ZENML\\_SECRETS\\_STORE\\_AUTH\\_CONFIG**: The AWS Service Connector configuration, in JSON format (e.g. `{\"aws_access_key_id\":\"<aws-key-id>\",\"aws_secret_access_key\":\"<aws-secret-key>\",\"region\":\"<aws-region>\"}`).\n\n> **Note:** The remaining configuration options are deprecated and may be removed in a future release. Instead, you should set the `ZENML_SECRETS_STORE_AUTH_METHOD` and `ZENML_SECRETS_STORE_AUTH_CONFIG` variables to use the AWS Service Connector authentication method.\n\n* **ZENML\\_SECRETS\\_STORE\\_REGION\\_NAME**: The AWS region to use. This must be set to the region where the AWS Secrets Manager service that you want to use is located.\n* **ZENML\\_SECRETS\\_STORE\\_AWS\\_ACCESS\\_KEY\\_ID**: The AWS access key ID to use for"}
{"input": " authentication. This must be set to a valid AWS access key ID that has access to the AWS Secrets Manager service that you want to use. If you are using an IAM role attached to an EKS cluster to authenticate, you can omit this variable.\n* **ZENML\\_SECRETS\\_STORE\\_AWS\\_SECRET\\_ACCESS\\_KEY**: The AWS secret access key to use for authentication. This must be set to a valid AWS secret access key that has access to the AWS Secrets Manager service that you want to use. If you are using an IAM role attached to an EKS cluster to authenticate, you can omit this variable.\n{% endtab %}\n\n{% tab title=\"GCP\" %}\nThese configuration options are only relevant if you're using the GCP Secrets Manager as the secrets store backend.\n\n* **ZENML\\_SECRETS\\_STORE\\_TYPE:** Set this to `gcp` in order to set this type of secret store.\n\nThe GCP Secrets Store uses the ZenML GCP Service Connector under the hood to authenticate with the GCP Secrets Manager API. This means that you can use any of the [authentication methods supported by the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md#authentication-methods) to authenticate with the GCP Secrets Manager API.\n\nThe minimum set of permissions that must be attached to the implicit or configured GCP credentials are as follows:\n\n* `secretmanager.secrets.create` for the target GCP project (i.e. no condition on the name prefix)\n* `secretmanager.secrets.get`, `secretmanager.secrets.update`, `secretmanager.versions.access`, `secretmanager.versions.add` and `secretmanager.secrets.delete` for the target GCP project and for secrets that have a name starting with `zenml-`\n\nThis can be achieved by creating two custom IAM roles and attaching them to the principal (e.g. user or service account) that will be used to access the GCP Secrets Manager API with a condition configured when attaching the second role to limit access to secrets with a name prefix of `zenml-`. The following `gcloud` CLI command examples can be used as a starting point:\n\n```bash\ngcloud iam roles create ZenMLServerSecretsStoreCreator \\\n  --project <your GCP project ID> \\\n  --title \"ZenML Server Secrets Store Creator\" \\\n  --description \"Allow the ZenML Server to create new secrets\" \\\n  --stage GA \\\n  --permissions \"secretmanager.secrets.create\"\n\ngcloud iam roles create ZenMLServerSecretsStoreEditor \\\n  --project <your GCP project ID> \\\n  --title \"ZenML Server Secrets Store Editor\" \\\n  --description \"Allow the ZenML Server to manage its secrets\" \\\n  --stage GA \\\n  --permissions \"secretmanager.secrets.get,secretmanager.secrets.update,secretmanager.versions.access,secretmanager.versions.add,secret"}
{"input": "manager.secrets.delete\"\n\ngcloud projects add-iam-policy-binding <your GCP project ID> \\\n  --member serviceAccount:<your GCP service account email> \\\n  --role projects/<your GCP project ID>/roles/ZenMLServerSecretsStoreCreator \\\n  --condition None\n\n# NOTE: use the GCP project NUMBER, not the project ID in the condition\ngcloud projects add-iam-policy-binding <your GCP project ID> \\\n  --member serviceAccount:<your GCP service account email> \\\n  --role projects/<your GCP project ID>/roles/ZenMLServerSecretsStoreEditor \\\n  --condition 'title=limit_access_zenml,description=\"Limit access to secrets with prefix zenml-\",expression=resource.name.startsWith(\"projects/<your GCP project NUMBER>/secrets/zenml-\")'\n```\n\nThe following configuration options are supported:\n\n* **ZENML\\_SECRETS\\_STORE\\_AUTH\\_METHOD**: The GCP Service Connector authentication method to use (e.g. `service-account`).\n* **ZENML\\_SECRETS\\_STORE\\_AUTH\\_CONFIG**: The GCP Service Connector configuration, in JSON format (e.g. `{\"project_id\":\"my-project\",\"service_account_json\":{ ... }}`).\n\n> **Note:** The remaining configuration options are deprecated and may be removed in a future release. Instead, you should set the `ZENML_SECRETS_STORE_AUTH_METHOD` and `ZENML_SECRETS_STORE_AUTH_CONFIG` variables to use the GCP Service Connector authentication method.\n\n* **ZENML\\_SECRETS\\_STORE\\_PROJECT\\_ID**: The GCP project ID to use. This must be set to the project ID where the GCP Secrets Manager service that you want to use is located.\n* **GOOGLE\\_APPLICATION\\_CREDENTIALS**: The path to the GCP service account credentials file to use for authentication. This must be set to a valid GCP service account credentials file that has access to the GCP Secrets Manager service that you want to use. If you are using a GCP service account attached to a GKE cluster to authenticate, you can omit this variable. NOTE: the path to the credentials file must be mounted into the container.\n{% endtab %}\n\n{% tab title=\"Azure\" %}\nThese configuration options are only relevant if you're using Azure Key Vault as the secrets store backend.\n\n* **ZENML\\_SECRETS\\_STORE\\_TYPE:** Set this to `azure` in order to set this type of secret store.\n* **ZENML\\_SECRETS\\_STORE\\_KEY\\_VAULT\\_NAME**: The name of the Azure Key Vault. This must be set to point to the Azure Key Vault instance that you want to use.\n\nThe Azure Secrets Store uses the ZenML Azure Service Connector under the hood to authenticate with the Azure Key Vault API. This means that you can use"}
{"input": " any of the [authentication methods supported by the Azure Service Connector](../../how-to/auth-management/azure-service-connector.md#authentication-methods) to authenticate with the Azure Key Vault API. The following configuration options are supported:\n\n* **ZENML\\_SECRETS\\_STORE\\_AUTH\\_METHOD**: The Azure Service Connector authentication method to use (e.g. `service-account`).\n* **ZENML\\_SECRETS\\_STORE\\_AUTH\\_CONFIG**: The Azure Service Connector configuration, in JSON format (e.g. `{\"tenant_id\":\"my-tenant-id\",\"client_id\":\"my-client-id\",\"client_secret\": \"my-client-secret\"}`).\n\n> **Note:** The remaining configuration options are deprecated and may be removed in a future release. Instead, you should set the `ZENML_SECRETS_STORE_AUTH_METHOD` and `ZENML_SECRETS_STORE_AUTH_CONFIG` variables to use the Azure Service Connector authentication method.\n\n* **ZENML\\_SECRETS\\_STORE\\_AZURE\\_CLIENT\\_ID**: The Azure application service principal client ID to use to authenticate with the Azure Key Vault API. If you are running the ZenML server hosted in Azure and are using a managed identity to access the Azure Key Vault service, you can omit this variable.\n* **ZENML\\_SECRETS\\_STORE\\_AZURE\\_CLIENT\\_SECRET**: The Azure application service principal client secret to use to authenticate with the Azure Key Vault API. If you are running the ZenML server hosted in Azure and are using a managed identity to access the Azure Key Vault service, you can omit this variable.\n* **ZENML\\_SECRETS\\_STORE\\_AZURE\\_TENANT\\_ID**: The Azure application service principal tenant ID to use to authenticate with the Azure Key Vault API. If you are running the ZenML server hosted in Azure and are using a managed identity to access the Azure Key Vault service, you can omit this variable.\n{% endtab %}\n\n{% tab title=\"Hashicorp\" %}\nThese configuration options are only relevant if you're using Hashicorp Vault as the secrets store backend.\n\n* **ZENML\\_SECRETS\\_STORE\\_TYPE:** Set this to `hashicorp` in order to set this type of secret store.\n* **ZENML\\_SECRETS\\_STORE\\_VAULT\\_ADDR**: The URL of the HashiCorp Vault server to connect to. NOTE: this is the same as setting the `VAULT_ADDR` environment variable.\n* **ZENML\\_SECRETS\\_STORE\\_VAULT\\_TOKEN**: The token to use to authenticate with the HashiCorp Vault server. NOTE: this is the same as setting the `VAULT_TOKEN` environment variable.\n* **ZENML\\_SECRETS\\_STORE\\_VAULT\\_NAMESPACE**: The Vault Enterprise namespace. Not required for Vault OSS. NOTE: this is the same as setting the `VAULT_NAMESPACE"}
{"input": "` environment variable.\n* **ZENML\\_SECRETS\\_STORE\\_MAX\\_VERSIONS**: The maximum number of secret versions to keep for each Vault secret. If not set, the default value of 1 will be used (only the latest version will be kept).\n{% endtab %}\n\n{% tab title=\"Custom\" %}\nThese configuration options are only relevant if you're using a custom secrets store backend implementation. For this to work, you must have [a custom implementation of the secrets store API](manage-the-deployed-services/custom-secret-stores.md) in the form of a class derived from `zenml.zen_stores.secrets_stores.base_secrets_store.BaseSecretsStore`. This class must be importable from within the ZenML server container, which means you most likely need to mount the directory containing the class into the container or build a custom container image that contains the class.\n\nThe following configuration option is required:\n\n* **ZENML\\_SECRETS\\_STORE\\_TYPE:** Set this to `custom` in order to set this type of secret store.\n* **ZENML\\_SECRETS\\_STORE\\_CLASS\\_PATH**: The fully qualified path to the class that implements the custom secrets store API (e.g. `my_package.my_module.MySecretsStore`).\n\nIf your custom secrets store implementation requires additional configuration options, you can pass them as environment variables using the following naming convention:\n\n* `ZENML_SECRETS_STORE_<OPTION_NAME>`: The name of the option to pass to the custom secrets store class. The option name must be in uppercase and any hyphens (`-`) must be replaced with underscores (`_`). ZenML will automatically convert the environment variable name to the corresponding option name by removing the prefix and converting the remaining characters to lowercase. For example, the environment variable `ZENML_SECRETS_STORE_MY_OPTION` will be converted to the option name `my_option` and passed to the custom secrets store class configuration.\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\n**ZENML\\_SECRETS\\_STORE\\_TYPE**: Set this variable to `none`to disable the secrets store functionality altogether.\n{% endhint %}\n\n#### Backup secrets store\n\n[A backup secrets store](secret-management.md#backup-secrets-store) back-end may be configured for high-availability and backup purposes. or as an intermediate step in the process of [migrating secrets to a different external location or secrets manager provider](secret-management.md#secrets-migration-strategy).\n\nTo configure a backup secrets store in the Docker container, use the same approach and instructions documented for the primary secrets store, but set the `**ZENML\\_BACKUP\\_SECRETS\\_STORE\\***` environment variables instead of `**ZENML\\_SECRETS\\_STORE\\***`, e.g.:\n\n```yaml\nZENML_BACKUP_SECRETS_STORE_TYPE"}
{"input": ": aws\nZENML_BACKUP_SECRETS_STORE_AUTH_METHOD: secret-key\nZENML_BACKUP_SECRETS_STORE_AUTH_CONFIG: '{\"aws_access_key_id\":\"<aws-key-id>\", \"aws_secret_access_key\",\"<aws-secret-key>\",\"role_arn\": \"<aws-role-arn>\"}`'\n```\n\n### Advanced server configuration options\n\nThese configuration options are not required for most use cases, but can be useful in certain scenarios that require mirroring the same ZenML server configuration across multiple container instances (e.g. a Kubernetes deployment with multiple replicas):\n\n*   **ZENML\\_SERVER\\_JWT\\_SECRET\\_KEY**: This is a secret key used to sign JWT tokens used for authentication. If not explicitly set, a random key is generated automatically by the server on startup and stored in the server's global configuration. This should be set to a random string with a recommended length of at least 32 characters, e.g.:\n\n    ```python\n    from secrets import token_hex\n    token_hex(32)\n    ```\n\n    or:\n\n    ```shell\n    openssl rand -hex 32\n    ```\n\nThe environment variables starting with _ZENML\\_SERVER\\_SECURE\\_HEADERS\\__\\* can be used to enable, disable or set custom values for security headers in the ZenML server's HTTP responses. The following values can be set for any of the supported secure headers configuration options:\n\n* `enabled`, `on`, `true` or `yes` - enables the secure header with the default value.\n* `disabled`, `off`, `false`, `none` or `no` - disables the secure header entirely, so that it is not set in the ZenML server's HTTP responses.\n* any other value - sets the secure header to the specified value.\n\nThe following secure headers environment variables are supported:\n\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_SERVER**: The `Server` HTTP header value used to identify the server. The default value is the ZenML server ID.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_HSTS**: The `Strict-Transport-Security` HTTP header value. The default value is `max-age=63072000; includeSubDomains`.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_XFO**: The `X-Frame-Options` HTTP header value. The default value is `SAMEORIGIN`.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_XXP**: The `X-XSS-Protection` HTTP header value. The default value is `0`. NOTE: this header is deprecated and should not be customized anymore. The `Content-Security-Policy` header should be used instead.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_CONTENT**: The `X-Content-Type-Options` HTTP header value. The default value is `nosniff`.\n"}
{"input": "* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_CSP**: The `Content-Security-Policy` HTTP header value. This is by default set to a strict CSP policy that only allows content from the origins required by the ZenML dashboard. NOTE: customizing this header is discouraged, as it may cause the ZenML dashboard to malfunction.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_REFERRER**: The `Referrer-Policy` HTTP header value. The default value is `no-referrer-when-downgrade`.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_CACHE**: The `Cache-Control` HTTP header value. The default value is `no-store, no-cache, must-revalidate`.\n* **ZENML\\_SERVER\\_SECURE\\_HEADERS\\_PERMISSIONS**: The `Permissions-Policy` HTTP header value. The default value is `accelerometer=(), camera=(), geolocation=(), gyroscope=(), magnetometer=(), microphone=(), payment=(), usb=()`.\n\nIf you prefer to activate the server automatically during the initial deployment and also automate the creation of the initial admin user account, this legacy behavior can be brought back by setting the following environment variables:\n\n* **ZENML\\_SERVER\\_AUTO\\_ACTIVATE**: Set this to `1` to automatically activate the server and create the initial admin user account when the server is first deployed. Defaults to `0`.\n* **ZENML\\_DEFAULT\\_USER\\_NAME**: The name of the initial admin user account created by the server on the first deployment, during database initialization. Defaults to `default`.\n* **ZENML\\_DEFAULT\\_USER\\_PASSWORD**: The password to use for the initial admin user account. Defaults to an empty password value, if not set.\n\n## Run the ZenML server with Docker\n\nAs previously mentioned, the ZenML server container image uses sensible defaults for most configuration options. This means that you can simply run the container with Docker without any additional configuration and it will work out of the box for most use cases:\n\n```bash\ndocker run -it -d -p 8080:8080 --name zenml zenmldocker/zenml-server\n```\n\n> **Note:** It is recommended to use a ZenML container image version that matches the version of your client, to avoid any potential API incompatibilities (e.g. `zenmldocker/zenml-server:0.21.1` instead of `zenmldocker/zenml-server`).\n\nThe above command will start a containerized ZenML server running on your machine that uses a temporary SQLite database file stored in the container. Temporary means that the database and all its contents (stacks, pipelines, pipeline runs, etc.) will be lost when the container is removed with `docker rm`.\n\nYou need to visit the ZenML dashboard at `http://localhost:8080` and"}
{"input": " activate the server by creating an initial admin user account. You can then connect your client to the server with the web login flow:\n\n```shell\n$ zenml connect --url http://localhost:8080\nConnecting to: 'http://localhost:8080'...\nIf your browser did not open automatically, please open the following URL into your browser to proceed with the authentication:\n\nhttp://localhost:8080/devices/verify?device_id=f7a7333a-3ef0-4f39-85a9-f190279456d3&user_code=9375f5cdfdaf36772ce981fe3ee6172c\n\nSuccessfully logged in.\nCreating default stack for user 'default' in workspace default...\nUpdated the global store configuration.\n```\n\n{% hint style=\"info\" %}\nThe `localhost` URL **will** work, even if you are using Docker-backed ZenML orchestrators in your stack, like [the local Docker orchestrator](../../component-guide/orchestrators/local-docker.md) or [a locally deployed Kubeflow orchestrator](../../component-guide/orchestrators/kubeflow.md).\n\nZenML makes use of specialized DNS entries such as `host.docker.internal` and `host.k3d.internal` to make the ZenML server accessible from the pipeline steps running inside other Docker containers on the same machine.\n{% endhint %}\n\nYou can manage the container with the usual Docker commands:\n\n* `docker logs zenml` to view the server logs\n* `docker stop zenml` to stop the server\n* `docker start zenml` to start the server again\n* `docker rm zenml` to remove the container\n\nIf you are looking for a customized ZenML server Docker deployment, you can configure one or more of [the supported environment variables](deploy-with-docker.md#zenml-server-configuration-options) and then pass them to the container using the `docker run` `--env` or `--env-file` arguments (see the [Docker documentation](https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file) for more details). For example:\n\n```shell\ndocker run -it -d -p 8080:8080 --name zenml \\\n    --env ZENML_STORE_URL=mysql://username:password@host:port/database \\\n    zenmldocker/zenml-server\n```\n\nIf you're looking for a quick way to run both the ZenML server and a MySQL database with Docker, you can [deploy the ZenML server with Docker Compose](deploy-with-docker.md#zenml-server-with-docker-compose).\n\nThe rest of this guide covers various advanced use cases for running the ZenML server with Docker.\n\n### Persisting the SQLite database\n\nDepending on your use case, you may also want to mount a persistent volume or directory from the host into the container to store the Zen"}
{"input": "ML SQLite database file. This can be done using the `--mount` flag (see the [Docker documentation](https://docs.docker.com/storage/volumes/) for more details). For example:\n\n```shell\nmkdir zenml-server\ndocker run -it -d -p 8080:8080 --name zenml \\\n    --mount type=bind,source=$PWD/zenml-server,target=/zenml/.zenconfig/local_stores/default_zen_store \\\n    zenmldocker/zenml-server\n```\n\nThis deployment has the advantage that the SQLite database file is persisted even when the container is removed with `docker rm`.\n\n### Docker MySQL database\n\nAs a recommended alternative to the SQLite database, you can run a MySQL database service as another Docker container and connect the ZenML server container to it.\n\nA command like the following can be run to start the containerized MySQL database service:\n\n```shell\ndocker run --name mysql -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password mysql:8.0\n```\n\nIf you also wish to persist the MySQL database data, you can mount a persistent volume or directory from the host into the container using the `--mount` flag, e.g.:\n\n```shell\nmkdir mysql-data\ndocker run --name mysql -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password \\\n    --mount type=bind,source=$PWD/mysql-data,target=/var/lib/mysql \\\n    mysql:8.0\n```\n\nConfiguring the ZenML server container to connect to the MySQL database is just a matter of setting the `ZENML_STORE_URL` environment variable. We use the special `host.docker.internal` DNS name that is resolved from within the Docker containers to the gateway IP address used by the Docker network (see the [Docker documentation](https://docs.docker.com/desktop/networking/#use-cases-and-workarounds-for-all-platforms) for more details). On Linux, this needs to be explicitly enabled in the `docker run` command with the `--add-host` argument:\n\n```shell\ndocker run -it -d -p 8080:8080 --name zenml \\\n    --add-host host.docker.internal:host-gateway \\\n    --env ZENML_STORE_URL=mysql://root:password@host.docker.internal/zenml \\\n    zenmldocker/zenml-server\n```\n\nYou need to visit the ZenML dashboard at `http://localhost:8080` and activate the server by creating an initial admin user account. You can then connect your client to the server with the web login flow:\n\n```shell\nzenml connect --url http://localhost:8080\n```\n\n### Direct MySQL database connection\n\nThis scenario is similar to the previous one, but instead of running a ZenML server, the client is configured to connect directly to a MySQL database running in a Docker container.\n\nAs previously covered, the containerized"}
{"input": " MySQL database service can be started with a command like the following:\n\n```shell\ndocker run --name mysql -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password mysql:8.0\n```\n\nThe ZenML client on the host machine can then be configured to connect directly to the database with a slightly different `zenml connect` command:\n\n```shell\nzenml connect --url mysql://127.0.0.1/zenml --username root --password password\n```\n\n> **Note** The `localhost` hostname will not work with MySQL databases. You need to use the `127.0.0.1` IP address instead.\n\n### ZenML server with `docker-compose`\n\nDocker compose offers a simpler way of managing multi-container setups on your local machine, which is the case for instance if you are looking to deploy the ZenML server container and connect it to a MySQL database service also running in a Docker container.\n\nTo use Docker Compose, you need to [install the docker-compose plugin](https://docs.docker.com/compose/install/linux/) on your machine first.\n\nA `docker-compose.yml` file like the one below can be used to start and manage the ZenML server container and the MySQL database service all at once:\n\n```yaml\nversion: \"3.9\"\n\nservices:\n  mysql:\n    image: mysql:8.0\n    ports:\n      - 3306:3306\n    environment:\n      - MYSQL_ROOT_PASSWORD=password\n  zenml:\n    image: zenmldocker/zenml-server\n    ports:\n      - \"8080:8080\"\n    environment:\n      - ZENML_STORE_URL=mysql://root:password@host.docker.internal/zenml\n    links:\n      - mysql\n    depends_on:\n      - mysql\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    restart: on-failure\n```\n\nNote the following:\n\n* `ZENML_STORE_URL` is set to the special Docker `host.docker.internal` hostname to instruct the server to connect to the database over the Docker network.\n* The `extra_hosts` section is needed on Linux to make the `host.docker.internal` hostname resolvable from the ZenML server container.\n\nTo start the containers, run the following command from the directory where the `docker-compose.yml` file is located:\n\n```shell\ndocker compose -p zenml up  -d\n```\n\nor, if you need to use a different filename or path:\n\n```shell\ndocker compose -f /path/to/docker-compose.yml -p zenml up -d\n```\n\nYou need to visit the ZenML dashboard at `http://localhost:8080` to activate the server by creating an initial admin account. You can then connect your client to the server with the web login flow:\n\n```shell\nzenml connect --url http://localhost:8080\n```\n\nTearing down the installation"}
{"input": " is as simple as running:\n\n```shell\ndocker compose -p zenml down\n```\n\n## Database backup and recovery\n\nAn automated database backup and recovery feature is enabled by default for all Docker deployments. The ZenML server will automatically back up the database in-memory before every database schema migration and restore it if the migration fails.\n\n{% hint style=\"info\" %}\nThe database backup automatically created by the ZenML server is only temporary and only used as an immediate recovery in case of database migration failures. It is not meant to be used as a long-term backup solution. If you need to back up your database for long-term storage, you should use a dedicated backup solution.\n{% endhint %}\n\nSeveral database backup strategies are supported, depending on where and how the backup is stored. The strategy can be configured by means of the `ZENML_STORE_BACKUP_STRATEGY` environment variable:\n\n* `disabled` - no backup is performed\n* `in-memory` - the database schema and data are stored in memory. This is the fastest backup strategy, but the backup is not persisted across container restarts, so no manual intervention is possible in case the automatic DB recovery fails after a failed DB migration. Adequate memory resources should be allocated to the ZenML server container when using this backup strategy with larger databases. This is the default backup strategy.\n* `database` - the database is copied to a backup database in the same database server. This requires the `ZENML_STORE_BACKUP_DATABASE` environment variable to be set to the name of the backup database. This backup strategy is only supported for MySQL compatible databases and the user specified in the database URL must have permissions to manage (create, drop, and modify) the backup database in addition to the main database.\n* `dump-file` - the database schema and data are dumped to a filesystem location inside the ZenML server container. This location can be customized by means of the `ZENML_STORE_BACKUP_DIRECTORY` environment variable. When this strategy is configured, users should mount a host directory in the container and point the `ZENML_STORE_BACKUP_DIRECTORY` variable to where it's mounted inside the container. If a host directory is not mounted, the dump file will be stored in the container's filesystem and will be lost when the container is removed.\n\nThe following additional rules are applied concerning the creation and lifetime of the backup:\n\n* a backup is not attempted if the database doesn't need to undergo a migration (e.g. when the ZenML server is upgraded to a new version that doesn't require a database schema change or if the ZenML version doesn't change at all).\n* a backup file or database is created before every database migration attempt (i.e. when the container starts). If a backup already exists (i.e. persisted in a mounted host directory or backup database), it is overwritten.\n* the persistent backup file or database is cleaned up after the migration is completed successfully or if the database doesn't"}
{"input": " need to undergo a migration. This includes backups created by previous failed migration attempts.\n* the persistent backup file or database is NOT cleaned up after a failed migration. This allows the user to manually inspect and/or apply the backup if the automatic recovery fails.\n\nThe following example shows how to deploy the ZenML server to use a mounted host directory to persist the database backup file during a database migration:\n\n```shell\nmkdir mysql-data\n\ndocker run --name mysql -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password \\\n    --mount type=bind,source=$PWD/mysql-data,target=/var/lib/mysql \\\n    mysql:8.0\n\ndocker run -it -d -p 8080:8080 --name zenml \\\n    --add-host host.docker.internal:host-gateway \\\n    --mount type=bind,source=$PWD/mysql-data,target=/db-dump \\\n    --env ZENML_STORE_URL=mysql://root:password@host.docker.internal/zenml \\\n    --env ZENML_STORE_BACKUP_STRATEGY=dump-file \\\n    --env ZENML_STORE_BACKUP_DIRECTORY=/db-dump \\\n    zenmldocker/zenml-server\n```\n\n## Troubleshooting\n\nYou can check the logs of the container to verify if the server is up and, depending on where you have deployed it, you can also access the dashboard at a `localhost` port (if running locally) or through some other service that exposes your container to the internet.\n\n### CLI Docker deployments\n\nIf you used the `zenml up --docker` CLI command to deploy the Docker ZenML server, you can check the logs with the command:\n\n```shell\nzenml logs -f\n```\n\n### Manual Docker deployments\n\nIf you used the `docker run` command to manually deploy the Docker ZenML server, you can check the logs with the command:\n\n```shell\ndocker logs zenml -f\n```\n\nIf you used the `docker compose` command to manually deploy the Docker ZenML server, you can check the logs with the command:\n\n```shell\ndocker compose -p zenml logs -f\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying ZenML in a Kubernetes cluster with Helm.\n---\n\n# Deploy with Helm\n\nIf you wish to manually deploy and manage ZenML in a Kubernetes cluster of your choice, ZenML also includes a Helm chart among its available deployment options.\n\nYou can find the chart on this [ArtifactHub repository](https://artifacthub.io/packages/helm/zenml/zenml), along with the templates, default values and instructions on how to install it. Read on to find detailed explanations on prerequisites, configuration, and deployment scenarios.\n\n## Prerequisites\n\nYou'll need the following:\n\n* A Kubernetes cluster\n* Optional, but recommended: a MySQL-compatible database reachable from the Kubernetes cluster (e.g. one of the managed databases offered by Google Cloud, AWS, or Azure). A MySQL server version of 8.0 or higher is required\n* the [Kubernetes client](https://kubernetes.io/docs/tasks/tools/#kubectl) already installed on your machine and configured to access your cluster\n* [Helm](https://helm.sh/docs/intro/install/) installed on your machine\n* Optional: an external Secrets Manager service (e.g. one of the managed secrets management services offered by Google Cloud, AWS, Azure, or HashiCorp Vault). By default, ZenML stores secrets inside the SQL database that it's connected to, but you also have the option of using an external cloud Secrets Manager service if you already happen to use one of those cloud or service providers\n\n## ZenML Helm Configuration\n\nYou can start by taking a look at the [`values.yaml` file](https://artifacthub.io/packages/helm/zenml/zenml?modal=values) and familiarize yourself with some of the configuration settings that you can customize for your ZenML deployment.\n\nIn addition to tools and infrastructure, you will also need to collect and [prepare information related to your database](deploy-with-helm.md#collect-information-from-your-sql-database-service) and [information related to your external secrets management service](deploy-with-helm.md#collect-information-from-your-secrets-management-service) to be used for the Helm chart configuration and you may also want to install additional [optional services in your cluster](deploy-with-helm.md#optional-cluster-services).\n\nWhen you are ready, you can proceed to the [installation](deploy-with-helm.md#zenml-helm-installation) section.\n\n### Collect information from your SQL database service\n\nUsing an external MySQL-compatible database service is optional, but is recommended for production deployments. If omitted, ZenML will default to using an embedded SQLite database, which has the following limitations:\n\n* the SQLite database is not persisted, meaning that it will be lost if the ZenML server pod is restarted or deleted\n* the SQLite database does not scale horizontally, meaning that you will not be able to use more than one replica at a time for the ZenML server pod\n\nIf you decide to use an external MySQL-compatible database service, you"}
{"input": " will need to collect and prepare the following information for the Helm chart configuration:\n\n* the hostname and port where the SQL database is reachable from the Kubernetes cluster\n* the username and password that will be used to connect to the database. It is recommended that you create a dedicated database user for the ZenML server and that you restrict its privileges to only access the database that will be used by ZenML. Enforcing secure SSL connections for the user/database is also recommended. See the [MySQL documentation](https://dev.mysql.com/doc/refman/5.7/en/access-control.html) for more information on how to set up users and privileges.\n* the name of the database that will be used by ZenML. The database does not have to exist prior to the deployment ( ZenML will create it on the first start). However, you need to create the database if you follow the best practice of restricting database user privileges to only access it.\n* if you plan on using SSL to secure the client database connection, you may also need to prepare additional SSL certificates and keys:\n  * the TLS CA certificate that was used to sign the server TLS certificate, if you're using a self-signed certificate or signed by a custom certificate authority that is not already trusted by default by most operating systems.\n  * the TLS client certificate and key. This is only needed if you decide to use client certificates for your DB connection (some managed DB services support this, CloudSQL is an example).\n\n### Collect information from your secrets management service\n\nUsing an externally managed secrets management service like those offered by Google Cloud, AWS, Azure or HashiCorp Vault is optional, but is recommended if you are already using those cloud service providers. If omitted, ZenML will default to using the SQL database to store secrets.\n\nIf you decide to use an external secrets management service, you will need to collect and prepare the following information for the Helm chart configuration (for supported back-ends only):\n\nFor the AWS secrets manager:\n\n* the AWS region that you want to use to store your secrets\n* an AWS access key ID and secret access key that provides full access to the AWS secrets manager service. You can create a dedicated IAM user for this purpose, or use an existing user with the necessary permissions. If you deploy the ZenML server in an EKS Kubernetes cluster that is already configured to use implicit authorization with an IAM role for service accounts, you can omit this step.\n\nFor the Google Cloud secrets manager:\n\n* the Google Cloud project ID that you want to use to store your secrets\n* a Google Cloud service account that has access to the secrets manager service. You can create a dedicated service account for this purpose, or use an existing service account with the necessary permissions.\n\nFor the Azure Key Vault:\n\n* the name of the Azure Key Vault that you want to use to store your secrets\n* the Azure tenant ID, client ID, and client secret associated with the Azure service principal that will be used to access the Azure Key"}
{"input": " Vault. You can create a dedicated application service principal for this purpose, or use an existing service principal with the necessary permissions. If you deploy the ZenML server in an AKS Kubernetes cluster that is already configured to use implicit authorization through the Azure-managed identity service, you can omit this step.\n\nFor the HashiCorp Vault:\n\n* the URL of the HashiCorp Vault server\n* the token that will be used to access the HashiCorp Vault server.\n\n### Optional cluster services\n\nIt is common practice to install additional infrastructure-related services in a Kubernetes cluster to support the deployment and long-term management of applications. For example:\n\n* an Ingress service like [nginx-ingress](https://kubernetes.github.io/ingress-nginx/deploy/) is recommended if you want to expose HTTP services to the internet. An Ingress is required if you want to use secure HTTPS for your ZenML deployment. The alternative is to use a LoadBalancer service to expose the ZenML service using plain HTTP, but this is not recommended for production.\n* a [cert-manager](https://cert-manager.io/docs/installation/) is recommended if you want to generate and manage TLS certificates for your ZenML deployment. It can be used to automatically provision TLS certificates from a certificate authority (CA) of your choice, such as [Let's Encrypt](https://letsencrypt.org/). As an alternative, the ZenML Helm chart can be configured to auto-generate self-signed or you can generate the certificates yourself and provide them to the Helm chart, but this makes it more difficult to manage the certificates and you need to manually renew them when they expire.\n\n## ZenML Helm Installation\n\n### Configure the Helm chart\n\nTo use the Helm chart with custom values that includes path to files like the database SSL certificates, you need to pull the chart to your local directory first. You can do this with the following command:\n\n```bash\nhelm pull oci://public.ecr.aws/zenml/zenml --version <VERSION> --untar\n```\n\nNext, to customize the Helm chart for your deployment, you should create a copy of the `values.yaml` file that you can find at `./zenml/values.yaml` (let\u2019s call this `custom-values.yaml`). You\u2019ll use this as a template to customize your configuration. Any values that you don\u2019t override you should simply remove from your `custom-values.yaml` file to keep it clean and compatible with future Helm chart releases.\n\nIn most cases, you\u2019ll need to change the following configuration values in `custom-values.yaml`:\n\n* the database configuration, if you mean to use an external database:\n  * the database URL, formatted as `mysql://<username>:<password>@<hostname>:<port>/<database>`\n  * CA and/or client TLS certificates, if you\u2019re using SSL to secure the connection to the database\n* the Ingress configuration, if enabled:\n  * enabling TLS\n  * enabling self-signed certificates\n"}
{"input": "  * configuring the hostname that will be used to access the ZenML server, if different from the IP address or hostname associated with the Ingress service installed in your cluster\n\n> **Note** All the file paths that you use in your helm chart (e.g. for certificates like `database.sslCa`) must be relative to the `./zenml` helm chart directory, meaning that you also have to copy these files there.\n\n### Install the Helm chart\n\nOnce everything is configured, you can run the following command in the `./zenml` folder to install the Helm chart.\n\n```\nhelm -n <namespace> install zenml-server . --create-namespace --values custom-values.yaml \n```\n\n### Connect to the deployed ZenML server\n\nImmediately after deployment, the ZenML server needs to be activated before it can be used. The activation process includes creating an initial admin user account and configuring some server settings. You can do this only by visiting the ZenML server URL in your browser and following the on-screen instructions. Connecting your local ZenML client to the server is not possible until the server is properly initialized.\n\nThe Helm chart should print out a message with the URL of the deployed ZenML server. You can use the URL to open the ZenML UI in your browser.\n\nTo connect your local client to the ZenML server, you can either pass the configuration as command line arguments or as a YAML file:\n\n```bash\nzenml connect --url=https://zenml.example.com:8080 --no-verify-ssl\n```\n\nor\n\n```bash\nzenml connect --config=/path/to/zenml_server_config.yaml\n```\n\nThe YAML file should have the following structure when connecting to a ZenML server:\n\n```yaml\nurl: <The URL of the ZenML server>\nverify_ssl: |\n  <Either a boolean, in which case it controls whether the\n  server's TLS certificate is verified, or a string, in which case it\n  must be a path to a CA certificate bundle to use or the CA bundle\n  value itself>\n```\n\nExample of a ZenML server YAML configuration file:\n\n```yaml\nurl: https://ac8ef63af203226194a7725ee71d85a-7635928635.us-east-1.elb.amazonaws.com/zenml\nverify_ssl: |\n  -----BEGIN CERTIFICATE-----\n...\n  -----END CERTIFICATE-----\n```\n\nTo disconnect from the current ZenML server and revert to using the local default database, use the following command:\n\n```bash\nzenml disconnect\n```\n\n## ZenML Helm Deployment Scenarios\n\nThis section covers some common Helm deployment scenarios for ZenML.\n\n### Minimal deployment\n\nThe example below is a minimal configuration for a ZenML server deployment that uses a temporary SQLite database and a ClusterIP service that is not exposed to the internet:\n\n```yaml\nzenml:\n\n  ingress:\n    enabled: false\n```\n\nOnce deployed, you have"}
{"input": " to use port-forwarding to access the ZenML server and to connect to it from your local machine:\n\n```bash\nkubectl -n zenml-server port-forward svc/zenml-server 8080:8080\nzenml connect --url=http://localhost:8080\n```\n\nThis is just a simple example only fit for testing and evaluation purposes. For production deployments, you should use an external database and an Ingress service with TLS certificates to secure and expose the ZenML server to the internet.\n\n### Basic deployment with local database\n\nThis deployment use-case still uses a local database, but it exposes the ZenML server to the internet using an Ingress service with TLS certificates generated by the cert-manager and signed by Let's Encrypt.\n\nFirst, you need to install cert-manager and nginx-ingress in your Kubernetes cluster. You can use the following commands to install them with their default configuration:\n\n```bash\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true\nhelm install nginx-ingress ingress-nginx/ingress-nginx --namespace nginx-ingress --create-namespace\n```\n\nNext, you need to create a ClusterIssuer resource that will be used by cert-manager to generate TLS certificates with Let's Encrypt:\n\n```bash\ncat <<EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\n  namespace: cert-manager\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: <your email address here>\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\nEOF\n```\n\nFinally, you can deploy the ZenML server with the following Helm values:\n\n```yaml\nzenml:\n\n  ingress:\n    enabled: true\n    annotations:\n      cert-manager.io/cluster-issuer: \"letsencrypt-staging\"\n    tls:\n      enabled: true\n      generateCerts: false\n```\n\n> **Note** This use-case exposes ZenML at the root URL path of the IP address or hostname of the Ingress service. You cannot share the same Ingress hostname and URL path for multiple applications. See the next section for a solution to this problem.\n\n### Shared Ingress controller\n\nIf the root URL path of your Ingress controller is already in use by another application, you cannot use it for ZenML. This section presents three possible solutions to this problem.\n\n#### Use a dedicated Ingress hostname for ZenML\n\nIf you know the IP address of the load balancer in use by your Ingress controller, you can use a service like https://nip.io"}
{"input": "/ to create a new DNS name associated with it and expose ZenML at this new root URL path. For example, if your Ingress controller has the IP address `192.168.10.20`, you can use a DNS name like `zenml.192.168.10.20.nip.io` to expose ZenML at the root URL path `https://zenml.192.168.10.20.nip.io`.\n\nTo find the IP address of your Ingress controller, you can use a command like the following:\n\n```bash\nkubectl -n nginx-ingress get svc nginx-ingress-ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n```\n\nYou can deploy the ZenML server with the following Helm values:\n\n```yaml\nzenml:\n\n  ingress:\n    enabled: true\n    annotations:\n      cert-manager.io/cluster-issuer: \"letsencrypt-staging\"\n    host: zenml.<nginx ingress IP address>.nip.io\n    tls:\n      enabled: true\n      generateCerts: false\n```\n\n> **Note** This method does not work if your Ingress controller is behind a load balancer that uses a hostname mapped to several IP addresses instead of an IP address.\n\n#### Use a dedicated Ingress URL path for ZenML\n\nIf you cannot use a dedicated Ingress hostname for ZenML, you can use a dedicated Ingress URL path instead. For example, you can expose ZenML at the URL path `https://<your ingress hostname>/zenml`.\n\nTo deploy the ZenML server with a dedicated Ingress URL path, you can use the following Helm values:\n\n```yaml\nzenml:\n\n  ingress:\n    enabled: true\n    annotations:\n      cert-manager.io/cluster-issuer: \"letsencrypt-staging\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n    path: /zenml/?(.*)\n    tls:\n      enabled: true\n      generateCerts: false\n```\n\n> **Note** This method has one current limitation: the ZenML UI does not support URL rewriting and will not work properly if you use a dedicated Ingress URL path. You can still connect your client to the ZenML server and use it to run pipelines as usual, but you will not be able to use the ZenML UI.\n\n#### Use a DNS service to map a different hostname to the Ingress controller\n\nThis method requires you to configure a DNS service like AWS Route 53 or Google Cloud DNS to map a different hostname to the Ingress controller. For example, you can map the hostname `zenml.<subdomain>` to the Ingress controller's IP address or hostname. Then, simply use the new hostname to expose ZenML at the root URL path.\n\n### Secret Store configuration\n\nUnless explicitly disabled or configured otherwise, the ZenML server will use the SQL database as [a secrets store backend](secret"}
{"input": "-management.md) where secret values are stored. If you want to use an external secrets management service like the AWS Secrets Manager, GCP Secrets Manager, Azure Key Vault, HashiCorp Vault or even your custom Secrets Store back-end implementation instead, you need to configure it in the Helm values. Depending on where you deploy your ZenML server and how your Kubernetes cluster is configured, you will also need to provide the credentials needed to access the secrets management service API.\n\n> **Important:** If you are updating the configuration of your ZenML Server deployment to use a different secrets store back-end or location, you should follow [the documented secrets migration strategy](secret-management.md#secrets-migration-strategy) to minimize downtime and to ensure that existing secrets are also properly migrated.\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n**Using the SQL database as a secrets store backend (default)**\n\nThe SQL database is used as the default location where the ZenML secrets store keeps the secret values. You only need to configure these options if you want to change the default behavior.\n\nIt is particularly recommended to enable encryption at rest for the SQL database if you plan on using it as a secrets store backend. You'll have to configure the secret key used to encrypt the secret values. If not set, encryption will not be used and passwords will be stored unencrypted in the database. This value should be set to a random string with a recommended length of at least 32 characters, e.g.:\n\n* generate a random string with Python:\n\n```python\nfrom secrets import token_hex\ntoken_hex(32)\n```\n\n* or with OpenSSL:\n\n```shell\nopenssl rand -hex 32\n```\n\n* then configure it in the Helm values:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Secrets store settings. This is used to store centralized secrets.\n   secretsStore:\n\n     # The type of the secrets store\n     type: sql\n\n     # Configuration for the SQL secrets store\n     sql:\n       encryptionKey: 0f00e4282a3181be32c108819e8a860a429b613e470ad58531f0730afff64545\n```\n\n> **Important:** If you configure encryption for your SQL database secrets store, you should keep the `encryptionKey` value somewhere safe and secure, as it will always be required by the ZenML Server to decrypt the secrets in the database. If you lose the encryption key, you will not be able to decrypt the secrets anymore and will have to reset them.\n{% endtab %}\n\n{% tab title=\"AWS\" %}\n**Using the AWS Secrets Manager as a secrets store backend**\n\nThe AWS Secrets Store uses the ZenML AWS Service Connector under the hood to authenticate with the AWS Secrets Manager API. This means that you can use any of the [authentication methods supported by the AWS Service Connector](../../how-to/auth-management/aws-service-connector.md#authentication-methods) to authenticate with"}
{"input": " the AWS Secrets Manager API.\n\nThe minimum set of permissions that must be attached to the implicit or configured AWS credentials are: `secretsmanager:CreateSecret`, `secretsmanager:GetSecretValue`, `secretsmanager:DescribeSecret`, `secretsmanager:PutSecretValue`, `secretsmanager:TagResource` and `secretsmanager:DeleteSecret` and they must be associated with secrets that have a name starting with `zenml/` in the target region and account. The following IAM policy example can be used as a starting point:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ZenMLSecretsStore\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:PutSecretValue\",\n                \"secretsmanager:TagResource\",\n                \"secretsmanager:DeleteSecret\"\n            ],\n            \"Resource\": \"arn:aws:secretsmanager:<AWS-region>:<AWS-account-id>:secret:zenml/*\"\n        }\n    ]\n}\n```\n\nExample configuration for the AWS Secrets Store:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Secrets store settings. This is used to store centralized secrets.\n   secretsStore:\n\n     # Set to false to disable the secrets store.\n     enabled: true\n\n     # The type of the secrets store\n     type: aws\n\n     # Configuration for the AWS Secrets Manager secrets store\n     aws:\n\n       # The AWS Service Connector authentication method to use.\n       authMethod: secret-key\n\n       # The AWS Service Connector configuration.\n       authConfig:\n        # The AWS region to use. This must be set to the region where the AWS\n        # Secrets Manager service that you want to use is located.\n        region: us-east-1\n\n        # The AWS credentials to use to authenticate with the AWS Secrets\n        aws_access_key_id: <your AWS access key ID>\n        aws_secret_access_key: <your AWS secret access key>\n```\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n**Using the GCP Secrets Manager as a secrets store backend**\n\nThe GCP Secrets Store uses the ZenML GCP Service Connector under the hood to authenticate with the GCP Secrets Manager API. This means that you can use any of the [authentication methods supported by the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md#authentication-methods) to authenticate with the GCP Secrets Manager API.\n\nThe minimum set of permissions that must be attached to the implicit or configured GCP credentials are as follows:\n\n* `secretmanager.secrets.create` for the target GCP project (i.e. no condition on the name prefix)\n* `secretmanager.secrets.get`, `secretmanager.secrets.update`, `secret"}
{"input": "manager.versions.access`, `secretmanager.versions.add` and `secretmanager.secrets.delete` for the target GCP project and for secrets that have a name starting with `zenml-`\n\nThis can be achieved by creating two custom IAM roles and attaching them to the principal (e.g. user or service account) that will be used to access the GCP Secrets Manager API with a condition configured when attaching the second role to limit access to secrets with a name prefix of `zenml-`. The following `gcloud` CLI command examples can be used as a starting point:\n\n```bash\ngcloud iam roles create ZenMLServerSecretsStoreCreator \\\n  --project <your GCP project ID> \\\n  --title \"ZenML Server Secrets Store Creator\" \\\n  --description \"Allow the ZenML Server to create new secrets\" \\\n  --stage GA \\\n  --permissions \"secretmanager.secrets.create\"\n\ngcloud iam roles create ZenMLServerSecretsStoreEditor \\\n  --project <your GCP project ID> \\\n  --title \"ZenML Server Secrets Store Editor\" \\\n  --description \"Allow the ZenML Server to manage its secrets\" \\\n  --stage GA \\\n  --permissions \"secretmanager.secrets.get,secretmanager.secrets.update,secretmanager.versions.access,secretmanager.versions.add,secretmanager.secrets.delete\"\n\ngcloud projects add-iam-policy-binding <your GCP project ID> \\\n  --member serviceAccount:<your GCP service account email> \\\n  --role projects/<your GCP project ID>/roles/ZenMLServerSecretsStoreCreator \\\n  --condition None\n\n# NOTE: use the GCP project NUMBER, not the project ID in the condition\ngcloud projects add-iam-policy-binding <your GCP project ID> \\\n  --member serviceAccount:<your GCP service account email> \\\n  --role projects/<your GCP project ID>/roles/ZenMLServerSecretsStoreEditor \\\n  --condition 'title=limit_access_zenml,description=\"Limit access to secrets with prefix zenml-\",expression=resource.name.startsWith(\"projects/<your GCP project NUMBER>/secrets/zenml-\")'\n```\n\nExample configuration for the GCP Secrets Store:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Secrets store settings. This is used to store centralized secrets.\n   secretsStore:\n\n     # Set to false to disable the secrets store.\n     enabled: true\n\n     # The type of the secrets store\n     type: gcp\n\n     # Configuration for the GCP Secrets Manager secrets store\n     gcp:\n\n       # The GCP Service Connector authentication method to use.\n       authMethod: service-account\n\n       # The GCP Service Connector configuration.\n       authConfig:\n\n          # The GCP project ID to use. This must be set to the project ID where the\n          # GCP Secrets Manager service that you want to use is located.\n"}
{"input": "          project_id: my-gcp-project\n\n          # GCP credentials JSON to use to authenticate with the GCP Secrets\n          # Manager instance. \n          google_application_credentials: |\n            {\n              \"type\": \"service_account\",\n              \"project_id\": \"my-project\",\n              \"private_key_id\": \"...\",\n              \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...=\\n-----END PRIVATE KEY-----\\n\",\n              \"client_email\": \"...\",\n              \"client_id\": \"...\",\n              \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n              \"token_uri\": \"https://oauth2.googleapis.com/token\",\n              \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n              \"client_x509_cert_url\": \"...\"\n            }\n\n serviceAccount:\n\n   # If you're using workload identity, you need to annotate the service\n   # account with the GCP service account name (see https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity)\n   annotations:\n     iam.gke.io/gcp-service-account: <SERVICE_ACCOUNT_NAME>@<PROJECT_NAME>.iam.gserviceaccount.com\n\n```\n{% endtab %}\n\n{% tab title=\"Azure\" %}\n**Using the Azure Key Vault as a secrets store backend**\n\nThe Azure Secrets Store uses the ZenML Azure Service Connector under the hood to authenticate with the Azure Key Vault API. This means that you can use any of the [authentication methods supported by the Azure Service Connector](../../how-to/auth-management/azure-service-connector.md#authentication-methods) to authenticate with the Azure Key Vault API.\n\nExample configuration for the Azure Key Vault Secrets Store:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Secrets store settings. This is used to store centralized secrets.\n   secretsStore:\n\n     # Set to false to disable the secrets store.\n     enabled: true\n\n     # The type of the secrets store\n     type: azure\n\n     # Configuration for the Azure Key Vault secrets store\n     azure:\n\n       # The name of the Azure Key Vault. This must be set to point to the Azure\n       # Key Vault instance that you want to use.\n       key_vault_name:\n\n       # The Azure Service Connector authentication method to use.\n       authMethod: service-principal\n\n       # The Azure Service Connector configuration.\n       authConfig:\n\n          # The Azure application service principal credentials to use to\n          # authenticate with the Azure Key Vault API.\n          client_id: <your Azure client ID>\n          client_secret: <your Azure client secret>\n          tenant_id: <your Azure tenant ID>\n```\n{% endtab %}\n\n{% tab title=\"Hashicorp\" %}\n**Using the HashiCorp Vault as a secrets store backend**\n\nTo use the HashiCorp Vault service as a Secrets Store back-end, it must be configured in the Helm values:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Secrets store"}
{"input": " settings. This is used to store centralized secrets.\n   secretsStore:\n\n     # Set to false to disable the secrets store.\n     enabled: true\n\n     # The type of the secrets store\n     type: hashicorp\n\n     # Configuration for the HashiCorp Vault secrets store\n     hashicorp:\n\n       # The url of the HashiCorp Vault server to use\n       vault_addr: https://vault.example.com\n       # The token used to authenticate with the Vault server\n       vault_token: <your Vault token>\n       # The Vault Enterprise namespace. Not required for Vault OSS.\n       vault_namespace: <your Vault namespace>\n```\n{% endtab %}\n\n{% tab title=\"Custom\" %}\n**Using a custom secrets store backend implementation**\n\nYou have the option of using [a custom implementation of the secrets store API](secret-management.md) as your secrets store back-end. This must come in the form of a class derived from `zenml.zen_stores.secrets_stores.base_secrets_store.BaseSecretsStore`. This class must be importable from within the ZenML server container, which means you most likely need to build a custom container image that contains the class. Then, you can configure the Helm values to use your custom secrets store as follows:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Secrets store settings. This is used to store centralized secrets.\n   secretsStore:\n\n     # Set to false to disable the secrets store.\n     enabled: true\n\n     # The type of the secrets store\n     type: custom\n\n     # Configuration for the HashiCorp Vault secrets store\n     custom:\n\n       # The class path of the custom secrets store implementation. This should\n       # point to a full Python class that extends the\n       # `zenml.zen_stores.secrets_stores.base_secrets_store.BaseSecretsStore`\n       # base class. The class should be importable from the container image\n       # that you are using for the ZenML server.\n       class_path: my.custom.secrets.store.MyCustomSecretsStore\n\n   # Extra environment variables used to configure the custom secrets store.\n   environment:\n     ZENML_SECRETS_STORE_OPTION_1: value1\n     ZENML_SECRETS_STORE_OPTION_2: value2\n\n   # Extra environment variables to set in the ZenML server container that\n   # should be kept secret and are used to configure the custom secrets store.\n   secretEnvironment:\n     ZENML_SECRETS_STORE_SECRET_OPTION_3: value3\n     ZENML_SECRETS_STORE_SECRET_OPTION_4: value4\n```\n{% endtab %}\n{% endtabs %}\n\n#### Backup secrets store\n\n[A backup secrets store](secret-management.md#backup-secrets-store) back-end may be configured for high-availability and backup purposes. or as an intermediate step in the process of [migrating secrets to a different external location or secrets"}
{"input": " manager provider](secret-management.md#secrets-migration-strategy).\n\nTo configure a backup secrets store in the Helm chart, use the same approach and instructions documented for the primary secrets store, but using the `backupSecretsStore` configuration section instead of `secretsStore`, e.g.:\n\n```yaml\n zenml:\n\n   # ...\n\n   # Backup secrets store settings. This is used as a backup for the primary\n   # secrets store.\n   backupSecretsStore:\n\n     # Set to true to enable the backup secrets store.\n     enabled: true\n\n     # The type of the backup secrets store\n     type: aws\n\n     # Configuration for the AWS Secrets Manager backup secrets store\n     aws:\n\n       # The AWS Service Connector authentication method to use.\n       authMethod: secret-key\n\n       # The AWS Service Connector configuration.\n       authConfig:\n        # The AWS region to use. This must be set to the region where the AWS\n        # Secrets Manager service that you want to use is located.\n        region: us-east-1\n\n        # The AWS credentials to use to authenticate with the AWS Secrets\n        aws_access_key_id: <your AWS access key ID>\n        aws_secret_access_key: <your AWS secret access key>\n```\n\n### Database backup and recovery\n\nAn automated database backup and recovery feature is enabled by default for all Helm deployments. The ZenML server will automatically back up the database before every upgrade and restore it if the upgrade fails in a way that affects the database.\n\n{% hint style=\"info\" %}\nThe database backup automatically created by the ZenML server is only temporary and only used as an immediate recovery in case of database migration failures. It is not meant to be used as a long-term backup solution. If you need to back up your database for long-term storage, you should use a dedicated backup solution.\n{% endhint %}\n\nSeveral database backup strategies are supported, depending on where and how the backup is stored. The strategy can be configured by means of the `zenml.database.backupStrategy` Helm value:\n\n* `disabled` - no backup is performed\n* `in-memory` - the database schema and data are stored in memory. This is the fastest backup strategy, but the backup is not persisted across pod restarts, so no manual intervention is possible in case the automatic DB recovery fails after a failed DB migration. Adequate memory resources should be allocated to the ZenML server pod when using this backup strategy with larger databases. This is the default backup strategy.\n* `database` - the database is copied to a backup database in the same database server. This requires the `backupDatabase` option to be set to the name of the backup database. This backup strategy is only supported for MySQL compatible databases and the user specified in the database URL must have permissions to manage (create, drop, and modify) the backup database in addition to the main database.\n* `dump-file` - the database schema and data are dumped"}
{"input": " to a file local to the database initialization and upgrade job. Users may optionally configure a persistent volume where the dump file will be stored by setting the `backupPVStorageSize` and optionally the `backupPVStorageClass` options. If a persistent volume is not configured, the dump file will be stored in an emptyDir volume, which is not persisted. If configured, the user is responsible for deleting the resulting PVC when uninstalling the Helm release.\n\n> **NOTE:** You should also set the `podSecurityContext.fsGroup` option if you are using a persistent volume to store the dump file.\n\nThe following additional rules are applied concerning the creation and lifetime of the backup:\n\n* a backup is not attempted if the database doesn't need to undergo a migration (e.g. when the ZenML server is upgraded to a new version that doesn't require a database schema change or if the ZenML version doesn't change at all).\n* a backup file or database is created before every database migration attempt (i.e. during every Helm upgrade). If a backup already exists (i.e. persisted in a persistent volume or backup database), it is overwritten.\n* the persistent backup file or database is cleaned up after the migration is completed successfully or if the database doesn't need to undergo a migration. This includes backups created by previous failed migration attempts.\n* the persistent backup file or database is NOT cleaned up after a failed migration. This allows the user to manually inspect and/or apply the backup if the automatic recovery fails.\n\nThe following example shows how to configure the ZenML server to use a persistent volume to store the database dump file:\n\n```yaml\n zenml:\n\n   # ...\n\n  database:\n    url: \"mysql://admin:password@my.database.org:3306/zenml\"\n\n    # Configure the database backup strategy\n    backupStrategy: dump-file\n    backupPVStorageSize: 1Gi\n\npodSecurityContext:\n  fsGroup: 1000 # if you're using a PVC for backup, this should necessarily be set.\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying ZenML to Huggingface Spaces.\n---\n\n# Deploy using HuggingFace Spaces\n\nA quick way to deploy ZenML and get started is to use [HuggingFace Spaces](https://huggingface.co/spaces). HuggingFace Spaces is a platform for hosting and sharing ML projects and workflows, and it also works to deploy ZenML. You can be up and running in minutes (for free) with a hosted ZenML server, so it's a good option if you want to try out ZenML without any infrastructure overhead.\n\n{% hint style=\"info\" %}\nIf you are planning to use HuggingFace Spaces for production use, make sure you have [persistent storage turned on](https://huggingface.co/docs/hub/en/spaces-storage) so as to prevent loss of data. See our [other deployment options](./README.md) if you want alternative options.\n{% endhint %}\n\n![ZenML on HuggingFace Spaces -- default deployment](../../.gitbook/assets/hf-spaces-chart.png)\n\nIn this diagram, you can see what the default deployment of ZenML on HuggingFace looks like.\n\n## Deploying ZenML on HuggingFace Spaces\n\nYou can deploy ZenML on HuggingFace Spaces with just a few clicks:\n\n[![](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=zenml/zenml)\n\nTo set up your ZenML app, you need to specify three main components: the Owner (either your personal account or an organization), a Space name, and the Visibility (a bit lower down the page). Note that the space visibility needs to be set to 'Public' if you wish to connect to the ZenML server from your local machine.\n\n![HuggingFace Spaces SDK interface](../../.gitbook/assets/hf-spaces-sdk.png)\n\nYou have the option here to select a higher-tier machine to use for your server. The advantage of selecting a paid CPU instance is that it is not subject to auto-shutdown policies and thus will stay up as long as you leave it up. In order to make use of a persistent CPU, you'll likely want to create and set up a MySQL database to connect to (see below).\n\nTo personalize your Space's appearance, such as the title, emojis, and colors, navigate to \"Files and Versions\" and modify the metadata in your README.md file. Full information on Spaces configuration parameters can be found on the HuggingFace [documentation reference guide](https://huggingface.co/docs/hub/spaces-config-reference).\n\nAfter creating your Space, you'll notice a 'Building' status along with logs displayed on the screen. When this switches to 'Running', your Space is ready for use. If the ZenML login UI isn't visible, try refreshing the page.\n\nIn the upper-right hand corner of your space you"}
{"input": "'ll see a button with three dots which, when you click on it, will offer you a menu option to \"Embed this Space\". (See [the HuggingFace documentation](https://huggingface.co/docs/hub/spaces-embed) for more details on this feature.) Copy the \"Direct URL\" shown in the box that you can now see on the screen. This should look something like this: `https://<YOUR_USERNAME>-<SPACE_NAME>.hf.space`. Open that URL and follow the instructions to initialize your ZenML server and set up an initial admin user account.\n\n## Connecting to your ZenML Server from your local machine\n\nOnce you have your ZenML server up and running, you can connect to it from your local machine. To do this, you'll need to get your Space's 'Direct URL' (see above).\n\n{% hint style=\"warning\" %}\nYour Space's URL will only be available and usable for connecting from your local machine if the visibility of the space is set to 'Public'.\n{% endhint %}\n\nYou can use the 'Direct URL' to connect to your ZenML server from your local machine with the following CLI command (after installing ZenML, and using your custom URL instead of the placeholder):\n\n```shell\nzenml connect --url '<YOUR_HF_SPACES_DIRECT_URL>'\n```\n\nYou can also use the Direct URL in your browser to use the ZenML dashboard as a fullscreen application (i.e. without the HuggingFace Spaces wrapper around it).\n\n## Extra configuration options\n\nBy default, the ZenML application will be configured to use an SQLite non-persistent database. If you want to use a persistent database, you can configure this by amending the `Dockerfile` to your Space's root directory. For full details on the various parameters you can change, see [our reference documentation](deploy-with-docker.md#advanced-server-configuration-options) on configuring ZenML when deployed with Docker.\n\n{% hint style=\"info\" %}\nIf you are using the space just for testing and experimentation, you don't need to make any changes to the configuration. Everything will work out of the box.\n{% endhint %}\n\nYou can also use an external secrets backend together with your HuggingFace Spaces as described in [our documentation](deploy-with-docker.md#advanced-server-configuration-options). You should be sure to use HuggingFace's inbuilt ' Repository secrets' functionality to configure any secrets you need to use in your`Dockerfile` configuration. [See the documentation](https://huggingface.co/docs/hub/spaces-sdks-docker#secret-management) for more details on how to set this up.\n\n{% hint style=\"warning\" %}\nIf you wish to use a cloud secrets backend together with ZenML for secrets management, **you must update your password** on your ZenML Server on the Dashboard. This is because the default user created by the HuggingFace Spaces deployment process has no"}
{"input": " password assigned to it and as the Space is publicly accessible (since the Space is public) _potentially anyone could access your secrets without this extra step_. To change your password navigate to the Settings page by clicking the button in the upper right-hand corner of the Dashboard and then click 'Update Password'.\n{% endhint %}\n\n## Troubleshooting\n\nIf you are having trouble with your ZenML server on HuggingFace Spaces, you can view the logs by clicking on the \"Open Logs\" button at the top of the space. This will give you more context of what's happening with your server.\n\nIf you have any other issues, please feel free to reach out to us on our [Slack channel](https://zenml.io/slack/) for more support.\n\n## Upgrading your ZenML Server on HF Spaces\n\nThe default space will use the latest version of ZenML automatically. If you want to update your version, you can simply select the 'Factory reboot' option within the 'Settings' tab of the space. Note that this will wipe any data contained within the space and so if you are not using a MySQL persistent database (as described above) you will lose any data contained within your ZenML deployment on the space. You can also configure the space to use an earlier version by updating the `Dockerfile`'s `FROM` import statement at the very top.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying ZenML with custom Docker images.\n---\n\n# Deploy with custom images\n\nIn most cases, deploying ZenML with the default `zenmlhub/zenml-server` Docker image should work just fine. However, there are some scenarios when you might need to deploy ZenML with a custom Docker image:\n\n* You have implemented a custom artifact store for which you want to enable [artifact visualizations](../../how-to/handle-data-artifacts/visualize-artifacts.md) or [step logs](../../how-to/setting-up-a-project-repository/best-practices.md#logging) in your dashboard.\n* You have forked the ZenML repository and want to deploy a ZenML server based on your own fork because you made changes to the server / database logic.\n\n{% hint style=\"warning\" %}\nDeploying ZenML with custom Docker images is only possible for [Docker](deploy-with-docker.md) or [Helm](deploy-with-helm.md) deployments.\n{% endhint %}\n\n### Build and Push Custom ZenML Server Docker Image\n\nHere is how you can build a custom ZenML server Docker image:\n\n1. Set up a container registry of your choice. E.g., as an indivial developer you could create a free [Docker Hub](https://hub.docker.com/) account and then set up a free Docker Hub repository.\n2.  Clone ZenML (or your ZenML fork) and checkout the branch that you want to deploy, e.g., if you want to deploy ZenML version 0.41.0, run\n\n    ```bash\n    git checkout release/0.41.0\n    ```\n3.  Copy the [ZenML base.Dockerfile](https://github.com/zenml-io/zenml/blob/main/docker/base.Dockerfile), e.g.:\n\n    ```bash\n    cp docker/base.Dockerfile docker/custom.Dockerfile\n    ```\n4.  Modify the copied Dockerfile:\n\n    * Add additional dependencies:\n\n    ```bash\n    RUN pip install <my_package>\n    ```\n\n    * (Forks only) install local files instead of official ZenML:\n\n    ```bash\n    RUN pip install -e .[server,secrets-aws,secrets-gcp,secrets-azure,secrets-hashicorp,s3fs,gcsfs,adlfs,connectors-aws,connectors-gcp,connectors-azure]\n    ```\n5.  Build and push an image based on your Dockerfile:\n\n    ```bash\n    docker build -f docker/custom.Dockerfile . -t <YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>:<IMAGE_TAG> --platform linux/amd64\n    docker push <YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>:<IMAGE_TAG>\n    ```\n\n{% hint style=\"info\" %}\nIf you want to verify your custom image locally, you can follow the [Deploy a custom ZenML image via Docker](deploy"}
{"input": "-with-custom-image.md#deploy-a-custom-zenml-image-via-docker) section below to deploy the ZenML server locally first.\n{% endhint %}\n\n### Deploy ZenML with your custom image\n\nNext, adjust your preferred deployment strategy to use the custom Docker image you just built.\n\n#### Deploy a custom ZenML image via CLI\n\nYou can deploy your custom image via the `zenml deploy` CLI command by setting the `--config` argument to a custom configuration file that has both `zenmlserver_image_repo` and `zenmlserver_image_tag` set:\n\n1.  Define a custom `config.yaml` based on the [base deployment configuration file](deploy-with-zenml-cli.md#base-configuration-file) and set `zenmlserver_image_repo` and `zenmlserver_image_tag` according to the custom image you built:\n\n    ```yaml\n    zenmlserver_image_repo: <YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>\n    zenmlserver_image_tag: <IMAGE_TAG>\n    ```\n2.  Run `zenml deploy` with the custom config file:\n\n    ```shell\n    zenml deploy --config=/PATH/TO/FILE\n    ```\n\nSee the general [ZenML CLI Deployment Guide](deploy-with-zenml-cli.md) for more information on how to use the `zenml deploy` CLI command and what other options can be configured.\n\n#### Deploy a custom ZenML image via Docker\n\nTo deploy your custom image via Docker, first familiarize yourself with the general [ZenML Docker Deployment Guide](deploy-with-docker.md).\n\nTo use your own image, follow the general guide step by step but replace all mentions of `zenmldocker/zenml-server` with your custom image reference `<YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>:<IMAGE_TAG>`. E.g.:\n\n* To run the ZenML server with Docker based on your custom image, do\n\n```bash\ndocker run -it -d -p 8080:8080 --name zenml <YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>:<IMAGE_TAG>\n```\n\n* To use `docker-compose`, adjust your `docker-compose.yml`:\n\n```yaml\nservices:\n  zenml:\n    image: <YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>:<IMAGE_TAG>\n```\n\n#### Deploy a custom ZenML image via Helm\n\nTo deploy your custom image via Helm, first familiarize yourself with the general [ZenML Helm Deployment Guide](deploy-with-helm.md).\n\nTo use your own image, the only thing you need to do differently is to modify the `image` section of your `values.yaml` file:\n\n```yaml\nzenml:\n  image:\n    repository: <YOUR_CONTAINER_REGISTRY>/<IMAGE_NAME>\n    tag: <IMAGE_TAG>\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee"}
{"input": "424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying ZenML on cloud using the ZenML CLI.\n---\n\n# Deploy with ZenML CLI\n\nThe easiest and fastest way to get running on the cloud is by using the `deploy` CLI command. It currently only supports deploying to Kubernetes on managed cloud services. You can check the [overview page](./README.md#deploying-a-zenml-server) to learn about other options that you have.\n\nBefore we begin, it will help to understand the [architecture](./README.md) around the ZenML server and the database that it uses. Now, depending on your setup, you may find one of the following scenarios relevant.\n\n## Option 1: Starting from scratch\n\nIf you don't have an existing Kubernetes cluster, you have the following two options to set it up:\n\n* Creating it manually using the documentation for your cloud provider. For convenience, here are links for [AWS](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html), [Azure](https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli), and [GCP](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster#before\\_you\\_begin).\n* Using a [stack recipe](../../how-to/stack-deployment/deploy-a-stack-using-mlstacks.md) that sets up a cluster along with other tools that you might need in your cloud stack like artifact stores and secret managers. Take a look at all [available stack recipes](https://github.com/zenml-io/mlstacks) to see if there's something that works for you.\n\n{% hint style=\"warning\" %}\nOnce you have created your cluster, make sure that you configure your [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) client to talk to it.\n{% endhint %}\n\nYou're now ready to deploy ZenML! Run the following command:\n\n```bash\nzenml deploy\n```\n\nYou will be prompted to provide a name for your deployment and details like what cloud provider you want to deploy to \u2014 and that's it! It creates the database and any VPCs, permissions, and more that are needed.\n\n{% hint style=\"info\" %}\nIn order to be able to run the `deploy` command, you should have your cloud provider's CLI configured locally with permissions to create resources like MySQL databases and networks.\n{% endhint %}\n\nReasonable defaults are in place for you already and if you wish to configure more settings, take a look at the next scenario that uses a config file.\n\n## Option 2: Using existing cloud resources\n\n### Existing Kubernetes cluster\n\nIf you already have an existing cluster without an ingress controller, you can jump straight to the `deploy` command above to get going with the defaults. Please make sure that you have your local `kubectl` configured to talk to your cluster.\n\n#### Having an existing NGINX In"}
{"input": "gress Controller\n\nThe `deploy` command, by default, tries to create an NGINX ingress controller on your cluster. If you already have an existing controller, you can tell ZenML to not re-deploy it through the use of a config file. This file can be found in the [Configuration File Templates](deploy-with-zenml-cli.md#configuration-file-templates) towards the end of this guide. It offers a host of configuration options that you can leverage for advanced use cases.\n\n*   Check if an ingress controller is running on your cluster by running the following command. You should see an entry in the output with the hostname populated.\n\n    ```bash\n    # change the namespace to any other where \n    # You might have the controller installed\n    kubectl get svc -n ingress-nginx\n    ```\n* Set `create_ingress_controller` to `false`.\n*   Supply your controller's hostname to the `ingress_controller_hostname` variable.\n\n    > **Note:** The address should not have a trailing `/`.\n*   You can now run the `deploy` command and pass the config file above, to it.\n\n    ```\n    zenml deploy --config=/PATH/TO/FILE\n    ```\n\n    > **Note:** To be able to run the deploy command, you should have your cloud provider's CLI configured locally with permissions to create resources like MySQL databases and networks.\n\n### Existing hosted SQL database\n\nIf you also already have a database that you would want to use with the deployment, you can choose to configure it with the use of the config file. Here, we will demonstrate setting the database.\n\n*   Fill the fields below from the config file with values from your database.\n\n    ```yaml\n    # The username and password for the database.\n    database_username: \n    database_password: \n\n    # The URL of the database to use for the ZenML server.\n    database_url: \n\n    # The path to the SSL CA certificate to use for the database connection.\n    database_ssl_ca: \n\n    # The path to the client SSL certificate to use for the database connection.\n    database_ssl_cert: \n\n    # The path to the client SSL key to use for the database connection.\n    database_ssl_key: \n\n    # Whether to verify the database server SSL certificate.\n    database_ssl_verify_server_cert: \n    ```\n*   Run the `deploy` command and pass the config file above to it.\n\n    ```\n    zenml deploy --config=/PATH/TO/FILE\n    ```\n\n    > **Note** To be able to run the deploy command, you should have your cloud provider's CLI configured locally with permissions to create resources like MySQL databases and networks.\n\n## Configuration file templates\n\n#### Base configuration file\n\nBelow is the general structure of a config file. Use this as a base and then add any cloud-specific parameters from the sections below.\n\n<details>\n\n<summary>General</summary>\n\n```yaml\n# Name of"}
{"input": " the server deployment.\nname:\n\n# The server provider type, one of aws, gcp or azure.\nprovider:\n\n# The path to the kubectl config file to use for deployment.\nkubectl_config_path:\n\n# The Kubernetes namespace to deploy the ZenML server to.\nnamespace: zenmlserver\n\n# The path to the ZenML server helm chart to use for deployment.\nhelm_chart:\n\n# The repository and tag to use for the ZenML server Docker image.\nzenmlserver_image_repo: zenmldocker/zenml\nzenmlserver_image_tag: latest\n\n# Whether to deploy an nginx ingress controller as part of the deployment.\ncreate_ingress_controller: true\n\n# Whether to use TLS for the ingress.\ningress_tls: true\n\n# Whether to generate self-signed TLS certificates for the ingress.\ningress_tls_generate_certs: true\n\n# The name of the Kubernetes secret to use for the ingress.\ningress_tls_secret_name: zenml-tls-certs\n\n# The ingress controller's IP address. The ZenML server will be exposed on a subdomain of this IP. For AWS, if you have a hostname instead, use the following command to get the IP address: `dig +short <hostname>`.\ningress_controller_ip:\n\n# Whether to create a SQL database service as part of the recipe.\ndeploy_db: true\n\n# The username and password for the database. \ndatabase_username: user\ndatabase_password:\n\n# The URL of the database to use for the ZenML server.\ndatabase_url:\n\n# The path to the SSL CA certificate to use for the database connection.\ndatabase_ssl_ca:\n\n# The path to the client SSL certificate to use for the database connection.\ndatabase_ssl_cert:\n\n# The path to the client SSL key to use for the database connection.\ndatabase_ssl_key:\n\n# Whether to verify the database server SSL certificate.\ndatabase_ssl_verify_server_cert: true\n\n# The log level to set the terraform client. Choose one of TRACE, \n# DEBUG, INFO, WARN, or ERROR (case insensitive).\nlog_level: ERROR \n```\n\n</details>\n\n{% hint style=\"info\" %}\nFeel free to include only those variables that you want to customize, in your file. For all other variables, the default values (shown above) will be used.\n{% endhint %}\n\n#### Cloud-specific settings\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n<pre class=\"language-yaml\"><code class=\"lang-yaml\"># The AWS region to deploy to.\nregion: eu-west-1 \n\n# The name of the RDS instance to create\n<strong>rds_name: zenmlserver\n</strong>\n<strong># Name of RDS database to create.\n</strong>db_name: zenmlserver\n\n# Type of RDS database to create.\ndb_type: mysql\n\n# Version of RDS database to create.\ndb_version: 5.7.38\n\n# Instance class of RDS database to create.\ndb_instance_class:"}
{"input": " db.t3.micro\n\n# Allocated storage of RDS database to create.\ndb_allocated_storage: 5\n</code></pre>\n\nThe `database_username` and `database_password` from the general config is used to set those variables for the AWS RDS instance.\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n<pre class=\"language-yaml\"><code class=\"lang-yaml\"># The project in GCP to deploy the server in.\nproject_id: \n\n<strong># The GCP region to deploy to.\n</strong>region: europe-west3\n\n# The name of the CloudSQL instance to create.\ncloudsql_name: zenmlserver\n\n# Name of CloudSQL database to create.\ndb_name: zenmlserver\n\n# Instance class of CloudSQL database to create.\ndb_instance_tier: db-n1-standard-1\n\n# Allocated storage of CloudSQL database, in GB, to create.\ndb_disk_size: 10\n\n# Whether or not to enable the Secrets Manager API. Disable this if you\n# don't have ListServices permissions on the project.\nenable_secrets_manager_api: true\n</code></pre>\n\n* The `project_id` is required to be set.\n* The `database_username` and `database_password` from the general config is used to set those variables for the CloudSQL instance.\n* SSL is disabled by default on the database and the option to enable it is coming soon!\n{% endtab %}\n\n{% tab title=\"Azure\" %}\n```yaml\n# The Azure resource_group to deploy to.\nresource_group: zenml\n\n# The name of the Flexible MySQL instance to create.\ndb_instance_name: zenmlserver\n\n# Name of RDS database to create.\ndb_name: zenmlserver\n\n# Version of MySQL database to create.\ndb_version: 5.7\n\n# The sku_name for the database resource.\ndb_sku_name: B_Standard_B1s\n\n# Allocated storage of MySQL database to create.\ndb_disk_size: 20\n```\n\nThe `database_username` and `database_password` from the general config is used to set those variables for the Azure Flexible MySQL server.\n{% endtab %}\n{% endtabs %}\n\n## Connecting to deployed ZenML\n\nImmediately after deployment, the ZenML server needs to be activated before it can be used. The activation process includes creating an initial admin user account and configuring some server settings. You can do this only by visiting the ZenML server URL in your browser and following the on-screen instructions. Connecting your local ZenML client to the server is not possible until the server is properly initialized.\n\nOnce ZenML is deployed, one or multiple users can connect to it with the `zenml connect` command.\n\n```bash\nzenml connect\n```\n\n{% hint style=\"info\" %}\nIf no arguments are supplied, ZenML will attempt to connect to the last ZenML server deployed from the local host using the `zenml deploy`"}
{"input": " command:\n{% endhint %}\n\nIn order to connect to a specific ZenML server, you can either pass the configuration as command line arguments or as a YAML file:\n\n```bash\nzenml connect --url=https://zenml.example.com:8080 --no-verify-ssl\n```\n\nor\n\n```bash\nzenml connect --config=/path/to/zenml_server_config.yaml\n```\n\nThe YAML file should have the following structure when connecting to a ZenML server:\n\n```yaml\n# The URL of the ZenML server\nurl:\n\n# Either a boolean, in which case it controls whether the server's TLS \n# certificate is verified, or a string, in which case it must be a path \n# to a CA certificate bundle to use or the CA bundle value itself\nverify_ssl: \n```\n\nHere is an example of a ZenML server YAML configuration file:\n\n```yaml\nurl: https://ac8ef63af203226194a7725ee71d85a-7635928635.us-east-1.elb.amazonaws.com/zenml\nverify_ssl: |\n  -----BEGIN CERTIFICATE-----\n...\n  -----END CERTIFICATE-----\n```\n\nTo disconnect from the current ZenML server and revert to using the local default database, use the following command:\n\n```bash\nzenml disconnect\n```\n\n## How does it work?\n\nHere's an architecture diagram that shows how the workflow looks like when you do `zenml deploy`.\n\n![Running zenml deploy](../../../.gitbook/assets/zenml\\_deploy.png)\n\nThe deploy CLI makes use of a \"recipe\" inside the `zenml-io/zenml` repository to deploy the server on the right cloud. Any configuration that you pass with the CLI, is sent to the recipe as input variables.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Why do we need to deploy ZenML?\n---\n\n# \ud83e\udd14 Deploying ZenML\n\nMoving your ZenML Server to a production environment offers several benefits over staying local:\n\n1. **Scalability**: Production environments are designed to handle large-scale workloads, allowing your models to process more data and deliver faster results.\n2. **Reliability**: Production-grade infrastructure ensures high availability and fault tolerance, minimizing downtime and ensuring consistent performance.\n3. **Collaboration**: A shared production environment enables seamless collaboration between team members, making it easier to iterate on models and share insights.\n\nDespite these advantages, transitioning to production can be challenging due to the complexities involved in setting up the needed infrastructure.\n\n### ZenML Server\n\nWhen you first get started with ZenML, it relies with the following architecture on your machine.\n\n![Scenario 1: ZenML default local configuration](../../.gitbook/assets/Scenario1.png)\n\nThe SQLite database that you can see in this diagram is used to store information about pipelines, pipeline runs, stacks, and other configurations. Users can run the `zenml up` command to spin up a local REST server to serve the dashboard. The diagram for this looks as follows:\n\n![Scenario 2: ZenML with a local REST Server](../../.gitbook/assets/Scenario2.png)\n\n{% hint style=\"info\" %}\nIn Scenario 2, the `zenml up` command implicitly connects the client to the server.\n{% endhint %}\n\n{% hint style=\"warning\" %}\nCurrently the ZenML server supports a legacy and a brand-new version of the dashboard. To use the legacy version simply use the\nfollowing command `zenml up --legacy`\n{% endhint %}\n\nIn order to move into production, the ZenML server needs to be deployed somewhere centrally so that the different cloud stack components can read from and write to the server. Additionally, this also allows all your team members to connect to it and share stacks and pipelines.\n\n![Scenario 3: Deployed ZenML Server](../../.gitbook/assets/Scenario3.2.png)\n\n### Deploying a ZenML Server\n\nDeploying the ZenML Server is a crucial step towards transitioning to a production-grade environment for your machine learning projects. By setting up a deployed ZenML Server instance, you gain access to powerful features, allowing you to use stacks with remote components, centrally track progress, collaborate effectively, and achieve reproducible results.\n\nCurrently, there are two main options to access a deployed ZenML server:\n\n1. **SaaS:** With the [Cloud](../zenml-pro/zenml-cloud.md) offering you can utilize a control plane to create ZenML servers, also known as tenants. These tenants are managed and maintained by ZenML's dedicated team, alleviating the burden of server management from your end. Importantly, your data remains securely within your stack, and ZenML's role is primarily to handle tracking of metadata and server maintenance.\n2. **Self"}
{"input": "-hosted Deployment:** Alternatively, you have the ability to deploy ZenML on your own self-hosted environment. This can be achieved through various methods, including using [our CLI](deploy-with-zenml-cli.md), [Docker](../../component-guide/model-registries/model-registries.md), [Helm](deploy-with-helm.md), or [HuggingFace Spaces](deploy-using-huggingface-spaces.md). We also offer our Pro version for self-hosted deployments, so you can use our full paid feature-set while staying fully in control with an airgapped solution on your infrastructure.\n\n{% hint style=\"warning\" %}\nCurrently the ZenML server supports a legacy and a brand-new version of the dashboard. To use the legacy version which supports stack registration from the dashboard simply set the following environment variable in the deployment environment: `export ZEN_SERVER_USE_LEGACY_DASHBOARD=True`.\n{% endhint %}\n\nBoth options offer distinct advantages, allowing you to choose the deployment approach that best aligns with your organization's needs and infrastructure preferences. Whichever path you select, ZenML facilitates a seamless and efficient way to take advantage of the ZenML Server and enhance your machine learning workflows for production-level success.\n\nChoose the most appropriate deployment strategy for you out of the following options to get started with the deployment:\n\n<table data-card-size=\"large\" data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden></th><th data-hidden data-type=\"content-ref\"></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><mark style=\"color:purple;\"><strong>Deploy with ZenML CLI</strong></mark></td><td>Deploying ZenML on cloud using the ZenML CLI.</td><td></td><td></td><td><a href=\"deploy-with-zenml-cli.md\">Broken link</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>Deploy with Docker</strong></mark></td><td>Deploying ZenML in a Docker container.</td><td></td><td></td><td><a href=\"deploy-with-docker.md\">Broken link</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>Deploy with Helm</strong></mark></td><td>Deploying ZenML in a Kubernetes cluster with Helm.</td><td></td><td></td><td><a href=\"deploy-with-helm.md\">Broken link</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>Deploy using HuggingFace Spaces</strong></mark></td><td>Deploying ZenML to Huggingface Spaces.</td><td></td><td></td><td><a href=\"deploy-using-huggingface-spaces.md\">Broken link</a></td></tr></tbody"}
{"input": "></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Registering and utilizing secrets.\n---\n\n# Secret management\n\n## What is a ZenML secret?\n\nZenML secrets are groupings of **key-value pairs** which are securely stored in the ZenML secrets store. Additionally, a secret always has a **name** that allows you to fetch or reference them in your pipelines and stacks.\n\n## Centralized secrets store\n\nZenML provides a centralized secrets management system that allows you to register and manage secrets in a secure way. The metadata of the ZenML secrets (e.g. name, ID, owner, scope etc.) is always stored in the ZenML server database, while the actual secret values are stored and managed separately, through the ZenML Secrets Store. This allows for a flexible deployment strategy that meets the security and compliance requirements of your organization.\n\nIn a local ZenML deployment, secret values are also stored in the local SQLite database. When connected to a remote ZenML server, the secret values are stored in the secrets management back-end that the server's Secrets Store is configured to use, while all access to the secrets is done through the ZenML server API.\n\n<figure><img src=\"../../.gitbook/assets/secrets-store-architecture.png\" alt=\"\"><figcaption><p>Basic Secrets Store Architecture</p></figcaption></figure>\n\nCurrently, the ZenML server can be configured to use one of the following supported secrets store back-ends:\n\n* the same SQL database that the ZenML server is using to store secrets metadata as well as other managed objects such as pipelines, stacks, etc. This is the default option.\n* the AWS Secrets Manager\n* the GCP Secret Manager\n* the Azure Key Vault\n* the HashiCorp Vault\n* a custom secrets store back-end implementation is also supported\n\n## Configuration and deployment\n\nConfiguring the specific secrets store back-end that the ZenML server uses is done at deployment time. This involves deciding on one of the supported back-ends and authentication mechanisms and configuring the ZenML server with the necessary credentials to authenticate with the back-end.\n\nThe ZenML secrets store reuses the [ZenML Service Connector](../../how-to/auth-management/service-connectors-guide.md) authentication mechanisms to authenticate with the secrets store back-end. This means that the same authentication methods and configuration parameters that are supported by the available Service Connectors are also reflected in the ZenML secrets store configuration. It is recommended to practice the principle of least privilege when configuring the ZenML secrets store and to use credentials with the documented minimum required permissions to access the secrets store back-end.\n\nThe ZenML secrets store configured for the ZenML Server can be updated at any time by updating the ZenML Server configuration and redeploying the server. This allows you to easily switch between different secrets store back-ends and authentication mechanisms. However, it is recommended to follow [the documented secret store migration strategy](secret-management.md#secrets-migration-strategy) to minimize downtime and to ensure that existing secrets are also properly migrated"}
{"input": ", in case the location where secrets are stored in the back-end changes.\n\nFor more information on how to deploy a ZenML server and configure the secrets store back-end, refer to your deployment strategy inside the deployment guide.\n\n## Backup secrets store\n\nThe ZenML Server deployment may be configured to optionally connect to _a second Secrets Store_ to provide additional features such as high-availability, backup and disaster recovery as well as an intermediate step in the process of migrating [secrets from one secrets store location to another](secret-management.md#secrets-migration-strategy). For example, the primary Secrets Store may be configured to use the internal database, while the backup Secrets Store may be configured to use the AWS Secrets Manager. Or two different AWS Secrets Manager accounts or regions may be used.\n\n{% hint style=\"warning\" %}\nAlways make sure that the backup Secrets Store is configured to use a different location than the primary Secrets Store. The location can be different in terms of the Secrets Store back-end type (e.g. internal database vs. AWS Secrets Manager) or the actual location of the Secrets Store back-end (e.g. different AWS Secrets Manager account or region, GCP Secret Manager project or Azure Key Vault's vault).\n\nUsing the same location for both the primary and backup Secrets Store will not provide any additional benefits and may even result in unexpected behavior.\n{% endhint %}\n\nWhen a backup secrets store is in use, the ZenML Server will always attempt to read and write secret values from/to the primary Secrets Store first while ensuring to keep the backup Secrets Store in sync. If the primary Secrets Store is unreachable, if the secret values are not found there or any otherwise unexpected error occurs, the ZenML Server falls back to reading and writing from/to the backup Secrets Store. Only if the backup Secrets Store is also unavailable, the ZenML Server will return an error.\n\nIn addition to the hidden backup operations, users can also explicitly trigger a backup operation by using the `zenml secret backup` CLI command. This command will attempt to read all secrets from the primary Secrets Store and write them to the backup Secrets Store. Similarly, the `zenml secret restore` CLI command can be used to restore secrets from the backup Secrets Store to the primary Secrets Store. These CLI commands are useful for migrating secrets from one Secrets Store to another.\n\n## Secrets migration strategy\n\nSometimes you may need to change the external provider or location where secrets values are stored by the Secrets Store. The immediate implication of this is that the ZenML server will no longer be able to access existing secrets with the new configuration until they are also manually copied to the new location. Some examples of such changes include:\n\n* switching Secrets Store back-end types (e.g. from internal SQL database to AWS Secrets Manager or Azure Key Vault)\n* switching back-end locations (e.g. changing the AWS Secrets Manager account or region, GCP Secret Manager project or Azure Key Vault's vault).\n\nIn such cases, it is not sufficient to"}
{"input": " simply reconfigure and redeploy the ZenML server with the new Secrets Store configuration. This is because the ZenML server will not automatically migrate existing secrets to the new location. Instead, you should follow a specific migration strategy to ensure that existing secrets are also properly migrated to the new location with minimal, even zero downtime.\n\nThe secrets migration process makes use of the fact that [a secondary Secrets Store](secret-management.md#backup-secrets-store) can be configured for the ZenML server for backup purposes. This secondary Secrets Store is used as an intermediate step in the migration process. The migration process is as follows (we'll refer to the Secrets Store that is currently in use as _Secrets Store A_ and the Secrets Store that will be used after the migration as _Secrets Store B_):\n\n1. Re-configure the ZenML server to use _Secrets Store B_ as the secondary Secrets Store.\n2. Re-deploy the ZenML server.\n3. Use the `zenml secret backup` CLI command to back up all secrets from _Secrets Store A_ to _Secrets Store B_. You don't have to worry about secrets that are created or updated by users during or after this process, as they will be automatically backed up to _Secrets Store B_. If you also wish to delete secrets from _Secrets Store A_ after they are successfully backed up to _Secrets Store B_, you should run `zenml secret backup --delete-secrets` instead.\n4. Re-configure the ZenML server to use _Secrets Store B_ as the primary Secrets Store and remove _Secrets Store A_ as the secondary Secrets Store.\n5. Re-deploy the ZenML server.\n\nThis migration strategy is not necessary if the actual location of the secrets values in the Secrets Store back-end does not change. For example:\n\n* updating the credentials used to authenticate with the Secrets Store back-end before or after they expire\n* switching to a different authentication method to authenticate with the same Secrets Store back-end (e.g. switching from an IAM account secret key to an IAM role in the AWS Secrets Manager)\n\nIf you are a [ZenML Pro](https://zenml.io/pro) user, you can configure your cloud backend based on your [deployment scenario](../zenml-pro/system-architectures.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to upgrade your server to a new version of ZenML for the different deployment options.\n---\n\n# Upgrade the version of the ZenML server\n\nThe way to upgrade your ZenML server depends a lot on how you deployed it.\n\n{% tabs %}\n{% tab title=\"ZenML CLI\" %}\nTo upgrade your ZenML server that was deployed with the `zenml deploy` command to a newer version, you can follow the\nsteps below.\n\n* In the config file, set `zenmlserver_image_tag` to the version that you want your ZenML server to be running.\n* Run the deploy command again with this config file:\n\n  ```bash\n  zenml deploy --config=/PATH/TO/FILE\n  ```\n\nAny database schema updates are automatically handled by ZenML and unless mentioned otherwise, all of your data is\nmigrated to the new version, intact.\n{% endtab %}\n\n{% tab title=\"Docker\" %}\nTo upgrade to a new version with docker, you have to delete the existing container and then run the new version of\nthe `zenml-server` image.\n\n{% hint style=\"danger\" %}\nCheck that your data is persisted (either on persistent storage or on an external MySQL instance) before doing this.\n\nOptionally also perform a backup before the upgrade.\n{% endhint %}\n\n* Delete the existing ZenML container, for example like this:\n\n  ```bash\n  # find your container ID\n  docker ps\n  ```\n\n  ```bash\n  # stop the container\n  docker stop <CONTAINER_ID>\n\n  # remove the container\n  docker rm <CONTAINER_ID>\n  ```\n* Deploy the version of the `zenml-server` image that you want to use. Find all\n  versions [here](https://hub.docker.com/r/zenmldocker/zenml-server/tags).\n\n  ```bash\n  docker run -it -d -p 8080:8080 --name <CONTAINER_NAME> zenmldocker/zenml-server:<VERSION>\n  ```\n\n{% endtab %}\n\n{% tab title=\"Helm\" %}\nTo upgrade your ZenML server Helm release to a new version, follow the steps below:\n\n* Pull the latest version of the Helm chart from the ZenML GitHub repository, or a version of your choice, e.g.:\n\n```bash\n# If you haven't cloned the ZenML repository yet\ngit clone https://github.com/zenml-io/zenml.git\n# Optional: checkout an explicit release tag\n# git checkout 0.21.1\ngit pull\n# Switch to the directory that hosts the helm chart\ncd src/zenml/zen_server/deploy/helm/\n```\n\n* Simply reuse the `custom-values.yaml` file that you used during the previous installation or upgrade. If you don't\n  have it handy, you can extract the values from the ZenML Helm deployment using the following command:\n\n"}
{"input": "  ```bash\n  helm -n <namespace> get values zenml-server > custom-values.yaml\n  ```\n* Upgrade the release using your modified values file. Make sure you are in the directory that hosts the helm chart:\n\n  ```bash\n  helm -n <namespace> upgrade zenml-server . -f custom-values.yaml\n  ```\n\n{% hint style=\"info\" %}\nIt is not recommended to change the container image tag in the Helm chart to custom values, since every Helm chart\nversion is tested to work only with the default image tag. However, if you know what you're doing you can change\nthe `zenml.image.tag` value in your `custom-values.yaml` file to the desired ZenML version (e.g. `0.32.0`).\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"warning\" %}\nDowngrading the server to an older version is not supported and can lead to unexpected behavior.\n{% endhint %}\n\n{% hint style=\"info\" %}\nThe version of the Python client that connects to the server should be kept at the same version as the server.\n{% endhint %}\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Once deployed, here's how to manage ZenML and its Stack Components\n---\n\n# Manage deployed services\n\nMaintaining and troubleshooting your ZenML deployment and the stack components deployed through ZenML is quite easy. Here are a few guides that will explain how:\n\n<table data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden></th><th data-hidden data-card-target data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><mark style=\"color:purple;\"><strong>Upgrade the version of the ZenML server</strong></mark></td><td>Learn how to upgrade your server to a new version of ZenML for the different deployment options.</td><td></td><td><a href=\"upgrade-the-version-of-the-zenml-server.md\">upgrade-the-version-of-the-zenml-server.md</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>Troubleshoot the deployed server</strong></mark></td><td>Troubleshooting tips for your ZenML deployment</td><td></td><td><a href=\"troubleshoot-your-deployed-server.md\">troubleshoot-your-deployed-server.md</a></td></tr><tr><td><mark style=\"color:purple;\"><strong>Troubleshoot stack components</strong></mark></td><td>Learn how to troubleshoot Stack Components deployed with ZenML.</td><td></td><td><a href=\"troubleshoot-stack-components.md\">troubleshoot-stack-components.md</a></td></tr></tbody></table>\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom secret store.\n---\n\n# Custom secret stores\n\nThe secrets store acts as the one-stop shop for all the secrets to which your pipeline or stack components might need access. It is responsible for storing, updating and deleting _only the secrets values_ for ZenML secrets, while the ZenML secret metadata is stored in the SQL database. The secrets store interface implemented by all available secrets store back-ends is defined in the `zenml.zen_stores.secrets_stores.secrets_store_interface` core module and looks more or less like this:\n\n```python\nclass SecretsStoreInterface(ABC):\n    \"\"\"ZenML secrets store interface.\n\n    All ZenML secrets stores must implement the methods in this interface.\n    \"\"\"\n\n    # ---------------------------------\n    # Initialization and configuration\n    # ---------------------------------\n\n    @abstractmethod\n    def _initialize(self) -> None:\n        \"\"\"Initialize the secrets store.\n\n        This method is called immediately after the secrets store is created.\n        It should be used to set up the backend (database, connection etc.).\n        \"\"\"\n\n    # ---------\n    # Secrets\n    # ---------\n\n    @abstractmethod\n    def store_secret_values(\n        self,\n        secret_id: UUID,\n        secret_values: Dict[str, str],\n    ) -> None:\n        \"\"\"Store secret values for a new secret.\n\n        Args:\n            secret_id: ID of the secret.\n            secret_values: Values for the secret.\n        \"\"\"\n\n    @abstractmethod\n    def get_secret_values(self, secret_id: UUID) -> Dict[str, str]:\n        \"\"\"Get the secret values for an existing secret.\n\n        Args:\n            secret_id: ID of the secret.\n\n        Returns:\n            The secret values.\n\n        Raises:\n            KeyError: if no secret values for the given ID are stored in the\n                secrets store.\n        \"\"\"\n\n    @abstractmethod\n    def update_secret_values(\n        self,\n        secret_id: UUID,\n        secret_values: Dict[str, str],\n    ) -> None:\n        \"\"\"Updates secret values for an existing secret.\n\n        Args:\n            secret_id: The ID of the secret to be updated.\n            secret_values: The new secret values.\n\n        Raises:\n            KeyError: if no secret values for the given ID are stored in the\n                secrets store.\n        \"\"\"\n\n    @abstractmethod\n    def delete_secret_values(self, secret_id: UUID) -> None:\n        \"\"\"Deletes secret values for an existing secret.\n\n        Args:\n            secret_id: The ID of the secret.\n\n        Raises:\n            KeyError: if no secret values for the given ID are stored in the\n                secrets store.\n        \"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the real interface which aims to highlight the abstraction layer. In order to see the full definition and get the complete docstrings, please check the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-zen"}
{"input": "\\_stores/#zenml.zen\\_stores.secrets\\_stores.secrets\\_store\\_interface.SecretsStoreInterface) .\n{% endhint %}\n\n#### Build your own custom secrets store\n\nIf you want to create your own custom secrets store implementation, you can follow the following steps:\n\n1. Create a class that inherits from the `zenml.zen_stores.secrets_stores.base_secrets_store.BaseSecretsStore` base class and implements the `abstractmethod`s shown in the interface above. Use `SecretsStoreType.CUSTOM` as the `TYPE` value for your secrets store class.\n2. If you need to provide any configuration, create a class that inherits from the `SecretsStoreConfiguration` class and add your configuration parameters there. Use that as the `CONFIG_TYPE` value for your secrets store class.\n3. To configure the ZenML server to use your custom secrets store, make sure your code is available in the container image that is used to run the ZenML server. Then, use environment variables or helm chart values to configure the ZenML server to use your custom secrets store, as covered in the [deployment guide](../README.md).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learn how to troubleshoot Stack Components deployed with ZenML.\n---\n\n# Troubleshoot stack components\n\nThere are two ways in which you can understand if something has gone wrong while deploying your stack or stack components.\n\n## Error logs from the CLI\n\nThe CLI will show any errors that the deployment runs into. Most of these would be coming from the underlying terraform library and could range from issues like resources with the same name existing in your cloud to a wrong naming scheme for some resource.\n\nMost of these are easy to fix and self-explanatory but feel free to ask any questions or doubts you may have to us on the ZenML Slack! \ud83d\ude4b\u200d\n\n## Debugging errors with already deployed components\n\nSometimes, an application might fail after an initial successful deployment. This section will cover steps on how to debug failures in such a case, for Kubernetes apps, since they form a majority of all tools deployed with the CLI.\n\n{% hint style=\"info\" %}\nOther components include cloud-specific apps like Vertex AI, Sagemaker, S3 buckets, and more. Information on what has gone wrong with them would be best found on the web console for the respective clouds.\n{% endhint %}\n\n### Getting access to the Kubernetes Cluster\n\nThe first step to figuring out the problem with a deployed Kubernetes app is to get access to the underlying cluster hosting it. When you deploy apps that require a cluster, ZenML creates a cluster for you and this is reused for all subsequent apps that need it.\n\n{% hint style=\"info\" %}\nIf you've used the `zenml stack deploy` flow to deploy your components, your local `kubectl` might already have access to the cluster. Check by running the following command:\n\n```\nkubectl get nodes\n```\n{% endhint %}\n\n#### Stack Component Deploy\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n1. Get the name of the deployed cluster.\\\n   `zenml stack recipe output eks-cluster-name`\n2. Figure out the region that the cluster is deployed to. By default, the region is set to `eu-west-1` , which you should use in the next step if you haven't supplied a custom value while creating the cluster.\n3. Run the following command.\\\n   `aws eks update-kubeconfig --name <NAME> --region <REGION>`\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n1. Get the name of the deployed cluster.\\\n   \\\n   `zenml stack recipe output gke-cluster-name`\\\\\n2. Figure out the region that the cluster is deployed to. By default, the region is set to `europe-west1`, which you should use in the next step if you haven't supplied a custom value while creating the cluster.\\\\\n3. Figure out the project that the cluster is deployed to. You must have passed in a project ID while creating a GCP resource for the first time.\\\\\n4."}
{"input": " Run the following command.\\\n   `gcloud container clusters get-credentials <NAME> --region <REGION> --project <PROJECT_ID>`\n{% endtab %}\n\n{% tab title=\"K3D\" %}\n{% hint style=\"info\" %}\nYou may already have your `kubectl` client configured with your cluster. Check by running `kubectl get nodes` before proceeding.\n{% endhint %}\n\n1. Get the name of the deployed cluster.\\\n   \\\n   `zenml stack recipe output k3d-cluster-name`\\\\\n2. Set the `KUBECONFIG` env variable to the `kubeconfig` file from the cluster.\\\n   \\\n   `export KUBECONFIG=$(k3d kubeconfig get <NAME>)`\\\\\n3. You can now use the `kubectl` client to talk to the cluster.\n{% endtab %}\n{% endtabs %}\n\n#### Stack Recipe Deploy\n\nThe steps for the stack recipe case should be the same as the ones listed above. The only difference that you need to take into account is the name of the outputs that contain your cluster name and the default regions.\n\nEach recipe might have its own values and here's how you can ascertain those values.\n\n* For the cluster name, go into the `outputs.tf` file in the root directory and search for the output that exposes the cluster name.\n* For the region, check out the `variables.tf` or the `locals.tf` file for the default value assigned to it.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Troubleshooting tips for your ZenML deployment\n---\n\n# Troubleshoot the deployed server\n\nIn this document, we will go over some common issues that you might face when deploying ZenML and how to solve them.\n\n## Viewing logs\n\nAnalyzing logs is a great way to debug issues. Depending on whether you have a Kubernetes (using Helm or `zenml deploy`)\nor a Docker deployment, you can view the logs in different ways.\n\n{% tabs %}\n{% tab title=\"Kubernetes\" %}\nIf you are using Kubernetes, you can view the logs of the ZenML server using the following method:\n\n* Check all pods that are running your ZenML deployment.\n\n```bash\nkubectl -n <KUBERNETES_NAMESPACE> get pods\n```\n\n* If you see that the pods aren't running, you can use the command below to get the logs for all pods at once.\n\n```bash\nkubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml\n```\n\nNote that the error can either be from the `zenml-db-init` container that connects to the MySQL database or from\nthe `zenml` container that runs the server code. If the get pods command shows that the pod is failing in the `Init`\nstate then use `zenml-db-init` as the container name, otherwise use `zenml`.\n\n```bash\nkubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml -c <CONTAINER_NAME>\n```\n\n{% hint style=\"info\" %}\nYou can also use the `--tail` flag to limit the number of lines to show or the `--follow` flag to follow the logs in\nreal-time.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"Docker\" %}\nIf you are using Docker, you can view the logs of the ZenML server using the following method:\n\n* If you used the `zenml up --docker` CLI command to deploy the Docker ZenML server, you can check the logs with the\n  command:\n\n  ```shell\n  zenml logs -f\n  ```\n* If you used the `docker run` command to manually deploy the Docker ZenML server, you can check the logs with the\n  command:\n\n  ```shell\n  docker logs zenml -f\n  ```\n* If you used the `docker compose` command to manually deploy the Docker ZenML server, you can check the logs with the\n  command:\n\n  ```shell\n  docker compose -p zenml logs -f\n  ```\n\n{% endtab %}\n{% endtabs %}\n\n## Fixing database connection problems\n\nIf you are using a MySQL database, you might face issues connecting to it. The logs from the `zenml-db-init` container\nshould give you a good idea of what the problem is. Here are some common issues and how to fix them"}
{"input": ":\n\n* If you see an error like `ERROR 1045 (28000): Access denied for user <USER> using password YES`, it means that the\n  username or password is incorrect. Make sure that the username and password are correctly set for whatever deployment\n  method you are using.\n* If you see an error like `ERROR 2003 (HY000): Can't connect to MySQL server on <HOST> (<IP>)`, it means that the host\n  is incorrect. Make sure that the host is correctly set for whatever deployment method you are using.\n\nYou can test the connection and the credentials by running the following command from your machine:\n\n```bash\nmysql -h <HOST> -u <USER> -p\n```\n\n{% hint style=\"info\" %}\nIf you are using a Kubernetes deployment, you can use the `kubectl port-forward` command to forward the MySQL port to\nyour local machine. This will allow you to connect to the database from your machine.\n{% endhint %}\n\n## Fixing database initialization problems\n\nIf you\u2019ve migrated from a newer ZenML version to an older version and see errors like `Revision not found` in\nyour `zenml-db-init` logs, one way out is to drop the database and create a new one with the same name.\n\n* Log in to your MySQL instance.\n\n  ```bash\n  mysql -h <HOST> -u <NAME> -p\n  ```\n* Drop the database for the server.\n\n  ```sql\n  drop database <NAME>;\n  ```\n* Create the database with the same name.\n\n  ```sql\n  create database <NAME>;\n  ```\n* Restart the Kubernetes pods or the docker container running your server to trigger the database initialization again.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Your one-stop MLOps control plane.\n---\n\n# ZenML SaaS\n\nOne of the most straightforward paths to start with a deployed ZenML server is\nto use [ZenML Pro](https://zenml.io/pro). The ZenML Pro offering eliminates the need\nfor you to dedicate time and resources to deploy and manage a ZenML server,\nallowing you to focus primarily on your MLOps workflows.\n\n<div data-full-width=\"false\">\n\n<figure>.gitbook/assets/zenml-cloud-tenant-overview.png\" alt=\"\"><figcaption><p>ZenML Pro comes equipped with powerful pro-only dashboard features</p></figcaption></figure>\n\n</div>\n\n{% hint style=\"info\" %}\nIf you're interested in assessing ZenML Pro, you can simply create\na [free account](https://cloud.zenml.io/?utm\\_source=docs\\&utm\\_medium=referral\\_link\\&utm\\_campaign=cloud\\_promotion\\&utm\\_content=signup\\_link).\nLearn more about ZenML Pro on the [ZenML Website](https://zenml.io/pro).\n{% endhint %}\n\n## Key features\n\nZenML Pro comes as a Software-as-a-Service (SaaS) platform that enhances the\nfunctionalities of the open-source ZenML product. It equips you with a\ncentralized interface to seamlessly launch and manage ZenML server instances.\nWhile it remains rooted in the robust open-source offering, ZenML Pro offers\nextra features designed to optimize your machine learning workflow.\n\n### Managed ZenML Server (Multi-tenancy)\n\nZenML Pro simplifies your machine learning workflows, enabling you to deploy a\nmanaged instance of ZenML servers with just one click. This eradicates the need\nto handle infrastructure complexities, making the set-up and management of your\nmachine learning pipelines a breeze. We handle all pertinent system updates and\nbackups, thus ensuring your system stays current and robust, allowing you to\nzero in on your essential MLOps tasks. As a ZenML Pro user, you'll also have\npriority support, giving you the necessary aid to fully utilize the platform.\n\n### Maximum data security\n\nAt ZenML Pro, your data security and privacy are our top priority. The\nplatform enables a secure connection to your infrastructure, tracking only\nmetadata via an encrypted connection to maintain the confidentiality of your\nsensitive information. ZenML Pro integrates smoothly with your cloud services\nvia service connectors, allowing a straightforward connection with various cloud\nresources without sacrificing data security. We hold your confidential\ninformation in a secure and isolated environment, offering an extra degree of\nprotection. If desired, you can\neven [supply your own secret store](../deploying-zenml/manage-the-deployed-services/custom-secret-stores.md).\n\nClick [here](./system-architectures.md) to understand about the ZenML Pro system\narchitecture.\n\n<figure><img src=\"https://static.s"}
{"input": "carf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "# User Management\n\nIn ZenML Pro, there is a slightly different entity hierarchy as compared to the open-source ZenML\nframework. This document walks you through the key differences and new concepts that are pro-only.\n\n## Organizations, Tenants, and Roles\n\nZenML Pro arranges various aspects of your work experience around the concept\nof an **Organization**. This is the top-most level structure within the ZenML\nCloud environment. Generally, an organization contains a group of users and one\nor more **tenants**. Tenants are individual, isolated deployments of the ZenML server.\n\nEvery user in an organization has a distinct role. Each role configures what\nthey can view, modify, and their level of involvement in collaborative tasks. A\nrole thus helps determine the level of access that a user has within an\norganization.\n\nThe `admin` has all permissions on an organization. They are allowed to add\nmembers, adjust the billing information and assign roles. The `editor` can still\nfully manage tenants and members but is not allowed to access the subscription\ninformation or delete the organization. The `viewer` Role allows you to allow\nusers to access the tenants within the organization with only view permissions.\n\n## Inviting Team Members\n\nInviting users to your organization to work on the organization's tenants is\neasy. Simply click `Add Member` in the Organization settings, and give them an\ninitial Role. The User will be sent an invitation email. If a user is part of an\norganization, they can utilize their login on all tenants they have authority to\naccess.\n\n![Image showing invite flow](../../.gitbook/assets/cloud-user-invite-flow.png)\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n\n\n"}
{"input": "# \u2601\ufe0f ZenML Pro\n\nThe Pro version of ZenML comes with a number of features that expand the\nfunctionality of the Open Source product\n\n### Role-based access control and permissions\n\nUtilizing ZenML Pro provides you with access to a robust control plane that\nunifies user management and optimizes your workflows. Efficiently manage access\nand permissions through centralized user administration. Create fine-grained\npermissions for resources such as stacks, pipelines, models, etc.\n\nSee the section on [user management](./user-management.md) to learn more.\n\n### A brand-new, modern MLOps experience\n\n![Walkthrough of ZenML Model Control Plane](../../.gitbook/assets/mcp_walkthrough.gif)\n\nWe have built the ZenML Pro experience from the ground-up. With ZenML Pro, you get\naccess to a new dashboard, with a better experience. The new dashboard features\nmore functionality such as\nthe [Model Control Plane](../../user-guide/starter-guide/track-ml-models.md)\nand [Artifact Control Plane](../../user-guide/starter-guide/manage-artifacts.md).\n\n### Run templates for running your pipelines from the dashboard or the API\n\nZenML Pro enables you to [create and run templates](../../how-to/create-and-run-templates/README.md).\nThis way, you can use the dashboard or our Client/REST API to run a pipeline with updated configuration\nwhich allows you to iterate quickly with minimal friction. \n\n### Triggers, CI/CD, Reports and more\n\nAdditionally, ZenML Pro users get exclusive access to an array of\ncloud-specific features, such as triggers, integrating with your code\nrepository CI/CD system, generating usage reports and more.\n\nLearn more about ZenML Pro on the [ZenML Website](https://zenml.io/pro).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Different variations of the ZenML architecture depending on your needs.\n---\n\n# System Architectures\n\n{% hint style=\"info\" %}\nIf you're interested in assessing ZenML Pro, you can create\na [free account](https://cloud.zenml.io/?utm\\_source=docs\\&utm\\_medium=referral\\_link\\&utm\\_campaign=cloud\\_promotion\\&utm\\_content=signup\\_link),\nwhich defaults to a [Scenario 1](./#scenario-1-full-saas) deployment. To upgrade\nto different scenarios, please [reach out to us](mailto:cloud@zenml.io).\n{% endhint %}\n\nThe ZenML Pro offers many additional features to increase your teams\nproductivity. No matter your specific needs, the hosting options for ZenML Pro\nrange from easy SaaS integration to completely airgapped deployments on your own\ninfrastructure.\n\nA ZenML Pro deployment consists of the following moving pieces for both the SaaS\nproduct as well as the self-hosted version.:\n\n* **ZenML Pro Control Plane**: This is a centralized MLOps control plane that includes a\n  managed ZenML dashboard and a special ZenML server optimized for production\n  MLOps workloads.\n* **Single Sign-On (SSO)**: The ZenML Pro API is integrated\n  with [Auth0](https://auth0.com/) as an SSO provider to manage user\n  authentication and authorization. Users can log in to the ZenML Pro\n  app using their social media accounts or their corporate credentials.\n* **Secrets Store**: All secrets and credentials required to access customer\n  infrastructure services are stored in a secure secrets store. The ZenML Pro\n  API has access to these secrets and uses them to access customer\n  infrastructure services on behalf of the ZenML Pro. The secrets store can be\n  hosted either by the ZenML Pro or by the customer.\n* **ML Metadata Store**: This is where all ZenML metadata is stored, including\n  ML metadata such as tracking and versioning information about pipelines and\n  models.\n\nThe above four interact with other MLOps stack components, secrets, and data in\nvarying scenarios described below.\n\n## Scenario 1: Full SaaS\n\n![Scenario 1: Full SaaS deployment](../../.gitbook/assets/cloud_architecture_scenario_1.png)\n\n\nIn this scenario, all services are hosted on infrastructure hosted by the ZenML Team,\nexcept the MLOps stack components.\nCustomer secrets and credentials required to access customer infrastructure are\nstored and managed by the ZenML Pro Control Plane.\n\nOn the ZenML Pro infrastructure, only ML _metadata_ (e.g. pipeline and\nmodel tracking and versioning information) is stored. All the actual ML data\nartifacts (e.g. data produced or consumed by pipeline steps, logs and\nvisualizations, models) are stored on the customer cloud."}
{"input": " This can be set up\nquite easily by configuring\nan [artifact store](../../component-guide/artifact-stores/artifact-stores.md)\nwith your MLOps stack.\n\nYour tenant only needs permissions to read from this data to display artifacts\non the ZenML dashboard. The tenant also needs direct access to parts of the\ncustomer infrastructure services to support dashboard control plane features\nsuch as CI/CD, triggering and running pipelines, triggering model deployments\netc.\n\nThis scenario is meant for customers who want to quickly get started with ZenML\nand can to a certain extent allow ingress connections into their infrastructure\nfrom an external SaaS provider.\n\n## Scenario 2: Hybrid SaaS with Customer Secret Store managed by ZenML\n\n![Scenario 2: Hybrid SaaS with Customer Secret Store managed by ZenML](../../.gitbook/assets/cloud_architecture_scenario_2.png)\n\nThis scenario is a version of Scenario 1. modified to store all sensitive\ninformation on the customer side. In this case, the customer connects their own\nsecret store directly to the ZenML server that is managed by us. All ZenML\nsecrets used by running pipelines to access infrastructure services and\nresources are stored in the customer secret store. This allows users to\nuse [service connectors](../../how-to/auth-management/service-connectors-guide.md)\nand the [secrets API](../../how-to/interact-with-secrets.md) to authenticate\nZenML pipelines and the ZenML Pro to 3rd party services and infrastructure\nwhile ensuring that credentials are always stored on the customer side.\n\nEven though they are stored customer side, access to ZenML secrets is fully\nmanaged by ZenML Pro. The individually deployed ZenML Servers can also allowed to use some of those\ncredentials to connect directly to customer infrastructure services to implement\ncontrol plane features such as artifact visualization or triggering pipelines.\nThis implies that the secret values are allowed to leave the customer\nenvironment to allow their access to be managed centrally by the ZenML Pro and\nto enforce access control policies, but the ZenML users and pipelines never have\ndirect access to the secret store.\n\nAll access to customer secrets is, of course, regulated through authentication\nand RBAC, so that only authorized users can access the secrets. This deployment\nscenario is meant for customers who want to use the ZenML Pro but want to keep\ntheir secrets on their own infrastructure.\n\n## Scenario 3: Fully On-prem\n\n![Scenario 3: Fully on-premises deployment](../../.gitbook/assets/cloud_architecture_scenario_5.png)\n\nIn this scenario, all services, data, and secrets are deployed on the customer\ncloud. This is the opposite of Scenario 1, and is meant for customers who\nrequire completely airgapped deployments, for the tightest security standards. \n[Reach out to us](mailto:cloud@zenml.io) if you want to set this up.\n\nAre you interested in ZenML Pro"}
{"input": "? [Sign up](https://cloud.zenml.io/?utm\\_source=docs\\&utm\\_medium=referral\\_link\\&utm\\_campaign=cloud\\_promotion\\&utm\\_content=signup\\_link)\nand get access to Scenario 1. with a free 14 day trial now!\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Overview of categories of MLOps components.\n---\n\n# \ud83d\udcdc Overview\n\nIf you are new to the world of MLOps, it is often daunting to be immediately faced with a sea of tools that seemingly all promise and do the same things. It is useful in this case to try to categorize tools in various groups in order to understand their value in your toolchain in a more precise manner.\n\nZenML tackles this problem by introducing the concept of [Stacks and Stack Components](../user-guide/production-guide/understand-stacks.md). These stack components represent categories, each of which has a particular function in your MLOps pipeline. ZenML realizes these stack components as base abstractions that standardize the entire workflow for your team. In order to then realize the benefit, one can write a concrete implementation of the [abstraction](../how-to/stack-deployment/implement-a-custom-stack-component.md), or use one of the many built-in [integrations](README.md) that implement these abstractions for you.\n\nHere is a full list of all stack components currently supported in ZenML, with a description of the role of that component in the MLOps process:\n\n| **Type of Stack Component**                                                                                                                              | **Description**                                                   |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |\n| [Orchestrator](./orchestrators/orchestrators.md)              | Orchestrating the runs of your pipeline                           |\n| [Artifact Store](./artifact-stores/artifact-stores.md)          | Storage for the artifacts created by your pipelines               |\n| [Container Registry](./container-registries/container-registries.md) | Store for your containers                                         |\n| [Step Operator](./step-operators/step-operators.md)            | Execution of individual steps in specialized runtime environments |\n| [Model Deployer](./model-deployers/model-deployers.md)          | Services/platforms responsible for online model serving           |\n| [Feature Store](./feature-stores/feature-stores.md)            | Management of your data/features                                  |\n| [Experiment Tracker](./experiment-trackers/experiment-trackers.md)  | Tracking your ML experiments                                      |\n| [Alerter](./alerters/alerters.md)                        | Sending alerts through specified channels                         |\n| [Annotator](./annotators/annotators.md)                    | Labeling and annotating data                                      |\n| [Data Validator](./data-validators/data-validators.md)          | Data and model validation                                         |\n| [Image Builder](./image-builders/image-builders.md)            | Builds container images.                                          |\n| [Model Registry](./model-registries/model-registries.md)         | Manage and interact with ML Models                                |\n\nEach pipeline run that you execute with ZenML will require a **stack** and each **stack** will be required to include at least an orchestrator and an artifact store. Apart"}
{"input": " from these two, the other components are optional and to be added as your pipeline evolves in MLOps maturity.\n\n## Writing custom component flavors\n\nYou can take control of how ZenML behaves by creating your own components. This is done by writing custom component `flavors`. To learn more, head over to [the general guide on writing component flavors](../how-to/stack-deployment/implement-a-custom-stack-component.md), or read more specialized guides for specific component types (e.g. the [custom orchestrator guide](orchestrators/custom.md)).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Overview of categories of MLOps components and third-party integrations.\n---\n\n# \ud83d\udcdc Overview\n\nIf you are new to the world of MLOps, it is often daunting to be immediately faced with a sea of tools that seemingly all promise and do the same things. It is useful in this case to try to categorize tools in various groups in order to understand their value in your toolchain in a more precise manner.\n\nZenML tackles this problem by introducing the concept of [Stacks and Stack Components](../user-guide/production-guide/understand-stacks.md). These stack components represent categories, each of which has a particular function in your MLOps pipeline. ZenML realizes these stack components as base abstractions that standardize the entire workflow for your team. In order to then realize the benefit, one can write a concrete implementation of the [abstraction](../how-to/stack-deployment/implement-a-custom-stack-component.md), or use one of the many built-in [integrations](README.md) that implement these abstractions for you.\n\nHere is a full list of all stack components currently supported in ZenML, with a description of the role of that component in the MLOps process:\n\n| **Type of Stack Component**                                                         | **Description**                                                   |\n| ----------------------------------------------------------------------------------- | ----------------------------------------------------------------- |\n| [Orchestrator](orchestrators/orchestrators.md)              | Orchestrating the runs of your pipeline                           |\n| [Artifact Store](artifact-stores/artifact-stores.md)          | Storage for the artifacts created by your pipelines               |\n| [Container Registry](container-registries/container-registries.md) | Store for your containers                                         |\n| [Data Validator](data-validators/data-validators.md)          | Data and model validation                                         |\n| [Experiment Tracker](experiment-trackers/experiment-trackers.md)  | Tracking your ML experiments                                      |\n| [Model Deployer](model-deployers/model-deployers.md)          | Services/platforms responsible for online model serving           |\n| [Step Operator](step-operators/step-operators.md)            | Execution of individual steps in specialized runtime environments |\n| [Alerter](alerters/alerters.md)                        | Sending alerts through specified channels                         |\n| [Image Builder](image-builders/image-builders.md)            | Builds container images.                                          |\n| [Annotator](annotators/annotators.md)                    | Labeling and annotating data                                      |\n| [Model Registry](model-registries/model-registries.md)         | Manage and interact with ML Models                                |\n| [Feature Store](feature-stores/feature-stores.md)            | Management of your data/features                                  |\n\nEach pipeline run that you execute with ZenML will require a **stack** and each **stack** will be required to include at least an orchestrator and an artifact store. Apart from these two, the other components are optional"}
{"input": " and to be added as your pipeline evolves in MLOps maturity.\n\n## Writing custom component flavors\n\nYou can take control of how ZenML behaves by creating your own components. This is done by writing custom component `flavors`. To learn more, head over to [the general guide on writing component flavors](../how-to/stack-deployment/implement-a-custom-stack-component.md), or read more specialized guides for specific component types (e.g. the [custom orchestrator guide](orchestrators/custom.md)).\n\n## Integrations\n\nCategorizing the MLOps stack is a good way to write abstractions for an MLOps pipeline and standardize your processes. But ZenML goes further and also provides concrete implementations of these categories by **integrating** with various tools for each category. Once code is organized into a ZenML pipeline, you can supercharge your ML workflows with the best-in-class solutions from various MLOps areas.\n\nFor example, you can orchestrate your ML pipeline workflows using [Airflow](orchestrators/airflow.md) or [Kubeflow](orchestrators/kubeflow.md), track experiments using [MLflow Tracking](experiment-trackers/mlflow.md) or [Weights & Biases](experiment-trackers/wandb.md), and transition seamlessly from a local [MLflow deployment](model-deployers/mlflow.md) to a deployed model on Kubernetes using [Seldon Core](model-deployers/seldon.md).\n\nThere are lots of moving parts for all the MLOps tooling and infrastructure you require for ML in production and ZenML brings them all together and enables you to manage them in one place. This also allows you to delay the decision of which MLOps tool to use in your stack as you have no vendor lock-in with ZenML and can easily switch out tools as soon as your requirements change.\n\n![ZenML is the glue](../../book/.gitbook/assets/zenml-is-the-glue.jpeg)\n\n### Available integrations\n\nWe have a [dedicated webpage](https://zenml.io/integrations) that indexes all supported ZenML integrations and their categories.\n\nAnother easy way of seeing a list of integrations is to see the list of directories in the [integrations directory](https://github.com/zenml-io/zenml/tree/main/src/zenml/integrations) on our GitHub.\n\n### Installing ZenML integrations\n\nBefore you can use integrations, you first need to install them using `zenml integration install`, e.g., you can install [Kubeflow](orchestrators/kubeflow.md), [MLflow Tracking](experiment-trackers/mlflow.md), and [Seldon Core](model-deployers/seldon.md), using:\n\n```\nzenml integration install kubeflow mlflow seldon -y\n```\n\nUnder the hood, this simply installs the preferred versions of all integrations using pip, i.e"}
{"input": "., it executes in a sub-process call:\n\n```\npip install kubeflow==<PREFERRED_VERSION> mlflow==<PREFERRED_VERSION> seldon==<PREFERRED_VERSION>\n```\n\n{% hint style=\"info\" %}\n* The `-y` flag confirms all `pip install` commands without asking you for\n\nYou can run `zenml integration --help` to see a full list of CLI commands that ZenML provides for interacting with integrations.\n{% endhint %}\n\nNote, that you can also install your dependencies directly, but please note that there is no guarantee that ZenML internals with work with any arbitrary version of any external library.\n\n#### Experimental: Use `uv` for package installation\n\nYou can use [`uv`](https://github.com/astral-sh/uv) as a package manager if you want. Simply pass the `--uv` flag to the `zenml integration ...` command and it'll use `uv` for installation, upgrades and uninstallations. Note that `uv` must be installed for this to work. This is an experimental option that we've added for users wishing to use `uv` but given that it is relatively new as an option there might be certain packages that don't work well with `uv`. We will monitor how this performs and update as `uv` becomes more stable.\n\n### Upgrade ZenML integrations\n\nYou can upgrade all integrations to their latest possible version using:\n\n```bash\nzenml integration upgrade mlflow pytorch -y\n```\n\n{% hint style=\"info\" %}\n* The `-y` flag confirms all `pip install --upgrade` commands without asking you for confirmation.\n* If no integrations are specified, all installed integrations will be upgraded.\n{% endhint %}\n\n### Help us with integrations!\n\nThere are countless tools in the ML / MLOps field. We have made an initial prioritization of which tools to support with integrations that are visible on our public [roadmap](https://zenml.io/roadmap).\n\nWe also welcome community contributions. Check our [Contribution Guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md) and [External Integration Guide](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/README.md) for more details on how to best contribute to new integrations.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Overview of third-party ZenML integrations.\n---\n\n# Integration overview\n\nCategorizing the MLOps stack is a good way to write abstractions for an MLOps pipeline and standardize your processes. But ZenML goes further and also provides concrete implementations of these categories by **integrating** with various tools for each category. Once code is organized into a ZenML pipeline, you can supercharge your ML workflows with the best-in-class solutions from various MLOps areas.\n\nFor example, you can orchestrate your ML pipeline workflows using [Airflow](orchestrators/airflow.md) or [Kubeflow](orchestrators/kubeflow.md), track experiments using [MLflow Tracking](experiment-trackers/mlflow.md) or [Weights & Biases](experiment-trackers/wandb.md), and transition seamlessly from a local [MLflow deployment](model-deployers/mlflow.md) to a deployed model on Kubernetes using [Seldon Core](model-deployers/seldon.md).\n\nThere are lots of moving parts for all the MLOps tooling and infrastructure you require for ML in production and ZenML brings them all together and enables you to manage them in one place. This also allows you to delay the decision of which MLOps tool to use in your stack as you have no vendor lock-in with ZenML and can easily switch out tools as soon as your requirements change.\n\n![ZenML is the glue](../.gitbook/assets/zenml-is-the-glue.jpeg)\n\n## Available integrations\n\nWe have a [dedicated webpage](https://zenml.io/integrations) that indexes all supported ZenML integrations and their categories.\n\nAnother easy way of seeing a list of integrations is to see the list of directories in the [integrations directory](https://github.com/zenml-io/zenml/tree/main/src/zenml/integrations) on our GitHub.\n\n## Installing ZenML integrations\n\nBefore you can use integrations, you first need to install them using `zenml integration install`, e.g., you can install [Kubeflow](orchestrators/kubeflow.md), [MLflow Tracking](experiment-trackers/mlflow.md), and [Seldon Core](model-deployers/seldon.md), using:\n\n```\nzenml integration install kubeflow mlflow seldon -y\n```\n\nUnder the hood, this simply installs the preferred versions of all integrations using pip, i.e., it executes in a sub-process call:\n\n```\npip install kubeflow==<PREFERRED_VERSION> mlflow==<PREFERRED_VERSION> seldon==<PREFERRED_VERSION>\n```\n\n{% hint style=\"info\" %}\n* The `-y` flag confirms all `pip install` commands without asking you for\n\nYou can run `zenml integration --help` to see a full list of CLI commands that ZenML provides for interacting with integr"}
{"input": "ations.\n{% endhint %}\n\nNote, that you can also install your dependencies directly, but please note that there is no guarantee that ZenML internals with work with any arbitrary version of any external library.\n\n### Experimental: Use `uv` for package installation\n\nYou can use [`uv`](https://github.com/astral-sh/uv) as a package manager if you want. Simply pass the `--uv` flag to the `zenml integration ...` command and it'll use `uv` for installation, upgrades and uninstallations. Note that `uv` must be installed for this to work. This is an experimental option that we've added for users wishing to use `uv` but given that it is relatively new as an option there might be certain packages that don't work well with `uv`. We will monitor how this performs and update as `uv` becomes more stable.\n\n## Upgrade ZenML integrations\n\nYou can upgrade all integrations to their latest possible version using:\n\n```bash\nzenml integration upgrade mlflow pytorch -y\n```\n\n{% hint style=\"info\" %}\n* The `-y` flag confirms all `pip install --upgrade` commands without asking you for confirmation.\n* If no integrations are specified, all installed integrations will be upgraded.\n{% endhint %}\n\n## Help us with integrations!\n\nThere are countless tools in the ML / MLOps field. We have made an initial prioritization of which tools to support with integrations that are visible on our public [roadmap](https://zenml.io/roadmap).\n\nWe also welcome community contributions. Check our [Contribution Guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md) and [External Integration Guide](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/README.md) for more details on how to best contribute to new integrations.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  Deploying models to Huggingface Inference Endpoints with Hugging Face\n  :hugging_face:.\n---\n\n# Hugging Face\n\nHugging Face Inference Endpoints provides a secure production solution to easily deploy any `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models).\n\nThis service provides dedicated and autoscaling infrastructure managed by Hugging Face, allowing you to deploy models without dealing with containers and GPUs.\n\n## When to use it?\n\nYou should use Hugging Face Model Deployer:\n\n* if you want to deploy [Transformers, Sentence-Transformers, or Diffusion models](https://huggingface.co/docs/inference-endpoints/supported\\_tasks) on dedicated and secure infrastructure.\n* if you prefer a fully-managed production solution for inference without the need to handle containers and GPUs.\n* if your goal is to turn your models into production-ready APIs with minimal infrastructure or MLOps involvement\n* Cost-effectiveness is crucial, and you want to pay only for the raw compute resources you use.\n* Enterprise security is a priority, and you need to deploy models into secure offline endpoints accessible only via a direct connection to your Virtual Private Cloud (VPCs).\n\nIf you are looking for a more easy way to deploy your models locally, you can use the [MLflow Model Deployer](mlflow.md) flavor.\n\n## How to deploy it?\n\nThe Hugging Face Model Deployer flavor is provided by the Hugging Face ZenML integration, so you need to install it on your local machine to be able to deploy your models. You can do this by running the following command:\n\n```bash\nzenml integration install huggingface -y\n```\n\nTo register the Hugging Face model deployer with ZenML you need to run the following command:\n\n```bash\nzenml model-deployer register <MODEL_DEPLOYER_NAME> --flavor=huggingface --token=<YOUR_HF_TOKEN> --namespace=<YOUR_HF_NAMESPACE>\n```\n\nHere,\n\n* `token` parameter is the Hugging Face authentication token. It can be managed through [Hugging Face settings](https://huggingface.co/settings/tokens).\n* `namespace` parameter is used for listing and creating the inference endpoints. It can take any of the following values, username or organization name or `*` depending on where the inference endpoint should be created.\n\nWe can now use the model deployer in our stack.\n\n```bash\nzenml stack update <CUSTOM_STACK_NAME> --model-deployer=<MODEL_DEPLOYER_NAME>\n```\n\nSee the [huggingface\\_model\\_deployer\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-seldon/#zenml.integrations.h"}
{"input": "uggingface.steps.huggingface\\_deployer.huggingface\\_model\\_deployer\\_step) for an example of using the Hugging Face Model Deployer to deploy a model inside a ZenML pipeline step.\n\n## Configuration\n\nWithin the `HuggingFaceServiceConfig` you can configure:\n\n* `model_name`: the name of the model in ZenML.\n* `endpoint_name`: the name of the inference endpoint. We add a prefix `zenml-` and first 8 characters of the service uuid as a suffix to the endpoint name.\n* `repository`: The repository name in the user\u2019s namespace (`{username}/{model_id}`) or in the organization namespace (`{organization}/{model_id}`) from the Hugging Face hub.\n* `framework`: The machine learning framework used for the model (e.g. `\"custom\"`, `\"pytorch\"` )\n* `accelerator`: The hardware accelerator to be used for inference. (e.g. `\"cpu\"`, `\"gpu\"`)\n* `instance_size`: The size of the instance to be used for hosting the model (e.g. `\"large\"`, `\"xxlarge\"`)\n* `instance_type`: Inference Endpoints offers a selection of curated CPU and GPU instances. (e.g. `\"c6i\"`, `\"g5.12xlarge\"`)\n* `region`: The cloud region in which the Inference Endpoint will be created (e.g. `\"us-east-1\"`, `\"eu-west-1\"` for `vendor = aws` and `\"eastus\"` for Microsoft Azure vendor.).\n* `vendor`: The cloud provider or vendor where the Inference Endpoint will be hosted (e.g. `\"aws\"`).\n* `token`: The Hugging Face authentication token. It can be managed through [huggingface settings](https://huggingface.co/settings/tokens). The same token can be passed used while registering the Hugging Face model deployer.\n* `account_id`: (Optional) The account ID used to link a VPC to a private Inference Endpoint (if applicable).\n* `min_replica`: (Optional) The minimum number of replicas (instances) to keep running for the Inference Endpoint. Defaults to `0`.\n* `max_replica`: (Optional) The maximum number of replicas (instances) to scale to for the Inference Endpoint. Defaults to `1`.\n* `revision`: (Optional) The specific model revision to deploy on the Inference Endpoint for the Hugging Face repository .\n* `task`: Select a supported [Machine Learning Task](https://huggingface.co/docs/inference-endpoints/supported\\_tasks). (e.g. `\"text-classification\"`, `\"text-generation\"`)\n* `custom_image`: (Optional) A custom Docker image to use for the Inference Endpoint.\n* `namespace`: The namespace where the Inference Endpoint will be created. The same namespace can be passed used while registering the Hugging Face model"}
{"input": " deployer.\n* `endpoint_type`: (Optional) The type of the Inference Endpoint, which can be `\"protected\"`, `\"public\"` (default) or `\"private\"`.\n\nFor more information and a full list of configurable attributes of the Hugging Face Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-huggingface/#zenml.integrations.huggingface.model\\_deployers) and Hugging Face endpoint [code](https://github.com/huggingface/huggingface\\_hub/blob/5e3b603ccc7cd6523d998e75f82848215abf9415/src/huggingface\\_hub/hf\\_api.py#L6957).\n\n### Run inference on a provisioned inference endpoint\n\nThe following code example shows how to run inference against a provisioned inference endpoint:\n\n```python\nfrom typing import Annotated\nfrom zenml import step, pipeline\nfrom zenml.integrations.huggingface.model_deployers import HuggingFaceModelDeployer\nfrom zenml.integrations.huggingface.services import HuggingFaceDeploymentService\n\n\n# Load a prediction service deployed in another pipeline\n@step(enable_cache=False)\ndef prediction_service_loader(\n    pipeline_name: str,\n    pipeline_step_name: str,\n    running: bool = True,\n    model_name: str = \"default\",\n) -> HuggingFaceDeploymentService:\n    \"\"\"Get the prediction service started by the deployment pipeline.\n\n    Args:\n        pipeline_name: name of the pipeline that deployed the MLflow prediction\n            server\n        step_name: the name of the step that deployed the MLflow prediction\n            server\n        running: when this flag is set, the step only returns a running service\n        model_name: the name of the model that is deployed\n    \"\"\"\n    # get the Hugging Face model deployer stack component\n    model_deployer = HuggingFaceModelDeployer.get_active_model_deployer()\n\n    # fetch existing services with same pipeline name, step name and model name\n    existing_services = model_deployer.find_model_server(\n        pipeline_name=pipeline_name,\n        pipeline_step_name=pipeline_step_name,\n        model_name=model_name,\n        running=running,\n    )\n\n    if not existing_services:\n        raise RuntimeError(\n            f\"No Hugging Face inference endpoint deployed by step \"\n            f\"'{pipeline_step_name}' in pipeline '{pipeline_name}' with name \"\n            f\"'{model_name}' is currently running.\"\n        )\n\n    return existing_services[0]\n\n\n# Use the service for inference\n@step\ndef predictor(\n    service: HuggingFaceDeploymentService,\n    data: str\n) -> Annotated[str, \"predictions\"]:\n    \"\"\"Run a inference request against a prediction service\"\"\"\n\n    prediction = service.predict(data)\n    return prediction\n\n\n@pipeline\ndef huggingface_deployment_inference_pipeline(\n    pipeline_name:"}
{"input": " str, pipeline_step_name: str = \"huggingface_model_deployer_step\",\n):\n    inference_data = ...\n    model_deployment_service = prediction_service_loader(\n        pipeline_name=pipeline_name,\n        pipeline_step_name=pipeline_step_name,\n    )\n    predictions = predictor(model_deployment_service, inference_data)\n```\n\nFor more information and a full list of configurable attributes of the Hugging Face Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-huggingface/#zenml.integrations.huggingface.model\\_deployers).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying your models locally with BentoML.\n---\n\n# BentoML\n\nBentoML is an open-source framework for machine learning model serving. it can be used to deploy models locally, in a cloud environment, or in a Kubernetes environment.\n\nThe BentoML Model Deployer is one of the available flavors of the [Model Deployer](./model-deployers.md) stack component. Provided with the BentoML integration it can be used to deploy and manage [BentoML models](https://docs.bentoml.org/en/latest/concepts/model.html) or [Bento](https://docs.bentoml.org/en/latest/concepts/bento.html) on a local running HTTP server.\n\n{% hint style=\"warning\" %}\nThe BentoML Model Deployer can be used to deploy models for local development and production use cases. While the integration mainly works in a local environment where pipelines are run, the used [Bento](https://docs.bentoml.org/en/latest/concepts/bento.html) can be exported and containerized, and deployed in a remote environment. Within the BentoML ecosystem, [Yatai](https://github.com/bentoml/Yatai) and [`bentoctl`](https://github.com/bentoml/bentoctl) are the tools responsible for deploying the Bentos into the Kubernetes cluster and Cloud Platforms. Full support for these advanced tools is in progress and will be available soon.\n{% endhint %}\n\n## When to use it?\n\nYou should use the BentoML Model Deployer to:\n\n* Standardize the way you deploy your models to production within your organization.\n* if you are looking to deploy your models in a simple way, while you are still able to transform your model into a production-ready solution when that time comes.\n\nIf you are looking to deploy your models with other Kubernetes-based solutions, you can take a look at one of the other [Model Deployer Flavors](./model-deployers.md#model-deployers-flavors) available in ZenML.\n\nBentoML also allows you to deploy your models in a more complex production-grade setting. [Bentoctl](https://github.com/bentoml/bentoctl) is one of the tools that can help you get there. Bentoctl takes your built Bento from a ZenML pipeline and deploys it with `bentoctl` into a cloud environment such as AWS Lambda, AWS SageMaker, Google Cloud Functions, Google Cloud AI Platform, or Azure Functions. Read more about this in the [From Local to Cloud with `bentoctl` section](bentoml.md#from-local-to-cloud-with-bentoctl).\n\n{% hint style=\"info\" %}\nThe `bentoctl` integration implementation is still in progress and will be available soon. The integration will allow you to deploy your models to a specific cloud provider with just a few lines of code using ZenML built-in"}
{"input": " steps.\n{% endhint %}\n\n## How do you deploy it?\n\nWithin ZenML you can quickly get started with BentoML by simply creating Model Deployer Stack Component with the BentoML flavor. To do so you'll need to install the required Python packages on your local machine to be able to deploy your models:\n\n```bash\nzenml integration install bentoml -y\n```\n\nTo register the BentoML model deployer with ZenML you need to run the following command:\n\n```bash\nzenml model-deployer register bentoml_deployer --flavor=bentoml\n```\n\nThe ZenML integration will provision a local HTTP deployment server as a daemon process that will continue to run in the background to serve the latest models and Bentos.\n\n## How do you use it?\n\nThe recommended flow to use the BentoML model deployer is to first [create a BentoML Service](bentoml.md#bentoml-service-and-runner), then [use the `bento_builder_step`](bentoml.md#zenml-bento-builder-step) to build the model and service into a bento bundle, and finally [deploy the bundle with the `bentoml_model_deployer_step`](bentoml.md#zenml-bentoml-deployer-step).\n\n### BentoML Service and Runner\n\nThe first step to being able to deploy your models and use BentoML is to create a [bento service](https://docs.bentoml.org/en/latest/concepts/service.html) which is the main logic that defines how your model will be served, and a [bento runner](https://docs.bentoml.org/en/latest/concepts/runner.html) which represents a unit of execution for your model on a remote Python worker.\n\nThe following example shows how to create a basic bento service and runner that will be used to serve a basic scikit-learn model.\n\n```python\nimport numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n\n```\n\n### ZenML Bento Builder step\n\nOnce you have your bento service and runner defined, we can use the built-in bento builder step to build the bento bundle that will be used to serve the model. The following example shows how can call the built-in bento builder step within a ZenML pipeline.\n\n```python\nfrom zenml import pipeline, step\nfrom zenml.integrations.bentoml.steps import bento_builder_step\n\n@pipeline\n"}
{"input": "def bento_builder_pipeline():\n    model = ...\n    bento = bento_builder_step(\n        model=model,\n        model_name=\"pytorch_mnist\",  # Name of the model\n        model_type=\"pytorch\",  # Type of the model (pytorch, tensorflow, sklearn, xgboost..)\n        service=\"service.py:svc\",  # Path to the service file within zenml repo\n        labels={  # Labels to be added to the bento bundle\n            \"framework\": \"pytorch\",\n            \"dataset\": \"mnist\",\n            \"zenml_version\": \"0.21.1\",\n        },\n        exclude=[\"data\"],  # Exclude files from the bento bundle\n        python={\n            \"packages\": [\"zenml\", \"torch\", \"torchvision\"],\n        },  # Python package requirements of the model\n    )\n```\n\nThe Bento Builder step can be used in any orchestration pipeline that you create with ZenML. The step will build the bento bundle and save it to the used artifact store. Which can be used to serve the model in a local setting using the BentoML Model Deployer Step, or in a remote setting using the `bentoctl` or Yatai. This gives you the flexibility to package your model in a way that is ready for different deployment scenarios.\n\n### ZenML BentoML Deployer step\n\nWe have now built our bento bundle, and we can use the built-in `bentoml_model_deployer_step` to deploy the bento bundle to our local HTTP server. The following example shows how to call the built-in bento deployer step within a ZenML pipeline.\n\nNote: the `bentoml_model_deployer_step` can only be used in a local environment.\n\n```python\nfrom zenml import pipeline, step\nfrom zenml.integrations.bentoml.steps import bentoml_model_deployer_step\n\n@pipeline\ndef bento_deployer_pipeline():\n    bento = ...\n    deployed_model = bentoml_model_deployer_step(\n        bento=bento\n        model_name=\"pytorch_mnist\",  # Name of the model\n        port=3001,  # Port to be used by the http server\n    )\n```\n\n### ZenML BentoML Pipeline examples\n\nOnce all the steps have been defined, we can create a ZenML pipeline and run it. The bento builder step expects to get the trained model as an input, so we need to make sure either we have a previous step that trains the model and outputs it or loads the model from a previous run. Then the deployer step expects to get the bento bundle as an input, so we need to make sure either we have a previous step that builds the bento bundle and outputs it or load the bento bundle from a previous run or external source.\n\nThe following example shows how to create a ZenML pipeline"}
{"input": " that trains a model, builds a bento bundle, and deploys it to a local HTTP server.\n\n```python\n# Import the pipeline to use the pipeline decorator\nfrom zenml.pipelines import pipeline\n\n\n# Pipeline definition\n@pipeline\ndef bentoml_pipeline(\n        importer,\n        trainer,\n        evaluator,\n        deployment_trigger,\n        bento_builder,\n        deployer,\n):\n    \"\"\"Link all the steps and artifacts together\"\"\"\n    train_dataloader, test_dataloader = importer()\n    model = trainer(train_dataloader)\n    accuracy = evaluator(test_dataloader=test_dataloader, model=model)\n    decision = deployment_trigger(accuracy=accuracy)\n    bento = bento_builder(model=model)\n    deployer(deploy_decision=decision, bento=bento)\n\n```\n\nIn more complex scenarios, you might want to build a pipeline that trains a model and builds a bento bundle in a remote environment. Then creates a new pipeline that retrieves the bento bundle and deploys it to a local http server, or to a cloud provider. The following example shows a pipeline example that does exactly that.\n\n```python\n# Import the pipeline to use the pipeline decorator\nfrom zenml.pipelines import pipeline\n\n\n# Pipeline definition\n@pipeline\ndef remote_train_pipeline(\n        importer,\n        trainer,\n        evaluator,\n        bento_builder,\n):\n    \"\"\"Link all the steps and artifacts together\"\"\"\n    train_dataloader, test_dataloader = importer()\n    model = trainer(train_dataloader)\n    accuracy = evaluator(test_dataloader=test_dataloader, model=model)\n    bento = bento_builder(model=model)\n\n\n@pipeline\ndef local_deploy_pipeline(\n        bento_loader,\n        deployer,\n):\n    \"\"\"Link all the steps and artifacts together\"\"\"\n    bento = bento_loader()\n    deployer(deploy_decision=decision, bento=bento)\n\n```\n\n### Predicting with the local deployed model\n\nOnce the model has been deployed we can use the BentoML client to send requests to the deployed model. ZenML will automatically create a BentoML client for you and you can use it to send requests to the deployed model by simply calling the service to predict the method and passing the input data and the API function name.\n\nThe following example shows how to use the BentoML client to send requests to the deployed model.\n\n```python\n@step\ndef predictor(\n        inference_data: Dict[str, List],\n        service: BentoMLDeploymentService,\n) -> None:\n    \"\"\"Run an inference request against the BentoML prediction service.\n\n    Args:\n        service: The BentoML service.\n        data: The data to predict.\n    \"\"\"\n\n    service.start(timeout=10)  # should be a NOP if already started\n    for img, data in inference_data.items():\n        prediction = service.predict(\"predict_ndarray\", np.array(data))\n        result = to_labels(prediction[0])\n        rich_print(f\"Prediction for {img} is {"}
{"input": "result}\")\n```\n\nDeploying and testing locally is a great way to get started and test your model. However, a real-world scenario will most likely require you to deploy your model to a remote environment. The next section will show you how to deploy the Bento you built with ZenML pipelines to a cloud environment using the `bentoctl` CLI.\n\n### From Local to Cloud with `bentoctl`\n\nBentoctl helps deploy any machine learning models as production-ready API endpoints into the cloud. It is a command line tool that provides a simple interface to manage your BentoML bundles.\n\nThe `bentoctl` CLI provides a list of operators which are plugins that interact with cloud services, some of these operators are:\n\n* [AWS Lambda](https://github.com/bentoml/aws-lambda-deploy)\n* [AWS SageMaker](https://github.com/bentoml/aws-sagemaker-deploy)\n* [AWS EC2](https://github.com/bentoml/aws-ec2-deploy)\n* [Google Cloud Run](https://github.com/bentoml/google-cloud-run-deploy)\n* [Google Compute Engine](https://github.com/bentoml/google-compute-engine-deploy)\n* [Azure Container Instances](https://github.com/bentoml/azure-container-instances-deploy)\n* [Heroku](https://github.com/bentoml/heroku-deploy)\n\nTo deploy your BentoML bundle to the cloud, you need to install the `bentoctl` CLI and the operator plugin for the cloud service you want to deploy to.\n\n```bash\n# Install bentoctl CLI\npip install bentoctl\n# Install a choose operator\nbentoctl operator install $OPERATOR # example: aws-lambda\n```\n\nOnce you have the `bentoctl` CLI and the operator plugin installed, you can use the `bentoctl` CLI to deploy your BentoML bundle to the cloud.\n\n```bash\n# Let's get the name of the BentoML bundle we want to deploy\nbentoml list\n\n# Generate deployment configuration file\nbentoctl init\n\n# Build and push the Docker image to the cloud\nbentoctl build -b $BENTO_TAG -f deployment_config.yaml\n\n# Deploy to the cloud\nbentoctl apply -f deployment_config.yaml\n```\n\nFor more information and a full list of configurable attributes of the BentoML Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-bentoml/#zenml.integrations.bentoml.model_deployers.bentoml_model_deployer) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML"}
{"input": " Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying your models and serve real-time predictions.\n---\n\n# Model Deployers\n\nModel Deployment is the process of making a machine learning model available to make predictions and decisions on\nreal-world data. Getting predictions from trained models can be done in different ways depending on the use case, a\nbatch prediction is used to generate predictions for a large amount of data at once, while a real-time prediction is\nused to generate predictions for a single data point at a time.\n\nModel deployers are stack components responsible for serving models on a real-time or batch basis.\n\nOnline serving is the process of hosting and loading machine-learning models as part of a managed web service and\nproviding access to the models through an API endpoint like HTTP or GRPC. Once deployed, model inference can be\ntriggered at any time, and you can send inference requests to the model through the web service's API and receive fast,\nlow-latency responses.\n\nBatch inference or offline inference is the process of making a machine learning model make predictions on a batch of\nobservations. This is useful for generating predictions for a large amount of data at once. The predictions are usually\nstored as files or in a database for end users or business applications.\n\n### When to use it?\n\nThe model deployers are optional components in the ZenML stack. They are used to deploy machine learning models to a\ntarget environment, either a development (local) or a production (Kubernetes or cloud) environment. The model deployers are mainly used to deploy models for real-time inference use cases. With the model deployers and other stack components, you can build pipelines that are continuously trained and deployed to production.\n\n### How model deployers slot into the stack\n\nHere is an architecture diagram that shows how model deployers fit into the overall story of a remote stack.\n\n![Model Deployers](../../.gitbook/assets/Remote_with_deployer.png)\n\n#### Model Deployers Flavors\n\nZenML comes with a `local` MLflow model deployer which is a simple model deployer that deploys models to a local MLflow\nserver. Additional model deployers that can be used to deploy models on production environments are provided by\nintegrations:\n\n| Model Deployer                     | Flavor    | Integration   | Notes                                                                        |\n|------------------------------------|-----------|---------------|------------------------------------------------------------------------------|\n| [MLflow](mlflow.md)                | `mlflow`  | `mlflow`      | Deploys ML Model locally                                                     |\n| [BentoML](bentoml.md)              | `bentoml` | `bentoml`     | Build and Deploy ML models locally or for production grade (Cloud, K8s)      |\n| [Seldon Core](seldon.md)           | `seldon`  | `seldon Core` | Built on top of Kubernetes to deploy models for production grade environment |\n| [Hugging Face](huggingface.md) | `"}
{"input": "huggingface` | `huggingface` | Deploys ML model on Hugging Face Inference Endpoints |\n| [Custom Implementation](custom.md) | _custom_  |               | Extend the Artifact Store abstraction and provide your own implementation    |\n\n{% hint style=\"info\" %}\nEvery model deployer may have different attributes that must be configured in order to interact with the model serving\ntool, framework, or platform (e.g. hostnames, URLs, references to credentials, and other client-related configuration\nparameters). The following example shows the configuration of the MLflow and Seldon Core model deployers:\n\n```shell\n# Configure MLflow model deployer\nzenml model-deployer register mlflow --flavor=mlflow\n\n# Configure Seldon Core model deployer\nzenml model-deployer register seldon --flavor=seldon \\\n--kubernetes_context=zenml-eks --kubernetes_namespace=zenml-workloads \\\n--base_url=http://abb84c444c7804aa98fc8c097896479d-377673393.us-east-1.elb.amazonaws.com\n...\n```\n\n{% endhint %}\n\n#### The role that a model deployer plays in a ZenML Stack\n\n* Seamless Model Deployment: Facilitates the deployment of machine learning models to various serving environments, such as local servers, Kubernetes clusters, or cloud platforms, ensuring that models can be deployed and managed efficiently in accordance with the specific requirements of the serving infrastructure by holds all the stack-related configuration attributes required to interact with the remote model serving tool, service, or platform (e.g. hostnames, URLs, references to credentials, and other client-related configuration parameters). The following are examples of configuring the MLflow and Seldon Core Model Deployers and registering them as a Stack component:\n\n   ```bash\n   zenml integration install mlflow\n   zenml model-deployer register mlflow --flavor=mlflow\n   zenml stack register local_with_mlflow -m default -a default -o default -d mlflow --set\n   ```\n\n   ```bash\n   zenml integration install seldon\n   zenml model-deployer register seldon --flavor=seldon \\\n   --kubernetes_context=zenml-eks --kubernetes_namespace=zenml-workloads \\\n   --base_url=http://abb84c444c7804aa98fc8c097896479d-377673393.us-east-1.elb.amazonaws.com\n   ...\n   zenml stack register seldon_stack -m default -a aws -o default -d seldon\n   ```\n\n* Lifecycle Management: Provides mechanisms for comprehensive lifecycle management of model servers, including the ability to start, stop, and delete model servers, as well as to update existing servers with new model versions, thereby optimizing resource utilization and facilitating continuous delivery of model updates. Some core methods that can be used to interact with the remote model server"}
{"input": " include:\n\n`deploy_model` - Deploys a model to the serving environment and returns a Service object that represents the deployed model server.\n`find_model_server` - Finds and returns a list of Service objects that represent model servers that have been deployed to the serving environment, the \nservices are stored in the DB and can be used as a reference to know what and where the model is deployed.\n`stop_model_server` - Stops a model server that is currently running in the serving environment.\n`start_model_server` - Starts a model server that has been stopped in the serving environment.\n`delete_model_server` - Deletes a model server from the serving environment and from the DB.\n\n{% hint style=\"info\" %}\nZenML uses the Service object to represent a model server that has been deployed to a serving environment. The Service object is saved in the DB and can be used as a reference to know what and where the model is deployed. The Service object consists of 2 main attributes, the `config` and the `status`. The `config` attribute holds all the deployment configuration attributes required to create a new deployment, while the `status` attribute holds the operational status of the deployment, such as the last error message, the prediction URL, and the deployment status.\n{% endhint %}\n\n   ```python\n   from zenml.integrations.huggingface.model_deployers import HuggingFaceModelDeployer\n\n   model_deployer = HuggingFaceModelDeployer.get_active_model_deployer()\n   services = model_deployer.find_model_server(\n       pipeline_name=\"LLM_pipeline\",\n       pipeline_step_name=\"huggingface_model_deployer_step\",\n       model_name=\"LLAMA-7B\",\n   )\n   if services:\n       if services[0].is_running:\n           print(\n               f\"Model server {services[0].config['model_name']} is running at {services[0].status['prediction_url']}\"\n              )\n        else:\n            print(f\"Model server {services[0].config['model_name']} is not running\")\n            model_deployer.start_model_server(services[0])\n    else:\n        print(\"No model server found\")\n        service = model_deployer.deploy_model(\n            pipeline_name=\"LLM_pipeline\",\n            pipeline_step_name=\"huggingface_model_deployer_step\",\n            model_name=\"LLAMA-7B\",\n            model_uri=\"s3://zenprojects/huggingface_model_deployer_step/output/884/huggingface\",\n            revision=\"main\",\n            task=\"text-classification\",\n            region=\"us-east-1\",\n            vendor=\"aws\",\n            token=\"huggingface_token\",\n            namespace=\"zenml-workloads\",\n            endpoint_type=\"public\",\n        )\n        print(f\"Model server {service.config['model_name']} is deployed at {service.status['prediction_url']}\")\n   ```\n\n#### &#x20;How to Interact with a model deployer after deployment?\n\nWhen a Model Deployer is"}
{"input": " part of the active ZenML Stack, it is also possible to interact with it from the CLI to list,\nstart, stop, or delete the model servers that is managed:\n\n```\n$ zenml model-deployer models list\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 STATUS \u2502 UUID                                 \u2502 PIPELINE_NAME                  \u2502 PIPELINE_STEP_NAME         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \u2705   \u2502 8cbe671b-9fce-4394-a051-68e001f92765 \u2502 seldon_deployment_pipeline     \u2502 seldon_model_deployer_step \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n$ zenml model-deployer models describe 8cbe671b-9fce-4394-a051-68e001f92765\n                          Properties of Served Model 8cbe671b-9fce-4394-a051-68e001f92765                          \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 MODEL SERVICE PROPERTY \u2502 VALUE                                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 MODEL_NAME             \u2502 mnist                                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 MODEL_URI              \u2502 s3://zenprojects/seldon_model_deployer_step/output/884/seldon                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 PIPELINE_NAME          \u2502 seldon_deployment_pipeline                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 RUN_NAME               \u2502 seldon_deployment_pipeline-11_Apr_22-09_39_27_648527                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 PIPELINE_STEP_NAME     \u2502 seldon_model_deployer"}
{"input": "_step                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 PREDICTION_URL         \u2502 http://abb84c444c7804aa98fc8c097896479d-377673393.us-east-1.elb.amazonaws.com/seldon/\u2026 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 SELDON_DEPLOYMENT      \u2502 zenml-8cbe671b-9fce-4394-a051-68e001f92765                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 STATUS                 \u2502 \u2705                                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 STATUS_MESSAGE         \u2502 Seldon Core deployment 'zenml-8cbe671b-9fce-4394-a051-68e001f92765' is available       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UUID                   \u2502 8cbe671b-9fce-4394-a051-68e001f92765                                                   \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n$ zenml model-deployer models get-url 8cbe671b-9fce-4394-a051-68e001f92765\n  Prediction URL of Served Model 8cbe671b-9fce-4394-a051-68e001f92765 is:\n  http://abb84c444c7804aa98fc8c097896479d-377673393.us-east-1.elb.amazonaws.com/seldon/zenml-workloads/zenml-8cbe67\n1b-9fce-4394-a051-68e001f92765/api/v0.1/predictions\n\n$ zenml model-deployer models delete 8cbe671b-9fce-4394-a051-68e001f92765\n```\n\nIn Python, you can alternatively discover the prediction URL of a deployed model by inspecting the metadata of the step\nthat deployed the model:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"<PIPELINE_RUN_NAME>\")\ndeployer_step = pipeline_run.steps[\"<NAME_OF_MODEL_DEPLOYER_STEP>\"]\ndeployed_model_url = deployer_step.run_metadata[\"deployed_model_url\"].value\n```\n\nThe ZenML integr"}
{"input": "ations that provide Model Deployer stack components also include standard pipeline steps that can\ndirectly be inserted into any pipeline to achieve a continuous model deployment workflow. These steps take care of all\nthe aspects of continuously deploying models to an external server and saving the Service configuration into the\nArtifact Store, where they can be loaded at a later time and re-create the initial conditions used to serve a particular\nmodel.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  Deploying models to Databricks Inference Endpoints with Databricks\n---\n\n# Databricks\n\n\nDatabricks Model Serving or Mosaic AI Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nThis service provides dedicated and autoscaling infrastructure managed by Databricks, allowing you to deploy models without dealing with containers and GPUs.\n\n\n{% hint style=\"info\" %}\nDatabricks Model deployer can be considered as a managed service for deploying models using MLflow, This means you can switch between MLflow and Databricks Model Deployers without changing your pipeline code even for custom complex models.\n{% endhint %}\n\n## When to use it?\n\nYou should use Databricks Model Deployer:\n\n*   You are already using Databricks for your data and ML workloads.\n*   If you want to deploy AI models without dealing with containers and GPUs, Databricks Model Deployer provides a unified interface to deploy, govern, and query models.\n*   Databricks Model Deployer offers dedicated and autoscaling infrastructure managed by Databricks, making it easier to deploy models at scale.\n*   Enterprise security is a priority, and you need to deploy models into secure offline endpoints accessible only via a direct connection to your Virtual Private Cloud (VPCs).\n*   if your goal is to turn your models into production-ready APIs with minimal infrastructure or MLOps involvement.\n\n\nIf you are looking for a more easy way to deploy your models locally, you can use the [MLflow Model Deployer](mlflow.md) flavor.\n\n## How to deploy it?\n\nThe Databricks Model Deployer flavor is provided by the Databricks ZenML integration, so you need to install it on your local machine to be able to deploy your models. You can do this by running the following command:\n\n```bash\nzenml integration install databricks -y\n```\n\nTo register the Databricks model deployer with ZenML you need to run the following command:\n\n```bash\nzenml model-deployer register <MODEL_DEPLOYER_NAME> --flavor=databricks --host=<HOST> --client_id={{databricks.client_id}} --client_secret={{databricks.client_secret}}\n```\n\n{% hint style=\"info\" %}\nWe recommend creating a Databricks service account with the necessary permissions to create and run jobs. You can find more information on how to create a service account [here](https://docs.databricks.com/dev-tools/api/latest/authentication.html). You can generate a client_id and client_secret for the service account and use them to authenticate with Databricks.\n{% endhint %}\n\nWe can now use the model deployer in our stack.\n\n```bash\nzenml stack update <CUSTOM_STACK_NAME> --model-deployer=<MODEL_DEPLOYER_NAME>\n``"}
{"input": "`\n\nSee the [databricks\\_model\\_deployer\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-databricks/#zenml.integrations.databricks.steps.databricks\\_deployer.databricks\\_model\\_deployer\\_step) for an example of using the Databricks Model Deployer to deploy a model inside a ZenML pipeline step.\n\n## Configuration\n\nWithin the `DatabricksServiceConfig` you can configure:\n\n\n* `model_name`: The name of the model that will be served, this will be used to identify the model in the Databricks Model Registry.\n* `model_version`: The version of the model that will be served, this will be used to identify the model in the Databricks Model Registry.\n* `workload_size`: The size of the workload that the model will be serving. This can be `Small`, `Medium`, or `Large`.\n* `scale_to_zero_enabled`: A boolean flag to enable or disable the scale to zero feature.\n* `env_vars`: A dictionary of environment variables to be passed to the model serving container.\n* `workload_type`: The type of workload that the model will be serving. This can be `CPU`, `GPU_LARGE`, `GPU_MEDIUM`, `GPU_SMALL`, or `MULTIGPU_MEDIUM`.\n* `endpoint_secret_name`: The name of the secret that will be used to secure the endpoint and authenticate requests.\n\nFor more information and a full list of configurable attributes of the Databricks Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-databricks/#zenml.integrations.databricks.model\\_deployers) and Databricks endpoint [code](https://github.com/databricks/databricks\\_hub/blob/5e3b603ccc7cd6523d998e75f82848215abf9415/src/databricks\\_hub/hf\\_api.py#L6957).\n\n### Run inference on a provisioned inference endpoint\n\nThe following code example shows how to run inference against a provisioned inference endpoint:\n\n```python\nfrom typing import Annotated\nfrom zenml import step, pipeline\nfrom zenml.integrations.databricks.model_deployers import DatabricksModelDeployer\nfrom zenml.integrations.databricks.services import DatabricksDeploymentService\n\n\n# Load a prediction service deployed in another pipeline\n@step(enable_cache=False)\ndef prediction_service_loader(\n    pipeline_name: str,\n    pipeline_step_name: str,\n    running: bool = True,\n    model_name: str = \"default\",\n) -> DatabricksDeploymentService:\n    \"\"\"Get the prediction service started by the deployment pipeline.\n\n    Args:\n        pipeline_name: name of the pipeline that deployed the MLflow prediction\n            server\n        step_name: the name of the step that deployed the MLflow prediction"}
{"input": "\n            server\n        running: when this flag is set, the step only returns a running service\n        model_name: the name of the model that is deployed\n    \"\"\"\n    # get the Databricks model deployer stack component\n    model_deployer = DatabricksModelDeployer.get_active_model_deployer()\n\n    # fetch existing services with same pipeline name, step name and model name\n    existing_services = model_deployer.find_model_server(\n        pipeline_name=pipeline_name,\n        pipeline_step_name=pipeline_step_name,\n        model_name=model_name,\n        running=running,\n    )\n\n    if not existing_services:\n        raise RuntimeError(\n            f\"No Databricks inference endpoint deployed by step \"\n            f\"'{pipeline_step_name}' in pipeline '{pipeline_name}' with name \"\n            f\"'{model_name}' is currently running.\"\n        )\n\n    return existing_services[0]\n\n\n# Use the service for inference\n@step\ndef predictor(\n    service: DatabricksDeploymentService,\n    data: str\n) -> Annotated[str, \"predictions\"]:\n    \"\"\"Run a inference request against a prediction service\"\"\"\n\n    prediction = service.predict(data)\n    return prediction\n\n\n@pipeline\ndef databricks_deployment_inference_pipeline(\n    pipeline_name: str, pipeline_step_name: str = \"databricks_model_deployer_step\",\n):\n    inference_data = ...\n    model_deployment_service = prediction_service_loader(\n        pipeline_name=pipeline_name,\n        pipeline_step_name=pipeline_step_name,\n    )\n    predictions = predictor(model_deployment_service, inference_data)\n```\n\nFor more information and a full list of configurable attributes of the Databricks Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-databricks/#zenml.integrations.databricks.model\\_deployers).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying models to Kubernetes with Seldon Core.\n---\n\n# Seldon\n\n[Seldon Core](https://github.com/SeldonIO/seldon-core) is a production grade source-available model serving platform. It packs a wide range of features built around deploying models to REST/GRPC microservices that include monitoring and logging, model explainers, outlier detectors and various continuous deployment strategies such as A/B testing, canary deployments and more.\n\nSeldon Core also comes equipped with a set of built-in model server implementations designed to work with standard formats for packaging ML models that greatly simplify the process of serving models for real-time inference.\n\n{% hint style=\"warning\" %}\nThe Seldon Core model deployer integration is currently not supported under **MacOS**.\n{% endhint %}\n\n## When to use it?\n\n[Seldon Core](https://github.com/SeldonIO/seldon-core) is a production-grade source-available model serving platform. It packs a wide range of features built around deploying models to REST/GRPC microservices that include monitoring and logging, model explainers, outlier detectors, and various continuous deployment strategies such as A/B testing, canary deployments, and more.\n\nSeldon Core also comes equipped with a set of built-in model server implementations designed to work with standard formats for packaging ML models that greatly simplify the process of serving models for real-time inference.\n\nYou should use the Seldon Core Model Deployer:\n\n* If you are looking to deploy your model on a more advanced infrastructure like Kubernetes.\n* If you want to handle the lifecycle of the deployed model with no downtime, including updating the runtime graph, scaling, monitoring, and security.\n* Looking for more advanced API endpoints to interact with the deployed model, including REST and GRPC endpoints.\n* If you want more advanced deployment strategies like A/B testing, canary deployments, and more.\n* if you have a need for a more complex deployment process that can be customized by the advanced inference graph that includes custom [TRANSFORMER](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/overview.html) and [ROUTER](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/routers.html?highlight=routers).\n\nIf you are looking for a more easy way to deploy your models locally, you can use the [MLflow Model Deployer](mlflow.md) flavor.\n\n## How to deploy it?\n\nZenML provides a Seldon Core flavor build on top of the Seldon Core Integration to allow you to deploy and use your models in a production-grade environment. In order to use the integration you need to install it on your local machine to be able to register a Seldon Core Model deployer with ZenML and add it to your stack:\n\n```bash\nzenml integration install seldon -y\n```\n\nTo deploy and make use of the Seldon Core integration we need to have the following prerequisites:\n\n1. access to a Kubernetes cluster. This can be configured"}
{"input": " using the `kubernetes_context` configuration attribute to point to a local `kubectl` context or an in-cluster configuration, but the recommended approach is to [use a Service Connector](seldon.md#using-a-service-connector) to link the Seldon Deployer Stack Component to a Kubernetes cluster.\n2. Seldon Core needs to be preinstalled and running in the target Kubernetes cluster. Check out the [official Seldon Core installation instructions](https://github.com/SeldonIO/seldon-core/tree/master/examples/auth#demo-setup) or the [EKS installation example below](seldon.md#installing-seldon-core-eg-in-an-eks-cluster).\n3. models deployed with Seldon Core need to be stored in some form of persistent shared storage that is accessible from the Kubernetes cluster where Seldon Core is installed (e.g. AWS S3, GCS, Azure Blob Storage, etc.). You can use one of the supported [remote artifact store flavors](../artifact-stores/artifact-stores.md) to store your models as part of your stack. For a smoother experience running Seldon Core with a cloud artifact store, we also recommend configuring explicit credentials for the artifact store. The Seldon Core model deployer knows how to automatically convert those credentials in the format needed by Seldon Core model servers to authenticate to the storage back-end where models are stored.\n\nSince the Seldon Model Deployer is interacting with the Seldon Core model server deployed on a Kubernetes cluster, you need to provide a set of configuration parameters. These parameters are:\n\n* kubernetes\\_context: the Kubernetes context to use to contact the remote Seldon Core installation. If not specified, the active Kubernetes context is used or the in-cluster configuration is used if the model deployer is running in a Kubernetes cluster. The recommended approach is to [use a Service Connector](seldon.md#using-a-service-connector) to link the Seldon Deployer Stack Component to a Kubernetes cluster and to skip this parameter.\n* kubernetes\\_namespace: the Kubernetes namespace where the Seldon Core deployment servers are provisioned and managed by ZenML. If not specified, the namespace set in the current configuration is used.\n* base\\_url: the base URL of the Kubernetes ingress used to expose the Seldon Core deployment servers.\n\nIn addition to these parameters, the Seldon Core Model Deployer may also require additional configuration to be set up to allow it to authenticate to the remote artifact store or persistent storage service where model artifacts are located. This is covered in the [Managing Seldon Core Authentication](seldon.md#managing-seldon-core-authentication) section.\n\n{% hint style=\"info\" %}\nConfiguring Seldon Core in a Kubernetes cluster can be a complex and error-prone process, so we have provided a set of Terraform-based recipes to quickly provision popular combinations of MLOps tools. More information about these recipes can be found in the [MLOps Stack Recipes](https://github.com/"}
{"input": "zenml-io/mlstacks).\n{% endhint %}\n\n### Infrastructure Deployment\n\nThe Seldon Model Deployer can be deployed directly from the ZenML CLI:\n\n```shell\nzenml model-deployer deploy seldon_deployer --flavor=seldon --provider=<YOUR_PROVIDER> ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the [dedicated documentation section](../../how-to/stack-deployment/README.md).\n\n### Seldon Core Installation Example\n\nThe following example briefly shows how you can install Seldon in an EKS Kubernetes cluster. It assumes that the EKS cluster itself is already set up and configured with IAM access. For more information or tutorials for other clouds, check out the [official Seldon Core installation instructions](https://github.com/SeldonIO/seldon-core/tree/master/examples/auth#demo-setup).\n\n1. Configure EKS cluster access locally, e.g:\n\n```bash\naws eks --region us-east-1 update-kubeconfig --name zenml-cluster --alias zenml-eks\n```\n\n2. Install Istio 1.5.0 (required for the latest Seldon Core version):\n\n```bash\ncurl -L [https://istio.io/downloadIstio](https://istio.io/downloadIstio) | ISTIO_VERSION=1.5.0 sh -\ncd istio-1.5.0/\nbin/istioctl manifest apply --set profile=demo\n```\n\n3. Set up an Istio gateway for Seldon Core:\n\n```bash\ncurl https://raw.githubusercontent.com/SeldonIO/seldon-core/master/notebooks/resources/seldon-gateway.yaml | kubectl apply -f -\n```\n\n4. Install Seldon Core:\n\n```bash\nhelm install seldon-core seldon-core-operator \\\n    --repo https://storage.googleapis.com/seldon-charts \\\n    --set usageMetrics.enabled=true \\\n    --set istio.enabled=true \\\n    --namespace seldon-system\n```\n\n5. Test that the installation is functional\n\n```bash\nkubectl apply -f iris.yaml\n```\n\nwith `iris.yaml` defined as follows:\n\n```yaml\napiVersion: machinelearning.seldon.io/v1\nkind: SeldonDeployment\nmetadata:\n  name: iris-model\n  namespace: default\nspec:\n  name: iris\n  predictors:\n  - graph:\n      implementation: SKLEARN_SERVER\n      modelUri: gs://seldon-models/v1.14.0-dev/sklearn/iris\n      name: classifier\n    name: default\n    replicas: 1\n```\n\nThen extract the URL where the model server exposes its prediction API:\n\n```bash\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath"}
{"input": "='{.status.loadBalancer.ingress[0].hostname}')\n```\n\nAnd use curl to send a test prediction API request to the server:\n\n```bash\ncurl -X POST http://$INGRESS_HOST/seldon/default/iris-model/api/v1.0/predictions \\\n         -H 'Content-Type: application/json' \\\n         -d '{ \"data\": { \"ndarray\": [[1,2,3,4]] } }'\n```\n\n### Using a Service Connector\n\nTo set up the Seldon Core Model Deployer to authenticate to a remote Kubernetes cluster, it is recommended to leverage the many features provided by [the Service Connectors](../../how-to/auth-management/README.md) such as auto-configuration, local client login, best security practices regarding long-lived credentials and fine-grained access control and reusing the same credentials across multiple stack components.\n\nDepending on where your target Kubernetes cluster is running, you can use one of the following Service Connectors:\n\n* [the AWS Service Connector](../../how-to/auth-management/aws-service-connector.md), if you are using an AWS EKS cluster.\n* [the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md), if you are using a GKE cluster.\n* [the Azure Service Connector](../../how-to/auth-management/azure-service-connector.md), if you are using an AKS cluster.\n* [the generic Kubernetes Service Connector](../../how-to/auth-management/kubernetes-service-connector.md) for any other Kubernetes cluster.\n\nIf you don't already have a Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure a Service Connector that can be used to access more than one Kubernetes cluster or even more than one type of cloud resource:\n\n```sh\nzenml service-connector register -i\n```\n\nA non-interactive CLI example that leverages [the AWS CLI configuration](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) on your local machine to auto-configure an AWS Service Connector targeting a single EKS cluster is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type aws --resource-type kubernetes-cluster --resource-name <EKS_CLUSTER_NAME> --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register eks-zenhacks --type aws --resource-type kubernetes-cluster --resource-id zenhacks-cluster --auto-configure\n\u283c Registering service connector 'eks-zenhacks'...\nSuccessfully registered service connector `eks-zenhacks` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503     RESOURCE TYPE     \u2502 RESOURCE NAMES   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ufffd"}
{"input": "\ufffd\n\u2503 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAlternatively, you can configure a Service Connector through the ZenML dashboard:\n\n![AWS Service Connector Type](../../.gitbook/assets/aws-service-connector-type.png) ![AWS EKS Service Connector Configuration](../../.gitbook/assets/aws-eks-service-connector-configuration.png)\n\n> **Note**: Please remember to grant the entity associated with your cloud credentials permissions to access the Kubernetes cluster and to list accessible Kubernetes clusters. For a full list of permissions required to use a AWS Service Connector to access one or more Kubernetes cluster, please refer to the [documentation for your Service Connector of choice](../../how-to/auth-management/README.md) or read the documentation available in the interactive CLI commands and dashboard. The Service Connectors supports many different authentication methods with different levels of security and convenience. You should pick the one that best fits your use-case.\n\nIf you already have one or more Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the Kubernetes cluster that you want to use for your Seldon Core Model Deployer by running e.g.:\n\n```sh\nzenml service-connector list-resources --resource-type kubernetes-cluster\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES                                \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bdf1dc76-e36b-4ab4-b5a6-5a9afea4822f \u2502 eks-zenhacks   \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 b57f5f5c-0378-434c-8d50-34b492486f30 \u2502 gcp-multi      \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zen"}
{"input": "ml-test-cluster                            \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 d6fc6004-eb76-4fd7-8fa1-ec600cced680 \u2502 azure-multi    \u2502 \ud83c\udde6 azure       \u2502 \ud83c\udf00 kubernetes-cluster \u2502 demo-zenml-demos/demo-zenml-terraform-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on a Service Connector to use to connect to the target Kubernetes cluster where Seldon Core is installed, you can register the Seldon Core Model Deployer as follows:\n\n```sh\n# Register the Seldon Core Model Deployer\nzenml model-deployer register <MODEL_DEPLOYER_NAME> --flavor=seldon \\\n  --kubernetes_namespace=<KUBERNETES-NAMESPACE> \\\n  --base_url=http://$INGRESS_HOST\n\n# Connect the Seldon Core Model Deployer to the target cluster via a Service Connector\nzenml model-deployer connect <MODEL_DEPLOYER_NAME> -i\n```\n\nA non-interactive version that connects the Seldon Core Model Deployer to a target Kubernetes cluster through a Service Connector:\n\n```sh\nzenml model-deployer connect <MODEL_DEPLOYER_NAME> --connector <CONNECTOR_ID> --resource-id <CLUSTER_NAME>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml model-deployer connect seldon-test --connector gcp-multi --resource-id zenml-test-cluster\nSuccessfully connected model deployer `seldon-test` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 b57f5f5c-0378-434c-8d50-34b492486f30 \u2502 gcp-multi      \u2502 \ud83d\udd35 gcp         \u2502 \ufffd"}
{"input": "\ufffd\ufffd kubernetes-cluster \u2502 zenml-test-cluster \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nA similar experience is available when you configure the Seldon Core Model Deployer through the ZenML dashboard:\n\n![Seldon Core Model Deployer Configuration](../../.gitbook/assets/seldon-model-deployer-service-connector.png)\n\n### Managing Seldon Core Authentication\n\nThe Seldon Core Model Deployer requires access to the persistent storage where models are located. In most cases, you will use the Seldon Core model deployer to serve models that are trained through ZenML pipelines and stored in the ZenML Artifact Store, which implies that the Seldon Core model deployer needs to access the Artifact Store.\n\nIf Seldon Core is already running in the same cloud as the Artifact Store (e.g. S3 and an EKS cluster for AWS, or GCS and a GKE cluster for GCP), there are ways of configuring cloud workloads to have implicit access to other cloud resources like persistent storage without requiring explicit credentials. However, if Seldon Core is running in a different cloud, or on-prem, or if implicit in-cloud workload authentication is not enabled, then you need to configure explicit credentials for the Artifact Store to allow other components like the Seldon Core model deployer to authenticate to it. Every cloud Artifact Store flavor supports some way of configuring explicit credentials and this is documented for each individual flavor in the [Artifact Store documentation](../artifact-stores/artifact-stores.md).\n\nWhen explicit credentials are configured in the Artifact Store, the Seldon Core Model Deployer doesn't need any additional configuration and will use those credentials automatically to authenticate to the same persistent storage service used by the Artifact Store. If the Artifact Store doesn't have explicit credentials configured, then Seldon Core will default to using whatever implicit authentication method is available in the Kubernetes cluster where it is running. For example, in AWS this means using the IAM role attached to the EC2 or EKS worker nodes, and in GCP this means using the service account attached to the GKE worker nodes.\n\n{% hint style=\"warning\" %}\nIf the Artifact Store used in combination with the Seldon Core Model Deployer in the same ZenML stack does not have explicit credentials configured, then the Seldon Core Model Deployer might not be able to authenticate to the Artifact Store which will"}
{"input": " cause the deployed model servers to fail.\n\nTo avoid this, we recommend that you use Artifact Stores with explicit credentials in the same stack as the Seldon Core Model Deployer. Alternatively, if you're running Seldon Core in one of the cloud providers, you should configure implicit authentication for the Kubernetes nodes.\n{% endhint %}\n\nIf you want to use a custom persistent storage with Seldon Core, or if you prefer to manually manage the authentication credentials attached to the Seldon Core model servers, you can use the approach described in the next section.\n\n**Advanced: Configuring a Custom Seldon Core Secret**\n\nThe Seldon Core model deployer stack component allows configuring an additional `secret` attribute that can be used to specify custom credentials that Seldon Core should use to authenticate to the persistent storage service where models are located. This is useful if you want to connect Seldon Core to a persistent storage service that is not supported as a ZenML Artifact Store, or if you don't want to configure or use the same credentials configured for your Artifact Store. The `secret` attribute must be set to the name of [a ZenML secret](../../how-to/interact-with-secrets.md) containing credentials configured in the format supported by Seldon Core.\n\n{% hint style=\"info\" %}\nThis method is not recommended, because it limits the Seldon Core model deployer to a single persistent storage service, whereas using the Artifact Store credentials gives you more flexibility in combining the Seldon Core model deployer with any Artifact Store in the same ZenML stack.\n{% endhint %}\n\nSeldon Core model servers use [`rclone`](https://rclone.org/) to connect to persistent storage services and the credentials that can be configured in the ZenML secret must also be in the configuration format supported by `rclone`. This section covers a few common use cases and provides examples of how to configure the ZenML secret to support them, but for more information on supported configuration options, you can always refer to the [`rclone` documentation for various providers](https://rclone.org/).\n\n<details>\n\n<summary>Seldon Core Authentication Secret Examples</summary>\n\nExample of configuring a Seldon Core secret for AWS S3:\n\n```shell\nzenml secret create s3-seldon-secret \\\n--rclone_config_s3_type=\"s3\" \\ # set to 's3' for S3 storage.\n--rclone_config_s3_provider=\"aws\" \\ # the S3 provider (e.g. aws, Ceph, Minio).\n--rclone_config_s3_env_auth=False \\ # set to true to use implicit AWS authentication from EC2/ECS meta data\n# (i.e. with IAM roles configuration). Only applies if access_key_id and secret_access_key are blank.\n--rclone_config_s3_access_key_id=\"<AWS-ACCESS-KEY-ID>\" \\ # AWS Access Key ID.\n--rclone_config_s3_secret_access_key=\"<AWS-SECRET-ACCESS-"}
{"input": "KEY>\" \\ # AWS Secret Access Key.\n--rclone_config_s3_session_token=\"\" \\ # AWS Session Token.\n--rclone_config_s3_region=\"\" \\ # region to connect to.\n--rclone_config_s3_endpoint=\"\" \\ # S3 API endpoint.\n\n# Alternatively for providing key-value pairs, you can utilize the '--values' option by specifying a file path containing \n# key-value pairs in either JSON or YAML format.\n# File content example: {\"rclone_config_s3_type\":\"s3\",...}\nzenml secret create s3-seldon-secret \\\n    --values=@path/to/file.json\n```\n\nExample of configuring a Seldon Core secret for GCS:\n\n```shell\nzenml secret create gs-seldon-secret \\\n--rclone_config_gs_type=\"google cloud storage\" \\ # set to 'google cloud storage' for GCS storage.\n--rclone_config_gs_client_secret=\"\" \\  # OAuth client secret. \n--rclone_config_gs_token=\"\" \\ # OAuth Access Token as a JSON blob.\n--rclone_config_gs_project_number=\"\" \\ # project number.\n--rclone_config_gs_service_account_credentials=\"\" \\ #service account credentials JSON blob.\n--rclone_config_gs_anonymous=False \\ # Access public buckets and objects without credentials. \n# Set to True if you just want to download files and don't configure credentials.\n--rclone_config_gs_auth_url=\"\" \\ # auth server URL.\n\n# Alternatively for providing key-value pairs, you can utilize the '--values' option by specifying a file path containing \n# key-value pairs in either JSON or YAML format.\n# File content example: {\"rclone_config_gs_type\":\"google cloud storage\",...}\nzenml secret create gs-seldon-secret \\\n    --values=@path/to/file.json\n```\n\nExample of configuring a Seldon Core secret for Azure Blob Storage:\n\n```shell\nzenml secret create az-seldon-secret \\\n--rclone_config_az_type=\"azureblob\" \\ # set to 'azureblob' for Azure Blob Storage.\n--rclone_config_az_account=\"\" \\ # storage Account Name. Leave blank to\n# use SAS URL or MSI.\n--rclone_config_az_key=\"\" \\ # storage Account Key. Leave blank to\n# use SAS URL or MSI.\n--rclone_config_az_sas_url=\"\" \\ # SAS URL for container level access\n# only. Leave blank if using account/key or MSI.\n--rclone_config_az_use_msi=\"\" \\ # use a managed service identity to\n# authenticate (only works in Azure).\n--rclone_config_az_client_id=\"\" \\ # client ID of the service principal\n# to use for authentication.\n--rclone_config_az_client_secret=\"\" \\ # client secret of the service\n# principal to use for authentication.\n--rclone_config_az_tenant=\"\" \\ # tenant ID of the service principal\n# to use for authentication.\n\n# Alternatively for providing key-value pairs, you can utilize the '--values' option by specifying"}
{"input": " a file path containing \n# key-value pairs in either JSON or YAML format.\n# File content example: {\"rclone_config_az_type\":\"azureblob\",...}\nzenml secret create az-seldon-secret \\\n    --values=@path/to/file.json\n```\n\n</details>\n\n## How do you use it?\n\n### Requirements\n\nTo run pipelines that deploy models to Seldon, you need the following tools installed locally:\n\n* [Docker](https://www.docker.com)\n* [K3D](https://k3d.io/v5.2.1/#installation) (can be installed by running `curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash`).\n\n### Stack Component Registration\n\nFor registering the model deployer, we need the URL of the Istio Ingress Gateway deployed on the Kubernetes cluster. We can get this URL by running the following command (assuming that the service name is `istio-ingressgateway`, deployed in the `istio-system` namespace):\n\n```bash\n# For GKE clusters, the host is the GKE cluster IP address.\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n# For EKS clusters, the host is the EKS cluster IP hostname.\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n```\n\nNow register the model deployer:\n\n> **Note**: If you chose to configure your own custom credentials to authenticate to the persistent storage service where models are stored, as covered in the [Advanced: Configuring a Custom Seldon Core Secret](seldon.md#managing-seldon-core-authentication) section, you will need to specify a ZenML secret reference when you configure the Seldon Core model deployer below:\n>\n> ```shell\n> zenml model-deployer register seldon_deployer --flavor=seldon \\\n>  --kubernetes_context=<KUBERNETES-CONTEXT> \\\n>  --kubernetes_namespace=<KUBERNETES-NAMESPACE> \\\n>  --base_url=http://$INGRESS_HOST \\\n>  --secret=<zenml-secret-name> \n> ```\n\n```bash\n# Register the Seldon Core Model Deployer\nzenml model-deployer register seldon_deployer --flavor=seldon \\\n  --kubernetes_context=<KUBERNETES-CONTEXT> \\\n  --kubernetes_namespace=<KUBERNETES-NAMESPACE> \\\n  --base_url=http://$INGRESS_HOST \\\n```\n\nWe can now use the model deployer in our stack.\n\n```bash\nzenml stack update seldon_stack --model-deployer=seldon_deployer\n```\n\nSee the [seldon\\_model\\_deployer\\_step](https"}
{"input": "://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-seldon/#zenml.integrations.seldon.steps.seldon\\_deployer.seldon\\_model\\_deployer\\_step) for an example of using the Seldon Core Model Deployer to deploy a model inside a ZenML pipeline step.\n\n### Configuration\n\nWithin the `SeldonDeploymentConfig` you can configure:\n\n* `model_name`: the name of the model in the Seldon cluster and in ZenML.\n* `replicas`: the number of replicas with which to deploy the model\n* `implementation`: the type of Seldon inference server to use for the model. The implementation type can be one of the following: `TENSORFLOW_SERVER`, `SKLEARN_SERVER`, `XGBOOST_SERVER`, `custom`.\n* `parameters`: an optional list of parameters (`SeldonDeploymentPredictorParameter`) to pass to the deployment predictor in the form of:\n  * `name`\n  * `type`\n  * `value`\n* `resources`: the resources to be allocated to the model. This can be configured by passing a `SeldonResourceRequirements` object with the `requests` and `limits` properties. The values for these properties can be a dictionary with the `cpu` and `memory` keys. The values for these keys can be a string with the amount of CPU and memory to be allocated to the model.\n* `serviceAccount` The name of the Service Account applied to the deployment.\n\nFor more information and a full list of configurable attributes of the Seldon Core Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-seldon/#zenml.integrations.seldon.model\\_deployers) .\n\n### Custom Code Deployment\n\nZenML enables you to deploy your pre- and post-processing code into the deployment environment together with the model by defining a custom predict function that will be wrapped in a Docker container and executed on the model deployment server, e.g.:\n\n```python\ndef custom_predict(\n    model: Any,\n    request: Array_Like,\n) -> Array_Like:\n    \"\"\"Custom Prediction function.\n\n    The custom predict function is the core of the custom deployment, the \n    function is called by the custom deployment class defined for the serving \n    tool. The current implementation requires the function to get the model \n    loaded in the memory and a request with the data to predict.\n\n    Args:\n        model: The model to use for prediction.\n        request: The prediction response of the model is an array-like format.\n    Returns:\n        The prediction in an array-like format.\n    \"\"\"\n    inputs = []\n    for instance in request:\n        input = np.array(instance)\n        if not isinstance(input, np.ndarray):\n            raise Exception(\"The request must be a NumPy array\")\n        processed_input = pre_process(input)\n        prediction = model.predict(process"}
{"input": "ed_input)\n        postprocessed_prediction = post_process(prediction)\n        inputs.append(postprocessed_prediction)\n    return inputs\n\n\ndef pre_process(input: np.ndarray) -> np.ndarray:\n    \"\"\"Pre process the data to be used for prediction.\"\"\"\n    input = input / 255.0\n    return input[None, :, :]\n\n\ndef post_process(prediction: np.ndarray) -> str:\n    \"\"\"Pre process the data\"\"\"\n    classes = [str(i) for i in range(10)]\n    prediction = tf.nn.softmax(prediction, axis=-1)\n    maxindex = np.argmax(prediction.numpy())\n    return classes[maxindex]\n```\n\n{% hint style=\"info\" %}\nThe custom predict function should get the model and the input data as arguments and return the model predictions. ZenML will automatically take care of loading the model into memory and starting the `seldon-core-microservice` that will be responsible for serving the model and running the predict function.\n{% endhint %}\n\nAfter defining your custom predict function in code, you can use the `seldon_custom_model_deployer_step` to automatically build your function into a Docker image and deploy it as a model server by setting the `predict_function` argument to the path of your `custom_predict` function:\n\n```python\nfrom zenml.integrations.seldon.steps import seldon_custom_model_deployer_step\nfrom zenml.integrations.seldon.services import SeldonDeploymentConfig\nfrom zenml import pipeline\n\n@pipeline\ndef seldon_deployment_pipeline():\n    model = ...\n    seldon_custom_model_deployer_step(\n        model=model,\n        predict_function=\"<PATH.TO.custom_predict>\",  # TODO: path to custom code\n        service_config=SeldonDeploymentConfig(\n            model_name=\"<MODEL_NAME>\",  # TODO: name of the deployed model\n            replicas=1,\n            implementation=\"custom\",\n            resources=SeldonResourceRequirements(\n                limits={\"cpu\": \"200m\", \"memory\": \"250Mi\"}\n            ),\n            serviceAccountName=\"kubernetes-service-account\",\n        ),\n    )\n```\n\n#### Advanced Custom Code Deployment with Seldon Core Integration\n\n{% hint style=\"warning\" %}\nBefore creating your custom model class, you should take a look at the [custom Python model](https://docs.seldon.io/projects/seldon-core/en/latest/python/python\\_wrapping\\_docker.html) section of the Seldon Core documentation.\n{% endhint %}\n\nThe built-in Seldon Core custom deployment step is a good starting point for deploying your custom models. However, if you want to deploy more than the trained model, you can create your own custom class and a custom step to achieve this.\n\nSee the [ZenML custom Seldon model class](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-seldon/#zenml.integrations.seldon.custom_deployer.zenml_custom_model.ZenMLCustomModel) as a reference.\n\n<figure><img src=\"https://static.s"}
{"input": "carf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Deploying your models locally with MLflow.\n---\n\n# MLflow\n\nThe MLflow Model Deployer is one of the available flavors of the [Model Deployer](./model-deployers.md) stack component. Provided with the MLflow integration it can be used to deploy and manage [MLflow models](https://www.mlflow.org/docs/latest/python\\_api/mlflow.deployments.html) on a local running MLflow server.\n\n{% hint style=\"warning\" %}\nThe MLflow Model Deployer is not yet available for use in production. This is a work in progress and will be available soon. At the moment it is only available for use in a local development environment.\n{% endhint %}\n\n## When to use it?\n\nMLflow is a popular open-source platform for machine learning. It's a great tool for managing the entire lifecycle of your machine learning. One of the most important features of MLflow is the ability to package your model and its dependencies into a single artifact that can be deployed to a variety of deployment targets.\n\nYou should use the MLflow Model Deployer:\n\n* if you want to have an easy way to deploy your models locally and perform real-time predictions using the running MLflow prediction server.\n* if you are looking to deploy your models in a simple way without the need for a dedicated deployment environment like Kubernetes or advanced infrastructure configuration.\n\nIf you are looking to deploy your models in a more complex way, you should use one of the other [Model Deployer Flavors](./model-deployers.md#model-deployers-flavors) available in ZenML.\n\n## How do you deploy it?\n\nThe MLflow Model Deployer flavor is provided by the MLflow ZenML integration, so you need to install it on your local machine to be able to deploy your models. You can do this by running the following command:\n\n```bash\nzenml integration install mlflow -y\n```\n\nTo register the MLflow model deployer with ZenML you need to run the following command:\n\n```bash\nzenml model-deployer register mlflow_deployer --flavor=mlflow\n```\n\nThe ZenML integration will provision a local MLflow deployment server as a daemon process that will continue to run in the background to serve the latest MLflow model.\n\n## How do you use it?\n\n### Deploy a logged model\n\nFollowing [MLflow's documentation](https://mlflow.org/docs/latest/deployment/deploy-model-locally.html#deploy-mlflow-model-as-a-local-inference-server), if we want to deploy a model as a local inference server, we need the model to be logged in the MLflow experiment tracker first. Once the model is logged, we can use the model URI either from the artifact path saved with the MLflow run or using model name and version if a model is registered in the MLflow model registry.\n\nIn the following examples, we will show how to deploy a model using the MLflow Model Deploy"}
{"input": "er, in two different scenarios:\n\n1. We already know the logged model URI and we want to deploy it as a local inference server.\n\n```python\nfrom zenml import pipeline, step, get_step_context\nfrom zenml.client import Client\n\n@step\ndef deploy_model() -> Optional[MLFlowDeploymentService]:\n    # Deploy a model using the MLflow Model Deployer\n    zenml_client = Client()\n    model_deployer = zenml_client.active_stack.model_deployer\n    mlflow_deployment_config = MLFlowDeploymentConfig(\n        name: str = \"mlflow-model-deployment-example\",\n        description: str = \"An example of deploying a model using the MLflow Model Deployer\",\n        pipeline_name: str = get_step_context().pipeline_name,\n        pipeline_step_name: str = get_step_context().step_name,\n        model_uri: str = \"runs:/<run_id>/model\" or \"models:/<model_name>/<model_version>\",\n        model_name: str = \"model\",\n        workers: int = 1\n        mlserver: bool = False\n        timeout: int = DEFAULT_SERVICE_START_STOP_TIMEOUT\n    )\n    service = model_deployer.deploy_model(\n        config=mlflow_deployment_config, \n        service_type=MLFlowDeploymentService.SERVICE_TYPE\n    )\n    logger.info(f\"The deployed service info: {model_deployer.get_model_server_info(service)}\")\n    return service\n```\n\n2. We don't know the logged model URI, since the model was logged in a previous step. We want to deploy the model as a local inference server. ZenML provides set of functionalities that would make it easier to get the model URI from the current run and deploy it.\n\n```python\nfrom zenml import pipeline, step, get_step_context\nfrom zenml.client import Client\nfrom mlflow.tracking import MlflowClient, artifact_utils\n\n\n@step\ndef deploy_model() -> Optional[MLFlowDeploymentService]:\n    # Deploy a model using the MLflow Model Deployer\n    zenml_client = Client()\n    model_deployer = zenml_client.active_stack.model_deployer\n    experiment_tracker = zenml_client.active_stack.experiment_tracker\n    # Let's get the run id of the current pipeline\n    mlflow_run_id = experiment_tracker.get_run_id(\n        experiment_name=get_step_context().pipeline_name,\n        run_name=get_step_context().run_name,\n    )\n    # Once we have the run id, we can get the model URI using mlflow client\n    experiment_tracker.configure_mlflow()\n    client = MlflowClient()\n    model_name = \"model\" # set the model name that was logged\n    model_uri = artifact_utils.get_artifact_uri(\n        run_id=mlflow_run_id, artifact_path=model_name\n    )\n    mlflow_deployment_config = MLFlowDeploymentConfig(\n        name: str = \"mlflow-model-deployment-example\",\n       "}
{"input": " description: str = \"An example of deploying a model using the MLflow Model Deployer\",\n        pipeline_name: str = get_step_context().pipeline_name,\n        pipeline_step_name: str = get_step_context().step_name,\n        model_uri: str = model_uri,\n        model_name: str = model_name,\n        workers: int = 1,\n        mlserver: bool = False,\n        timeout: int = 300,\n    )\n    service = model_deployer.deploy_model(\n        config=mlflow_deployment_config, \n        service_type=MLFlowDeploymentService.SERVICE_TYPE\n    )\n    return service\n```\n\n#### Configuration\n\nWithin the `MLFlowDeploymentService` you can configure:\n\n* `name`: The name of the deployment.\n* `description`: The description of the deployment.\n* `pipeline_name`: The name of the pipeline that deployed the MLflow prediction server.\n* `pipeline_step_name`: The name of the step that deployed the MLflow prediction server.\n* `model_name`: The name of the model that is deployed in case of model registry the name must be a valid registered model name.\n* `model_version`: The version of the model that is deployed in case of model registry the version must be a valid registered model version.\n* `silent_daemon`: set to True to suppress the output of the daemon (i.e., redirect stdout and stderr to /dev/null). If False, the daemon output will be redirected to a log file.\n* `blocking`: set to True to run the service in the context of the current process and block until the service is stopped instead of running the service as a daemon process. Useful for operating systems that do not support daemon processes.\n* `model_uri`: The URI of the model to be deployed. This can be a local file path, a run ID, or a model name and version.\n* `workers`: The number of workers to be used by the MLflow prediction server.\n* `mlserver`: If True, the MLflow prediction server will be started as a MLServer instance.\n* `timeout`: The timeout in seconds to wait for the MLflow prediction server to start or stop.\n\n### Run inference on a deployed model\n\nThe following code example shows how you can load a deployed model in Python and run inference against it:\n\n1. Load a prediction service deployed in another pipeline\n\n```python\nimport json\nimport requests\nfrom zenml import step\nfrom zenml.integrations.mlflow.model_deployers.mlflow_model_deployer import (\n    MLFlowModelDeployer,\n)\nfrom zenml.integrations.mlflow.services import MLFlowDeploymentService\n\n\n# Load a prediction service deployed in another pipeline\n@step(enable_cache=False)\ndef prediction_service_loader(\n    pipeline_name: str,\n    pipeline_step_name: str,\n    model_name: str = \"model\",\n) -> None:\n    \"\"\"Get the prediction service started by the deployment pipeline.\n\n    Args:\n        pipeline_name"}
{"input": ": name of the pipeline that deployed the MLflow prediction\n            server\n        step_name: the name of the step that deployed the MLflow prediction\n            server\n        running: when this flag is set, the step only returns a running service\n        model_name: the name of the model that is deployed\n    \"\"\"\n    # get the MLflow model deployer stack component\n    model_deployer = MLFlowModelDeployer.get_active_model_deployer()\n\n    # fetch existing services with same pipeline name, step name and model name\n    existing_services = model_deployer.find_model_server(\n        pipeline_name=pipeline_name,\n        pipeline_step_name=pipeline_step_name,\n        model_name=model_name,\n    )\n\n    if not existing_services:\n        raise RuntimeError(\n            f\"No MLflow prediction service deployed by step \"\n            f\"'{pipeline_step_name}' in pipeline '{pipeline_name}' with name \"\n            f\"'{model_name}' is currently running.\"\n        )\n\n    service = existing_services[0]\n\n    # Let's try run a inference request against the prediction service\n\n    payload = json.dumps(\n        {\n            \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"Tell a joke!\"}]},\n            \"params\": {\n                \"temperature\": 0.5,\n                \"max_tokens\": 20,\n            },\n        }\n    )\n    response = requests.post(\n        url=service.get_prediction_url(),\n        data=payload,\n        headers={\"Content-Type\": \"application/json\"},\n    )\n\n    response.json()\n```\n\n2. Within the same pipeline, use the service from previous step to run inference this time using pre-built predict method\n\n```python\nfrom typing_extensions import Annotated\nimport numpy as np\nfrom zenml import step\nfrom zenml.integrations.mlflow.services import MLFlowDeploymentService\n\n# Use the service for inference\n@step\ndef predictor(\n    service: MLFlowDeploymentService,\n    data: np.ndarray,\n) -> Annotated[np.ndarray, \"predictions\"]:\n    \"\"\"Run a inference request against a prediction service\"\"\"\n\n    prediction = service.predict(data)\n    prediction = prediction.argmax(axis=-1)\n\n    return prediction\n```\n\nFor more information and a full list of configurable attributes of the MLflow Model Deployer, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-mlflow/#zenml.integrations.mlflow.model\\_deployers) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom model deployer.\n---\n\n# Develop a Custom Model Deployer\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\nTo deploy and manage your trained machine-learning models, ZenML provides a stack component called `Model Deployer`. This component is responsible for interacting with the deployment tool, framework, or platform.\n\nWhen present in a stack, the model deployer can also act as a registry for models that are served with ZenML. You can use the model deployer to list all models that are currently deployed for online inference or filtered according to a particular pipeline run or step, or to suspend, resume or delete an external model server managed through ZenML.\n\n### Base Abstraction\n\nIn ZenML, the base abstraction of the model deployer is built on top of three major criteria:\n\n1. It needs to ensure efficient deployment and management of models in accordance with the specific requirements of the serving infrastructure, by holding all the stack-related configuration attributes required to interact with the remote model serving tool, service, or platform.\n2. It needs to implement the continuous deployment logic necessary to deploy models in a way that updates an existing model server that is already serving a previous version of the same model instead of creating a new model server for every new model version (see the `deploy_model` abstract method). This functionality can be consumed directly from ZenML pipeline steps, but it can also be used outside the pipeline to deploy ad-hoc models. It is also usually coupled with a standard model deployer step, implemented by each integration, that hides the details of the deployment process from the user.\n3. It needs to act as a ZenML BaseService registry, where every BaseService instance is used as an internal representation of a remote model server (see the `find_model_server` abstract method). To achieve this, it must be able to re-create the configuration of a BaseService from information that is persisted externally, alongside, or even as part of the remote model server configuration itself. For example, for model servers that are implemented as Kubernetes resources, the BaseService instances can be serialized and saved as Kubernetes resource annotations. This allows the model deployer to keep track of all externally running model servers and to re-create their corresponding BaseService instance representations at any given time. The model deployer also defines methods that implement basic life-cycle management on remote model servers outside the coverage of a pipeline (see `stop_model_server` , `start_model_server` and `delete_model_server`).\n\nPutting all these considerations together, we end up with the following interface:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Type\nfrom uuid"}
{"input": " import UUID\n\nfrom zenml.enums import StackComponentType\nfrom zenml.services import BaseService, ServiceConfig\nfrom zenml.stack import StackComponent, StackComponentConfig, Flavor\n\nDEFAULT_DEPLOYMENT_START_STOP_TIMEOUT = 300\n\n\nclass BaseModelDeployerConfig(StackComponentConfig):\n    \"\"\"Base class for all ZenML model deployer configurations.\"\"\"\n\n\nclass BaseModelDeployer(StackComponent, ABC):\n    \"\"\"Base class for all ZenML model deployers.\"\"\"\n\n    @abstractmethod\n    def perform_deploy_model(\n        self,\n        id: UUID,\n        config: ServiceConfig,\n        timeout: int = DEFAULT_DEPLOYMENT_START_STOP_TIMEOUT,\n    ) -> BaseService:\n        \"\"\"Abstract method to deploy a model.\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_model_server_info(\n            service: BaseService,\n    ) -> Dict[str, Optional[str]]:\n        \"\"\"Give implementation-specific way to extract relevant model server\n        properties for the user.\"\"\"\n\n    @abstractmethod\n    def perform_stop_model(\n        self,\n        service: BaseService,\n        timeout: int = DEFAULT_DEPLOYMENT_START_STOP_TIMEOUT,\n        force: bool = False,\n    ) -> BaseService:\n        \"\"\"Abstract method to stop a model server.\"\"\"\n\n    @abstractmethod\n    def perform_start_model(\n        self,\n        service: BaseService,\n        timeout: int = DEFAULT_DEPLOYMENT_START_STOP_TIMEOUT,\n    ) -> BaseService:\n        \"\"\"Abstract method to start a model server.\"\"\"\n\n    @abstractmethod\n    def perform_delete_model(\n        self,\n        service: BaseService,\n        timeout: int = DEFAULT_DEPLOYMENT_START_STOP_TIMEOUT,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Abstract method to delete a model server.\"\"\"\n\n\nclass BaseModelDeployerFlavor(Flavor):\n    \"\"\"Base class for model deployer flavors.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self):\n        \"\"\"Returns the name of the flavor.\"\"\"\n\n    @property\n    def type(self) -> StackComponentType:\n        \"\"\"Returns the flavor type.\n\n        Returns:\n            The flavor type.\n        \"\"\"\n        return StackComponentType.MODEL_DEPLOYER\n\n    @property\n    def config_class(self) -> Type[BaseModelDeployerConfig]:\n        \"\"\"Returns `BaseModelDeployerConfig` config class.\n\n        Returns:\n                The config class.\n        \"\"\"\n        return BaseModelDeployerConfig\n\n    @property\n    @abstractmethod\n    def implementation_class(self) -> Type[BaseModelDeployer]:\n        \"\"\"The class that implements the model deployer.\"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the abstraction layer. In order to see the full implementation and get the complete docstrings, please check the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-model\\_deployers/#zenml.model\\_deployers.base\\_model\\_deployer.BaseModelDeployer) .\n{% endhint %}\n\n### Building"}
{"input": " your own model deployers\n\nIf you want to create your own custom flavor for a model deployer, you can follow the following steps:\n\n1. Create a class that inherits from the `BaseModelDeployer` class and implements the abstract methods.\n2. If you need to provide any configuration, create a class that inherits from the `BaseModelDeployerConfig` class and add your configuration parameters.\n3. Bring both the implementation and the configuration together by inheriting from the `BaseModelDeployerFlavor` class. Make sure that you give a `name` to the flavor through its abstract property.\n4. Create a service class that inherits from the `BaseService` class and implements the abstract methods. This class will be used to represent the deployed model server in ZenML.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml model-deployer flavor register <path.to.MyModelDeployerFlavor>\n```\n\nFor example, if your flavor class `MyModelDeployerFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml model-deployer flavor register flavors.my_flavor.MyModelDeployerFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually, it's better to not have to rely on this mechanism and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml model-deployer flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomModelDeployerFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomModelDeployerConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomModelDeployer** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors"}
{"input": " and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomModelDeployerFlavor` and the `CustomModelDeployerConfig` are implemented in a different module/path than the actual `CustomModelDeployer`).\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Building container images for your ML workflow.\n---\n\n# Image Builders\n\nThe image builder is an essential part of most remote MLOps stacks. It is used to build container images such that your\nmachine-learning pipelines and steps can be executed in remote environments.\n\n### When to use it\n\nThe image builder is needed whenever other components of your stack need to build container images. Currently, this is\nthe case for most of ZenML's remote [orchestrators](../orchestrators/orchestrators.md)\n, [step operators](../step-operators/step-operators.md), and\nsome [model deployers](../model-deployers/model-deployers.md). These containerize your pipeline code and therefore\nrequire an image builder to build [Docker](https://www.docker.com/) images.\n\n### Image Builder Flavors\n\nOut of the box, ZenML comes with a `local` image builder that builds Docker images on your client machine. Additional\nimage builders are provided by integrations:\n\n| Image Builder                      | Flavor   | Integration | Notes                                                                    |\n|------------------------------------|----------|-------------|--------------------------------------------------------------------------|\n| [LocalImageBuilder](local.md)      | `local`  | _built-in_  | Builds your Docker images locally.                                       |\n| [KanikoImageBuilder](kaniko.md)    | `kaniko` | `kaniko`    | Builds your Docker images in Kubernetes using Kaniko.                    |\n| [GCPImageBuilder](gcp.md)          | `gcp`    | `gcp`       | Builds your Docker images using Google Cloud Build.                      |\n| [Custom Implementation](custom.md) | _custom_ |             | Extend the image builder abstraction and provide your own implementation |\n\nIf you would like to see the available flavors of image builders, you can use the command:\n\n```shell\nzenml image-builder flavor list\n```\n\n### How to use it\n\nYou don't need to directly interact with any image builder in your code. As long as the image builder that you want to\nuse is part of your active [ZenML stack](/docs/book/user-guide/production-guide/understand-stacks.md), it will be used\nautomatically by any component that needs to build container images.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Building container images with Google Cloud Build\n---\n\n# Google Cloud Image Builder\n\nThe Google Cloud image builder is an [image builder](./image-builders.md) flavor provided by the ZenML `gcp` integration that uses [Google Cloud Build](https://cloud.google.com/build) to build container images.\n\n### When to use it\n\nYou should use the Google Cloud image builder if:\n\n* you're **unable** to install or use [Docker](https://www.docker.com) on your client machine.\n* you're already using GCP.\n* your stack is mainly composed of other Google Cloud components such as the [GCS Artifact Store](../artifact-stores/gcp.md) or the [Vertex Orchestrator](../orchestrators/vertex.md).\n\n### How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding the Google Cloud image builder? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML GCP Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nIn order to use the ZenML Google Cloud image builder you need to enable Google Cloud Build relevant APIs on the Google Cloud project.\n\n### How to use it\n\nTo use the Google Cloud image builder, we need:\n\n*   The ZenML `gcp` integration installed. If you haven't done so, run:\n\n    ```shell\n    zenml integration install gcp\n    ```\n* A [GCP Artifact Store](../artifact-stores/gcp.md) where the build context will be uploaded, so Google Cloud Build can access it.\n* A [GCP container registry](../container-registries/gcp.md) where the built image will be pushed.\n* Optionally, the GCP project ID in which you want to run the build and a service account with the needed permissions to run the build. If not provided, then the project ID and credentials will be inferred from the environment.\n* Optionally, you can change:\n  * the Docker image used by Google Cloud Build to execute the steps to build and push the Docker image. By default, the builder image will be `'gcr.io/cloud-builders/docker'`.\n  * The network to which the container used to build the ZenML pipeline Docker image will be attached. More information: [Cloud build network](https://cloud.google.com/build/docs/build-config-file-schema#network).\n  * The build timeout for the build, and for the blocking operation waiting for the build to finish. More information: [Build Timeout](https://cloud.google.com/build/docs/build-config-file-schema#timeout\\_2).\n\nWe can"}
{"input": " register the image builder and use it in our active stack:\n\n```shell\nzenml image-builder register <IMAGE_BUILDER_NAME> \\\n    --flavor=gcp \\\n    --cloud_builder_image=<BUILDER_IMAGE_NAME> \\\n    --network=<DOCKER_NETWORK> \\\n    --build_timeout=<BUILD_TIMEOUT_IN_SECONDS>\n\n# Register and activate a stack with the new image builder\nzenml stack register <STACK_NAME> -i <IMAGE_BUILDER_NAME> ... --set\n```\n\nYou also need to set up [authentication](gcp.md#authentication-methods) required to access the Cloud Build GCP services.\n\n#### Authentication Methods\n\nIntegrating and using a GCP Image Builder in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Local Authentication_ method. However, the recommended way to authenticate to the GCP cloud platform is through [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the GCP Image Builder with other remote stack components also running in GCP.\n\n{% tabs %}\n{% tab title=\"Implicit Authentication\" %}\nThis method uses the implicit GCP authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure a GCP Image Builder. You don't need to supply credentials explicitly when you register the GCP Image Builder, as it leverages the local credentials and configuration that the Google Cloud CLI stores on your local machine. However, you will need to install and set up the Google Cloud CLI on your machine as a prerequisite, as covered in [the Google Cloud documentation](https://cloud.google.com/sdk/docs/install-sdk) , before you register the GCP Image Builder.\n\n{% hint style=\"warning\" %}\nStacks using the GCP Image Builder set up with local authentication are not portable across environments. To make ZenML pipelines fully portable, it is recommended to use [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) to authenticate your GCP Image Builder to the GCP cloud platform.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"GCP Service Connector (recommended)\" %}\nTo set up the GCP Image Builder to authenticate to GCP and access the GCP Cloud Build services, it is recommended to leverage the many features provided by [the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) such as auto-configuration, best security practices regarding long-lived credentials and reusing the same credentials across multiple stack components.\n\nIf you don't already have a GCP Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You also have the option to configure a GCP Service Connector that can be used to access more than just the GCP Cloud Build service:\n\n```"}
{"input": "sh\nzenml service-connector register --type gcp -i\n```\n\nA non-interactive CLI example that leverages [the Google Cloud CLI configuration](https://cloud.google.com/sdk/docs/install-sdk) on your local machine to auto-configure a GCP Service Connector for the GCP Cloud Build service:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type gcp --resource-type gcp-generic --resource-name <GCS_BUCKET_NAME> --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register gcp-generic --type gcp --resource-type gcp-generic --auto-configure\nSuccessfully registered service connector `gcp-generic` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n> **Note**: Please remember to grant the entity associated with your GCP credentials permissions to access the Cloud Build API and to run Cloud Builder jobs (e.g. the [Cloud Build Editor IAM role](https://cloud.google.com/build/docs/iam-roles-permissions#predefined\\_roles)). The GCP Service Connector supports [many different authentication methods](../../how-to/auth-management/gcp-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use case.\n\nIf you already have one or more GCP Service Connectors configured in your ZenML deployment, you can check which of them can be used to access generic GCP resources like the GCP Image Builder required for your GCP Image Builder by running e.g.:\n\n```sh\nzenml service-connector list-resources --resource-type gcp-generic\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'gcp-generic' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bfdb657d-d808-47e7-9974-9ba6e4919d83 \u2502 gcp-generic"}
{"input": "    \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on a GCP Service Connector to use to authenticate to GCP, you can register the GCP Image Builder as follows:\n\n```sh\nzenml image-builder register <IMAGE_BUILDER_NAME> \\\n    --flavor=gcp \\\n    --cloud_builder_image=<BUILDER_IMAGE_NAME> \\\n    --network=<DOCKER_NETWORK> \\\n    --build_timeout=<BUILD_TIMEOUT_IN_SECONDS>\n\n# Connect the GCP Image Builder to GCP via a GCP Service Connector\nzenml image-builder connect <IMAGE_BUILDER_NAME> -i\n```\n\nA non-interactive version that connects the GCP Image Builder to a target GCP Service Connector:\n\n```sh\nzenml image-builder connect <IMAGE_BUILDER_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml image-builder connect gcp-image-builder --connector gcp-generic\nSuccessfully connected image builder `gcp-image-builder` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE  \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 bfdb657d-d808-47e7-9974-9ba6e4919d83 \u2502 gcp-generic    \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udd35 gcp-generic \u2502 zenml-core     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the GCP Image Builder in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new image builder\nzenml stack register <STACK_NAME> -i <IMAGE_BUILDER_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"GCP Credentials\" %}\nWhen"}
{"input": " you register the GCP Image Builder, you can [generate a GCP Service Account Key](https://cloud.google.com/docs/authentication/application-default-credentials#attached-sa), save it to a local file and then reference it in the Image Builder configuration.\n\nThis method has the advantage that you don't need to install and configure the GCP CLI on your host, but it's still not as secure as using a GCP Service Connector and the stack component configuration is not portable to other hosts.\n\nFor this method, you need to [create a user-managed GCP service account](https://cloud.google.com/iam/docs/service-accounts-create), and grant it privileges to access the Cloud Build API and to run Cloud Builder jobs (e.g. the [Cloud Build Editor IAM role](https://cloud.google.com/build/docs/iam-roles-permissions#predefined\\_roles).\n\nWith the service account key downloaded to a local file, you can register the GCP Image Builder as follows:\n\n```shell\nzenml image-builder register <IMAGE_BUILDER_NAME> \\\n    --flavor=gcp \\\n    --project=<GCP_PROJECT_ID> \\\n    --service_account_path=<PATH_TO_SERVICE_ACCOUNT_KEY> \\\n    --cloud_builder_image=<BUILDER_IMAGE_NAME> \\\n    --network=<DOCKER_NETWORK> \\\n    --build_timeout=<BUILD_TIMEOUT_IN_SECONDS>\n\n# Register and set a stack with the new image builder\nzenml stack register <STACK_NAME> -i <IMAGE_BUILDER_NAME> ... --set\n```\n{% endtab %}\n{% endtabs %}\n\n### Caveats\n\nAs described in this [Google Cloud Build documentation page](https://cloud.google.com/build/docs/build-config-file-schema#network), Google Cloud Build uses containers to execute the build steps which are automatically attached to a network called `cloudbuild` that provides some Application Default Credentials (ADC), that allow the container to be authenticated and therefore use other GCP services.\n\nBy default, the GCP Image Builder is executing the build command of the ZenML Pipeline Docker image with the option `--network=cloudbuild`, so the ADC provided by the `cloudbuild` network can also be used in the build. This is useful if you want to install a private dependency from a GCP Artifact Registry, but you will also need to use a [custom base parent image](../../how-to/customize-docker-builds/docker-settings-on-a-pipeline.md) with the [`keyrings.google-artifactregistry-auth`](https://pypi.org/project/keyrings.google-artifactregistry-auth/) installed, so `pip` can connect and authenticate in the private artifact registry to download the dependency.\n\n```dockerfile\nFROM zenmldocker/zenml:latest\n\nRUN pip install keyrings.google-artifactregistry-auth\n```\n\n{% hint style=\"warning\" %}\nThe above `Dockerfile` uses `zenmldocker/zenml:latest` as a"}
{"input": " base image, but is recommended to change the tag to specify the ZenML version and Python version like `0.33.0-py3.10`.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom image builder.\n---\n\n# Develop a Custom Image Builder\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n### Base Abstraction\n\nThe `BaseImageBuilder` is the abstract base class that needs to be subclassed in order to create a custom component that can be used to build Docker images. As image builders can come in many shapes and forms, the base class exposes a deliberately basic and generic interface:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Optional, Type, cast\n\nfrom zenml.container_registries import BaseContainerRegistry\nfrom zenml.enums import StackComponentType\nfrom zenml.image_builders import BuildContext\nfrom zenml.stack import Flavor, StackComponent\nfrom zenml.stack.stack_component import StackComponentConfig\n\n\nclass BaseImageBuilder(StackComponent, ABC):\n    \"\"\"Base class for all ZenML image builders.\"\"\"\n\n    @property\n    def build_context_class(self) -> Type[\"BuildContext\"]:\n        \"\"\"Build context class to use.\n\n        The default build context class creates a build context that works\n        for the Docker daemon. Override this method if your image builder\n        requires a custom context.\n\n        Returns:\n            The build context class.\n        \"\"\"\n        return BuildContext\n\n    @abstractmethod\n    def build(\n            self,\n            image_name: str,\n            build_context: \"BuildContext\",\n            docker_build_options: Dict[str, Any],\n            container_registry: Optional[\"BaseContainerRegistry\"] = None,\n    ) -> str:\n        \"\"\"Builds a Docker image.\n\n        If a container registry is passed, the image will be pushed to that\n        registry.\n\n        Args:\n            image_name: Name of the image to build.\n            build_context: The build context to use for the image.\n            docker_build_options: Docker build options.\n            container_registry: Optional container registry to push to.\n\n        Returns:\n            The Docker image repo digest or name.\n        \"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the abstraction layer. In order to see the full implementation and get the complete docstrings, please check [the source code on GitHub](https://github.com/zenml-io/zenml/blob/main/src/zenml/image\\_builders/base\\_image\\_builder.py) .\n{% endhint %}\n\n### Build your own custom image builder\n\nIf you want to create your own custom flavor for an image builder, you can follow the following steps:\n\n1. Create a class that inherits from the `BaseImageBuilder` class and implement the abstract `build` method. This method"}
{"input": " should use the given build context and build a Docker image with it. If additionally a container registry is passed to the `build` method, the image builder is also responsible for pushing the image there.\n2. If you need to provide any configuration, create a class that inherits from the `BaseImageBuilderConfig` class and adds your configuration parameters.\n3. Bring both the implementation and the configuration together by inheriting from the `BaseImageBuilderFlavor` class. Make sure that you give a `name` to the flavor through its abstract property.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml image-builder flavor register <path.to.MyImageBuilderFlavor>\n```\n\nFor example, if your flavor class `MyImageBuilderFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml image-builder flavor register flavors.my_flavor.MyImageBuilderFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually it's better to not have to rely on this mechanism, and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml image-builder flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomImageBuilderFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomImageBuilderConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomImageBuilder** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomImageBuilderFlavor` and the `CustomImageBuilderConfig` are implemented in a different module/path than the actual `CustomImageBuilder`).\n{% endhint %}\n\n#### Using a custom"}
{"input": "-build context\n\nThe `BaseImageBuilder` abstraction uses the `build_context_class` to provide a class that should be used as the build context. In case your custom image builder requires a different build context than the default Docker build context, you can subclass the `BuildContext` class to customize the structure of your build context. In your image builder implementation, you can then overwrite the `build_context_class` property to specify your build context subclass.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Building container images locally.\n---\n\n# Local Image Builder\n\nThe local image builder is an [image builder](./image-builders.md) flavor that comes built-in with ZenML and uses the local Docker installation on your client machine to build container images.\n\n{% hint style=\"info\" %}\nZenML uses the official Docker Python library to build and push your images. This library loads its authentication credentials to push images from the default config location: `$HOME/.docker/config.json`. If your Docker configuration is stored in a different directory, you can use the environment variable `DOCKER_CONFIG` to override this behavior:\n\n```shell\nexport DOCKER_CONFIG=/path/to/config_dir\n```\n\nThe directory that you specify here must contain your Docker configuration in a file called `config.json`.\n{% endhint %}\n\n### When to use it\n\nYou should use the local image builder if:\n\n* you're able to install and use [Docker](https://www.docker.com) on your client machine.\n* you want to use remote components that require containerization without the additional hassle of configuring infrastructure for an additional component.\n\n### How to deploy it\n\nThe local image builder comes with ZenML and works without any additional setup.\n\n### How to use it\n\nTo use the Local image builder, we need:\n\n* [Docker](https://www.docker.com) installed and running.\n* The Docker client authenticated to push to the container registry that you intend to use in the same stack.\n\nWe can then register the image builder and use it to create a new stack:\n\n```shell\nzenml image-builder register <NAME> --flavor=local\n\n# Register and activate a stack with the new image builder\nzenml stack register <STACK_NAME> -i <NAME> ... --set\n```\n\nFor more information and a full list of configurable attributes of the local image builder, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-image\\_builders/#zenml.image\\_builders.local\\_image\\_builder.LocalImageBuilder) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Building container images with Kaniko.\n---\n\n# Kaniko Image Builder\n\nThe Kaniko image builder is an [image builder](./image-builders.md) flavor provided by the ZenML `kaniko` integration that uses [Kaniko](https://github.com/GoogleContainerTools/kaniko) to build container images.\n\n### When to use it\n\nYou should use the Kaniko image builder if:\n\n* you're **unable** to install or use [Docker](https://www.docker.com) on your client machine.\n* you're familiar with/already using Kubernetes.\n\n### How to deploy it\n\nIn order to use the Kaniko image builder, you need a deployed Kubernetes cluster.\n\n### How to use it\n\nTo use the Kaniko image builder, we need:\n\n*   The ZenML `kaniko` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install kaniko\n    ```\n* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) installed.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* By default, the Kaniko image builder transfers the build context using the Kubernetes API. If you instead want to transfer the build context by storing it in the artifact store, you need to register it with the `store_context_in_artifact_store` attribute set to `True`. In this case, you also need a [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* Optionally, you can change the timeout (in seconds) until the Kaniko pod is running in the orchestrator using the `pod_running_timeout` attribute.\n\nWe can then register the image builder and use it in our active stack:\n\n```shell\nzenml image-builder register <NAME> \\\n    --flavor=kaniko \\\n    --kubernetes_context=<KUBERNETES_CONTEXT>\n    [ --pod_running_timeout=<POD_RUNNING_TIMEOUT_IN_SECONDS> ]\n\n# Register and activate a stack with the new image builder\nzenml stack register <STACK_NAME> -i <NAME> ... --set\n```\n\nFor more information and a full list of configurable attributes of the Kaniko image builder, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kaniko/#zenml.integrations.kaniko.image\\_builders.kaniko\\_image\\_builder.KanikoImageBuilder) .\n\n#### Authentication for the container registry and artifact store\n\nThe Kaniko image builder will create a Kubernetes pod that is running the build. This build pod needs to be able to pull from/push to certain container registries, and depending on the stack component configuration also needs to be able to read from the artifact store:\n\n* The pod needs to be authenticated to push to the container registry in your active stack.\n*"}
{"input": " In case the [parent image](../../how-to/customize-docker-builds/docker-settings-on-a-pipeline.md#using-a-custom-parent-image) you use in your `DockerSettings` is stored in a private registry, the pod needs to be authenticated to pull from this registry.\n* If you configured your image builder to store the build context in the artifact store, the pod needs to be authenticated to read files from the artifact store storage.\n\nZenML is not yet able to handle setting all of the credentials of the various combinations of container registries and artifact stores on the Kaniko build pod, which is you're required to set this up yourself for now. The following section outlines how to handle it in the most straightforward (and probably also most common) scenario, when the Kubernetes cluster you're using for the Kaniko build is hosted on the same cloud provider as your container registry (and potentially the artifact store). For all other cases, check out the [official Kaniko repository](https://github.com/GoogleContainerTools/kaniko) for more information.\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n* Add permissions to push to ECR by attaching the `EC2InstanceProfileForImageBuilderECRContainerBuilds` policy to your [EKS node IAM role](https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html).\n* Configure the image builder to set some required environment variables on the Kaniko build pod:\n\n```shell\n# register a new image builder with the environment variables\nzenml image-builder register <NAME> \\\n    --flavor=kaniko \\\n    --kubernetes_context=<KUBERNETES_CONTEXT> \\\n    --env='[{\"name\": \"AWS_SDK_LOAD_CONFIG\", \"value\": \"true\"}, {\"name\": \"AWS_EC2_METADATA_DISABLED\", \"value\": \"true\"}]'\n\n# or update an existing one\nzenml image-builder update <NAME> \\\n    --env='[{\"name\": \"AWS_SDK_LOAD_CONFIG\", \"value\": \"true\"}, {\"name\": \"AWS_EC2_METADATA_DISABLED\", \"value\": \"true\"}]'\n```\n\nCheck out [the Kaniko docs](https://github.com/GoogleContainerTools/kaniko#pushing-to-amazon-ecr) for more information.\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n* [Enable workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable\\_on\\_cluster) for your cluster\n* Follow the steps described [here](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#authenticating\\_to) to create a Google service account, a Kubernetes service account as well as an IAM policy binding between them.\n* Grant the Google service account permissions to push to your GCR registry and read from your GCP bucket.\n* Configure the image builder to run in the correct namespace and use the correct service account:\n\n```"}
{"input": "shell\n# register a new image builder with namespace and service account\nzenml image-builder register <NAME> \\\n    --flavor=kaniko \\\n    --kubernetes_context=<KUBERNETES_CONTEXT> \\\n    --kubernetes_namespace=<KUBERNETES_NAMESPACE> \\\n    --service_account_name=<KUBERNETES_SERVICE_ACCOUNT_NAME>\n    # --executor_args='[\"--compressed-caching=false\", \"--use-new-run=true\"]'\n\n# or update an existing one\nzenml image-builder update <NAME> \\\n    --kubernetes_namespace=<KUBERNETES_NAMESPACE> \\\n    --service_account_name=<KUBERNETES_SERVICE_ACCOUNT_NAME>\n```\n\nCheck out [the Kaniko docs](https://github.com/GoogleContainerTools/kaniko#pushing-to-google-gcr) for more information.\n{% endtab %}\n\n{% tab title=\"Azure\" %}\n* Create a Kubernetes `configmap` for a Docker config that uses the Azure credentials helper:\n\n```shell\nkubectl create configmap docker-config --from-literal='config.json={ \"credHelpers\": { \"mycr.azurecr.io\": \"acr-env\" } }'\n```\n\n* Follow [these steps](https://learn.microsoft.com/en-us/azure/aks/use-managed-identity) to configure your cluster to use a managed identity\n* Configure the image builder to mount the `configmap` in the Kaniko build pod:\n\n```shell\n# register a new image builder with the mounted configmap\nzenml image-builder register <NAME> \\\n    --flavor=kaniko \\\n    --kubernetes_context=<KUBERNETES_CONTEXT> \\\n    --volume_mounts='[{\"name\": \"docker-config\", \"mountPath\": \"/kaniko/.docker/\"}]' \\\n    --volumes='[{\"name\": \"docker-config\", \"configMap\": {\"name\": \"docker-config\"}}]'\n    # --executor_args='[\"--compressed-caching=false\", \"--use-new-run=true\"]'\n\n# or update an existing one\nzenml image-builder update <NAME> \\\n    --volume_mounts='[{\"name\": \"docker-config\", \"mountPath\": \"/kaniko/.docker/\"}]' \\\n    --volumes='[{\"name\": \"docker-config\", \"configMap\": {\"name\": \"docker-config\"}}]'\n```\n\nCheck out [the Kaniko docs](https://github.com/GoogleContainerTools/kaniko#pushing-to-azure-container-registry) for more information.\n{% endtab %}\n{% endtabs %}\n\n#### Passing additional parameters to the Kaniko build\n\nYou can pass additional parameters to the Kaniko build by setting the `executor_args` attribute of the image builder.\n\n```shell\nzenml image-builder register <NAME> \\\n    --flavor=kaniko \\\n    --kubernetes_context=<KUBERNETES_CONTEXT> \\\n    --executor_args='[\"--label\", \""}
{"input": "key=value\"]' # Adds a label to the final image\n```\n\nList of some possible additional flags:\n\n* `--cache`: Set to `false` to disable caching. Defaults to `true`.\n* `--cache-dir`: Set the directory where to store cached layers. Defaults to `/cache`.\n* `--cache-repo`: Set the repository where to store cached layers. Defaults to `gcr.io/kaniko-project/executor`.\n* `--cache-ttl`: Set the cache expiration time. Defaults to `24h`.\n* `--cleanup`: Set to `false` to disable cleanup of the working directory. Defaults to `true`.\n* `--compressed-caching`: Set to `false` to disable compressed caching. Defaults to `true`.\n\nFor a full list of possible flags, check out the [Kaniko additional flags](https://github.com/GoogleContainerTools/kaniko#additional-flags)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing container images locally.\n---\n\n# Default Container Registry\n\nThe Default container registry is a [container registry](./container-registries.md) flavor that comes built-in with ZenML and allows container registry URIs of any format.\n\n### When to use it\n\nYou should use the Default container registry if you want to use a **local** container registry or when using a remote container registry that is not covered by other [container registry flavors](./container-registries.md#container-registry-flavors).\n\n### Local registry URI format\n\nTo specify a URI for a local container registry, use the following format:\n\n```shell\nlocalhost:<PORT>\n\n# Examples:\nlocalhost:5000\nlocalhost:8000\nlocalhost:9999\n```\n\n### How to use it\n\nTo use the Default container registry, we need:\n\n* [Docker](https://www.docker.com) installed and running.\n* The registry URI. If you're using a local container registry, check out\n* the [previous section](default.md#local-registry-uri-format) on the URI format.\n\nWe can then register the container registry and use it in our active stack:\n\n```shell\nzenml container-registry register <NAME> \\\n    --flavor=default \\\n    --uri=<REGISTRY_URI>\n\n# Add the container registry to the active stack\nzenml stack update -c <NAME>\n```\n\nYou may also need to set up [authentication](default.md#authentication-methods) required to log in to the container registry.\n\n#### Authentication Methods\n\nIf you are using a private container registry, you will need to configure some form of authentication to login to the registry. If you're looking for a quick way to get started locally, you can use the _Local Authentication_ method. However, the recommended way to authenticate to a remote private container registry is through [a Docker Service Connector](../../how-to/auth-management/docker-service-connector.md).\n\nIf your target private container registry comes from a cloud provider like AWS, GCP or Azure, you should use the [container registry flavor](./container-registries.md#container-registry-flavors) targeted at that cloud provider. For example, if you're using AWS, you should use the [AWS Container Registry](aws.md) flavor. These cloud provider flavors also use specialized cloud provider Service Connectors to authenticate to the container registry.\n\n{% tabs %}\n{% tab title=\"Local Authentication\" %}\nThis method uses the Docker client authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure a Default Container Registry. You don't need to supply credentials explicitly when you register the Default Container Registry, as it leverages the local credentials and configuration that the Docker client stores on your local machine.\n\nTo log in to the container registry so Docker can pull and push images, you'll need to run the `docker login` command and supply your credentials, e.g.:\n\n"}
{"input": "```shell\ndocker login --username <USERNAME> --password-stdin <REGISTRY_URI>\n```\n\n{% hint style=\"warning\" %}\nStacks using the Default Container Registry set up with local authentication are not portable across environments. To make ZenML pipelines fully portable, it is recommended to use [a Docker Service Connector](../../how-to/auth-management/docker-service-connector.md) to link your Default Container Registry to the remote private container registry.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"Docker Service Connector (recommended)\" %}\nTo set up the Default Container Registry to authenticate to and access a private container registry, it is recommended to leverage the features provided by [the Docker Service Connector](../../how-to/auth-management/docker-service-connector.md) such as local login and reusing the same credentials across multiple stack components.\n\nIf you don't already have a Docker Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command:\n\n```sh\nzenml service-connector register --type docker -i\n```\n\nA non-interactive CLI example is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type docker --username=<USERNAME> --password=<PASSWORD_OR_TOKEN>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register dockerhub --type docker --username=username --password=password\nSuccessfully registered service connector `dockerhub` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 docker.io      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nIf you already have one or more Docker Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the container registry you want to use for your Default Container Registry by running e.g.:\n\n```sh\nzenml service-connector list-resources --connector-type docker --resource-id <REGISTRY_URI>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector list-resources --connector-type docker --resource-id docker.io\nThe  resource with name 'docker.io' can be accessed by 'docker' service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE     "}
{"input": " \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 cf55339f-dbc8-4ee6-862e-c25aff411292 \u2502 dockerhub      \u2502 \ud83d\udc33 docker      \u2502 \ud83d\udc33 docker-registry \u2502 docker.io      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on a Docker Service Connector to use to connect to the target container registry, you can register the Docker Container Registry as follows:\n\n```sh\n# Register the container registry and reference the target registry URI\nzenml container-registry register <CONTAINER_REGISTRY_NAME> -f default \\\n    --uri=<REGISTRY_URL>\n\n# Connect the container registry to the target registry via a Docker Service Connector\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> -i\n```\n\nA non-interactive version that connects the Default Container Registry to a target registry through a Docker Service Connector:\n\n```sh\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml container-registry connect dockerhub --connector dockerhub\nSuccessfully connected container registry `dockerhub` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 cf55339f-dbc8-4ee6-862e-c25aff411292 \u2502 dockerhub      \u2502 \ud83d\udc33 docker      \u2502 \ud83d\udc33 docker-registry \u2502 docker.io      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the Default Container Registry in a ZenML Stack:\n\n```sh\n# Register and set a"}
{"input": " stack with the new container registry\nzenml stack register <STACK_NAME> -c <CONTAINER_REGISTRY_NAME> ... --set\n```\n\n{% hint style=\"info\" %}\nLinking the Default Container Registry to a Service Connector means that your local Docker client is no longer authenticated to access the remote registry. If you need to manually interact with the remote registry via the Docker CLI, you can use the [local login Service Connector feature](../../how-to/auth-management/service-connectors-guide.md#configure-local-clients) to temporarily authenticate your local Docker client to the remote registry:\n\n```sh\nzenml service-connector login <CONNECTOR_NAME>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector login dockerhub\n\u2839 Attempting to configure local client using service connector 'dockerhub'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'dockerhub' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more information and a full list of configurable attributes of the Default container registry, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.container\\_registries.default\\_container\\_registry.DefaultContainerRegistry) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing container images in GitHub.\n---\n\n# GitHub Container Registry\n\nThe GitHub container registry is a [container registry](./container-registries.md) flavor that comes built-in with ZenML and uses the [GitHub Container Registry](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) to store container images.\n\n### When to use it\n\nYou should use the GitHub container registry if:\n\n* one or more components of your stack need to pull or push container images.\n* you're using GitHub for your projects. If you're not using GitHub, take a look at the other [container registry flavors](./container-registries.md#container-registry-flavors).\n\n### How to deploy it\n\nThe GitHub container registry is enabled by default when you create a GitHub account.\n\n### How to find the registry URI\n\nThe GitHub container registry URI should have the following format:\n\n```shell\nghcr.io/<USER_OR_ORGANIZATION_NAME>\n\n# Examples:\nghcr.io/zenml\nghcr.io/my-username\nghcr.io/my-organization\n```\n\nTo figure our the URI for your registry:\n\n* Use the GitHub user or organization name to fill the template `ghcr.io/<USER_OR_ORGANIZATION_NAME>` and get your URI.\n\n### How to use it\n\nTo use the GitHub container registry, we need:\n\n* [Docker](https://www.docker.com) installed and running.\n* The registry URI. Check out the [previous section](github.md#how-to-find-the-registry-uri) on the URI format and how to get the URI for your registry.\n* Our Docker client configured, so it can pull and push images. Follow [this guide](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to create a personal access token and login to the container registry.\n\nWe can then register the container registry and use it in our active stack:\n\n```shell\nzenml container-registry register <NAME> \\\n    --flavor=github \\\n    --uri=<REGISTRY_URI>\n\n# Add the container registry to the active stack\nzenml stack update -c <NAME>\n```\n\nFor more information and a full list of configurable attributes of the GitHub container registry, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-container\\_registries/#zenml.container\\_registries.github\\_container\\_registry.GitHubContainerRegistry) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing container images in GCP.\n---\n\n# Google Cloud Container Registry\n\nThe GCP container registry is a [container registry](./container-registries.md) flavor that comes built-in with ZenML and uses the [Google Artifact Registry](https://cloud.google.com/artifact-registry).\n\n{% hint style=\"warning\" %}\n**Important Notice: Google Container Registry** [**is being replaced by Artifact Registry**](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr)**. Please start using Artifact Registry for your containers. As per Google's documentation, \"after May 15, 2024, Artifact Registry will host images for the gcr.io domain in Google Cloud projects without previous Container Registry usage. After March 18, 2025, Container Registry will be shut down.\"** The terms `container registry` and `artifact registry` will be used interchangeably throughout this document.\n{% endhint %}\n\n### When to use it\n\nYou should use the GCP container registry if:\n\n* one or more components of your stack need to pull or push container images.\n* you have access to GCP. If you're not using GCP, take a look at the other [container registry flavors](./container-registries.md#container-registry-flavors).\n\n### How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding a Google Artifact Registry? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML GCP Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nWhen using the Google Artifact Registry, you need to:\n\n* enable it [here](https://console.cloud.google.com/marketplace/product/google/artifactregistry.googleapis.com)\n* go [here](https://console.cloud.google.com/artifacts) and create a `Docker` repository.\n\n### Infrastructure Deployment\n\nA GCP Container Registry can be deployed directly from the ZenML CLI:\n\n```shell\nzenml container-registry deploy gcp_container_registry --flavor=gcp --provider=gcp ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the [dedicated documentation section](../../how-to/stack-deployment/deploy-a-stack-using-mlstacks.md).\n\n## How to find the registry URI\n\nWhen using the Google Artifact Registry, the GCP container registry URI should have the following format:\n\n```shell\n<REGION>-docker.pkg.dev/<"}
{"input": "PROJECT_ID>/<REPOSITORY_NAME>\n\n# Examples:\neurope-west1-docker.pkg.dev/zenml/my-repo\nsouthamerica-east1-docker.pkg.dev/zenml/zenml-test\nasia-docker.pkg.dev/my-project/another-repo\n```\n\nTo figure out the URI for your registry:\n\n* Go [here](https://console.cloud.google.com/artifacts) and select the repository that you want to use to store Docker images. If you don't have a repository yet, take a look at the [deployment section](gcp.md#how-to-deploy-it).\n* On the top, click the copy button to copy the full repository URL.\n\n#### Infrastructure Deployment\n\nA GCP Container Registry can be deployed directly from the ZenML CLI:\n\n```shell\nzenml container-registry deploy gcp_container_registry --flavor=gcp --provider=gcp ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n### How to use it\n\nTo use the GCP container registry, we need:\n\n* [Docker](https://www.docker.com) installed and running.\n* The registry URI. Check out the [previous section](gcp.md#how-to-find-the-registry-uri) on the URI format and how to get the URI for your registry.\n\nWe can then register the container registry and use it in our active stack:\n\n```shell\nzenml container-registry register <NAME> \\\n    --flavor=gcp \\\n    --uri=<REGISTRY_URI>\n\n# Add the container registry to the active stack\nzenml stack update -c <NAME>\n```\n\nYou also need to set up [authentication](gcp.md#authentication-methods) required to log in to the container registry.\n\n#### Authentication Methods\n\nIntegrating and using a GCP Container Registry in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Local Authentication_ method. However, the recommended way to authenticate to the GCP cloud platform is through [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the GCP Container Registry with other remote stack components also running in GCP.\n\n{% tabs %}\n{% tab title=\"Local Authentication\" %}\nThis method uses the Docker client authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure a GCP Container Registry. You don't need to supply credentials explicitly when you register the GCP Container Registry, as it leverages the local credentials and configuration that the GCP CLI and Docker client store on your local machine. However, you will need"}
{"input": " to install and set up the GCP CLI on your machine as a prerequisite, as covered in [the GCP CLI documentation](https://cloud.google.com/sdk/docs/install-sdk), before you register the GCP Container Registry.\n\nWith the GCP CLI installed and set up with credentials, we'll need to configure Docker, so it can pull and push images:\n\n*   for a Google Container Registry:\n\n    ```shell\n    gcloud auth configure-docker\n    ```\n*   for a Google Artifact Registry:\n\n    ```shell\n    gcloud auth configure-docker <REGION>-docker.pkg.dev\n    ```\n\n{% hint style=\"warning\" %}\nStacks using the GCP Container Registry set up with local authentication are not portable across environments. To make ZenML pipelines fully portable, it is recommended to use [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) to link your GCP Container Registry to the remote GCR registry.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"GCP Service Connector (recommended)\" %}\nTo set up the GCP Container Registry to authenticate to GCP and access a GCR registry, it is recommended to leverage the many features provided by [the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) such as auto-configuration, local login, best security practices regarding long-lived credentials and reusing the same credentials across multiple stack components.\n\n{% hint style=\"warning\" %}\nThe GCP Service Connector does not support the Google Artifact Registry yet. If you need to connect your GCP Container Registry to a Google Artifact Registry, you can use the _Local Authentication_ method instead.\n{% endhint %}\n\nIf you don't already have a GCP Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure a GCP Service Connector that can be used to access a GCR registry or even more than one type of GCP resource:\n\n```sh\nzenml service-connector register --type gcp -i\n```\n\nA non-interactive CLI example that leverages [the GCP CLI configuration](https://docs.gcp.amazon.com/cli/latest/userguide/getting-started-install.html) on your local machine to auto-configure a GCP Service Connector targeting a GCR registry is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type gcp --resource-type docker-registry --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register gcp-zenml-core --type gcp --resource-type docker-registry --auto-configure\n\u2838 Registering service connector 'gcp-zenml-core'...\nSuccessfully registered service connector `gcp-zenml-core` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 gcr.io/zenml-core                               \u2503\n\u2503                    \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                    \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                    \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                    \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                    \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                    \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                    \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                    \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n> **Note**: Please remember to grant the entity associated with your GCP credentials permissions to read and write to your GCR registry. For a full list of permissions required to use a GCP Service Connector to access a GCR registry, please refer to the [GCP Service Connector GCR registry resource type documentation](../../how-to/auth-management/gcp-service-connector.md#gcr-container-registry) or read the documentation available in the interactive CLI commands and dashboard. The GCP Service Connector supports [many different authentication methods](../../how-to/auth-management/gcp-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use-case.\n\nIf you already have one or more GCP Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the GCR registry you want to use for your GCP Container Registry by running e.g.:\n\n```sh\nzenml service-connector list-resources --connector-type gcp --resource-type docker-registry\n``` \n\n{% code title=\"Example Command Output\" %}\n```text\nThe following 'docker-registry' resources can be accessed by 'gcp' service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME   \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 ffc01795-0c0a-4f1d-af80-b84aceabcfcf \u2502 gcp-implicit     \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udc33 docker-registry \u2502 gcr.io/zenml-core                               \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 561b776a-af8b-491c-a4ed-14349b440f30 \u2502 gcp-zenml-core   \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udc33 docker-registry \u2502 gcr.io/zenml-core                               \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 us.gcr.io/zenml-core                            \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 eu.gcr.io/zenml-core                            \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 asia.gcr.io/zenml-core                          \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 asia-docker.pkg.dev/zenml-core/asia.gcr.io      \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 europe-docker.pkg.dev/zenml-core/eu.gcr.io      \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 europe-west1-docker.pkg.dev/zenml-core/test     \u2503\n\u2503                                      \u2502                  \u2502                \u2502                    \u2502 us-docker.pkg.dev/zenml-core/gcr.io             \u2503\n\u2503                                      \u2502                  \u2502"}
{"input": "                \u2502                    \u2502 us-docker.pkg.dev/zenml-core/us.gcr.io          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on a GCP Service Connector to use to connect to the target GCR registry, you can register the GCP Container Registry as follows:\n\n```sh\n# Register the GCP container registry and reference the target GCR registry URI\nzenml container-registry register <CONTAINER_REGISTRY_NAME> -f gcp \\\n    --uri=<REGISTRY_URL>\n\n# Connect the GCP container registry to the target GCR registry via a GCP Service Connector\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> -i\n```\n\nA non-interactive version that connects the GCP Container Registry to a target GCR registry through a GCP Service Connector:\n\n```sh\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> --connector <CONNECTOR_ID>\n```\n\n{% hint style=\"info\" %}\nLinking the GCP Container Registry to a Service Connector means that your local Docker client is no longer authenticated to access the remote registry. If you need to manually interact with the remote registry via the Docker CLI, you can use the [local login Service Connector feature](../../how-to/auth-management/service-connectors-guide.md#configure-local-clients) to temporarily authenticate your local Docker client to the remote registry:\n\n```sh\nzenml service-connector login <CONNECTOR_NAME> --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```text\n$ zenml service-connector login gcp-zenml-core --resource-type docker-registry\n\u280b Attempting to configure local client using service connector 'gcp-zenml-core'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'gcp-zenml-core' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n{% endhint %}\n\n{% code title=\"Example Command Output\" %}\n```text\n$ zenml container-registry connect gcp-zenml-core --connector gcp-zenml-core \nSuccessfully connected container registry `gcp-zenml-core` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 561b776a-af8b-491c-a4ed-14349b440f30 \u2502 gcp-zenml-core \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udc33 docker-registry \u2502 europe-west1-docker.pkg.dev/zenml-core/test \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the GCP Container Registry in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new container registry\nzenml stack register <STACK_NAME> -c <CONTAINER_REGISTRY_NAME> ... --set\n```\n{% endtab %}\n{% endtabs %}\n\nFor more information and a full list of configurable attributes of the GCP container registry, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-container\\_registries/#zenml.container\\_registries.gcp\\_container\\_registry.GCPContainerRegistry) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom container registry.\n---\n\n# Develop a custom container registry\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n### Base Abstraction\n\nIn the current version of ZenML, container registries have a rather basic base abstraction. In essence, their base configuration only features a `uri` and their implementation features a non-abstract `prepare_image_push` method for validation.\n\n```python\nfrom abc import abstractmethod\nfrom typing import Type\n\nfrom zenml.enums import StackComponentType\nfrom zenml.stack import Flavor\nfrom zenml.stack.authentication_mixin import (\n    AuthenticationConfigMixin,\n    AuthenticationMixin,\n)\nfrom zenml.utils import docker_utils\n\n\nclass BaseContainerRegistryConfig(AuthenticationConfigMixin):\n    \"\"\"Base config for a container registry.\"\"\"\n\n    uri: str\n\n\nclass BaseContainerRegistry(AuthenticationMixin):\n    \"\"\"Base class for all ZenML container registries.\"\"\"\n\n    def prepare_image_push(self, image_name: str) -> None:\n        \"\"\"Conduct necessary checks/preparations before an image gets pushed.\"\"\"\n\n    def push_image(self, image_name: str) -> str:\n        \"\"\"Pushes a Docker image.\"\"\"\n        if not image_name.startswith(self.config.uri):\n            raise ValueError(\n                f\"Docker image `{image_name}` does not belong to container \"\n                f\"registry `{self.config.uri}`.\"\n            )\n\n        self.prepare_image_push(image_name)\n        return docker_utils.push_image(image_name)\n\n\nclass BaseContainerRegistryFlavor(Flavor):\n    \"\"\"Base flavor for container registries.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Returns the name of the flavor.\"\"\"\n\n    @property\n    def type(self) -> StackComponentType:\n        \"\"\"Returns the flavor type.\"\"\"\n        return StackComponentType.CONTAINER_REGISTRY\n\n    @property\n    def config_class(self) -> Type[BaseContainerRegistryConfig]:\n        \"\"\"Config class for this flavor.\"\"\"\n        return BaseContainerRegistryConfig\n\n    @property\n    def implementation_class(self) -> Type[BaseContainerRegistry]:\n        \"\"\"Implementation class.\"\"\"\n        return BaseContainerRegistry\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the abstraction layer. In order to see the full implementation and get the complete docstrings, please check the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-container\\_registries/#zenml.container\\_registries.base\\_container\\_registry.BaseContainerRegistry) .\n{% endhint %}\n\n### Building your own container registry\n\nIf you want to create your own custom flavor for a container registry,"}
{"input": " you can follow the following steps:\n\n1. Create a class that inherits from the `BaseContainerRegistry` class and if you need to execute any checks/validation before the image gets pushed, you can define these operations in the `prepare_image_push` method. As an example, you can check the `AWSContainerRegistry`.\n2. If you need further configuration, you can create a class which inherits from the `BaseContainerRegistryConfig` class.\n3. Bring both the implementation and the configuration together by inheriting from the `BaseContainerRegistryFlavor` class.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml container-registry flavor register <path.to.MyContainerRegistryFlavor>\n```\n\nFor example, your flavor class `MyContainerRegistryFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml container-registry flavor register flavors.my_flavor.MyContainerRegistryFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually it's better to not have to rely on this mechanism, and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml container-registry flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomContainerRegistryFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomContainerRegistryConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` object are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomContainerRegistry** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomContainerRegistryFlavor` and the `CustomContainerRegistryConfig` are implemented in a different module/path than the actual `CustomContainerRegistry`).\n{% endhint %}\n\n"}
{"input": "<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Setting up a storage for Docker images.\n---\n\n# Container Registries\n\nThe container registry is an essential part of most remote MLOps stacks. It is used to store container images that are\nbuilt to run machine learning pipelines in remote environments. Containerization of the pipeline code creates a portable\nenvironment that allows code to run in an isolated manner.\n\n### When to use it\n\nThe container registry is needed whenever other components of your stack need to push or pull container images.\nCurrently, this is the case for most of ZenML's remote [orchestrators](../orchestrators/orchestrators.md)\n, [step operators](../step-operators/step-operators.md), and\nsome [model deployers](../model-deployers/model-deployers.md). These containerize your pipeline code and therefore\nrequire a container registry to store the resulting [Docker](https://www.docker.com/) images. Take a look at the\ndocumentation page of the component you want to use in your stack to see if it requires a container registry or even a\nspecific container registry flavor.\n\n### Container Registry Flavors\n\nZenML comes with a few container registry flavors that you can use:\n\n* Default flavor: Allows any URI without validation. Use this if you want to use a local container registry or when\n  using a remote container registry that is not covered by other flavors.\n* Specific flavors: Validates your container registry URI and performs additional checks to ensure you're able to push\n  to the registry.\n\n{% hint style=\"warning\" %}\nWe highly suggest using the specific container registry flavors in favor of the `default` one to make use of the\nadditional URI validations.\n{% endhint %}\n\n| Container Registry                         | Flavor      | Integration | URI example                               |\n|--------------------------------------------|-------------|-------------|-------------------------------------------|\n| [DefaultContainerRegistry](default.md)     | `default`   | _built-in_  | -                                         |\n| [DockerHubContainerRegistry](dockerhub.md) | `dockerhub` | _built-in_  | docker.io/zenml                           |\n| [GCPContainerRegistry](gcp.md)             | `gcp`       | _built-in_  | gcr.io/zenml                              |\n| [AzureContainerRegistry](azure.md)         | `azure`     | _built-in_  | zenml.azurecr.io                          |\n| [GitHubContainerRegistry](github.md)       | `github`    | _built-in_  | ghcr.io/zenml                             |\n| [AWSContainerRegistry](aws.md)             | `aws`       | `aws`       | 123456789.dkr.ecr.us-east-1.amazonaws.com |\n\nIf you would like to see the available flavors of container registries, you can use the command:\n\n```shell\nzenml container-registry flavor list\n```\n\n<!-- For scarf -->\n<figure><img alt=\""}
{"input": "ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Storing container images in Amazon ECR.\n---\n\n# Amazon Elastic Container Registry (ECR)\n\nThe AWS container registry is a [container registry](./container-registries.md) flavor provided with the ZenML `aws` integration and uses [Amazon ECR](https://aws.amazon.com/ecr/) to store container images.\n\n### When to use it\n\nYou should use the AWS container registry if:\n\n* one or more components of your stack need to pull or push container images.\n* you have access to AWS ECR. If you're not using AWS, take a look at the other [container registry flavors](./container-registries.md#container-registry-flavors).\n\n### How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding an AWS ECR container registry? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML AWS Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nThe ECR registry is automatically activated once you create an AWS account. However, you'll need to create a `Repository` in order to push container images to it:\n\n* Go to the [ECR website](https://console.aws.amazon.com/ecr).\n* Make sure the correct region is selected on the top right.\n* Click on `Create repository`.\n* Create a private repository. The name of the repository depends on the [orchestrator](../orchestrators/orchestrators.md) or [step operator](../step-operators/step-operators.md) you're using in your stack.\n\n### URI format\n\nThe AWS container registry URI should have the following format:\n\n```shell\n<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com\n# Examples:\n123456789.dkr.ecr.eu-west-2.amazonaws.com\n987654321.dkr.ecr.ap-south-1.amazonaws.com\n135792468.dkr.ecr.af-south-1.amazonaws.com\n```\n\nTo figure out the URI for your registry:\n\n* Go to the [AWS console](https://console.aws.amazon.com/) and click on your user account in the top right to see the `Account ID`.\n* Go [here](https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints) and choose the region in which you would like to store your container images. Make sure to choose a nearby region for faster access.\n* Once you have both these values, fill in the values in this template `<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com` to get your container registry URI.\n\n#### Infrastructure Deployment"}
{"input": "\n\nAn AWS ECR Container Registry can be deployed directly from the ZenML CLI:\n\n```shell\nzenml container-registry deploy ecr_container_registry --flavor=aws --provider=aws ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n### How to use it\n\nTo use the AWS container registry, we need:\n\n*   The ZenML `aws` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install aws\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* The registry URI. Check out the [previous section](aws.md#how-to-deploy-it) on the URI format and how to get the URI for your registry.\n\nWe can then register the container registry and use it in our active stack:\n\n```shell\nzenml container-registry register <NAME> \\\n    --flavor=aws \\\n    --uri=<REGISTRY_URI>\n\n# Add the container registry to the active stack\nzenml stack update -c <NAME>\n```\n\nYou also need to set up [authentication](aws.md#authentication-methods) required to log in to the container registry.\n\n#### Authentication Methods\n\nIntegrating and using an AWS Container Registry in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Local Authentication_ method. However, the recommended way to authenticate to the AWS cloud platform is through [an AWS Service Connector](../../how-to/auth-management/aws-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the AWS Container Registry with other remote stack components also running in AWS.\n\n{% tabs %}\n{% tab title=\"Local Authentication\" %}\nThis method uses the Docker client authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure an AWS Container Registry. You don't need to supply credentials explicitly when you register the AWS Container Registry, as it leverages the local credentials and configuration that the AWS CLI and Docker client store on your local machine. However, you will need to install and set up the AWS CLI on your machine as a prerequisite, as covered in [the AWS CLI documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), before you register the AWS Container Registry.\n\nWith the AWS CLI installed and set up with credentials, we'll need to log in to the container registry so Docker can pull and push images:\n\n```shell\n# Fill your REGISTRY_URI and REGION in the placeholders in the following command.\n# You can find the REGION as part of your REGISTRY_URI: `<ACCOUNT_ID>.dkr.ecr.<"}
{"input": "REGION>.amazonaws.com`\naws ecr get-login-password --region <REGION> | docker login --username AWS --password-stdin <REGISTRY_URI>\n```\n\n{% hint style=\"warning\" %}\nStacks using the AWS Container Registry set up with local authentication are not portable across environments. To make ZenML pipelines fully portable, it is recommended to use [an AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) to link your AWS Container Registry to the remote ECR registry.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"AWS Service Connector (recommended)\" %}\nTo set up the AWS Container Registry to authenticate to AWS and access an ECR registry, it is recommended to leverage the many features provided by [the AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) such as auto-configuration, local login, best security practices regarding long-lived credentials and fine-grained access control and reusing the same credentials across multiple stack components.\n\nIf you don't already have an AWS Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure an AWS Service Connector that can be used to access an ECR registry or even more than one type of AWS resource:\n\n```sh\nzenml service-connector register --type aws -i\n```\n\nA non-interactive CLI example that leverages [the AWS CLI configuration](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) on your local machine to auto-configure an AWS Service Connector targeting an ECR registry is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type aws --resource-type docker-registry --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register aws-us-east-1 --type aws --resource-type docker-registry --auto-configure\n\u2838 Registering service connector 'aws-us-east-1'...\nSuccessfully registered service connector `aws-us-east-1` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n> **Note**: Please remember to grant the entity associated with your AWS credentials permissions to read and write to one or more ECR repositories as well as to"}
{"input": " list accessible ECR repositories. For a full list of permissions required to use an AWS Service Connector to access an ECR registry, please refer to the [AWS Service Connector ECR registry resource type documentation](../../how-to/auth-management/aws-service-connector.md#ecr-container-registry) or read the documentation available in the interactive CLI commands and dashboard. The AWS Service Connector supports [many different authentication methods](../../how-to/auth-management/aws-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use case.\n\nIf you already have one or more AWS Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the ECR registry you want to use for your AWS Container Registry by running e.g.:\n\n```sh\nzenml service-connector list-resources --connector-type aws --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'docker-registry' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME          \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 37c97fa0-fa47-4d55-9970-e2aa6e1b50cf \u2502 aws-secret-key          \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 d400e0c6-a8e7-4b95-ab34-0359229c5d36 \u2502 aws-us-east-1           \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on an AWS Service Connector to use to connect to the target ECR registry, you can register the AWS Container Registry as follows:\n\n```sh\n# Register the AWS container registry and reference the target ECR registry URI\nzenml container-registry register <CONTAINER_REGISTRY_NAME> -f aws \\\n    --uri=<REGISTRY_URL>\n\n# Connect the AWS container registry to the target ECR registry via an AWS Service Connector\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> -i\n```\n\nA non-interactive version that connects the AWS Container Registry to a target ECR registry through an AWS Service Connector:\n\n```sh\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml container-registry connect aws-us-east-1 --connector aws-us-east-1 \nSuccessfully connected container registry `aws-us-east-1` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 d400e0c6-a8e7-4b95-ab34-0359229c5d36 \u2502 aws-us-east-1  \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udc33 docker-registry \u2502 715803424590.dkr.ecr.us-east-1.amazonaws.com \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the AWS Container Registry in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new container registry\nzenml stack register <STACK_NAME> -c <CONTAINER_REGISTRY_NAME> ... --set\n```\n\n{% hint style=\"info\" %}\nLinking the AWS Container Registry to a Service Connector means that your local Docker client is no longer authenticated to access"}
{"input": " the remote registry. If you need to manually interact with the remote registry via the Docker CLI, you can use the [local login Service Connector feature](../../how-to/auth-management/service-connectors-guide.md#configure-local-clients) to temporarily authenticate your local Docker client to the remote registry:\n\n```sh\nzenml service-connector login <CONNECTOR_NAME> --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector login aws-us-east-1 --resource-type docker-registry\n\u283c Attempting to configure local client using service connector 'aws-us-east-1'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'aws-us-east-1' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more information and a full list of configurable attributes of the AWS container registry, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.container\\_registries.aws\\_container\\_registry.AWSContainerRegistry).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing container images in DockerHub.\n---\n\n# DockerHub\n\nThe DockerHub container registry is a [container registry](./container-registries.md) flavor that comes built-in with ZenML and uses [DockerHub](https://hub.docker.com/) to store container images.\n\n### When to use it\n\nYou should use the DockerHub container registry if:\n\n* one or more components of your stack need to pull or push container images.\n* you have a DockerHub account. If you're not using DockerHub, take a look at the other [container registry flavors](./container-registries.md#container-registry-flavors).\n\n### How to deploy it\n\nTo use the DockerHub container registry, all you need to do is create a [DockerHub](https://hub.docker.com/) account.\n\nWhen this container registry is used in a ZenML stack, the Docker images that are built will be published in a \\*\\* public\\*\\* repository and everyone will be able to pull your images. If you want to use a **private** repository instead, you'll have to [create a private repository](https://docs.docker.com/docker-hub/repos/#creating-repositories) on the website before running the pipeline. The repository name depends on the remote [orchestrator](../orchestrators/orchestrators.md) or [step operator](../step-operators/step-operators.md) that you're using in your stack.\n\n### How to find the registry URI\n\nThe DockerHub container registry URI should have one of the two following formats:\n\n```shell\n<ACCOUNT_NAME>\n# or\ndocker.io/<ACCOUNT_NAME>\n\n# Examples:\nzenml\nmy-username\ndocker.io/zenml\ndocker.io/my-username\n```\n\nTo figure out the URI for your registry:\n\n* Find out the account name of your [DockerHub](https://hub.docker.com/) account.\n* Use the account name to fill the template `docker.io/<ACCOUNT_NAME>` and get your URI.\n\n### How to use it\n\nTo use the Azure container registry, we need:\n\n* [Docker](https://www.docker.com) installed and running.\n* The registry URI. Check out the [previous section](dockerhub.md#how-to-find-the-registry-uri) on the URI format and how to get the URI for your registry.\n\nWe can then register the container registry and use it in our active stack:\n\n```shell\nzenml container-registry register <NAME> \\\n    --flavor=dockerhub \\\n    --uri=<REGISTRY_URI>\n\n# Add the container registry to the active stack\nzenml stack update -c <NAME>\n```\n\nAdditionally, we'll need to log in to the container registry so Docker can pull and push images. This will require your DockerHub account name and either your password or preferably a [personal access token](https://docs.docker.com/docker-hub/access-tokens/).\n\n```shell"}
{"input": "\ndocker login\n```\n\nFor more information and a full list of configurable attributes of the `dockerhub` container registry, check out the [SDK Docs](https://apidocs.zenml.io/latest/core\\_code\\_docs/core-container\\_registries/#zenml.container\\_registries.dockerhub\\_container\\_registry.DockerHubContainerRegistry) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing container images in Azure.\n---\n\n# Azure Container Registry\n\nThe Azure container registry is a [container registry](./container-registries.md) flavor that comes built-in with ZenML and uses the [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/) to store container images.\n\n### When to use it\n\nYou should use the Azure container registry if:\n\n* one or more components of your stack need to pull or push container images.\n* you have access to Azure. If you're not using Azure, take a look at the other [container registry flavors](./container-registries.md#container-registry-flavors).\n\n### How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding an Azure container registry? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Azure Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nGo [here](https://portal.azure.com/#create/Microsoft.ContainerRegistry) and choose a subscription, resource group, location, and registry name. Then click on `Review + Create` and to create your container registry.\n\n### How to find the registry URI\n\nThe Azure container registry URI should have the following format:\n\n```shell\n<REGISTRY_NAME>.azurecr.io\n# Examples:\nzenmlregistry.azurecr.io\nmyregistry.azurecr.io\n```\n\nTo figure out the URI for your registry:\n\n* Go to the [Azure portal](https://portal.azure.com/#home).\n* In the search bar, enter `container registries` and select the container registry you want to use. If you don't have any container registries yet, check out the [deployment section](azure.md#how-to-deploy-it) on how to create one.\n* Use the name of your registry to fill the template `<REGISTRY_NAME>.azurecr.io` and get your URI.\n\n### How to use it\n\nTo use the Azure container registry, we need:\n\n* [Docker](https://www.docker.com) installed and running.\n* The registry URI. Check out the [previous section](azure.md#how-to-find-the-registry-uri) on the URI format and how to get the URI for your registry.\n\nWe can then register the container registry and use it in our active stack:\n\n```shell\nzenml container-registry register <NAME> \\\n    --flavor=azure \\\n    --uri=<REGISTRY_URI>\n\n# Add the container registry to the active stack\nzenml stack update -c <NAME>\n```\n\nYou also need to set up [authentication](azure.md"}
{"input": "#authentication-methods) required to log in to the container registry.\n\n#### Authentication Methods\n\nIntegrating and using an Azure Container Registry in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Local Authentication_ method. However, the recommended way to authenticate to the Azure cloud platform is through [an Azure Service Connector](../../how-to/auth-management/azure-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the Azure Container Registry with other remote stack components also running in Azure.\n\n{% tabs %}\n{% tab title=\"Local Authentication\" %}\nThis method uses the Docker client authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure an Azure Container Registry. You don't need to supply credentials explicitly when you register the Azure Container Registry, as it leverages the local credentials and configuration that the Azure CLI and Docker client store on your local machine. However, you will need to install and set up the Azure CLI on your machine as a prerequisite, as covered in [the Azure CLI documentation](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli), before you register the Azure Container Registry.\n\nWith the Azure CLI installed and set up with credentials, you need to login to the container registry so Docker can pull and push images:\n\n```shell\n# Fill your REGISTRY_NAME in the placeholder in the following command.\n# You can find the REGISTRY_NAME as part of your registry URI: `<REGISTRY_NAME>.azurecr.io`\naz acr login --name=<REGISTRY_NAME>\n```\n\n{% hint style=\"warning\" %}\nStacks using the Azure Container Registry set up with local authentication are not portable across environments. To make ZenML pipelines fully portable, it is recommended to use [an Azure Service Connector](../../how-to/auth-management/azure-service-connector.md) to link your Azure Container Registry to the remote ACR registry.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"Azure Service Connector (recommended)\" %}\nTo set up the Azure Container Registry to authenticate to Azure and access an ACR registry, it is recommended to leverage the many features provided by [the Azure Service Connector](../../how-to/auth-management/azure-service-connector.md) such as auto-configuration, local login, best security practices regarding long-lived credentials and reusing the same credentials across multiple stack components.\n\nIf you don't already have an Azure Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure an Azure Service Connector that can be used to access a ACR registry or even more than one type of Azure resource:\n\n```sh\nzenml service-connector register --type azure -i\n```\n\nA non-interactive CLI example that uses [Azure Service Principal credentials](https://learn.microsoft.com/en-us/azure"}
{"input": "/active-directory/develop/app-objects-and-service-principals) to configure an Azure Service Connector targeting a single ACR registry is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type azure --auth-method service-principal --tenant_id=<AZURE_TENANT_ID> --client_id=<AZURE_CLIENT_ID> --client_secret=<AZURE_CLIENT_SECRET> --resource-type docker-registry --resource-id <REGISTRY_URI>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register azure-demo --type azure --auth-method service-principal --tenant_id=a79f3633-8f45-4a74-a42e-68871c17b7fb --client_id=8926254a-8c3f-430a-a2fd-bdab234d491e --client_secret=AzureSuperSecret --resource-type docker-registry --resource-id demozenmlcontainerregistry.azurecr.io\n\u2838 Registering service connector 'azure-demo'...\nSuccessfully registered service connector `azure-demo` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE    \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udc33 docker-registry \u2502 demozenmlcontainerregistry.azurecr.io \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n> **Note**: Please remember to grant the entity associated with your Azure credentials permissions to read and write to your ACR registry as well as to list accessible ACR registries. For a full list of permissions required to use an Azure Service Connector to access a ACR registry, please refer to the [Azure Service Connector ACR registry resource type documentation](../../how-to/auth-management/azure-service-connector.md#acr-container-registry) or read the documentation available in the interactive CLI commands and dashboard. The Azure Service Connector supports [many different authentication methods](../../how-to/auth-management/azure-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use case.\n\nIf you already have one or more Azure Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the ACR registry you want to use for your Azure Container Registry by running e.g.:\n\n```sh\nzenml service-connector list-resources --connector-type azure --resource-type docker-registry\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'docker-registry' resources can"}
{"input": " be accessed by 'azure' service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 db5821d0-a658-4504-ae96-04c3302d8f85 \u2502 azure-demo     \u2502 \ud83c\udde6 azure       \u2502 \ud83d\udc33 docker-registry \u2502 demozenmlcontainerregistry.azurecr.io \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on an Azure Service Connector to use to connect to the target ACR registry, you can register the Azure Container Registry as follows:\n\n```sh\n# Register the Azure container registry and reference the target ACR registry URI\nzenml container-registry register <CONTAINER_REGISTRY_NAME> -f azure \\\n    --uri=<REGISTRY_URL>\n\n# Connect the Azure container registry to the target ACR registry via an Azure Service Connector\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> -i\n```\n\nA non-interactive version that connects the Azure Container Registry to a target ACR registry through an Azure Service Connector:\n\n```sh\nzenml container-registry connect <CONTAINER_REGISTRY_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml container-registry connect azure-demo --connector azure-demo\nSuccessfully connected container registry `azure-demo` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE      \u2502 RESOURCE NAMES                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 db5821d0-a658-4504-ae96-04c3302d8f85 \u2502 azure-demo     \u2502 \ud83c\udde6  azure       \u2502 \ud83d\udc33 docker-registry \u2502 demozenmlcontainerregistry.azurecr.io \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the Azure Container Registry in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new container registry\nzenml stack register <STACK_NAME> -c <CONTAINER_REGISTRY_NAME> ... --set\n```\n\n{% hint style=\"info\" %}\nLinking the Azure Container Registry to a Service Connector means that your local Docker client is no longer authenticated to access the remote registry. If you need to manually interact with the remote registry via the Docker CLI, you can use the [local login Service Connector feature](../../how-to/auth-management/service-connectors-guide.md#configure-local-clients) to temporarily authenticate your local Docker client to the remote registry:\n\n```sh\nzenml service-connector login <CONNECTOR_NAME> --resource-type docker-registry --resource-id <CONTAINER_REGISTRY_URI>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector login azure-demo --resource-type docker-registry --resource-id demozenmlcontainerregistry.azurecr.io\n\u2839 Attempting to configure local client using service connector 'azure-demo'...\nWARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nThe 'azure-demo' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.\n```\n{% endcode %}\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more information and a full list of configurable attributes of the Azure container registry, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-container\\_registries/#zenml.container\\_registries.azure\\_container\\_registry.AzureContainerRegistry) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Annotating the data in your workflow.\n---\n\n# Annotators\n\nAnnotators are a stack component that enables the use of data annotation as part of your ZenML stack and pipelines. You\ncan use the associated CLI command to launch annotation, configure your datasets and get stats on how many labeled tasks\nyou have ready for use.\n\nData annotation/labeling is a core part of MLOps that is frequently left out of the conversation. ZenML will\nincrementally start to build features that support an iterative annotation workflow that sees the people doing\nlabeling (and their workflows/behaviors) as integrated parts of their ML process(es).\n\n![When and where to annotate.](../../.gitbook/assets/annotation-when-where.png)\n\nThere are a number of different places in the ML lifecycle where this can happen:\n\n* **At the start**: You might be starting out without any data, or with a ton of data but no clear sense of which parts\n  of it are useful to your particular problem. It\u2019s not uncommon to have a lot of data but to be lacking accurate labels\n  for that data. So you can start and get great value from bootstrapping your model: label some data, train your model,\n  and use your model to suggest labels allowing you to speed up your labeling, iterating on and on in this way. Labeling\n  data early on in the process also helps clarify and condense down your specific rules and standards. For example, you\n  might realize that you need to have specific definitions for certain concepts so that your labeling efforts are\n  consistent across your team.\n* **As new data comes in**: New data will likely continue to come in, and you might want to check in with the labeling\n  process at regular intervals to expose yourself to this new data. (You\u2019ll probably also want to have some kind of\n  automation around detecting data or concept drift, but for certain kinds of unstructured data you probably can never\n  completely abandon the instant feedback of actual contact with the raw data.)\n* **Samples generated for inference**: Your model will be making predictions on real-world data being passed in. If you\n  store and label this data, you\u2019ll gain a valuable set of data that you can use to compare your labels with what the\n  model was predicting, another possible way to flag drifts of various kinds. This data can then (subject to\n  privacy/user consent) be used in retraining or fine-tuning your model.\n* **Other ad hoc interventions**: You will probably have some kind of process to identify bad labels, or to find the\n  kinds of examples that your model finds really difficult to make correct predictions. For these, and for areas where\n  you have clear class imbalances, you might want to do ad hoc annotation to supplement the raw materials your model has\n  to learn from.\n\nZenML currently offers standard steps that"}
{"input": " help you tackle the above use cases, but the stack component and abstraction\nwill continue to be developed to make it easier to use.\n\n### When to use it\n\nThe annotator is an optional stack component in the ZenML Stack. We designed our abstraction to fit into the larger ML\nuse cases, particularly the training and deployment parts of the lifecycle.\n\nThe core parts of the annotation workflow include:\n\n* using labels or annotations in your training steps in a seamless way\n* handling the versioning of annotation data\n* allow for the conversion of annotation data to and from custom formats\n* handle annotator-specific tasks, for example, the generation of UI config files that Label Studio requires for the web\n  annotation interface\n\n### List of available annotators\n\nFor production use cases, some more flavors can be found in specific `integrations` modules. In terms of annotators,\nZenML features integrations with `label_studio` and `pigeon`.\n\n| Annotator                               | Flavor         | Integration    | Notes                                                                |\n|-----------------------------------------|----------------|----------------|----------------------------------------------------------------------|\n| [ArgillaAnnotator](argilla.md)           | `argilla`       | `argilla`       | Connect ZenML with Argilla                                             |\n| [LabelStudioAnnotator](label-studio.md) | `label_studio` | `label_studio` | Connect ZenML with Label Studio                                      |\n| [PigeonAnnotator](pigeon.md) | `pigeon` | `pigeon` | Connect ZenML with Pigeon. Notebook only & for image and text classification tasks.      |\n| [ProdigyAnnotator](prodigy.md)           | `prodigy`       | `prodigy`       | Connect ZenML with [Prodigy](https://prodi.gy/)                                             |\n| [Custom Implementation](custom.md)      | _custom_       |                | Extend the annotator abstraction and provide your own implementation |\n\nIf you would like to see the available flavors for annotators, you can use the command:\n\n```shell\nzenml annotator flavor list\n```\n\n### How to use it\n\nThe available implementation of the annotator is built on top of the Label\nStudio integration, which means that using an annotator currently is no\ndifferent from what's described on the [Label Studio page: How to use\nit?](label-studio.md#how-do-you-use-it). ([Pigeon](pigeon.md) is also supported, but has a\nvery limited functionality and only works within Jupyter notebooks.)\n\n### A note on names\n\nThe various annotation tools have mostly standardized around the naming of key concepts as part of how they build their\ntools. Unfortunately, this hasn't been completely unified so ZenML takes an opinion on which names we use for our stack\ncomponents and integrations. Key differences to note:\n\n* Label Studio refers to the grouping of"}
{"input": " a set of annotations/tasks as a 'Project', whereas most other tools use the\n  term 'Dataset', so ZenML also calls this grouping a 'Dataset'.\n* The individual meta-unit for 'an annotation + the source data' is referred to in different ways, but at ZenML (and\n  with Label Studio) we refer to them as 'tasks'.\n\nThe remaining core concepts ('annotation' and 'prediction', in particular) are broadly used among annotation tools.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Annotating data using Prodigy.\n---\n\n# Prodigy\n\n[Prodigy](https://prodi.gy/) is a modern annotation tool for creating training\nand evaluation data for machine learning models. You can also use Prodigy to\nhelp you inspect and clean your data, do error analysis and develop rule-based\nsystems to use in combination with your statistical models.\n\n![Prodigy Annotator](../../.gitbook/assets/prodigy-annotator.png)\n\n{% hint style=\"info\" %} Prodigy is a paid annotation tool. You will need a\nProdigy is a paid tool. A license is required to download and use it with ZenML.\n{% endhint %}\n\n\nThe Prodigy Python library includes a range of pre-built workflows and\ncommand-line commands for various tasks, and well-documented components for\nimplementing your own workflow scripts. Your scripts can specify how the data is\nloaded and saved, change which questions are asked in the annotation interface,\nand can even define custom HTML and JavaScript to change the behavior of the\nfront-end. The web application is optimized for fast, intuitive and efficient\nannotation.\n\n### When would you want to use it?\n\nIf you need to label data as part of your ML workflow, that is the point at\nwhich you could consider adding the optional annotator stack component as part\nof your ZenML stack.\n\n### How to deploy it?\n\nThe Prodigy Annotator flavor is provided by the Prodigy ZenML integration. You\nneed to install it to be able to register it as an Annotator and add it to your\nstack:\n\n```shell\nzenml integration export-requirements --output-file prodigy-requirements.txt prodigy\n```\n\nNote that you'll need to install Prodigy separately since it requires a license.\nPlease [visit the Prodigy docs](https://prodi.gy/docs/install) for information\non how to install it. Currently Prodigy also requires the `urllib3<2`\ndependency, so make sure to install that.\n\nThen register your annotator with ZenML:\n\n```shell\nzenml annotator register prodigy --flavor prodigy\n# optionally also pass in --custom_config_path=\"<PATH_TO_CUSTOM_CONFIG_FILE>\"\n```\n\nSee https://prodi.gy/docs/install#config for more on custom Prodigy config\nfiles. Passing a `custom_config_path` allows you to override the default Prodigy\nconfig.\n\nFinally, add all these components to a stack and set it as your active stack.\nFor example:\n\n```shell\nzenml stack copy default annotation\nzenml stack update annotation -an prodigy\nzenml stack set annotation\n# optionally also\nzenml stack describe\n```\n\nNow if you run a simple CLI command like `zenml annotator dataset list` this\nshould work without any errors. You're ready to use your annotator"}
{"input": " in your ML\nworkflow!\n\n### How do you use it?\n\nWith Prodigy, there is no need to specially start the annotator ahead of time\nlike with [Label Studio](label-studio.md). Instead, just use Prodigy as per the\n[Prodigy docs](https://prodi.gy) and then you can use the ZenML wrapper / API to\nget your labelled data etc using our Python methods.\n\nZenML supports access to your data and annotations via the `zenml annotator ...`\nCLI command.\n\nYou can access information about the datasets you're using with the `zenml\nannotator dataset list`. To work on annotation for a particular dataset, you can\nrun `zenml annotator dataset annotate <DATASET_NAME> <CUSTOM_COMMAND>`. This is\nthe equivalent of running `prodigy <CUSTOM_COMMAND>` in the terminal. For\nexample, you might run:\n\n```shell\nzenml annotator dataset annotate your_dataset --command=\"textcat.manual news_topics ./news_headlines.jsonl --label Technology,Politics,Economy,Entertainment\"\n```\n\nThis would launch the Prodigy interface for the `textcat.manual` recipe with the\n`news_topics` dataset and the labels `Technology`, `Politics`, `Economy`, and\n`Entertainment`. The data would be loaded from the `news_headlines.jsonl` file.\n\nA common workflow for Prodigy is to annotate data as you would usually do, and\nthen use the connection into ZenML to import those annotations within a step in\nyour pipeline (if running locally). For example, within a ZenML step:\n\n```python\nfrom typing import List, Dict, Any\n\nfrom zenml import step\nfrom zenml.client import Client\n\n@step\ndef import_annotations() -> List[Dict[str, Any]:\n    zenml_client = Client()\n    annotations = zenml_client.active_stack.annotator.get_labeled_data(dataset_name=\"my_dataset\")\n    # Do something with the annotations\n    return annotations\n```\n\nIf you're running in a cloud environment, you can manually export the\nannotations, store them somewhere in a cloud environment and then reference or\nuse those within ZenML. The precise way you do this will be very case-dependent,\nhowever, so it's difficult to provide a one-size-fits-all solution.\n\n#### Prodigy Annotator Stack Component\n\nOur Prodigy annotator component inherits from the `BaseAnnotator` class. There\nare some methods that are core methods that must be defined, like being able to\nregister or get a dataset. Most annotators handle things like the storage of\nstate and have their own custom features, so there are quite a few extra methods\nspecific to Prodigy.\n\nThe core Prodigy functionality that's currently enabled from within the\n`annotator` stack component interface includes a way to register your datasets\nand export any annotations for use"}
{"input": " in separate steps.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Annotating data using Argilla.\n---\n\n# Argilla\n\n[Argilla](https://github.com/argilla-io/argilla) is an open-source data curation\nplatform designed to enhance the development of both small and large language\nmodels (LLMs) and NLP tasks in general. It enables users to build robust\nlanguage models through faster data curation using both human and machine\nfeedback, providing support for each step in the MLOps cycle, from data labeling\nto model monitoring.\n\n![Argilla Annotator](../../.gitbook/assets/argilla_annotator.png)\n\nArgilla distinguishes itself for its focus on specific use cases and\nhuman-in-the-loop approaches. While it does offer programmatic features,\nArgilla's core value lies in actively involving human experts in the\ntool-building process, setting it apart from other competitors.\n\n### When would you want to use it?\n\nIf you need to label textual data as part of your ML workflow, that is the point\nat which you could consider adding the Argilla annotator stack component as part\nof your ZenML stack.\n\nWe currently support the use of annotation at the various stages described in\n[the main annotators docs page](annotators.md). The Argilla integration\ncurrently is built to support annotation using a local (Docker-backed) instance\nof Argilla as well as a deployed instance of Argilla. There is an easy way to\ndeploy Argilla as a [Hugging Face\nSpace](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla), for\ninstance, which is documented in the [Argilla\ndocumentation](https://docs.argilla.io/en/latest/getting_started/installation/deployments/huggingface-spaces.html).\n\n### How to deploy it?\n\nThe Argilla Annotator flavor is provided by the Argilla ZenML integration. You\nneed to install it to be able to register it as an Annotator and add it to your\nstack:\n\n```shell\nzenml integration install argilla\n```\n\nYou can either pass the `api_key` directly into the `zenml annotator register`\ncommand or you can register it as a secret and pass the secret name into the\ncommand. We recommend the latter approach for security reasons. If you want to\ntake the latter approach, be sure to register a secret for whichever artifact\nstore you choose, and then you should make sure to pass the name of that secret\ninto the annotator as the `--authentication_secret`. For example, you'd run:\n\n```shell\nzenml secret create argilla_secrets --api_key=\"<your_argilla_api_key>\"\n```\n\n(Visit the Argilla documentation and interface to obtain your API key.)\n\nThen register your annotator with ZenML:\n\n```shell\nzenml annotator register argilla --flavor argilla --authentication_secret=argilla_secrets\n"}
{"input": "```\n\nWhen using a deployed instance of Argilla, the instance URL must be specified\nwithout any trailing `/` at the end. If you are using a Hugging Face Spaces\ninstance and its visibility is set to private, you must also set the\n`extra_headers` parameter which would include a Hugging Face token. For example:\n\n```shell\nzenml annotator register argilla --flavor argilla --authentication_secret=argilla_secrets --instance_url=\"https://[your-owner-name]-[your_space_name].hf.space\" --extra_headers=\"{\"Authorization\": f\"Bearer {<your_hugging_face_token>}\"}\"\n```\n\nFinally, add all these components to a stack and set it as your active stack.\nFor example:\n\n```shell\nzenml stack copy default annotation\n# this must be done separately so that the other required stack components are first registered\nzenml stack update annotation -an <YOUR_ARGILLA_ANNOTATOR>\nzenml stack set annotation\n# optionally also\nzenml stack describe\n```\n\nNow if you run a simple CLI command like `zenml annotator dataset list` this\nshould work without any errors. You're ready to use your annotator in your ML\nworkflow!\n\n### How do you use it?\n\nZenML supports access to your data and annotations via the `zenml annotator ...`\nCLI command. We have also implemented an interface to some of the common Argilla\nfunctionality via the ZenML SDK.\n\nYou can access information about the datasets you're using with the `zenml\nannotator dataset list`. To work on annotation for a particular dataset, you can\nrun `zenml annotator dataset annotate <dataset_name>`. What follows is an\noverview of some key components to the Argilla integration and how it can be\nused.\n\n#### Argilla Annotator Stack Component\n\nOur Argilla annotator component inherits from the `BaseAnnotator` class. There\nare some methods that are core methods that must be defined, like being able to\nregister or get a dataset. Most annotators handle things like the storage of\nstate and have their own custom features, so there are quite a few extra methods\nspecific to Argilla.\n\nThe core Argilla functionality that's currently enabled includes a way to\nregister your datasets, export any annotations for use in separate steps as well\nas start the annotator daemon process. (Argilla requires a server to be running\nin order to use the web interface, and ZenML handles the connection to this\nserver using the details you passed in when registering the component.)\n\n#### Argilla Annotator SDK\n\nVisit [the SDK\ndocs](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-argilla/)\nto learn more about the methods that ZenML exposes for the Argilla annotator. To\naccess the SDK through Python, you would first get the client object and then\ncall"}
{"input": " the methods you need. For example:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\nannotator = client.active_stack.annotator\n\n# list dataset names\ndataset_names = annotator.get_dataset_names()\n\n# get a specific dataset\ndataset = annotator.get_dataset(\"dataset_name\")\n\n# get the annotations for a dataset\nannotations = annotator.get_labeled_data(dataset_name=\"dataset_name\")\n```\n\nFor more detailed information on how to use the Argilla annotator and the\nfunctionality it provides, visit the [Argilla\ndocumentation](https://docs.argilla.io/en/latest/).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Annotating data using Pigeon.\n---\n\n# Pigeon\n\nPigeon is a lightweight, open-source annotation tool designed for quick and easy labeling of data directly within Jupyter notebooks. It provides a simple and intuitive interface for annotating various types of data, including:\n\n* Text Classification\n* Image Classification\n* Text Captioning\n\n### When would you want to use it?\n\n![Pigeon annotator interface](../../.gitbook/assets/pigeon.png)\n\nIf you need to label a small to medium-sized dataset as part of your ML workflow and prefer the convenience of doing it directly within your Jupyter notebook, Pigeon is a great choice. It is particularly useful for:\n\n* Quick labeling tasks that don't require a full-fledged annotation platform\n* Iterative labeling during the exploratory phase of your ML project\n* Collaborative labeling within a Jupyter notebook environment\n\n### How to deploy it?\n\nTo use the Pigeon annotator, you first need to install the ZenML Pigeon integration:\n\n```shell\nzenml integration install pigeon\n```\n\nNext, register the Pigeon annotator with ZenML, specifying the output directory where the annotation files will be stored:\n\n```shell\nzenml annotator register pigeon --flavor pigeon --output_dir=\"path/to/dir\"\n```\n\nNote that the `output_dir` is relative to the repository or notebook root.\n\nFinally, add the Pigeon annotator to your stack and set it as the active stack:\n\n```shell\nzenml stack update <YOUR_STACK_NAME> --annotator pigeon\n```\n\nNow you're ready to use the Pigeon annotator in your ML workflow!\n\n### How do you use it?\n\nWith the Pigeon annotator registered and added to your active stack, you can easily access it using the ZenML client within your Jupyter notebook.\n\nFor text classification tasks, you can launch the Pigeon annotator as follows:\n\n````python\nfrom zenml.client import Client\n\nannotator = Client().active_stack.annotator\n\nannotations = annotator.launch(\n    data=[\n        'I love this movie',\n        'I was really disappointed by the book'\n    ],\n    options=[\n        'positive',\n        'negative'\n    ]\n)\n````\n\nFor image classification tasks, you can provide a custom display function to render the images:\n\n````python\nfrom zenml.client import Client\nfrom IPython.display import display, Image\n\nannotator = Client().active_stack.annotator\n\nannotations = annotator.launch(\n    data=[\n        '/path/to/image1.png',\n        '/path/to/image2.png'\n    ],\n    options=[\n        'cat',\n        'dog'\n    ],\n    display_fn=lambda filename: display(Image(filename))\n)\n````\n\nThe `launch` method returns the annotations as a list of tuples, where each tuple contains the data item and its corresponding label.\n\nYou can also use the `zenml"}
{"input": " annotator dataset` commands to manage your datasets:\n\n* `zenml annotator dataset list` - List all available datasets\n* `zenml annotator dataset delete <dataset_name>` - Delete a specific dataset\n* `zenml annotator dataset stats <dataset_name>` - Get statistics for a specific dataset\n\nAnnotation files are saved as JSON files in the specified output directory. Each\nannotation file represents a dataset, with the filename serving as the dataset\nname.\n\n## Acknowledgements\n\nPigeon was created by [Anastasis Germanidis](https://github.com/agermanidis) and\nreleased as a [Python package](https://pypi.org/project/pigeon-jupyter/) and\n[Github repository](https://github.com/agermanidis/pigeon). It is licensed under\nthe Apache License. It has been updated to work with more recent `ipywidgets`\nversions and some small UI improvements were added. We are grateful to Anastasis\nfor creating this tool and making it available to the community.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom annotator.\n---\n\n# Develop a Custom Annotator\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\nAnnotators are a stack component that enables the use of data annotation as part of your ZenML stack and pipelines. You can use the associated CLI command to launch annotation, configure your datasets and get stats on how many labeled tasks you have ready for use.\n\n{% hint style=\"warning\" %}\n**Base abstraction in progress!**\n\nWe are actively working on the base abstraction for the annotators, which will be available soon. As a result, their extension is not possible at the moment. If you would like to use an annotator in your stack, please check the list of already available feature stores down below.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Annotating data using Label Studio.\n---\n\n# Label Studio\n\nLabel Studio is one of the leading open-source annotation platforms available to data scientists and ML practitioners.\nIt is used to create or edit datasets that you can then use as part of training or validation workflows. It supports a\nbroad range of annotation types, including:\n\n* Computer Vision (image classification, object detection, semantic segmentation)\n* Audio & Speech (classification, speaker diarization, emotion recognition, audio transcription)\n* Text / NLP (classification, NER, question answering, sentiment analysis)\n* Time Series (classification, segmentation, event recognition)\n* Multi-Modal / Domain (dialogue processing, OCR, time series with reference)\n\n### When would you want to use it?\n\nIf you need to label data as part of your ML workflow, that is the point at which you could consider adding the optional\nannotator stack component as part of your ZenML stack.\n\nWe currently support the use of annotation at the various stages described\nin [the main annotators docs page](annotators.md), and also offer custom utility functions to generate Label Studio\nlabel config files for image classification and object detection. (More will follow in due course.)\n\nThe Label Studio integration currently is built to support workflows using the following three cloud artifact stores:\nAWS S3, GCP/GCS, and Azure Blob Storage. Purely local stacks will currently _not_ work if you want to do add the\nannotation stack component as part of your stack.\n\n### How to deploy it?\n\nThe Label Studio Annotator flavor is provided by the Label Studio ZenML integration, you need to install it, to be able\nto register it as an Annotator and add it to your stack:\n\n```shell\nzenml integration install label_studio\n```\n\nYou will then need to obtain your Label Studio API key. This will give you access to the web annotation interface. (The\nfollowing steps apply to a local instance of Label Studio, but feel free to obtain your API key directly from your\ndeployed instance if that's what you are using.)\n\n```shell\ngit clone https://github.com/HumanSignal/label-studio.git\ncd label-studio\ndocker-compose up -d # starts label studio at http://localhost:8080\n```\n\nThen visit [http://localhost:8080/](http://localhost:8080/) to log in, and then\nvisit [http://localhost:8080/user/account](http://localhost:8080/user/account) and get your Label Studio API key (from\nthe upper right-hand corner). You will need it for the next step. Keep the Label Studio server running, because the\nZenML Label Studio annotator will use it as the backend.\n\nAt this point you should register the API key under a custom secret name, making sure to replace the two parts in `<>`\nwith whatever you choose:\n\n```shell\nzenml secret"}
{"input": " create label_studio_secrets --api_key=\"<your_label_studio_api_key>\"\n```\n\nThen register your annotator with ZenML:\n\n```shell\nzenml annotator register label_studio --flavor label_studio --api_key=\"{{label_studio_secrets.api_key}}\"\n\n# for deployed instances of Label Studio, you can also pass in the URL as follows, for example:\n# zenml annotator register label_studio --flavor label_studio --authentication_secret=\"<LABEL_STUDIO_SECRET_NAME>\" --instance_url=\"<your_label_studio_url>\" --port=80\n```\n\nWhen using a deployed instance of Label Studio, the instance URL must be specified without any trailing `/` at the end.\nYou should specify the port, for example, port 80 for a standard HTTP\nconnection. For a Hugging Face deployment (the easiest way to get going with\nLabel Studio), please read the [Hugging Face deployment documentation](https://huggingface.co/docs/hub/spaces-sdks-docker-label-studio).\n\nFinally, add all these components to a stack and set it as your active stack. For example:\n\n```shell\nzenml stack copy default annotation\nzenml stack update annotation -a <YOUR_CLOUD_ARTIFACT_STORE>\n# this must be done separately so that the other required stack components are first registered\nzenml stack update annotation -an <YOUR_LABEL_STUDIO_ANNOTATOR>\nzenml stack set annotation\n# optionally also\nzenml stack describe\n```\n\nNow if you run a simple CLI command like `zenml annotator dataset list` this should work without any errors. You're\nready to use your annotator in your ML workflow!\n\n### How do you use it?\n\nZenML assumes that users have registered a cloud artifact store and an annotator as described above. ZenML currently\nonly supports this setup, but we will add in the fully local stack option in the future.\n\nZenML supports access to your data and annotations via the `zenml annotator ...` CLI command.\n\nYou can access information about the datasets you're using with the `zenml annotator dataset list`. To work on\nannotation for a particular dataset, you can run `zenml annotator dataset annotate <dataset_name>`.\n\n[Our computer vision end to end example](https://github.com/zenml-io/zenml-projects/tree/main/end-to-end-computer-vision) \nis the best place to see how all the pieces of making this integration work fit together. What follows is an overview of \nsome key components to the Label Studio integration and how it can be used.\n\n#### Label Studio Annotator Stack Component\n\nOur Label Studio annotator component inherits from the `BaseAnnotator` class. There are some methods that are core\nmethods that must be defined, like being able to register or get a dataset. Most annotators handle things like the\nstorage of state and have their own custom"}
{"input": " features, so there are quite a few extra methods specific to Label Studio.\n\nThe core Label Studio functionality that's currently enabled includes a way to register your datasets, export any\nannotations for use in separate steps as well as start the annotator daemon process. (Label Studio requires a server to\nbe running in order to use the web interface, and ZenML handles the provisioning of this server locally using the\ndetails you passed in when registering the component unless you've specified that you want to use a deployed instance.)\n\n#### Standard Steps\n\nZenML offers some standard steps (and their associated config objects) which will get you up and running with the Label\nStudio integration quickly. These include:\n\n* `LabelStudioDatasetRegistrationConfig` - a step config object to be used when registering a dataset with Label studio\n  using the `get_or_create_dataset` step\n* `LabelStudioDatasetSyncConfig` - a step config object to be used when registering a dataset with Label studio using\n  the `sync_new_data_to_label_studio` step. Note that this requires a ZenML secret to have been pre-registered with your\n  artifact store as being the one that holds authentication secrets specific to your particular cloud provider. (Label\n  Studio provides some documentation on what permissions these secrets\n  require [here](https://labelstud.io/guide/tasks.html).)\n* `get_or_create_dataset` step - This takes a `LabelStudioDatasetRegistrationConfig` config object which includes the\n  name of the dataset. If it exists, this step will return the name, but if it doesn't exist then ZenML will register\n  the dataset along with the appropriate label config with Label Studio.\n* `get_labeled_data` step - This step will get all labeled data available for a particular dataset. Note that these are\n  output in a Label Studio annotation format, which will subsequently be converted into a format appropriate for your\n  specific use case.\n* `sync_new_data_to_label_studio` step - This step is for ensuring that ZenML is handling the annotations and that the\n  files being used are stored and synced with the ZenML cloud artifact store. This is an important step as part of a\n  continuous annotation workflow since you want all the subsequent steps of your workflow to remain in sync with\n  whatever new annotations are being made or have been created.\n\n#### Helper Functions\n\nLabel Studio requires the use of what it calls 'label config' when you are creating/registering your dataset. These are\nstrings containing HTML-like syntax that allow you to define a custom interface for your annotation. ZenML provides\nthree helper functions that will construct these label config strings in the case of object detection, image\nclassification, and OCR. See the\n[`integrations.label_studio.label_config_generators`](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/label_studio/label_config_generators/"}
{"input": "label_config_generators.py)\nmodule for those three functions.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Storing artifacts using GCP Cloud Storage.\n---\n\n# Google Cloud Storage (GCS)\n\nThe GCS Artifact Store is an [Artifact Store](./artifact-stores.md) flavor provided with the GCP ZenML integration that uses [the Google Cloud Storage managed object storage service](https://cloud.google.com/storage/docs/introduction) to store ZenML artifacts in a GCP Cloud Storage bucket.\n\n### When would you want to use it?\n\nRunning ZenML pipelines with [the local Artifact Store](local.md) is usually sufficient if you just want to evaluate ZenML or get started quickly without incurring the trouble and the cost of employing cloud storage services in your stack. However, the local Artifact Store becomes insufficient or unsuitable if you have more elaborate needs for your project:\n\n* if you want to share your pipeline run results with other team members or stakeholders inside or outside your organization\n* if you have other components in your stack that are running remotely (e.g. a Kubeflow or Kubernetes Orchestrator running in a public cloud).\n* if you outgrow what your local machine can offer in terms of storage space and need to use some form of private or public storage service that is shared with others\n* if you are running pipelines at scale and need an Artifact Store that can handle the demands of production-grade MLOps\n\nIn all these cases, you need an Artifact Store that is backed by a form of public cloud or self-hosted shared object storage service.\n\nYou should use the GCS Artifact Store when you decide to keep your ZenML artifacts in a shared object storage and if you have access to the Google Cloud Storage managed service. You should consider one of the other [Artifact Store flavors](./artifact-stores.md#artifact-store-flavors) if you don't have access to the GCP Cloud Storage service.\n\n### How do you deploy it?\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding a GCS Artifact Store? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML GCP Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\n\nThe GCS Artifact Store flavor is provided by the GCP ZenML integration, you need to install it on your local machine to be able to register a GCS Artifact Store and add it to your stack:\n\n```shell\nzenml integration install gcp -y\n```\n\nThe only configuration parameter mandatory for registering a GCS Artifact Store is the root path URI, which needs to point to a GCS bucket and take the form `gs://bucket-name`. Please read [the Google Cloud Storage documentation](https"}
{"input": "://cloud.google.com/storage/docs/creating-buckets) on how to configure a GCS bucket.\n\nWith the URI to your GCS bucket known, registering a GCS Artifact Store can be done as follows:\n\n```shell\n# Register the GCS artifact store\nzenml artifact-store register gs_store -f gcp --path=gs://bucket-name\n\n# Register and set a stack with the new artifact store\nzenml stack register custom_stack -a gs_store ... --set\n```\n\nDepending on your use case, however, you may also need to provide additional configuration parameters pertaining to [authentication](gcp.md#authentication-methods) to match your deployment scenario.\n\n#### Infrastructure Deployment\n\nA GCS Artifact Store can be deployed directly from the ZenML CLI:\n\n```shell\nzenml artifact-store deploy gcs_artifact_store --flavor=gcp --provider=gcp ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the [dedicated documentation section](../../how-to/stack-deployment/README.md).\n\n#### Authentication Methods\n\nIntegrating and using a GCS Artifact Store in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Implicit Authentication_ method. However, the recommended way to authenticate to the GCP cloud platform is through [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the GCS Artifact Store with other remote stack components also running in GCP.\n\n{% tabs %}\n{% tab title=\"Implicit Authentication\" %}\nThis method uses the implicit GCP authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure a GCS Artifact Store. You don't need to supply credentials explicitly when you register the GCS Artifact Store, as it leverages the local credentials and configuration that the Google Cloud CLI stores on your local machine. However, you will need to install and set up the Google Cloud CLI on your machine as a prerequisite, as covered in [the Google Cloud documentation](https://cloud.google.com/sdk/docs/install-sdk) , before you register the GCS Artifact Store.\n\n{% hint style=\"warning\" %}\nCertain dashboard functionality, such as visualizing or deleting artifacts, is not available when using an implicitly authenticated artifact store together with a deployed ZenML server because the ZenML server will not have permission to access the filesystem.\n\nThe implicit authentication method also needs to be coordinated with other stack components that are highly dependent on the Artifact Store and need to interact with it directly to the function. If these components are not running on your machine, they do not have access to the local Google Cloud CLI configuration and"}
{"input": " will encounter authentication failures while trying to access the GCS Artifact Store:\n\n* [Orchestrators](../orchestrators/orchestrators.md) need to access the Artifact Store to manage pipeline artifacts\n* [Step Operators](../step-operators/step-operators.md) need to access the Artifact Store to manage step-level artifacts\n* [Model Deployers](../model-deployers/model-deployers.md) need to access the Artifact Store to load served models\n\nTo enable these use cases, it is recommended to use [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) to link your GCS Artifact Store to the remote GCS bucket.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"GCP Service Connector (recommended)\" %}\nTo set up the GCS Artifact Store to authenticate to GCP and access a GCS bucket, it is recommended to leverage the many features provided by [the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) such as auto-configuration, best security practices regarding long-lived credentials and reusing the same credentials across multiple stack components.\n\nIf you don't already have a GCP Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure a GCP Service Connector that can be used to access more than one GCS bucket or even more than one type of GCP resource:\n\n```sh\nzenml service-connector register --type gcp -i\n```\n\nA non-interactive CLI example that leverages [the Google Cloud CLI configuration](https://cloud.google.com/sdk/docs/install-sdk) on your local machine to auto-configure a GCP Service Connector targeting a single GCS bucket is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type gcp --resource-type gcs-bucket --resource-name <GCS_BUCKET_NAME> --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register gcs-zenml-bucket-sl --type gcp --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl --auto-configure\n\u2838 Registering service connector 'gcs-zenml-bucket-sl'...\nSuccessfully registered service connector `gcs-zenml-bucket-sl` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n>"}
{"input": " **Note**: Please remember to grant the entity associated with your GCP credentials permissions to read and write to your GCS bucket as well as to list accessible GCS buckets. For a full list of permissions required to use a GCP Service Connector to access one or more GCS buckets, please refer to the [GCP Service Connector GCS bucket resource type documentation](../../how-to/auth-management/gcp-service-connector.md#gcs-bucket) or read the documentation available in the interactive CLI commands and dashboard. The GCP Service Connector supports [many different authentication methods](../../how-to/auth-management/gcp-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use case.\n\nIf you already have one or more GCP Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the GCS bucket you want to use for your GCS Artifact Store by running e.g.:\n\n```sh\nzenml service-connector list-resources --resource-type gcs-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME      \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 7f0c69ba-9424-40ae-8ea6-04f35c2eba9d \u2502 gcp-user-account    \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl                            \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 gs://zenml-core.appspot.com                     \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 gs://zenml-core_cloudbuild                      \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 gs://zenml-datasets                             \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 gs://zenml-internal-artifact-store              \u2503\n\u2503                                      \u2502                     \u2502                \u2502               \u2502 gs://zenml-kubeflow-artifact-store              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 2a0bec1b-9787-4"}
{"input": "bd7-8d4a-9a47b6f61643 \u2502 gcs-zenml-bucket-sl \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl                            \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on a GCP Service Connector to use to connect to the target GCS bucket, you can register the GCS Artifact Store as follows:\n\n```sh\n# Register the GCS artifact-store and reference the target GCS bucket\nzenml artifact-store register <GCS_STORE_NAME> -f gcp \\\n    --path='gs://your-bucket'\n\n# Connect the GCS artifact-store to the target bucket via a GCP Service Connector\nzenml artifact-store connect <GCS_STORE_NAME> -i\n```\n\nA non-interactive version that connects the GCS Artifact Store to a target GCP Service Connector:\n\n```sh\nzenml artifact-store connect <GCS_STORE_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml artifact-store connect gcs-zenml-bucket-sl --connector gcs-zenml-bucket-sl\nSuccessfully connected artifact store `gcs-zenml-bucket-sl` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME      \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES       \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 2a0bec1b-9787-4bd7-8d4a-9a47b6f61643 \u2502 gcs-zenml-bucket-sl \u2502 \ud83d\udd35 gcp         \u2502 \ud83d\udce6 gcs-bucket \u2502 gs://zenml-bucket-sl \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\ufffd"}
{"input": "\ufffd\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the GCS Artifact Store in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new artifact store\nzenml stack register <STACK_NAME> -a <GCS_STORE_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"GCP Credentials\" %}\nWhen you register the GCS Artifact Store, you can [generate a GCP Service Account Key](https://cloud.google.com/docs/authentication/application-default-credentials#attached-sa), store it in a [ZenML Secret](../../getting-started/deploying-zenml/secret-management.md) and then reference it in the Artifact Store configuration.\n\nThis method has some advantages over the implicit authentication method:\n\n* you don't need to install and configure the GCP CLI on your host\n* you don't need to care about enabling your other stack components (orchestrators, step operators and model deployers) to have access to the artifact store through GCP Service Accounts and Workload Identity\n* you can combine the GCS artifact store with other stack components that are not running in GCP\n\nFor this method, you need to [create a user-managed GCP service account](https://cloud.google.com/iam/docs/service-accounts-create), grant it privileges to read and write to your GCS bucket (i.e. use the `Storage Object Admin` role) and then [create a service account key](https://cloud.google.com/iam/docs/keys-create-delete#creating).\n\nWith the service account key downloaded to a local file, you can register a ZenML secret and reference it in the GCS Artifact Store configuration as follows:\n\n```shell\n# Store the GCP credentials in a ZenML\nzenml secret create gcp_secret \\\n    --token=@path/to/service_account_key.json\n\n# Register the GCS artifact store and reference the ZenML secret\nzenml artifact-store register gcs_store -f gcp \\\n    --path='gs://your-bucket' \\\n    --authentication_secret=gcp_secret\n\n# Register and set a stack with the new artifact store\nzenml stack register custom_stack -a gs_store ... --set\n\n```\n{% endtab %}\n{% endtabs %}\n\nFor more, up-to-date information on the GCS Artifact Store implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-gcp/#zenml.integrations.gcp.artifact\\_stores.gcp\\_artifact\\_store) .\n\n### How do you use it?\n\nAside from the fact that the artifacts are stored in GCP Cloud Storage, using the GCS Artifact Store is no different from [using any other flavor of Artifact Store](./artifact-stores.md#how-to-use-it"}
{"input": ").\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing artifacts in an AWS S3 bucket.\n---\n\n# Amazon Simple Cloud Storage (S3)\n\nThe S3 Artifact Store is an [Artifact Store](./artifact-stores.md) flavor provided with the S3 ZenML integration that uses [the AWS S3 managed object storage service](https://aws.amazon.com/s3/) or one of the self-hosted S3 alternatives, such as [MinIO](https://min.io/) or [Ceph RGW](https://ceph.io/en/discover/technology/#object), to store artifacts in an S3 compatible object storage backend.\n\n### When would you want to use it?\n\nRunning ZenML pipelines with [the local Artifact Store](local.md) is usually sufficient if you just want to evaluate ZenML or get started quickly without incurring the trouble and the cost of employing cloud storage services in your stack. However, the local Artifact Store becomes insufficient or unsuitable if you have more elaborate needs for your project:\n\n* if you want to share your pipeline run results with other team members or stakeholders inside or outside your organization\n* if you have other components in your stack that are running remotely (e.g. a Kubeflow or Kubernetes Orchestrator running in a public cloud).\n* if you outgrow what your local machine can offer in terms of storage space and need to use some form of private or public storage service that is shared with others\n* if you are running pipelines at scale and need an Artifact Store that can handle the demands of production-grade MLOps\n\nIn all these cases, you need an Artifact Store that is backed by a form of public cloud or self-hosted shared object storage service.\n\nYou should use the S3 Artifact Store when you decide to keep your ZenML artifacts in a shared object storage and if you have access to the AWS S3 managed service or one of the S3 compatible alternatives (e.g. Minio, Ceph RGW). You should consider one of the other [Artifact Store flavors](./artifact-stores.md#artifact-store-flavors) if you don't have access to an S3-compatible service.\n\n### How do you deploy it?\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding an S3 Artifact Store? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML AWS Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nThe S3 Artifact Store flavor is provided by the S3 ZenML integration, you need to install it on your local machine to be able to register an S3 Artifact Store and add it to your stack:\n\n```shell\nzen"}
{"input": "ml integration install s3 -y\n```\n\nThe only configuration parameter mandatory for registering an S3 Artifact Store is the root path URI, which needs to point to an S3 bucket and take the form `s3://bucket-name`. Please read the documentation relevant to the S3 service that you are using on how to create an S3 bucket. For example, the AWS S3 documentation is available [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html).\n\nWith the URI to your S3 bucket known, registering an S3 Artifact Store and using it in a stack can be done as follows:\n\n```shell\n# Register the S3 artifact-store\nzenml artifact-store register s3_store -f s3 --path=s3://bucket-name\n\n# Register and set a stack with the new artifact store\nzenml stack register custom_stack -a s3_store ... --set\n```\n\nDepending on your use case, however, you may also need to provide additional configuration parameters pertaining to [authentication](s3.md#authentication-methods) or [pass advanced configuration parameters](s3.md#advanced-configuration) to match your S3-compatible service or deployment scenario.\n\n#### Infrastructure Deployment\n\nAn S3 Artifact Store can be deployed directly from the ZenML CLI:\n\n```shell\nzenml artifact-store deploy s3-artifact-store --flavor=s3 --provider=aws ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the [dedicated documentation section](../../how-to/stack-deployment/README.md).\n\n#### Authentication Methods\n\nIntegrating and using an S3-compatible Artifact Store in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Implicit Authentication_ method. However, the recommended way to authenticate to the AWS cloud platform is through [an AWS Service Connector](../../how-to/auth-management/aws-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the S3 Artifact Store with other remote stack components also running in AWS.\n\n{% tabs %}\n{% tab title=\"Implicit Authentication\" %}\nThis method uses the implicit AWS authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure an S3 Artifact Store. You don't need to supply credentials explicitly when you register the S3 Artifact Store, as it leverages the local credentials and configuration that the AWS CLI stores on your local machine. However, you will need to install and set up the AWS CLI on your machine as a prerequisite, as covered in [the AWS CLI documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), before you"}
{"input": " register the S3 Artifact Store.\n\n{% hint style=\"warning\" %}\nCertain dashboard functionality, such as visualizing or deleting artifacts, is not available when using an implicitly authenticated artifact store together with a deployed ZenML server because the ZenML server will not have permission to access the filesystem.\n\nThe implicit authentication method also needs to be coordinated with other stack components that are highly dependent on the Artifact Store and need to interact with it directly to work. If these components are not running on your machine, they do not have access to the local AWS CLI configuration and will encounter authentication failures while trying to access the S3 Artifact Store:\n\n* [Orchestrators](../orchestrators/orchestrators.md) need to access the Artifact Store to manage pipeline artifacts\n* [Step Operators](../step-operators/step-operators.md) need to access the Artifact Store to manage step-level artifacts\n* [Model Deployers](../model-deployers/model-deployers.md) need to access the Artifact Store to load served models\n\nTo enable these use-cases, it is recommended to use [an AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) to link your S3 Artifact Store to the remote S3 bucket.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"AWS Service Connector (recommended)\" %}\nTo set up the S3 Artifact Store to authenticate to AWS and access an S3 bucket, it is recommended to leverage the many features provided by [the AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) such as auto-configuration, best security practices regarding long-lived credentials and fine-grained access control and reusing the same credentials across multiple stack components.\n\nIf you don't already have an AWS Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure an AWS Service Connector that can be used to access more than one S3 bucket or even more than one type of AWS resource:\n\n```sh\nzenml service-connector register --type aws -i\n```\n\nA non-interactive CLI example that leverages [the AWS CLI configuration](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) on your local machine to auto-configure an AWS Service Connector targeting a single S3 bucket is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type aws --resource-type s3-bucket --resource-name <S3_BUCKET_NAME> --auto-configure\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register s3-zenfiles --type aws --resource-type s3-bucket --resource-id s3://zenfiles --auto-configure\n\u2838 Registering service connector 's3-zenfiles'...\nSuccessfully registered service connector `s3-zenfiles` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 RESOURCE TYPE \u2502 RESOURCE NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n> **Note**: Please remember to grant the entity associated with your AWS credentials permissions to read and write to your S3 bucket as well as to list accessible S3 buckets. For a full list of permissions required to use an AWS Service Connector to access one or more S3 buckets, please refer to the [AWS Service Connector S3 bucket resource type documentation](../../how-to/auth-management/aws-service-connector.md#s3-bucket) or read the documentation available in the interactive CLI commands and dashboard. The AWS Service Connector supports [many different authentication methods](../../how-to/auth-management/aws-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use case.\n\nIf you already have one or more AWS Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the S3 bucket you want to use for your S3 Artifact Store by running e.g.:\n\n```sh\nzenml service-connector list-resources --resource-type s3-bucket\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 's3-bucket' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME       \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE NAMES                                 \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 aeed6507-f94c-4329-8bc2-52b85cd8d94d \u2502 aws-s3               \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 9a810521-ef41-4e45-bb48-8569c5943dc6 \u2502 aws-implicit         \u2502"}
{"input": " \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://sagemaker-studio-907999144431-m11qlsdyqr8 \u2503\n\u2503                                      \u2502                      \u2502                \u2502               \u2502 s3://sagemaker-studio-d8a14tvjsmb              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 37c97fa0-fa47-4d55-9970-e2aa6e1b50cf \u2502 aws-secret-key       \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles                                  \u2503\n\u2503                                      \u2502                      \u2502                \u2502               \u2502 s3://zenml-demos                               \u2503\n\u2503                                      \u2502                      \u2502                \u2502               \u2502 s3://zenml-generative-chat                     \u2503\n\u2503                                      \u2502                      \u2502                \u2502               \u2502 s3://zenml-public-datasets                     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on an AWS Service Connector to use to connect to the target S3 bucket, you can register the S3 Artifact Store as follows:\n\n```sh\n# Register the S3 artifact-store and reference the target S3 bucket\nzenml artifact-store register <S3_STORE_NAME> -f s3 \\\n    --path='s3://your-bucket'\n\n# Connect the S3 artifact-store to the target bucket via an AWS Service Connector\nzenml artifact-store connect <S3_STORE_NAME> -i\n```\n\nA non-interactive version that connects the S3 Artifact Store to a target S3 bucket through an AWS Service Connector:\n\n```sh\nzenml artifact-store connect <S3_STORE_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml artifact-store connect s3-zenfiles --connector s3-zenfiles\nSuccessfully connected artifact store `s3-zenfiles` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE \u2502 RESOURCE"}
{"input": " NAMES \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 c4ee3f0a-bc69-4c79-9a74-297b2dd47d50 \u2502 s3-zenfiles    \u2502 \ud83d\udd36 aws         \u2502 \ud83d\udce6 s3-bucket  \u2502 s3://zenfiles  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the S3 Artifact Store in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new artifact store\nzenml stack register <STACK_NAME> -a <S3_STORE_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"ZenML Secret\" %}\nWhen you register the S3 Artifact Store, you can [generate an AWS access key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/), store it in a [ZenML Secret](../../getting-started/deploying-zenml/secret-management.md) and then reference it in the Artifact Store configuration.\n\nThis method has some advantages over the implicit authentication method:\n\n* you don't need to install and configure the AWS CLI on your host\n* you don't need to care about enabling your other stack components (orchestrators, step operators, and model deployers) to have access to the artifact store through IAM roles and policies\n* you can combine the S3 artifact store with other stack components that are not running in AWS\n\n> **Note**: When you create the IAM user for your AWS access key, please remember to grant the created IAM user permissions to read and write to your S3 bucket (i.e. at a minimum: `s3:PutObject`, `s3:GetObject`, `s3:ListBucket`, `s3:DeleteObject`)\n\nAfter having set up the IAM user and generated the access key, as described in the [AWS documentation](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/), you can register the S3 Artifact Store as follows:\n\n```shell\n# Store the AWS access key in a ZenML secret\nzenml secret create s3_secret \\\n    --aws_access_key_id='<YOUR_S3_ACCESS_KEY_ID>' \\\n    --aws_secret_access_key='<YOUR_S3_SECRET_KEY>'\n\n# Register the S3 artifact-store and reference the ZenML secret\nzenml artifact-store register s3_store -f s3 \\\n    --path='s3://your-bucket' \\\n    --authentication_secret"}
{"input": "=s3_secret\n\n# Register and set a stack with the new artifact store\nzenml stack register custom_stack -a s3_store ... --set\n```\n{% endtab %}\n{% endtabs %}\n\n#### Advanced Configuration\n\nThe S3 Artifact Store accepts a range of advanced configuration options that can be used to further customize how ZenML connects to the S3 storage service that you are using. These are accessible via the `client_kwargs`, `config_kwargs` and `s3_additional_kwargs` configuration attributes and are passed transparently to [the underlying S3Fs library](https://s3fs.readthedocs.io/en/latest/#s3-compatible-storage):\n\n* `client_kwargs`: arguments that will be transparently passed to [the botocore client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html#boto3.session.Session.client) . You can use it to configure parameters like `endpoint_url` and `region_name` when connecting to an S3-compatible endpoint (e.g. Minio).\n* `config_kwargs`: advanced parameters passed to [botocore.client.Config](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html).\n* `s3_additional_kwargs`: advanced parameters that are used when calling S3 API, typically used for things like `ServerSideEncryption` and `ACL`.\n\nTo include these advanced parameters in your Artifact Store configuration, pass them using JSON format during registration, e.g.:\n\n```shell\nzenml artifact-store register minio_store -f s3 \\\n    --path='s3://minio_bucket' \\\n    --authentication_secret=s3_secret \\\n    --client_kwargs='{\"endpoint_url\": \"http://minio.cluster.local:9000\", \"region_name\": \"us-east-1\"}'\n```\n\nFor more, up-to-date information on the S3 Artifact Store implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-s3/#zenml.integrations.s3.artifact\\_stores.s3\\_artifact\\_store) .\n\n### How do you use it?\n\nAside from the fact that the artifacts are stored in an S3 compatible backend, using the S3 Artifact Store is no different than [using any other flavor of Artifact Store](./artifact-stores.md#how-to-use-it).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom artifact store.\n---\n\n# Develop a custom artifact store\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\nZenML comes equipped with [Artifact Store implementations](./artifact-stores.md#artifact-store-flavors) that you can use to store artifacts on a local filesystem or in the managed AWS, GCP, or Azure cloud object storage services. However, if you need to use a different type of object storage service as a backend for your ZenML Artifact Store, you can extend ZenML to provide your own custom Artifact Store implementation.\n\n### Base Abstraction\n\nThe Artifact Store establishes one of the main components in every ZenML stack. Now, let us take a deeper dive into the fundamentals behind its abstraction, namely [the `BaseArtifactStore` class](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-artifact\\_stores/#zenml.artifact\\_stores.base\\_artifact\\_store.BaseArtifactStore):\n\n1. As ZenML only supports filesystem-based artifact stores, it features a configuration parameter called `path`, which will indicate the root path of the artifact store. When registering an artifact store, users will have to define this parameter.\n2. Moreover, there is another variable in the config class called `SUPPORTED_SCHEMES`. This is a class variable that needs to be defined in every subclass of the base artifact store configuration. It indicates the supported file path schemes for the corresponding implementation. For instance, for the Azure artifact store, this set will be defined as `{\"abfs://\", \"az://\"}`.\n3. Lastly, the base class features a set of `abstractmethod`s: `open`, `copyfile`,`exists`,`glob`,`isdir`,`listdir` ,`makedirs`,`mkdir`,`remove`, `rename`,`rmtree`,`stat`,`walk`. In the implementation of every `ArtifactStore` flavor, it is required to define these methods with respect to the flavor at hand.\n\nPutting all these considerations together, we end up with the following implementation:\n\n```python\n\nfrom zenml.enums import StackComponentType\nfrom zenml.stack import StackComponent, StackComponentConfig\n\nPathType = Union[bytes, str]\n\n\nclass BaseArtifactStoreConfig(StackComponentConfig):\n    \"\"\"Config class for `BaseArtifactStore`.\"\"\"\n\n    path: str\n\n    SUPPORTED_SCHEMES: ClassVar[Set[str]]\n\n\nclass BaseArtifactStore(StackComponent):\n    \"\"\"Base class for all ZenML artifact stores.\"\"\"\n\n    @abstractmethod\n    def open(self, name: PathType, mode: str = \"r\") -> Any:\n        \"\"\"Open a file at the given path.\"\"\"\n\n    @abstractmethod"}
{"input": "\n    def copyfile(\n            self, src: PathType, dst: PathType, overwrite: bool = False\n    ) -> None:\n        \"\"\"Copy a file from the source to the destination.\"\"\"\n\n    @abstractmethod\n    def exists(self, path: PathType) -> bool:\n        \"\"\"Returns `True` if the given path exists.\"\"\"\n\n    @abstractmethod\n    def glob(self, pattern: PathType) -> List[PathType]:\n        \"\"\"Return the paths that match a glob pattern.\"\"\"\n\n    @abstractmethod\n    def isdir(self, path: PathType) -> bool:\n        \"\"\"Returns whether the given path points to a directory.\"\"\"\n\n    @abstractmethod\n    def listdir(self, path: PathType) -> List[PathType]:\n        \"\"\"Returns a list of files under a given directory in the filesystem.\"\"\"\n\n    @abstractmethod\n    def makedirs(self, path: PathType) -> None:\n        \"\"\"Make a directory at the given path, recursively creating parents.\"\"\"\n\n    @abstractmethod\n    def mkdir(self, path: PathType) -> None:\n        \"\"\"Make a directory at the given path; parent directory must exist.\"\"\"\n\n    @abstractmethod\n    def remove(self, path: PathType) -> None:\n        \"\"\"Remove the file at the given path. Dangerous operation.\"\"\"\n\n    @abstractmethod\n    def rename(\n            self, src: PathType, dst: PathType, overwrite: bool = False\n    ) -> None:\n        \"\"\"Rename source file to destination file.\"\"\"\n\n    @abstractmethod\n    def rmtree(self, path: PathType) -> None:\n        \"\"\"Deletes dir recursively. Dangerous operation.\"\"\"\n\n    @abstractmethod\n    def stat(self, path: PathType) -> Any:\n        \"\"\"Return the stat descriptor for a given file path.\"\"\"\n\n    @abstractmethod\n    def walk(\n            self,\n            top: PathType,\n            topdown: bool = True,\n            onerror: Optional[Callable[..., None]] = None,\n    ) -> Iterable[Tuple[PathType, List[PathType], List[PathType]]]:\n        \"\"\"Return an iterator that walks the contents of the given directory.\"\"\"\n\n\nclass BaseArtifactStoreFlavor(Flavor):\n    \"\"\"Base class for artifact store flavors.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -> Type[\"BaseArtifactStore\"]:\n        \"\"\"Returns the name of the flavor.\"\"\"\n\n    @property\n    def type(self) -> StackComponentType:\n        \"\"\"Returns the flavor type.\"\"\"\n        return StackComponentType.ARTIFACT_STORE\n\n    @property\n    def config_class(self) -> Type[StackComponentConfig]:\n        \"\"\"Config class.\"\"\"\n        return BaseArtifactStoreConfig\n\n    @property\n    @abstractmethod\n    def implementation_class(self) -> Type[\"BaseArtifactStore\"]:\n        \"\"\"Implementation class.\"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the abstraction layer. In order to see the"}
{"input": " full implementation and get the complete docstrings, please check the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-artifact\\_stores/#zenml.artifact\\_stores.base\\_artifact\\_store.BaseArtifactStore) .\n{% endhint %}\n\n**The effect on the `zenml.io.fileio`**\n\nIf you created an instance of an artifact store, added it to your stack, and activated the stack, it will create a filesystem each time you run a ZenML pipeline and make it available to the `zenml.io.fileio` module.\n\nThis means that when you utilize a method such as `fileio.open(...)` with a file path that starts with one of the `SUPPORTED_SCHEMES` within your steps or materializers, it will be able to use the `open(...)` method that you defined within your artifact store.\n\n### Build your own custom artifact store\n\nIf you want to implement your own custom Artifact Store, you can follow the following steps:\n\n1. Create a class that inherits from [the `BaseArtifactStore` class](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-artifact\\_stores/#zenml.artifact\\_stores.base\\_artifact\\_store.BaseArtifactStore) and implements the abstract methods.\n2. Create a class that inherits from [the `BaseArtifactStoreConfig` class](custom.md) and fill in the `SUPPORTED_SCHEMES` based on your file system.\n3. Bring both of these classes together by inheriting from [the `BaseArtifactStoreFlavor` class](custom.md).\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml artifact-store flavor register <path.to.MyArtifactStoreFlavor>\n```\n\nFor example, if your flavor class `MyArtifactStoreFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml artifact-store flavor register flavors.my_flavor.MyArtifactStoreFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually, it's better to not have to rely on this mechanism and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new custom artifact store flavor in the list of available artifact store flavors:\n\n```shell\nzenml artifact-store flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to"}
{"input": " draw attention to when and how these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomArtifactStoreFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomArtifactStoreConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomArtifactStore** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomArtifactStoreFlavor` and the `CustomArtifactStoreConfig` are implemented in a different module/path than the actual `CustomArtifactStore`).\n{% endhint %}\n\n#### Enabling Artifact Visualizations with Custom Artifact Stores\n\nZenML automatically saves visualizations for many common data types and allows you to view these visualizations in the ZenML dashboard. Under the hood, this works by saving the visualizations together with the artifacts in the artifact store.\n\nIn order to load and display these visualizations, ZenML needs to be able to load and access the corresponding artifact store. This means that your custom artifact store needs to be configured in a way that allows authenticating to the back-end without relying on the local environment, e.g., by embedding the authentication credentials in the stack component configuration or by referencing a secret.\n\nFurthermore, for deployed ZenML instances, you need to install the package dependencies of your artifact store implementation in the environment where you have deployed ZenML. See the [Documentation on deploying ZenML with custom Docker images](../../getting-started/deploying-zenml/deploy-with-custom-image.md) for more information on how to do that.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing artifacts on your local filesystem.\n---\n\n# Local Artifact Store\n\nThe local Artifact Store is a built-in ZenML [Artifact Store](./artifact-stores.md) flavor that uses a folder on your local filesystem to store artifacts.\n\n### When would you want to use it?\n\nThe local Artifact Store is a great way to get started with ZenML, as it doesn't require you to provision additional local resources or to interact with managed object-store services like Amazon S3 and Google Cloud Storage. All you need is the local filesystem. You should use the local Artifact Store if you're just evaluating or getting started with ZenML, or if you are still in the experimental phase and don't need to share your pipeline artifacts (dataset, models, etc.) with others.\n\n{% hint style=\"warning\" %}\nThe local Artifact Store is not meant to be utilized in production. The local filesystem cannot be shared across your team and the artifacts stored in it cannot be accessed from other machines. This also means that [artifact visualizations](../../how-to/visualize-artifacts/README.md) will not be available when using a local Artifact Store through a [ZenML instance deployed in the cloud](../../getting-started/deploying-zenml/README.md).\n\nFurthermore, the local Artifact Store doesn't cover services like high-availability, scalability, backup and restore and other features that are expected from a production grade MLOps system.\n\nThe fact that it stores artifacts on your local filesystem also means that not all stack components can be used in the same stack as a local Artifact Store:\n\n* only [Orchestrators](../orchestrators/orchestrators.md) running on the local machine, such as the [local Orchestrator](../orchestrators/local.md), a [local Kubeflow Orchestrator](../orchestrators/kubeflow.md), or a [local Kubernetes Orchestrator](../orchestrators/kubernetes.md) can be combined with a local Artifact Store\n* only [Model Deployers](../model-deployers/model-deployers.md) that are running locally, such as the [MLflow Model Deployer](../model-deployers/mlflow.md), can be used in combination with a local Artifact Store\n* [Step Operators](../step-operators/step-operators.md): none of the Step Operators can be used in the same stack as a local Artifact Store, given that their very purpose is to run ZenML steps in remote specialized environments\n\nAs you transition to a team setting or a production setting, you can replace the local Artifact Store in your stack with one of the other flavors that are better suited for these purposes, with no changes required in your code.\n{% endhint %}\n\n### How do you deploy it?\n\nThe `default` stack that comes pre-configured with ZenML already contains a local Artifact Store:\n\n```\n$ zenml stack list\nRunning without an active repository root.\nUsing the"}
{"input": " default local database.\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 STACK NAME \u2502 ARTIFACT_STORE \u2502 ORCHESTRATOR \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default    \u2502 default        \u2502 default      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n$ zenml artifact-store describe\nRunning without an active repository root.\nUsing the default local database.\nRunning with active stack: 'default'\nNo component name given; using `default` from active stack.\n                           ARTIFACT_STORE Component Configuration (ACTIVE)                           \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 COMPONENT_PROPERTY \u2502 VALUE                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 TYPE               \u2502 artifact_store                                                               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 FLAVOR             \u2502 local                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 NAME               \u2502 default                                                                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UUID               \u2502 2b7773eb-d371-4f24-96f1-fad15e74fd6e                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 PATH               \u2502 /home/stefan/.config/zenml/local_stores/2b7773eb-d371-4f24-96f1-fad15e74fd6e \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nAs shown by the `PATH` value in the `zenml artifact-store describe` output, the artifacts are stored inside a folder on your local filesystem.\n\nYou can create additional instances of local Artifact Stores and use them in your stacks as you see fit, e.g.:\n\n```shell\n# Register the local artifact store\nzenml artifact-store register custom_local --flavor local\n\n"}
{"input": "# Register and set a stack with the new artifact store\nzenml stack register custom_stack -o default -a custom_local --set\n```\n\n{% hint style=\"warning\" %}\nSame as all other Artifact Store flavors, the local Artifact Store does take in a `path` configuration parameter that can be set during registration to point to a custom path on your machine. However, it is highly recommended that you rely on the default `path` value, otherwise, it may lead to unexpected results. Other local stack components depend on the convention used for the default path to be able to access the local Artifact Store.\n{% endhint %}\n\nFor more, up-to-date information on the local Artifact Store implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-artifact\\_stores/#zenml.artifact\\_stores.local\\_artifact\\_store) .\n\n### How do you use it?\n\nAside from the fact that the artifacts are stored locally, using the local Artifact Store is no different from [using any other flavor of Artifact Store](./artifact-stores.md#how-to-use-it).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Setting up a persistent storage for your artifacts.\n---\n\n# Artifact Stores\n\nThe Artifact Store is a central component in any MLOps stack. As the name suggests, it acts as a data persistence layer where artifacts (e.g. datasets, models) ingested or generated by the machine learning pipelines are stored.\n\nZenML automatically serializes and saves the data circulated through your pipelines in the Artifact Store: datasets, models, data profiles, data and model validation reports, and generally any object that is returned by a pipeline step. This is coupled with tracking in ZenML to provide extremely useful features such as caching and provenance/lineage tracking and pipeline reproducibility.\n\n{% hint style=\"info\" %}\nNot all objects returned by pipeline steps are physically stored in the Artifact Store, nor do they have to be. How artifacts are serialized and deserialized and where their contents are stored are determined by the particular implementation of the [Materializer](../../how-to/handle-data-artifacts/handle-custom-data-types.md) associated with the artifact data type. The majority of Materializers shipped with ZenML use the Artifact Store which is part of the active Stack as the location where artifacts are kept.\n\nIf you need to store _a particular type of pipeline artifact_ in a different medium (e.g. use an external model registry to store model artifacts, or an external data lake or data warehouse to store dataset artifacts), you can write your own [Materializer](../../how-to/handle-data-artifacts/handle-custom-data-types.md) to implement the custom logic required for it. In contrast, if you need to use an entirely different storage backend to store artifacts, one that isn't already covered by one of the ZenML integrations, you can [extend the Artifact Store abstraction](custom.md) to provide your own Artifact Store implementation.\n{% endhint %}\n\nIn addition to pipeline artifacts, the Artifact Store may also be used as storage backed by other specialized stack components that need to store their data in the form of persistent object storage. The [Great Expectations Data Validator](../data-validators/great-expectations.md) is such an example.\n\nRelated concepts:\n\n* the Artifact Store is a type of Stack Component that needs to be registered as part of your ZenML [Stack](../../user-guide/production-guide/understand-stacks.md).\n* the objects circulated through your pipelines are serialized and stored in the Artifact Store using [Materializers](../../how-to/handle-data-artifacts/handle-custom-data-types.md). Materializers implement the logic required to serialize and deserialize the artifact contents and to store them and retrieve their contents to/from the Artifact Store.\n\n### When to use it\n\nThe Artifact Store is a mandatory component in the ZenML stack. It is used to store all artifacts produced by pipeline runs, and you are required to configure it in all of your stacks.\n\n#### Artifact Store Flavors\n\nOut of the box, ZenML comes with a `local` artifact"}
{"input": " store already part of the default stack that stores artifacts on your local filesystem. Additional Artifact Stores are provided by integrations:\n\n| Artifact Store                     | Flavor   | Integration | URI Schema(s)      | Notes                                                                                                                            |\n| ---------------------------------- | -------- | ----------- | ------------------ | -------------------------------------------------------------------------------------------------------------------------------- |\n| [Local](local.md)                  | `local`  | _built-in_  | None               | This is the default Artifact Store. It stores artifacts on your local filesystem. Should be used only for running ZenML locally. |\n| [Amazon S3](s3.md)                 | `s3`     | `s3`        | `s3://`            | Uses AWS S3 as an object store backend                                                                                           |\n| [Google Cloud Storage](gcp.md)     | `gcp`    | `gcp`       | `gs://`            | Uses Google Cloud Storage as an object store backend                                                                             |\n| [Azure](azure.md)                  | `azure`  | `azure`     | `abfs://`, `az://` | Uses Azure Blob Storage as an object store backend                                                                               |\n| [Custom Implementation](custom.md) | _custom_ |             | _custom_           | Extend the Artifact Store abstraction and provide your own implementation                                                        |\n\nIf you would like to see the available flavors of Artifact Stores, you can use the command:\n\n```shell\nzenml artifact-store flavor list\n```\n\n{% hint style=\"info\" %}\nEvery Artifact Store has a `path` attribute that must be configured when it is registered with ZenML. This is a URI pointing to the root path where all objects are stored in the Artifact Store. It must use a URI schema that is supported by the Artifact Store flavor. For example, the S3 Artifact Store will need a URI that contains the `s3://` schema:\n\n```shell\nzenml artifact-store register s3_store -f s3 --path s3://my_bucket\n```\n{% endhint %}\n\n### How to use it\n\nThe Artifact Store provides low-level object storage services for other ZenML mechanisms. When you develop ZenML pipelines, you normally don't even have to be aware of its existence or interact with it directly. ZenML provides higher-level APIs that can be used as an alternative to store and access artifacts:\n\n* return one or more objects from your pipeline steps to have them automatically saved in the active Artifact Store as pipeline artifacts.\n* [retrieve pipeline artifacts](../../how-to/handle-data-artifacts/load-artifacts-into-memory.md) from the active Artifact Store after a pipeline run is complete.\n\nYou will probably need to interact with the [low-level Artifact Store API](artifact-stores.md#the-artifact-store-api) directly:\n\n* if you implement custom [Materializers](../../how-to/handle-data-artifacts/handle-custom-data-types.md) for your artifact data types\n* if you want to store custom objects in the Artifact"}
{"input": " Store\n\n#### The Artifact Store API\n\nAll ZenML Artifact Stores implement [the same IO API](custom.md) that resembles a standard file system. This allows you to access and manipulate the objects stored in the Artifact Store in the same manner you would normally handle files on your computer and independently of the particular type of Artifact Store that is configured in your ZenML stack.\n\nAccessing the low-level Artifact Store API can be done through the following Python modules:\n\n* `zenml.io.fileio` provides low-level utilities for manipulating Artifact Store objects (e.g. `open`, `copy`, `rename` , `remove`, `mkdir`). These functions work seamlessly across Artifact Stores types. They have the same signature as the [Artifact Store abstraction methods](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-artifact\\_stores/#zenml.artifact\\_stores.base\\_artifact\\_store.BaseArtifactStore) ( in fact, they are one and the same under the hood).\n* [zenml.utils.io\\_utils](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-utils/#zenml.utils.io\\_utils) includes some higher-level helper utilities that make it easier to find and transfer objects between the Artifact Store and the local filesystem or memory.\n\n{% hint style=\"info\" %}\nWhen calling the Artifact Store API, you should always use URIs that are relative to the Artifact Store root path, otherwise, you risk using an unsupported protocol or storing objects outside the store. You can use the `Repository` singleton to retrieve the root path of the active Artifact Store and then use it as a base path for artifact URIs, e.g.:\n\n```python\nimport os\nfrom zenml.client import Client\nfrom zenml.io import fileio\n\nroot_path = Client().active_stack.artifact_store.path\n\nartifact_contents = \"example artifact\"\nartifact_path = os.path.join(root_path, \"artifacts\", \"examples\")\nartifact_uri = os.path.join(artifact_path, \"test.txt\")\nfileio.makedirs(artifact_path)\nwith fileio.open(artifact_uri, \"w\") as f:\n    f.write(artifact_contents)\n```\n\nWhen using the Artifact Store API to write custom Materializers, the base artifact URI path is already provided. See the documentation on [Materializers](../../how-to/handle-data-artifacts/handle-custom-data-types.md) for an example.\n{% endhint %}\n\nThe following are some code examples showing how to use the Artifact Store API for various operations:\n\n* creating folders, writing and reading data directly to/from an artifact store object\n\n```python\nimport os\nfrom zenml.utils import io_utils\nfrom zenml.io import fileio\n\nfrom zenml.client import Client\n\nroot_path = Client().active_stack.artifact_store.path\n\nartifact_contents = \"example artifact\"\nartifact_path = os.path.join(root_path, \"artifacts\", \"examples\")\nartifact_uri = os.path.join(artifact_path, \""}
{"input": "test.txt\")\nfileio.makedirs(artifact_path)\nio_utils.write_file_contents_as_string(artifact_uri, artifact_contents)\n```\n\n```python\nimport os\nfrom zenml.utils import io_utils\n\nfrom zenml.client import Client\n\nroot_path = Client().active_stack.artifact_store.path\n\nartifact_path = os.path.join(root_path, \"artifacts\", \"examples\")\nartifact_uri = os.path.join(artifact_path, \"test.txt\")\nartifact_contents = io_utils.read_file_contents_as_string(artifact_uri)\n```\n\n* using a temporary local file/folder to serialize and copy in-memory objects to/from the artifact store (heavily used in Materializers to transfer information between the Artifact Store and external libraries that don't support writing/reading directly to/from the artifact store backend):\n\n```python\nimport os\nimport tempfile\nimport external_lib\n\nroot_path = Repository().active_stack.artifact_store.path\n\nartifact_path = os.path.join(root_path, \"artifacts\", \"examples\")\nartifact_uri = os.path.join(artifact_path, \"test.json\")\nfileio.makedirs(artifact_path)\n\nwith tempfile.NamedTemporaryFile(\n        mode=\"w\", suffix=\".json\", delete=True\n) as f:\n    external_lib.external_object.save_to_file(f.name)\n    # Copy it into artifact store\n    fileio.copy(f.name, artifact_uri)\n```\n\n```python\nimport os\nimport tempfile\nimport external_lib\n\nroot_path = Repository().active_stack.artifact_store.path\n\nartifact_path = os.path.join(root_path, \"artifacts\", \"examples\")\nartifact_uri = os.path.join(artifact_path, \"test.json\")\n\nwith tempfile.NamedTemporaryFile(\n        mode=\"w\", suffix=\".json\", delete=True\n) as f:\n    # Copy the serialized object from the artifact store\n    fileio.copy(artifact_uri, f.name)\n    external_lib.external_object.load_from_file(f.name)\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Storing artifacts using Azure Blob Storage\n---\n\n# Azure Blob Storage\n\nThe Azure Artifact Store is an [Artifact Store](./artifact-stores.md) flavor provided with the Azure ZenML integration that uses [the Azure Blob Storage managed object storage service](https://azure.microsoft.com/en-us/services/storage/blobs/) to store ZenML artifacts in an Azure Blob Storage container.\n\n### When would you want to use it?\n\nRunning ZenML pipelines with [the local Artifact Store](local.md) is usually sufficient if you just want to evaluate ZenML or get started quickly without incurring the trouble and the cost of employing cloud storage services in your stack. However, the local Artifact Store becomes insufficient or unsuitable if you have more elaborate needs for your project:\n\n* if you want to share your pipeline run results with other team members or stakeholders inside or outside your organization\n* if you have other components in your stack that are running remotely (e.g. a Kubeflow or Kubernetes Orchestrator running in a public cloud).\n* if you outgrow what your local machine can offer in terms of storage space and need to use some form of private or public storage service that is shared with others\n* if you are running pipelines at scale and need an Artifact Store that can handle the demands of production-grade MLOps\n\nIn all these cases, you need an Artifact Store that is backed by a form of public cloud or self-hosted shared object storage service.\n\nYou should use the Azure Artifact Store when you decide to keep your ZenML artifacts in a shared object storage and if you have access to the Azure Blob Storage managed service. You should consider one of the other [Artifact Store flavors](./artifact-stores.md#artifact-store-flavors) if you don't have access to the Azure Blob Storage service.\n\n### How do you deploy it?\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding an Azure Artifact Store? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Azure Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nThe Azure Artifact Store flavor is provided by the Azure ZenML integration, you need to install it on your local machine to be able to register an Azure Artifact Store and add it to your stack:\n\n```shell\nzenml integration install azure -y\n```\n\nThe only configuration parameter mandatory for registering an Azure Artifact Store is the root path URI, which needs to point to an Azure Blog Storage container and take the form `az://container-name` or `abfs://container-name`. Please read [the Azure Blob Storage documentation](https://docs.microsoft.com/en"}
{"input": "-us/azure/storage/blobs/storage-quickstart-blobs-portal) on how to configure an Azure Blob Storage container.\n\nWith the URI to your Azure Blob Storage container known, registering an Azure Artifact Store can be done as follows:\n\n```shell\n# Register the Azure artifact store\nzenml artifact-store register az_store -f azure --path=az://container-name\n\n# Register and set a stack with the new artifact store\nzenml stack register custom_stack -a az_store ... --set\n```\n\nDepending on your use case, however, you may also need to provide additional configuration parameters pertaining to [authentication](azure.md#authentication-methods) to match your deployment scenario.\n\n#### Authentication Methods\n\nIntegrating and using an Azure Artifact Store in your pipelines is not possible without employing some form of authentication. If you're looking for a quick way to get started locally, you can use the _Implicit Authentication_ method. However, the recommended way to authenticate to the Azure cloud platform is through [an Azure Service Connector](../../how-to/auth-management/azure-service-connector.md). This is particularly useful if you are configuring ZenML stacks that combine the Azure Artifact Store with other remote stack components also running in Azure.\n\nYou will need the following information to configure Azure credentials for ZenML, depending on which type of Azure credentials you want to use:\n\n* an Azure connection string\n* an Azure account key\n* the client ID, client secret and tenant ID of the Azure service principal\n\nFor more information on how to retrieve information about your Azure Storage Account and Access Key or connection string, please refer to this [Azure guide](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?tabs=environment-variable-windows#copy-your-credentials-from-the-azure-portal).\n\nFor information on how to configure an Azure service principal, please consult the [Azure documentation](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).\n\n{% tabs %}\n{% tab title=\"Implicit Authentication\" %}\nThis method uses the implicit Azure authentication available _in the environment where the ZenML code is running_. On your local machine, this is the quickest way to configure an Azure Artifact Store. You don't need to supply credentials explicitly when you register the Azure Artifact Store, instead, you have to set one of the following sets of environment variables:\n\n* to use [an Azure storage account key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage) , set `AZURE_STORAGE_ACCOUNT_NAME` to your account name and one of `AZURE_STORAGE_ACCOUNT_KEY` or `AZURE_STORAGE_SAS_TOKEN` to the Azure key value.\n* to use [an Azure storage account key connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage) , set `AZURE_STORAGE_CONNECTION_STRING` to your Azure Storage Key connection string\n* to use ["}
{"input": "Azure Service Principal credentials](https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals) , [create an Azure Service Principal](https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal) and then set `AZURE_STORAGE_ACCOUNT_NAME` to your account name and `AZURE_STORAGE_CLIENT_ID` , `AZURE_STORAGE_CLIENT_SECRET` and `AZURE_STORAGE_TENANT_ID` to the client ID, secret and tenant ID of your service principal\n\n{% hint style=\"warning\" %}\nCertain dashboard functionality, such as visualizing or deleting artifacts, is not available when using an implicitly authenticated artifact store together with a deployed ZenML server because the ZenML server will not have permission to access the filesystem.\n\nThe implicit authentication method also needs to be coordinated with other stack components that are highly dependent on the Artifact Store and need to interact with it directly to the function. If these components are not running on your machine, they do not have access to the local environment variables and will encounter authentication failures while trying to access the Azure Artifact Store:\n\n* [Orchestrators](../orchestrators/orchestrators.md) need to access the Artifact Store to manage pipeline artifacts\n* [Step Operators](../step-operators/step-operators.md) need to access the Artifact Store to manage step-level artifacts\n* [Model Deployers](../model-deployers/model-deployers.md) need to access the Artifact Store to load served models\n\nTo enable these use cases, it is recommended to use [an Azure Service Connector](../../how-to/auth-management/azure-service-connector.md) to link your Azure Artifact Store to the remote Azure Blob storage container.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"Azure Service Connector (recommended)\" %}\nTo set up the Azure Artifact Store to authenticate to Azure and access an Azure Blob storage container, it is recommended to leverage the many features provided by [the Azure Service Connector](../../how-to/auth-management/azure-service-connector.md) such as auto-configuration, best security practices regarding long-lived credentials and reusing the same credentials across multiple stack components.\n\nIf you don't already have an Azure Service Connector configured in your ZenML deployment, you can register one using the interactive CLI command. You have the option to configure an Azure Service Connector that can be used to access more than one Azure blob storage container or even more than one type of Azure resource:\n\n```sh\nzenml service-connector register --type azure -i\n```\n\nA non-interactive CLI example that uses [Azure Service Principal credentials](https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals) to configure an Azure Service Connector targeting a single Azure Blob storage container is:\n\n```sh\nzenml service-connector register <CONNECTOR_NAME> --type azure --auth-method service-principal --tenant"}
{"input": "_id=<AZURE_TENANT_ID> --client_id=<AZURE_CLIENT_ID> --client_secret=<AZURE_CLIENT_SECRET> --resource-type blob-container --resource-id <BLOB_CONTAINER_NAME>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml service-connector register azure-blob-demo --type azure --auth-method service-principal --tenant_id=a79f3633-8f45-4a74-a42e-68871c17b7fb --client_id=8926254a-8c3f-430a-a2fd-bdab234d491e --client_secret=AzureSuperSecret --resource-type blob-container --resource-id az://demo-zenmlartifactstore\nSuccessfully registered service connector `azure-blob-demo` with access to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   RESOURCE TYPE   \u2502 RESOURCE NAMES               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 \ud83d\udce6 blob-container \u2502 az://demo-zenmlartifactstore \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\n> **Note**: Please remember to grant the Azure service principal permissions to read and write to your Azure Blob storage container as well as to list accessible storage accounts and Blob containers. For a full list of permissions required to use an AWS Service Connector to access one or more S3 buckets, please refer to the [Azure Service Connector Blob storage container resource type documentation](../../how-to/auth-management/azure-service-connector.md#azure-blob-storage-container) or read the documentation available in the interactive CLI commands and dashboard. The Azure Service Connector supports [many different authentication methods](../../how-to/auth-management/azure-service-connector.md#authentication-methods) with different levels of security and convenience. You should pick the one that best fits your use-case.\n\nIf you already have one or more Azure Service Connectors configured in your ZenML deployment, you can check which of them can be used to access the Azure Blob storage container you want to use for your Azure Artifact Store by running e.g.:\n\n```sh\nzenml service-connector list-resources --resource-type blob-container\n```\n\n{% code title=\"Example Command Output\" %}\n```\nThe following 'blob-container' resources can be accessed by service connectors configured in your workspace:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME          \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE     \u2502 RESOURCE NAMES               \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 273d2812-2643-4446-82e6-6098b8ccdaa4 \u2502 azure-service-principal \u2502 \ud83c\udde6  azure       \u2502 \ud83d\udce6 blob-container \u2502 az://demo-zenmlartifactstore \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 f6b329e1-00f7-4392-94c9-264119e672d0 \u2502 azure-blob-demo         \u2502 \ud83c\udde6  azure       \u2502 \ud83d\udce6 blob-container \u2502 az://demo-zenmlartifactstore \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAfter having set up or decided on an Azure Service Connector to use to connect to the target Azure Blob storage container, you can register the Azure Artifact Store as follows:\n\n```sh\n# Register the Azure artifact-store and reference the target blob storage container\nzenml artifact-store register <AZURE_STORE_NAME> -f azure \\\n    --path='az://your-container'\n\n# Connect the Azure artifact-store to the target container via an Azure Service Connector\nzenml artifact-store connect <AZURE_STORE_NAME> -i\n```\n\nA non-interactive version that connects the Azure Artifact Store to a target blob storage container through an Azure Service Connector:\n\n```sh\nzenml artifact-store connect <S3_STORE_NAME> --connector <CONNECTOR_ID>\n```\n\n{% code title=\"Example Command Output\" %}\n```\n$ zenml artifact-store connect azure-blob-demo --connector azure-blob-demo\nSuccessfully connected artifact store `azure-blob-demo` to the following resources:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503             CONNECTOR ID             \u2502 CONNECTOR NAME  \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE     \u2502 RESOURCE NAMES               \u2503\n"}
{"input": "\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 f6b329e1-00f7-4392-94c9-264119e672d0 \u2502 azure-blob-demo \u2502 \ud83c\udde6  azure       \u2502 \ud83d\udce6 blob-container \u2502 az://demo-zenmlartifactstore \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n{% endcode %}\n\nAs a final step, you can use the Azure Artifact Store in a ZenML Stack:\n\n```sh\n# Register and set a stack with the new artifact store\nzenml stack register <STACK_NAME> -a <AZURE_STORE_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"ZenML Secret\" %}\nWhen you register the Azure Artifact Store, you can create a [ZenML Secret](../../getting-started/deploying-zenml/secret-management.md) to store a variety of Azure credentials and then reference it in the Artifact Store configuration:\n\n* to use [an Azure storage account key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage) , set `account_name` to your account name and one of `account_key` or `sas_token` to the Azure key or SAS token value as attributes in the ZenML secret\n* to use [an Azure storage account key connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage) , configure the `connection_string` attribute in the ZenML secret to your Azure Storage Key connection string\n* to use [Azure Service Principal credentials](https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals) , [create an Azure Service Principal](https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal) and then set `account_name` to your account name and `client_id`, `client_secret` and `tenant_id` to the client ID, secret and tenant ID of your service principal in the ZenML secret\n\nThis method has some advantages over the implicit authentication method:\n\n* you don't need to install and configure the Azure CLI on your host\n* you don't need to care about enabling your other stack components (orchestrators, step operators and model deployers) to have access to the artifact store through Azure Managed Identities\n* you can combine the Azure artifact store with other stack components that are not running in Azure\n\nConfiguring Azure credentials in"}
{"input": " a ZenML secret and then referencing them in the Artifact Store configuration could look like this:\n\n```shell\n# Store the Azure storage account key in a ZenML secret\nzenml secret create az_secret \\\n    --account_name='<YOUR_AZURE_ACCOUNT_NAME>' \\\n    --account_key='<YOUR_AZURE_ACCOUNT_KEY>'\n\n# or if you want to use a connection string\nzenml secret create az_secret \\\n    --connection_string='<YOUR_AZURE_CONNECTION_STRING>'\n\n# or if you want to use Azure ServicePrincipal credentials\nzenml secret create az_secret \\\n    --account_name='<YOUR_AZURE_ACCOUNT_NAME>' \\\n    --tenant_id='<YOUR_AZURE_TENANT_ID>' \\\n    --client_id='<YOUR_AZURE_CLIENT_ID>' \\\n    --client_secret='<YOUR_AZURE_CLIENT_SECRET>'\n\n# Alternatively for providing key-value pairs, you can utilize the '--values' option by specifying a file path containing \n# key-value pairs in either JSON or YAML format.\n# File content example: {\"account_name\":\"<YOUR_AZURE_ACCOUNT_NAME>\",...}\nzenml secret create az_secret \\\n    --values=@path/to/file.txt\n\n# Register the Azure artifact store and reference the ZenML secret\nzenml artifact-store register az_store -f azure \\\n    --path='az://your-container' \\\n    --authentication_secret=az_secret\n\n# Register and set a stack with the new artifact store\nzenml stack register custom_stack -a az_store ... --set\n```\n{% endtab %}\n{% endtabs %}\n\nFor more, up-to-date information on the Azure Artifact Store implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-azure/#zenml.integrations.azure.artifact\\_stores) .\n\n### How do you use it?\n\nAside from the fact that the artifacts are stored in Azure Blob Storage, using the Azure Artifact Store is no different from [using any other flavor of Artifact Store](./artifact-stores.md#how-to-use-it).\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Logging and visualizing experiments with neptune.ai\n---\n\n# Neptune\n\nThe Neptune Experiment Tracker is an [Experiment Tracker](./experiment-trackers.md) flavor provided with the Neptune-ZenML integration that uses [neptune.ai](https://neptune.ai/product/experiment-tracking) to log and visualize information from your pipeline steps (e.g. models, parameters, metrics).\n\n### When would you want to use it?\n\n[Neptune](https://neptune.ai/product/experiment-tracking) is a popular tool that you would normally use in the iterative ML experimentation phase to track and visualize experiment results or as a model registry for your production-ready models. Neptune can also track and visualize the results produced by your automated pipeline runs, as you make the transition towards a more production-oriented workflow.\n\nYou should use the Neptune Experiment Tracker:\n\n* if you have already been using neptune.ai to track experiment results for your project and would like to continue doing so as you are incorporating MLOps workflows and best practices in your project through ZenML.\n* if you are looking for a more visually interactive way of navigating the results produced from your ZenML pipeline runs (e.g. models, metrics, datasets)\n* if you would like to connect ZenML to neptune.ai to share the artifacts and metrics logged by your pipelines with your team, organization, or external stakeholders\n\nYou should consider one of the other [Experiment Tracker flavors](./experiment-trackers.md#experiment-tracker-flavors) if you have never worked with neptune.ai before and would rather use another experiment tracking tool that you are more familiar with.\n\n### How do you deploy it?\n\nThe Neptune Experiment Tracker flavor is provided by the Neptune-ZenML integration. You need to install it on your local machine to be able to register the Neptune Experiment Tracker and add it to your stack:\n\n```shell\nzenml integration install neptune -y\n```\n\nThe Neptune Experiment Tracker needs to be configured with the credentials required to connect to Neptune using an API token.\n\n#### Authentication Methods\n\nYou need to configure the following credentials for authentication to Neptune:\n\n* `api_token`: [API key token](https://docs.neptune.ai/setup/setting\\_api\\_token) of your Neptune account. You can create a free Neptune account [here](https://app.neptune.ai/register). If left blank, Neptune will attempt to retrieve the token from your environment variables.\n* `project`: The name of the project where you're sending the new run, in the form \"workspace-name/project-name\". If the project is not specified, Neptune will attempt to retrieve it from your environment variables.\n\n{% tabs %}\n{% tab title=\"Basic Authentication\" %}\nThis option configures the credentials for neptune.ai directly as stack component attributes.\n\n{% hint style=\"warning\" %}\nThis is not recommended for production settings as the credentials won't be stored securely and will be clearly visible in the stack configuration.\n{% endhint %}\n\n```shell\n# Register the Neptune"}
{"input": " experiment tracker\nzenml experiment-tracker register neptune_experiment_tracker --flavor=neptune \\ \n    --project=<project_name> --api_token=<token>\n\n# Register and set a stack with the new experiment tracker\nzenml stack register custom_stack -e neptune_experiment_tracker ... --set\n```\n{% endtab %}\n\n{% tab title=\"ZenML Secret (Recommended)\" %}\nThis method requires you to [configure a ZenML secret](../../how-to/interact-with-secrets.md) to store the Neptune tracking service credentials securely.\n\nYou can create the secret using the `zenml secret create` command:\n\n```shell\nzenml secret create neptune_secret \\\n    --project=<PROJECT>\n    --api_token=<API_TOKEN>\n```\n\nOnce the secret is created, you can use it to configure the `neptune` Experiment Tracker:\n\n```shell\n# Reference the project and api-token in our experiment tracker component\nzenml experiment-tracker register neptune_secret \\\n    --flavor=neptune \\\n    --project={{neptune_secret.project}} \\\n    --api_token={{neptune_secret.api_token}}\n    ...\n```\n\n{% hint style=\"info\" %}\nRead more about [ZenML Secrets](../../how-to/interact-with-secrets.md) in the ZenML documentation.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more, up-to-date information on the Neptune Experiment Tracker implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-neptune/#zenml.integrations.neptune.experiment\\_trackers.neptune\\_experiment\\_tracker) .\n\n### How do you use it?\n\nTo log information from a ZenML pipeline step using the Neptune Experiment Tracker component in the active stack, you need to enable an experiment tracker using the `@step` decorator. Then fetch the Neptune run object and use logging capabilities as you would normally do. For example:\n\n```python\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom neptune_tensorflow_keras import NeptuneCallback\n\nfrom zenml.integrations.neptune.experiment_trackers.run_state import (\n    get_neptune_run,\n)\nfrom zenml import step\n\n\n@step(experiment_tracker=\"<NEPTUNE_TRACKER_STACK_COMPONENT_NAME>\")\ndef tf_trainer(\n    x_train: np.ndarray,\n    y_train: np.ndarray,\n    x_val: np.ndarray,\n    y_val: np.ndarray,\n    epochs: int = 5,\n    lr: float = 0.001\n) -> tf.keras.Model:\n    ...\n    neptune_run = get_neptune_run()\n    model.fit(\n        x_train,\n        y_train,\n        epochs=epochs,\n        validation_data=(x_val, y_val),\n        callbacks=[\n            NeptuneCallback(run=neptune_run),\n        ],\n    )\n\n    metric = ...\n\n    neptune_run[\"<METRIC"}
{"input": "_NAME>\"] = metric\n```\n\n{% hint style=\"info\" %}\nInstead of hardcoding an experiment tracker name, you can also use the [Client](../../reference/python-client.md) to dynamically use the experiment tracker of your active stack:\n\n```python\nfrom zenml.client import Client\n\nexperiment_tracker = Client().active_stack.experiment_tracker\n\n@step(experiment_tracker=experiment_tracker.name)\ndef tf_trainer(...):\n    ...\n```\n{% endhint %}\n\n#### Additional configuration\n\nYou can pass a set of tags to the Neptune run by using the `NeptuneExperimentTrackerSettings` class, like in the example below:\n\n```python\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom zenml import step\nfrom zenml.integrations.neptune.experiment_trackers.run_state import (\n    get_neptune_run,\n)\nfrom zenml.integrations.neptune.flavors import NeptuneExperimentTrackerSettings\n\nneptune_settings = NeptuneExperimentTrackerSettings(tags={\"keras\", \"mnist\"})\n\n\n@step(\n    experiment_tracker=\"<NEPTUNE_TRACKER_STACK_COMPONENT_NAME>\",\n    settings={\n        \"experiment_tracker.neptune\": neptune_settings\n    }\n)\ndef my_step(\n    x_test: np.ndarray,\n    y_test: np.ndarray,\n    model: tf.keras.Model,\n) -> float:\n    \"\"\"Log metadata to Neptune run\"\"\"\n    neptune_run = get_neptune_run()\n    ...\n```\n\n### Neptune UI\n\nNeptune comes with a web-based UI that you can use to find further details about your tracked experiments. Each pipeline run will be logged as a separate experiment run in Neptune, which you can inspect in the Neptune UI.\n\nYou can find the URL of the Neptune run linked to a specific ZenML run printed on the console whenever a Neptune run is initialized.\n\n### Further reading\n\nCheck [Neptune's docs](https://docs.neptune.ai/integrations/zenml/) for further information on how to use this integration and Neptune in general.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Logging and visualizing experiments with MLflow.\n---\n\n# MLflow\n\nThe MLflow Experiment Tracker is an [Experiment Tracker](./experiment-trackers.md) flavor provided with the MLflow ZenML integration that uses [the MLflow tracking service](https://mlflow.org/docs/latest/tracking.html) to log and visualize information from your pipeline steps (e.g. models, parameters, metrics).\n\n## When would you want to use it?\n\n[MLflow Tracking](https://www.mlflow.org/docs/latest/tracking.html) is a very popular tool that you would normally use in the iterative ML experimentation phase to track and visualize experiment results. That doesn't mean that it cannot be repurposed to track and visualize the results produced by your automated pipeline runs, as you make the transition toward a more production-oriented workflow.\n\nYou should use the MLflow Experiment Tracker:\n\n* if you have already been using MLflow to track experiment results for your project and would like to continue doing so as you are incorporating MLOps workflows and best practices in your project through ZenML.\n* if you are looking for a more visually interactive way of navigating the results produced from your ZenML pipeline runs (e.g. models, metrics, datasets)\n* if you or your team already have a shared MLflow Tracking service deployed somewhere on-premise or in the cloud, and you would like to connect ZenML to it to share the artifacts and metrics logged by your pipelines\n\nYou should consider one of the other [Experiment Tracker flavors](./experiment-trackers.md#experiment-tracker-flavors) if you have never worked with MLflow before and would rather use another experiment tracking tool that you are more familiar with.\n\n## How do you deploy it?\n\nThe MLflow Experiment Tracker flavor is provided by the MLflow ZenML integration, you need to install it on your local machine to be able to register an MLflow Experiment Tracker and add it to your stack:\n\n```shell\nzenml integration install mlflow -y\n```\n\nThe MLflow Experiment Tracker can be configured to accommodate the following [MLflow deployment scenarios](https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded):\n\n* [Scenario 1](https://mlflow.org/docs/latest/tracking.html#scenario-1-mlflow-on-localhost): This scenario requires that you use a [local Artifact Store](../artifact-stores/local.md) alongside the MLflow Experiment Tracker in your ZenML stack. The local Artifact Store comes with limitations regarding what other types of components you can use in the same stack. This scenario should only be used to run ZenML locally and is not suitable for collaborative and production settings. No parameters need to be supplied when configuring the MLflow Experiment Tracker, e.g:\n\n```shell\n# Register the MLflow experiment tracker\nzenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow\n\n# Register and set a stack with the new experiment"}
{"input": " tracker\nzenml stack register custom_stack -e mlflow_experiment_tracker ... --set\n```\n\n* [Scenario 5](https://mlflow.org/docs/latest/tracking.html#scenario-5-mlflow-tracking-server-enabled-with-proxied-artifact-storage-access): This scenario assumes that you have already deployed an MLflow Tracking Server enabled with proxied artifact storage access. There is no restriction regarding what other types of components it can be combined with. This option requires [authentication-related parameters](mlflow.md#authentication-methods) to be configured for the MLflow Experiment Tracker.\n\n{% hint style=\"warning\" %}\nDue to a [critical severity vulnerability](https://github.com/advisories/GHSA-xg73-94fp-g449) found in older versions of MLflow, we recommend using MLflow version 2.2.1 or higher.\n{% endhint %}\n\n* [Databricks scenario](https://www.databricks.com/product/managed-mlflow): This scenario assumes that you have a Databricks workspace, and you want to use the managed MLflow Tracking server it provides. This option requires [authentication-related parameters](mlflow.md#authentication-methods) to be configured for the MLflow Experiment Tracker.\n\n### Infrastructure Deployment\n\nThe MLflow Experiment Tracker can be deployed directly from the ZenML CLI:\n\n```shell\n# optionally assigning an existing bucket to the MLflow Experiment Tracker\nzenml experiment-tracker deploy mlflow_tracker --flavor=mlflow -x mlflow_bucket=gs://my_bucket --provider=<YOUR_PROVIDER>\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the [dedicated documentation section](../../how-to/stack-deployment/README.md).\n\n### Authentication Methods\n\nYou need to configure the following credentials for authentication to a remote MLflow tracking server:\n\n* `tracking_uri`: The URL pointing to the MLflow tracking server. If using an MLflow Tracking Server managed by Databricks, then the value of this attribute should be `\"databricks\"`.\n* `tracking_username`: Username for authenticating with the MLflow tracking server.\n* `tracking_password`: Password for authenticating with the MLflow tracking server.\n* `tracking_token` (in place of `tracking_username` and `tracking_password`): Token for authenticating with the MLflow tracking server.\n* `tracking_insecure_tls` (optional): Set to skip verifying the MLflow tracking server SSL certificate.\n* `databricks_host`: The host of the Databricks workspace with the MLflow-managed server to connect to. This is only required if the `tracking_uri` value is set to `\"databricks\"`. More information: [Access the MLflow tracking server from outside Databricks](https://docs.databricks.com"}
{"input": "/applications/mlflow/access-hosted-tracking-server.html)\n\nEither `tracking_token` or `tracking_username` and `tracking_password` must be specified.\n\n{% tabs %}\n{% tab title=\"Basic Authentication\" %}\nThis option configures the credentials for the MLflow tracking service directly as stack component attributes.\n\n{% hint style=\"warning\" %}\nThis is not recommended for production settings as the credentials won't be stored securely and will be clearly visible in the stack configuration.\n{% endhint %}\n\n```shell\n# Register the MLflow experiment tracker\nzenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow \\ \n    --tracking_uri=<URI> --tracking_token=<token>\n\n# You can also register it like this:\n# zenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow \\ \n#    --tracking_uri=<URI> --tracking_username=<USERNAME> --tracking_password=<PASSWORD>\n\n# Register and set a stack with the new experiment tracker\nzenml stack register custom_stack -e mlflow_experiment_tracker ... --set\n```\n{% endtab %}\n\n{% tab title=\"ZenML Secret (Recommended)\" %}\nThis method requires you to [configure a ZenML secret](../../how-to/interact-with-secrets.md) to store the MLflow tracking service credentials securely.\n\nYou can create the secret using the `zenml secret create` command:\n\n```shell\n# Create a secret called `mlflow_secret` with key-value pairs for the\n# username and password to authenticate with the MLflow tracking server\nzenml secret create mlflow_secret \\\n    --username=<USERNAME> \\\n    --password=<PASSWORD>\n```\n\nOnce the secret is created, you can use it to configure the MLflow Experiment Tracker:\n\n```shell\n# Reference the username and password in our experiment tracker component\nzenml experiment-tracker register mlflow \\\n    --flavor=mlflow \\\n    --tracking_username={{mlflow_secret.username}} \\\n    --tracking_password={{mlflow_secret.password}} \\\n    ...\n```\n\n{% hint style=\"info\" %}\nRead more about [ZenML Secrets](../../how-to/interact-with-secrets.md) in the ZenML documentation.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more, up-to-date information on the MLflow Experiment Tracker implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-mlflow/#zenml.integrations.mlflow.experiment\\_trackers.mlflow\\_experiment\\_tracker) .\n\n## How do you use it?\n\nTo be able to log information from a ZenML pipeline step using the MLflow Experiment Tracker component in the active stack, you need to enable an experiment tracker using the `@step` decorator. Then use MLflow's logging or auto-logging capabilities as you would normally do, e.g.:\n\n```python\nimport ml"}
{"input": "flow\n\n\n@step(experiment_tracker=\"<MLFLOW_TRACKER_STACK_COMPONENT_NAME>\")\ndef tf_trainer(\n    x_train: np.ndarray,\n    y_train: np.ndarray,\n) -> tf.keras.Model:\n    \"\"\"Train a neural net from scratch to recognize MNIST digits return our\n    model or the learner\"\"\"\n\n    # compile model\n\n    mlflow.tensorflow.autolog()\n\n    # train model\n\n    # log additional information to MLflow explicitly if needed\n\n    mlflow.log_param(...)\n    mlflow.log_metric(...)\n    mlflow.log_artifact(...)\n\n    return model\n```\n\n{% hint style=\"info\" %}\nInstead of hardcoding an experiment tracker name, you can also use the [Client](../../reference/python-client.md) to dynamically use the experiment tracker of your active stack:\n\n```python\nfrom zenml.client import Client\n\nexperiment_tracker = Client().active_stack.experiment_tracker\n\n@step(experiment_tracker=experiment_tracker.name)\ndef tf_trainer(...):\n    ...\n```\n{% endhint %}\n\n### MLflow UI\n\nMLflow comes with its own UI that you can use to find further details about your tracked experiments.\n\nYou can find the URL of the MLflow experiment linked to a specific ZenML run via the metadata of the step in which the experiment tracker was used:\n\n```python\nfrom zenml.client import Client\n\nlast_run = client.get_pipeline(\"<PIPELINE_NAME>\").last_run\ntrainer_step = last_run.get_step(\"<STEP_NAME>\")\ntracking_url = trainer_step.run_metadata[\"experiment_tracker_url\"].value\nprint(tracking_url)\n```\n\nThis will be the URL of the corresponding experiment in your deployed MLflow instance, or a link to the corresponding mlflow experiment file if you are using local MLflow.\n\n{% hint style=\"info\" %}\nIf you are using local MLflow, you can use the `mlflow ui` command to start MLflow at [`localhost:5000`](http://localhost:5000/) where you can then explore the UI in your browser.\n\n```bash\nmlflow ui --backend-store-uri <TRACKING_URL>\n```\n{% endhint %}\n\n### Additional configuration\n\nFor additional configuration of the MLflow experiment tracker, you can pass `MLFlowExperimentTrackerSettings` to create nested runs or add additional tags to your MLflow runs:\n\n```python\nimport mlflow\nfrom zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import MLFlowExperimentTrackerSettings\n\nmlflow_settings = MLFlowExperimentTrackerSettings(\n    nested=True,\n    tags={\"key\": \"value\"}\n)\n\n\n@step(\n    experiment_tracker=\"<MLFLOW_TRACKER_STACK_COMPONENT_NAME>\",\n    settings={\n        \"experiment_tracker.mlflow\": mlflow_settings\n    }\n)\ndef step_one(\n    data: np.ndarray,\n) -> np.ndarray:\n    ...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-mlflow/#zen"}
{"input": "ml.integrations.mlflow.flavors.mlflow\\_experiment\\_tracker\\_flavor.MLFlowExperimentTrackerSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom experiment tracker.\n---\n\n# Develop a custom experiment tracker\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n{% hint style=\"warning\" %}\n**Base abstraction in progress!**\n\nWe are actively working on the base abstraction for the Experiment Tracker, which will be available soon. As a result, their extension is not recommended at the moment. When you are selecting an Experiment Tracker for your stack, you can use one of [the existing flavors](./experiment-trackers.md#experiment-tracker-flavors).\n\nIf you need to implement your own Experiment Tracker flavor, you can still do so, but keep in mind that you may have to refactor it when the base abstraction is released.\n{% endhint %}\n\n### Build your own custom experiment tracker\n\nIf you want to create your own custom flavor for an experiment tracker, you can follow the following steps:\n\n1. Create a class that inherits from the `BaseExperimentTracker` class and implements the abstract methods.\n2. If you need any configuration, create a class that inherits from the `BaseExperimentTrackerConfig` class and add your configuration parameters.\n3. Bring both the implementation and the configuration together by inheriting from the `BaseExperimentTrackerFlavor` class.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml experiment-tracker flavor register <path.to.MyExperimentTrackerFlavor>\n```\n\nFor example, if your flavor class `MyExperimentTrackerFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml experiment-tracker flavor register flavors.my_flavor.MyExperimentTrackerFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually, it's better to not have to rely on this mechanism and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml experiment-tracker flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions are coming into play in a"}
{"input": " ZenML workflow.\n\n* The **CustomExperimentTrackerFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomExperimentTrackerConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomExperimentTracker** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomExperimentTrackerFlavor` and the `CustomExperimentTrackerConfig` are implemented in a different module/path than the actual `CustomExperimentTracker`).\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Logging and visualizing ML experiments.\n---\n\n# Experiment Trackers\n\nExperiment trackers let you track your ML experiments by logging extended information about your models, datasets,\nmetrics, and other parameters and allowing you to browse them, visualize them and compare them between runs. In the\nZenML world, every pipeline run is considered an experiment, and ZenML facilitates the storage of experiment results\nthrough Experiment Tracker stack components. This establishes a clear link between pipeline runs and experiments.\n\nRelated concepts:\n\n* the Experiment Tracker is an optional type of Stack Component that needs to be registered as part of your\n  ZenML [Stack](/docs/book/user-guide/production-guide/understand-stacks.md).\n* ZenML already provides versioning and tracking for the pipeline artifacts by storing artifacts in\n  the [Artifact Store](../artifact-stores/artifact-stores.md).\n\n### When to use it\n\nZenML already records information about the artifacts circulated through your pipelines by means of the\nmandatory [Artifact Store](../artifact-stores/artifact-stores.md).\n\nHowever, these ZenML mechanisms are meant to be used programmatically and can be more difficult to work with without a\nvisual interface.\n\nExperiment Trackers on the other hand are tools designed with usability in mind. They include extensive UIs providing\nusers with an interactive and intuitive interface that allows them to browse and visualize the information logged during\nthe ML pipeline runs.\n\nYou should add an Experiment Tracker to your ZenML stack and use it when you want to augment ZenML with the visual\nfeatures provided by experiment tracking tools.\n\n### How they experiment trackers slot into the stack\n\nHere is an architecture diagram that shows how experiment trackers fit into the overall story of a remote stack.\n\n![Experiment Tracker](../../.gitbook/assets/Remote_with_exp_tracker.png)\n\n#### Experiment Tracker Flavors\n\nExperiment Trackers are optional stack components provided by integrations:\n\n| Experiment Tracker                 | Flavor    | Integration | Notes                                                                                           |\n|------------------------------------|-----------|-------------|-------------------------------------------------------------------------------------------------|\n| [Comet](comet.md)                | `comet`  | `comet`    | Add Comet experiment tracking and visualization capabilities to your ZenML pipelines           |\n| [MLflow](mlflow.md)                | `mlflow`  | `mlflow`    | Add MLflow experiment tracking and visualization capabilities to your ZenML pipelines           |\n| [Neptune](neptune.md)              | `neptune` | `neptune`   | Add Neptune experiment tracking and visualization capabilities to your ZenML pipelines          |\n| [Weights & Biases](wandb.md)       | `wandb`   | `wandb`     | Add Weights & Biases experiment tracking and visualization capabilities to your ZenML pipelines |\n| [Custom Implementation](custom.md) | _custom_  |             | _custom_                                                                                        |\n\nIf you would like to see the available flavors of Experiment Tracker, you can use the command:\n\n"}
{"input": "```shell\nzenml experiment-tracker flavor list\n```\n\n### How to use it\n\nEvery Experiment Tracker has different capabilities and uses a different way of logging information from your pipeline\nsteps, but it generally works as follows:\n\n* first, you have to configure and add an Experiment Tracker to your ZenML stack\n* next, you have to explicitly enable the Experiment Tracker for individual steps in your pipeline by decorating them\n  with the included decorator\n* in your steps, you have to explicitly log information (e.g. models, metrics, data) to the Experiment Tracker same as\n  you would if you were using the tool independently of ZenML\n* finally, you can access the Experiment Tracker UI to browse and visualize the information logged during your pipeline\n  runs. You can use the following code snippet to get the URL of the experiment tracker UI for the experiment linked to\n  a certain step of your pipeline run:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"<PIPELINE_RUN_NAME>\")\nstep = pipeline_run.steps[\"<STEP_NAME>\"]\nexperiment_tracker_url = step.run_metadata[\"experiment_tracker_url\"].value\n```\n\n{% hint style=\"info\" %}\nExperiment trackers will automatically declare runs as failed if the corresponding ZenML pipeline step fails.\n{% endhint %}\n\nConsult the documentation for the\nparticular [Experiment Tracker flavor](experiment-trackers.md#experiment-tracker-flavors) that you plan on using or are\nusing in your stack for detailed information about how to use it in your ZenML pipelines.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Logging and visualizing experiments with Comet.\n---\n\n# Comet\n\nThe Comet Experiment Tracker is an [Experiment Tracker](./experiment-trackers.md) flavor provided with the Comet ZenML integration that uses [the Comet experiment tracking platform](https://www.comet.com/site/products/ml-experiment-tracking/) to log and visualize information from your pipeline steps (e.g., models, parameters, metrics).\n\n### When would you want to use it?\n\n[Comet](https://www.comet.com/site/products/ml-experiment-tracking/) is a popular platform that you would normally use in the iterative ML experimentation phase to track and visualize experiment results. That doesn't mean that it cannot be repurposed to track and visualize the results produced by your automated pipeline runs, as you make the transition towards a more production-oriented workflow.\n\nYou should use the Comet Experiment Tracker:\n\n* if you have already been using Comet to track experiment results for your project and would like to continue doing so as you are incorporating MLOps workflows and best practices in your project through ZenML.\n* if you are looking for a more visually interactive way of navigating the results produced from your ZenML pipeline runs (e.g., models, metrics, datasets)\n* if you would like to connect ZenML to Comet to share the artifacts and metrics logged by your pipelines with your team, organization, or external stakeholders\n\nYou should consider one of the other [Experiment Tracker flavors](./experiment-trackers.md#experiment-tracker-flavors) if you have never worked with Comet before and would rather use another experiment tracking tool that you are more familiar with.\n\n### How do you deploy it?\n\nThe Comet Experiment Tracker flavor is provided by the Comet ZenML integration. You need to install it on your local machine to be able to register a Comet Experiment Tracker and add it to your stack:\n\n```bash\nzenml integration install comet -y\n```\n\nThe Comet Experiment Tracker needs to be configured with the credentials required to connect to the Comet platform using one of the available authentication methods.\n\n#### Authentication Methods\n\nYou need to configure the following credentials for authentication to the Comet platform:\n\n* `api_key`: Mandatory API key token of your Comet account.\n* `project_name`: The name of the project where you're sending the new experiment. If the project is not specified, the experiment is put in the default project associated with your API key.\n* `workspace`: Optional. The name of the workspace where your project is located. If not specified, the default workspace associated with your API key will be used.\n\n{% tabs %}\n{% tab title=\"Basic Authentication\" %}\nThis option configures the credentials for the Comet platform directly as stack component attributes.\n\n{% hint style=\"warning\" %}\nThis is not recommended for production settings as the credentials won't be stored securely and will be clearly visible in the stack configuration.\n{% endhint %}\n\n```bash\n# Register the Comet experiment tracker\nzenml experiment-tracker register comet_experiment_tracker --flavor=comet \\\n"}
{"input": "    --workspace=<workspace> --project_name=<project_name> --api_key=<key>\n\n# Register and set a stack with the new experiment tracker\nzenml stack register custom_stack -e comet_experiment_tracker ... --set\n```\n{% endtab %}\n\n{% tab title=\"ZenML Secret (Recommended)\" %}\nThis method requires you to [configure a ZenML secret](../../getting-started/deploying-zenml/secret-management.md) to store the Comet tracking service credentials securely.\n\nYou can create the secret using the `zenml secret create` command:\n\n```bash\nzenml secret create comet_secret \\\n    --workspace=<WORKSPACE> \\\n    --project_name=<PROJECT_NAME> \\\n    --api_key=<API_KEY>\n```\n\nOnce the secret is created, you can use it to configure the Comet Experiment Tracker:\n\n```bash\n# Reference the workspace, project, and api-key in our experiment tracker component\nzenml experiment-tracker register comet_tracker \\\n    --flavor=comet \\\n    --workspace={{comet_secret.workspace}} \\\n    --project_name={{comet_secret.project_name}} \\\n    --api_key={{comet_secret.api_key}}\n    ...\n```\n\n{% hint style=\"info\" %}\nRead more about [ZenML Secrets](../../getting-started/deploying-zenml/secret-management.md) in the ZenML documentation.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more up-to-date information on the Comet Experiment Tracker implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-comet/#zenml.integrations.comet.experiment\\_trackers.comet\\_experiment\\_tracker).\n\n### How do you use it?\n\nTo be able to log information from a ZenML pipeline step using the Comet Experiment Tracker component in the active stack, you need to enable an experiment tracker using the `@step` decorator. Then use Comet logging capabilities as you would normally do, e.g.:\n\n```python\nfrom zenml.client import Client\n\nexperiment_tracker = Client().active_stack.experiment_tracker\n\n@step(experiment_tracker=experiment_tracker.name)\ndef my_step():\n    ...\n    experiment_tracker.log_metrics({\"my_metric\": 42})\n    experiment_tracker.log_params({\"my_param\": \"hello\"})\n    ...\n```\n\n{% hint style=\"info\" %}\nInstead of hardcoding an experiment tracker name, you can also use the [Client](../../reference/python-client.md) to dynamically use the experiment tracker of your active stack, as shown in the example above.\n{% endhint %}\n\n### Comet UI\n\nComet comes with a web-based UI that you can use to find further details about your tracked experiments.\n\nEvery ZenML step that uses Comet should create a separate experiment which you can inspect in the Comet UI.\n\nYou can find the URL of the Comet experiment linked to a specific ZenML run via the metadata"}
{"input": " of the step in which the experiment tracker was used:\n\n```python\nfrom zenml.client import Client\n\nlast_run = client.get_pipeline(\"<PIPELINE_NAME>\").last_run\ntrainer_step = last_run.get_step(\"<STEP_NAME>\")\ntracking_url = trainer_step.run_metadata[\"experiment_tracker_url\"].value\nprint(tracking_url)\n```\n\nAlternatively, you can see an overview of all experiments at `https://www.comet.com/{WORKSPACE_NAME}/{PROJECT_NAME}/experiments/`.\n\n{% hint style=\"info\" %}\nThe naming convention of each Comet experiment is `{pipeline_run_name}_{step_name}` (e.g., `comet_example_pipeline-25_Apr_22-20_06_33_535737_my_step`), and each experiment will be tagged with both `pipeline_name` and `pipeline_run_name`, which you can use to group and filter experiments.\n{% endhint %}\n\n#### Additional configuration\n\nFor additional configuration of the Comet experiment tracker, you can pass `CometExperimentTrackerSettings` to provide additional tags for your experiments:\n\n```\nfrom zenml.integrations.comet.flavors.comet_experiment_tracker_flavor import CometExperimentTrackerSettings\n\ncomet_settings = CometExperimentTrackerSettings(\n    tags=[\"some_tag\"]\n)\n\n@step(\n    experiment_tracker=\"<COMET_TRACKER_STACK_COMPONENT_NAME>\",\n    settings={\n        \"experiment_tracker.comet\": comet_settings\n    }\n)\ndef my_step():\n    ...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-comet/#zenml.integrations.comet.flavors.comet\\_experiment\\_tracker\\_flavor.CometExperimentTrackerSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Logging and visualizing experiments with Weights & Biases.\n---\n\n# Weights & Biases\n\nThe Weights & Biases Experiment Tracker is an [Experiment Tracker](./experiment-trackers.md) flavor provided with the Weights & Biases ZenML integration that uses [the Weights & Biases experiment tracking platform](https://wandb.ai/site/experiment-tracking) to log and visualize information from your pipeline steps (e.g. models, parameters, metrics).\n\n### When would you want to use it?\n\n[Weights & Biases](https://wandb.ai/site/experiment-tracking) is a very popular platform that you would normally use in the iterative ML experimentation phase to track and visualize experiment results. That doesn't mean that it cannot be repurposed to track and visualize the results produced by your automated pipeline runs, as you make the transition towards a more production-oriented workflow.\n\nYou should use the Weights & Biases Experiment Tracker:\n\n* if you have already been using Weights & Biases to track experiment results for your project and would like to continue doing so as you are incorporating MLOps workflows and best practices in your project through ZenML.\n* if you are looking for a more visually interactive way of navigating the results produced from your ZenML pipeline runs (e.g. models, metrics, datasets)\n* if you would like to connect ZenML to Weights & Biases to share the artifacts and metrics logged by your pipelines with your team, organization, or external stakeholders\n\nYou should consider one of the other [Experiment Tracker flavors](./experiment-trackers.md#experiment-tracker-flavors) if you have never worked with Weights & Biases before and would rather use another experiment tracking tool that you are more familiar with.\n\n### How do you deploy it?\n\nThe Weights & Biases Experiment Tracker flavor is provided by the MLflow ZenML integration, you need to install it on your local machine to be able to register a Weights & Biases Experiment Tracker and add it to your stack:\n\n```shell\nzenml integration install wandb -y\n```\n\nThe Weights & Biases Experiment Tracker needs to be configured with the credentials required to connect to the Weights & Biases platform using one of the [available authentication methods](wandb.md#authentication-methods).\n\n#### Authentication Methods\n\nYou need to configure the following credentials for authentication to the Weights & Biases platform:\n\n* `api_key`: Mandatory API key token of your Weights & Biases account.\n* `project_name`: The name of the project where you're sending the new run. If the project is not specified, the run is put in an \"Uncategorized\" project.\n* `entity`: An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the"}
{"input": " run will be sent to your default entity, which is usually your username.\n\n{% tabs %}\n{% tab title=\"Basic Authentication\" %}\nThis option configures the credentials for the Weights & Biases platform directly as stack component attributes.\n\n{% hint style=\"warning\" %}\nThis is not recommended for production settings as the credentials won't be stored securely and will be clearly visible in the stack configuration.\n{% endhint %}\n\n```shell\n# Register the Weights & Biases experiment tracker\nzenml experiment-tracker register wandb_experiment_tracker --flavor=wandb \\ \n    --entity=<entity> --project_name=<project_name> --api_key=<key>\n\n# Register and set a stack with the new experiment tracker\nzenml stack register custom_stack -e wandb_experiment_tracker ... --set\n```\n{% endtab %}\n\n{% tab title=\"ZenML Secret (Recommended)\" %}\nThis method requires you to [configure a ZenML secret](../../how-to/interact-with-secrets.md) to store the Weights & Biases tracking service credentials securely.\n\nYou can create the secret using the `zenml secret create` command:\n\n```shell\nzenml secret create wandb_secret \\\n    --entity=<ENTITY> \\\n    --project_name=<PROJECT_NAME>\n    --api_key=<API_KEY>\n```\n\nOnce the secret is created, you can use it to configure the wandb Experiment Tracker:\n\n```shell\n# Reference the entity, project and api-key in our experiment tracker component\nzenml experiment-tracker register wandb_tracker \\\n    --flavor=wandb \\\n    --entity={{wandb_secret.entity}} \\\n    --project_name={{wandb_secret.project_name}} \\\n    --api_key={{wandb_secret.api_key}}\n    ...\n```\n\n{% hint style=\"info\" %}\nRead more about [ZenML Secrets](../../how-to/interact-with-secrets.md) in the ZenML documentation.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\nFor more, up-to-date information on the Weights & Biases Experiment Tracker implementation and its configuration, you can have a look at [the SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-wandb/#zenml.integrations.wandb.experiment\\_trackers.wandb\\_experiment\\_tracker) .\n\n### How do you use it?\n\nTo be able to log information from a ZenML pipeline step using the Weights & Biases Experiment Tracker component in the active stack, you need to enable an experiment tracker using the `@step` decorator. Then use Weights & Biases logging or auto-logging capabilities as you would normally do, e.g.:\n\n```python\nimport wandb\nfrom wandb.integration.keras import WandbCallback\n\n\n@step(experiment_tracker=\"<WANDB_TRACKER_STACK_COMPONENT_NAME>\")\ndef tf_trainer(\n    config: TrainerConfig,\n    x_train: np.ndarray"}
{"input": ",\n    y_train: np.ndarray,\n    x_val: np.ndarray,\n    y_val: np.ndarray,\n) -> tf.keras.Model:\n    ...\n\n    model.fit(\n        x_train,\n        y_train,\n        epochs=config.epochs,\n        validation_data=(x_val, y_val),\n        callbacks=[\n            WandbCallback(\n                log_evaluation=True,\n                validation_steps=16,\n                validation_data=(x_val, y_val),\n            )\n        ],\n    )\n\n    metric = ...\n\n    wandb.log({\"<METRIC_NAME>\": metric})\n```\n\n{% hint style=\"info\" %}\nInstead of hardcoding an experiment tracker name, you can also use the [Client](../../reference/python-client.md) to dynamically use the experiment tracker of your active stack:\n\n```python\nfrom zenml.client import Client\n\nexperiment_tracker = Client().active_stack.experiment_tracker\n\n@step(experiment_tracker=experiment_tracker.name)\ndef tf_trainer(...):\n    ...\n```\n{% endhint %}\n\n### Weights & Biases UI\n\nWeights & Biases comes with a web-based UI that you can use to find further details about your tracked experiments.\n\nEvery ZenML step that uses Weights & Biases should create a separate experiment run which you can inspect in the Weights & Biases UI:\n\n![WandB UI](../../.gitbook/assets/WandBUI.png)\n\nYou can find the URL of the Weights & Biases experiment linked to a specific ZenML run via the metadata of the step in which the experiment tracker was used:\n\n```python\nfrom zenml.client import Client\n\nlast_run = client.get_pipeline(\"<PIPELINE_NAME>\").last_run\ntrainer_step = last_run.get_step(\"<STEP_NAME>\")\ntracking_url = trainer_step.run_metadata[\"experiment_tracker_url\"].value\nprint(tracking_url)\n```\n\nAlternatively, you can see an overview of all experiment runs at https://wandb.ai/{ENTITY\\_NAME}/{PROJECT\\_NAME}/runs/.\n\n{% hint style=\"info\" %}\nThe naming convention of each Weights & Biases experiment run is `{pipeline_run_name}_{step_name}` (e.g. `wandb_example_pipeline-25_Apr_22-20_06_33_535737_tf_evaluator`) and each experiment run will be tagged with both `pipeline_name` and `pipeline_run_name`, which you can use to group and filter experiment runs.\n{% endhint %}\n\n#### Additional configuration\n\nFor additional configuration of the Weights & Biases experiment tracker, you can pass `WandbExperimentTrackerSettings` to overwrite the [wandb.Settings](https://github.com/wandb/client/blob/master/wandb/sdk/wandb\\_settings.py#L353) or pass additional tags for your runs:\n\n```python\nimport wandb\nfrom zenml.integrations.wandb.flavors.wandb_experiment_tracker_flavor import WandbExperimentTrackerSettings\n\nwandb_settings = WandbExperimentTrackerSettings(\n    settings=wand"}
{"input": "b.Settings(magic=True),\n    tags=[\"some_tag\"]\n)\n\n\n@step(\n    experiment_tracker=\"<WANDB_TRACKER_STACK_COMPONENT_NAME>\",\n    settings={\n        \"experiment_tracker.wandb\": wandb_settings\n    }\n)\ndef my_step(\n    x_test: np.ndarray,\n    y_test: np.ndarray,\n    model: tf.keras.Model,\n) -> float:\n    \"\"\"Everything in this step is auto-logged\"\"\"\n    ...\n```\n\nDoing the above auto-magically logs all the data, metrics, and results within the step, no further action is required!\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-wandb/#zenml.integrations.wandb.flavors.wandb\\_experiment\\_tracker\\_flavor.WandbExperimentTrackerSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Amazon Sagemaker.\n---\n\n# AWS Sagemaker Orchestrator\n\n[Sagemaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) is a serverless ML workflow tool running on AWS. It is an easy way to quickly run your code in a production-ready, repeatable cloud orchestrator that requires minimal setup without provisioning and paying for standby compute.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n## When to use it\n\nYou should use the Sagemaker orchestrator if:\n\n* you're already using AWS.\n* you're looking for a proven production-grade orchestrator.\n* you're looking for a UI in which you can track your pipeline runs.\n* you're looking for a managed solution for running your pipelines.\n* you're looking for a serverless solution for running your pipelines.\n\n## How it works\n\nThe ZenML Sagemaker orchestrator works with [Sagemaker Pipelines](https://aws.amazon.com/sagemaker/pipelines), which can be used to construct machine learning pipelines. Under the hood, for each ZenML pipeline step, it creates a SageMaker `PipelineStep`, which contains a Sagemaker Processing job. Currently, other step types are not supported.\n\n## How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding a Sagemaker orchestrator? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML AWS Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nIn order to use a Sagemaker AI orchestrator, you need to first deploy [ZenML to the cloud](../../getting-started/deploying-zenml/README.md). It would be recommended to deploy ZenML in the same region as you plan on using for Sagemaker, but it is not necessary to do so. You must ensure that you are connected to the remote ZenML server before using this stack component.\n\nThe only other thing necessary to use the ZenML Sagemaker orchestrator is enabling the relevant permissions for your particular role.\n\nIn order to quickly enable APIs, and create other resources necessary for to use this integration, we will soon provide a Sagemaker stack recipe via [our `mlstacks` repository](https://github.com/zenml-io/mlstacks), which will help you set up the"}
{"input": " infrastructure with one click.\n\n### Infrastructure Deployment\n\nA Sagemaker orchestrator can be deployed directly from the ZenML CLI:\n\n```shell\nzenml orchestrator deploy sagemaker_orchestrator --flavor=sagemaker --provider=aws ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n## How to use it\n\nTo use the Sagemaker orchestrator, we need:\n\n* The ZenML `aws` and `s3` integrations installed. If you haven't done so, run\n\n```shell\nzenml integration install aws s3\n```\n\n* [Docker](https://www.docker.com) installed and running.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack (configured with an `authentication_secret` attribute).\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* An IAM role or user with [an `AmazonSageMakerFullAccess` managed policy](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html) applied to it as well as `sagemaker.amazonaws.com` added as a Principal Service. Full details on these permissions can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) or use the ZenML recipe (when available) which will set up the necessary permissions for you.\n* The local client (whoever is running the pipeline) will also have to have the necessary permissions or roles to be able to launch Sagemaker jobs. (This would be covered by the `AmazonSageMakerFullAccess` policy suggested above.)\n\nThere are three ways you can authenticate your orchestrator and link it to the IAM role you have created:\n\n{% tabs %}\n{% tab title=\"Authentication via Service Connector\" %}\nThe recommended way to authenticate your SageMaker orchestrator is by registering an [AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) and connecting it to your SageMaker orchestrator:\n\n```shell\nzenml service-connector register <CONNECTOR_NAME> --type aws -i\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=sagemaker \\\n    --execution_role=<YOUR_IAM_ROLE_ARN>\nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"Explicit Authentication\" %}\nInstead of creating a service connector, you can also configure your AWS authentication credentials directly in the orchestrator:\n\n```shell\nzen"}
{"input": "ml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=sagemaker \\\n    --execution_role=<YOUR_IAM_ROLE_ARN> \\ \n    --aws_access_key_id=...\n    --aws_secret_access_key=...\n    --aws_region=...\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\nSee the [`SagemakerOrchestratorConfig` SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.flavors.sagemaker\\_orchestrator\\_flavor) for more information on available configuration options.\n{% endtab %}\n\n{% tab title=\"Implicit Authentication\" %}\nIf you neither connect your orchestrator to a service connector nor configure credentials explicitly, ZenML will try to implicitly authenticate to AWS via the `default` profile in your local [AWS configuration file](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html).\n\n```shell\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=sagemaker \\\n    --execution_role=<YOUR_IAM_ROLE_ARN>\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\npython run.py  # Authenticates with `default` profile in `~/.aws/config`\n```\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your pipeline steps in Sagemaker. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\nYou can now run any ZenML pipeline using the Sagemaker orchestrator:\n\n```shell\npython run.py\n```\n\nIf all went well, you should now see the following output:\n\n```\nSteps can take 5-15 minutes to start running when using the Sagemaker Orchestrator.\nYour orchestrator 'sagemaker' is running remotely. Note that the pipeline run will only show up on the ZenML dashboard once the first step has started executing on the remote infrastructure.\n```\n\n{% hint style=\"warning\" %}\nIf it is taking more than 15 minutes for your run to show up, it might be that a setup error occurred in SageMaker before the pipeline could be started. Checkout the [Debugging SageMaker Pipelines](sagemaker.md#debugging-sagemaker-pipelines) section for more information on how to debug this.\n{% endhint %}\n\n### Sagemaker UI\n\nSagemaker comes with its own UI that you can use to find further details about your pipeline runs, such as the logs of your steps"}
{"input": ".\n\nTo access the Sagemaker Pipelines UI, you will have to launch Sagemaker Studio via the AWS Sagemaker UI. Make sure that you are launching it from within your desired AWS region.\n\n![Sagemaker Studio launch](../../.gitbook/assets/sagemaker-studio-launch.png)\n\nOnce the Studio UI has launched, click on the 'Pipeline' button on the left side. From there you can view the pipelines that have been launched via ZenML:\n\n![Sagemaker Studio Pipelines](../../.gitbook/assets/sagemakerUI.png)\n\n### Debugging SageMaker Pipelines\n\nIf your SageMaker pipeline encounters an error before the first ZenML step starts, the ZenML run will not appear in the ZenML dashboard. In such cases, use the [SageMaker UI](sagemaker.md#sagemaker-ui) to review the error message and logs. Here's how:\n\n* Open the corresponding pipeline in the SageMaker UI as shown in the [SageMaker UI Section](sagemaker.md#sagemaker-ui),\n* Open the execution,\n* Click on the failed step in the pipeline graph,\n* Go to the 'Output' tab to see the error message or to 'Logs' to see the logs.\n\n![SageMaker Studio Logs](../../.gitbook/assets/sagemaker-logs.png)\n\nAlternatively, for a more detailed view of log messages during SageMaker pipeline executions, consider using [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/):\n\n* Search for 'CloudWatch' in the AWS console search bar.\n* Navigate to 'Logs > Log groups.'\n* Open the '/aws/sagemaker/ProcessingJobs' log group.\n* Here, you can find log streams for each step of your SageMaker pipeline executions.\n\n![SageMaker CloudWatch Logs](../../.gitbook/assets/sagemaker-cloudwatch-logs.png)\n\n### Run pipelines on a schedule\n\nThe ZenML Sagemaker orchestrator doesn't currently support running pipelines on a schedule. We maintain a public roadmap for ZenML, which you can find [here](https://zenml.io/roadmap). We welcome community contributions (see more [here](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md)) so if you want to enable scheduling for Sagemaker, please [do let us know](https://zenml.io/slack)!\n\n### Configuration at pipeline or step level\n\nWhen running your ZenML pipeline with the Sagemaker orchestrator, the configuration set when configuring the orchestrator as a ZenML component will be used by default. However, it is possible to provide additional configuration at the pipeline or step level. This allows you to run whole pipelines or individual steps with alternative configurations. For example, this allows you to run the training process with a heavier, GPU-enabled instance type, while running other steps with lighter instances.\n\nAdditional configuration for the Sagemaker orchestr"}
{"input": "ator can be passed via `SagemakerOrchestratorSettings`. Here, it is possible to configure `processor_args`, which is a dictionary of arguments for the Processor. For available arguments, see the [Sagemaker documentation](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.Processor) . Currently, it is not possible to provide custom configuration for the following attributes:\n\n* `image_uri`\n* `instance_count`\n* `sagemaker_session`\n* `entrypoint`\n* `base_job_name`\n* `env`\n\nFor example, settings can be provided in the following way:\n\n```python\nsagemaker_orchestrator_settings = SagemakerOrchestratorSettings(\n    processor_args={\n        \"instance_type\": \"ml.t3.medium\",\n        \"volume_size_in_gb\": 30\n    }\n)\n```\n\nThey can then be applied to a step as follows:\n\n```python\n@step(settings={\"orchestrator.sagemaker\": sagemaker_orchestrator_settings})\n```\n\nFor example, if your ZenML component is configured to use `ml.c5.xlarge` with 400GB additional storage by default, all steps will use it except for the step above, which will use `ml.t3.medium` with 30GB additional storage.\n\nCheck out [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings in general.\n\nFor more information and a full list of configurable attributes of the Sagemaker orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.orchestrators.sagemaker\\_orchestrator.SagemakerOrchestrator) .\n\n#### S3 data access in ZenML steps\n\nIn Sagemaker jobs, it is possible to [access data that is located in S3](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html). Similarly, it is possible to write data from a job to a bucket. The ZenML Sagemaker orchestrator supports this via the `SagemakerOrchestratorSettings` and hence at component, pipeline, and step levels.\n\n**Import: S3 -> job**\n\nImporting data can be useful when large datasets are available in S3 for training, for which manual copying can be cumbersome. Sagemaker supports `File` (default) and `Pipe` mode, with which data is either fully copied before the job starts or piped on the fly. See the Sagemaker documentation referenced above for more information about these modes.\n\nNote that data import and export can be used jointly with `processor_args` for maximum flexibility.\n\nA simple example of importing data from S3 to the Sagemaker job is as follows:\n\n```python\nsagem"}
{"input": "aker_orchestrator_settings = SagemakerOrchestratorSettings(\n    input_data_s3_mode=\"File\",\n    input_data_s3_uri=\"s3://some-bucket-name/folder\"\n)\n```\n\nIn this case, data will be available at `/opt/ml/processing/input/data` within the job.\n\nIt is also possible to split your input over channels. This can be useful if the dataset is already split in S3, or maybe even located in different buckets.\n\n```python\nsagemaker_orchestrator_settings = SagemakerOrchestratorSettings(\n    input_data_s3_mode=\"File\",\n    input_data_s3_uri={\n        \"train\": \"s3://some-bucket-name/training_data\",\n        \"val\": \"s3://some-bucket-name/validation_data\",\n        \"test\": \"s3://some-other-bucket-name/testing_data\"\n    }\n)\n```\n\nHere, the data will be available in `/opt/ml/processing/input/data/train`, `/opt/ml/processing/input/data/val` and `/opt/ml/processing/input/data/test`.\n\nIn the case of using `Pipe` for `input_data_s3_mode`, a file path specifying the pipe will be available as per the description written [here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html#model-access-training-data-input-modes) . An example of using this pipe file within a Python script can be found [here](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced\\_functionality/pipe\\_bring\\_your\\_own/train.py) .\n\n**Export: job -> S3**\n\nData from within the job (e.g. produced by the training process, or when preprocessing large data) can be exported as well. The structure is highly similar to that of importing data. Copying data to S3 can be configured with `output_data_s3_mode`, which supports `EndOfJob` (default) and `Continuous`.\n\nIn the simple case, data in `/opt/ml/processing/output/data` will be copied to S3 at the end of a job:\n\n```python\nsagemaker_orchestrator_settings = SagemakerOrchestratorSettings(\n    output_data_s3_mode=\"EndOfJob\",\n    output_data_s3_uri=\"s3://some-results-bucket-name/results\"\n)\n```\n\nIn a more complex case, data in `/opt/ml/processing/output/data/metadata` and `/opt/ml/processing/output/data/checkpoints` will be written away continuously:\n\n```python\nsagemaker_orchestrator_settings = SagemakerOrchestratorSettings(\n    output_data_s3_mode=\"Continuous\",\n    output_data_s3_uri={\n        \"metadata\": \"s3://some-results-bucket-name/metadata\",\n        \"checkpoints\": \"s3://some-results-bucket-name/checkpoints\"\n    }\n)\n```\n\n"}
{"input": "### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Kubernetes clusters.\n---\n\n# Kubernetes Orchestrator\n\nUsing the ZenML `kubernetes` integration, you can orchestrate and scale your ML pipelines on a [Kubernetes](https://kubernetes.io/) cluster without writing a single line of Kubernetes code.\n\nThis Kubernetes-native orchestrator is a minimalist, lightweight alternative to other distributed orchestrators like Airflow or Kubeflow.\n\nOverall, the Kubernetes orchestrator is quite similar to the Kubeflow orchestrator in that it runs each pipeline step in a separate Kubernetes pod. However, the orchestration of the different pods is not done by Kubeflow but by a separate master pod that orchestrates the step execution via topological sort.\n\nCompared to Kubeflow, this means that the Kubernetes-native orchestrator is faster and much simpler to start with since you do not need to install and maintain Kubeflow on your cluster. The Kubernetes-native orchestrator is an ideal choice for teams new to distributed orchestration that do not want to go with a fully-managed offering.\n\nHowever, since Kubeflow is much more mature, you should, in most cases, aim to move your pipelines to Kubeflow in the long run. A smooth way to production-grade orchestration could be to set up a Kubernetes cluster first and get started with the Kubernetes-native orchestrator. If needed, you can then install and set up Kubeflow later and simply switch out the orchestrator of your stack as soon as your full setup is ready.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n### When to use it\n\nYou should use the Kubernetes orchestrator if:\n\n* you're looking for a lightweight way of running your pipelines on Kubernetes.\n* you're not willing to maintain [Kubeflow Pipelines](kubeflow.md) on your Kubernetes cluster.\n* you're not interested in paying for managed solutions like [Vertex](vertex.md).\n\n### How to deploy it\n\nThe Kubernetes orchestrator requires a Kubernetes cluster in order to run. There are many ways to deploy a Kubernetes cluster using different cloud providers or on your custom infrastructure, and we can't possibly cover all of them, but you can check out our cloud guide\n\nIf the above Kubernetes cluster is deployed remotely on the cloud, then another pre-requisite to use this orchestrator would be to deploy and connect to a [remote ZenML server](../../getting-started/deploying-zenml/README.md).\n\n#### Infrastructure Deployment\n\nA Kubernetes orchestrator can be deployed directly from the ZenML CLI:\n\n```shell\nzenml orchestrator deploy k8s_orchestrator --flavor=kubernetes --provider=<YOUR_PROVIDER> ...\n```\n\nYou can pass other configurations specific"}
{"input": " to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n### How to use it\n\nTo use the Kubernetes orchestrator, we need:\n\n*   The ZenML `kubernetes` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install kubernetes\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) installed.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* A Kubernetes cluster [deployed](kubernetes.md#how-to-deploy-it)\n* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) installed and the name of the Kubernetes configuration context which points to the target cluster (i.e. run`kubectl config get-contexts` to see a list of available contexts) . This is optional (see below).\n\n{% hint style=\"info\" %}\nIt is recommended that you set up [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) and use it to connect ZenML Stack Components to the remote Kubernetes cluster, especially If you are using a Kubernetes cluster managed by a cloud provider like AWS, GCP or Azure, This guarantees that your Stack is fully portable on other environments and your pipelines are fully reproducible.\n{% endhint %}\n\nWe can then register the orchestrator and use it in our active stack. This can be done in two ways:\n\n1.  If you have [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) configured to access the remote Kubernetes cluster, you no longer need to set the `kubernetes_context` attribute to a local `kubectl` context. In fact, you don't need the local Kubernetes CLI at all. You can [connect the stack component to the Service Connector](../../how-to/auth-management/service-connectors-guide.md#connect-stack-components-to-resources) instead:\n\n    ```\n    $ zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubernetes\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully registered orchestrator `<ORCHESTRATOR_NAME>`.\n\n    $ zenml service-connector list-resources --resource-type kubernetes-cluster -e\n    The following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES      \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 e33c9fac-5daa-48b2-87bb-0187d3782cde \u2502 aws-iam-multi-eu      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 kubeflowmultitenant \u2503\n    \u2503                                      \u2502                       \u2502                \u2502                       \u2502 zenbox              \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster    \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 1c54b32a-4889-4417-abbd-42d3ace3d03a \u2502 gcp-sa-multi          \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster  \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    $ zenml orchestrator connect <ORCHESTRATOR_NAME> --connector aws-iam-multi-us\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully connected orchestrator `<ORCHESTRATOR_NAME>` to the following resources:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME   \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES   \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    # Register and activate a stack with the new orchestrator\n    $ zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n    ```\n2.  if you don't have a Service Connector on hand and you don't want to [register one](../../how-to/auth-management/service-connectors-guide.md#register-service-connectors) , the local Kubernetes `kubectl` client needs to be configured with a configuration context pointing to the remote cluster. The `kubernetes_context` stack component must also be configured with the value of that context:\n\n    ```shell\n    zenml orchestrator register <ORCHESTRATOR_NAME> \\\n        --flavor=kubernetes \\\n        --kubernetes_context=<KUBERNETES_CONTEXT>\n\n    # Register and activate a stack with the new orchestrator\n    zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n    ```\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your pipeline steps in Kubernetes. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\nYou can now run any ZenML pipeline using the Kubernetes orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\nIf all went well, you should now see the logs of all Kubernetes pods in your terminal, and when running `kubectl get pods -n zenml`, you should also see that a pod was created in your cluster for each pipeline step.\n\n#### Interacting with pods via kubectl\n\nFor debugging, it can sometimes be handy to interact with the Kubernetes pods directly via kubectl. To make this easier, we have added the following labels to all pods:\n\n* `run`: the name of the ZenML run.\n* `pipeline`: the name of the ZenML pipeline associated with this run.\n\nE.g., you can use these labels to manually delete all pods related to a specific pipeline:\n\n```shell\nkubectl"}
{"input": " delete pod -n zenml -l pipeline=kubernetes_example_pipeline\n```\n\n#### Additional configuration\n\nThe Kubernetes orchestrator will by default use a Kubernetes namespace called `zenml` to run pipelines. In that namespace, it will automatically create a Kubernetes service account called `zenml-service-account` and grant it `edit` RBAC role in that namespace. To customize these settings, you can configure the following additional attributes in the Kubernetes orchestrator:\n\n* `kubernetes_namespace`: The Kubernetes namespace to use for running the pipelines. The namespace must already exist in the Kubernetes cluster.\n* `service_account_name`: The name of a Kubernetes service account to use for running the pipelines. If configured, it must point to an existing service account in the default or configured `namespace` that has associated RBAC roles granting permissions to create and manage pods in that namespace. This can also be configured as an individual pipeline setting in addition to the global orchestrator setting.\n\nFor additional configuration of the Kubernetes orchestrator, you can pass `KubernetesOrchestratorSettings` which allows you to configure (among others) the following attributes:\n\n* `pod_settings`: Node selectors, labels, affinity, and tolerations, and image pull secrets to apply to the Kubernetes Pods running the steps of your pipeline. These can be either specified using the Kubernetes model objects or as dictionaries.\n\n* `orchestrator_pod_settings`:  Node selectors, labels, affinity, and tolerations, and image pull secrets to apply to the Kubernetes Pod that is responsible for orchestrating the pipeline and starting the other Pods. These can be either specified using the Kubernetes model objects or as dictionaries.\n\n```python\nfrom zenml.integrations.kubernetes.flavors.kubernetes_orchestrator_flavor import KubernetesOrchestratorSettings\nfrom kubernetes.client.models import V1Toleration\n\nkubernetes_settings = KubernetesOrchestratorSettings(\n    pod_settings={\n        \"node_selectors\": {\n            \"cloud.google.com/gke-nodepool\": \"ml-pool\",\n            \"kubernetes.io/arch\": \"amd64\"\n        },\n        \"affinity\": {\n            \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                    \"nodeSelectorTerms\": [\n                        {\n                            \"matchExpressions\": [\n                                {\n                                    \"key\": \"gpu-type\",\n                                    \"operator\": \"In\",\n                                    \"values\": [\"nvidia-tesla-v100\", \"nvidia-tesla-p100\"]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            }\n        },\n        \"tolerations\": [\n            V1Toleration(\n                key=\"gpu\",\n                operator=\"Equal\",\n                value=\"present\",\n                effect=\"NoSchedule\"\n            ),\n            V1Toleration(\n                key=\"high-priority\",\n                operator=\"Exists\",\n                effect=\"PreferNoSchedule\"\n            )\n        ],\n        \"resources\": {\n            \"requests\": {\n                \"cpu\": \"2\",\n               "}
{"input": " \"memory\": \"4Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            },\n            \"limits\": {\n                \"cpu\": \"4\",\n                \"memory\": \"8Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            }\n        },\n        \"annotations\": {\n            \"prometheus.io/scrape\": \"true\",\n            \"prometheus.io/port\": \"8080\"\n        },\n        \"volumes\": [\n            {\n                \"name\": \"data-volume\",\n                \"persistentVolumeClaim\": {\n                    \"claimName\": \"ml-data-pvc\"\n                }\n            },\n            {\n                \"name\": \"config-volume\",\n                \"configMap\": {\n                    \"name\": \"ml-config\"\n                }\n            }\n        ],\n        \"volume_mounts\": [\n            {\n                \"name\": \"data-volume\",\n                \"mountPath\": \"/mnt/data\"\n            },\n            {\n                \"name\": \"config-volume\",\n                \"mountPath\": \"/etc/ml-config\",\n                \"readOnly\": True\n            }\n        ],\n        \"host_ipc\": True,\n        \"image_pull_secrets\": [\"regcred\", \"gcr-secret\"],\n        \"labels\": {\n            \"app\": \"ml-pipeline\",\n            \"environment\": \"production\",\n            \"team\": \"data-science\"\n        }\n    },\n    orchestrator_pod_settings={\n        \"node_selectors\": {\n            \"cloud.google.com/gke-nodepool\": \"orchestrator-pool\"\n        },\n        \"resources\": {\n            \"requests\": {\n                \"cpu\": \"1\",\n                \"memory\": \"2Gi\"\n            },\n            \"limits\": {\n                \"cpu\": \"2\",\n                \"memory\": \"4Gi\"\n            }\n        },\n        \"labels\": {\n            \"app\": \"zenml-orchestrator\",\n            \"component\": \"pipeline-runner\"\n        }\n    },\n    kubernetes_namespace=\"ml-pipelines\",\n    service_account_name=\"zenml-pipeline-runner\"\n)\n\n@pipeline(\n    settings={\n        \"orchestrator.kubernetes\": kubernetes_settings\n    }\n)\ndef my_kubernetes_pipeline():\n    # Your pipeline steps here\n    ...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kubernetes/#zenml.integrations.kubernetes.flavors.kubernetes\\_orchestrator\\_flavor.KubernetesOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the Kubernetes orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kubernetes/#zenml.integrations.kubernetes.orchestrators.kubernetes\\_orchestrator.KubernetesOrchestrator) .\n\n#### Enabling CUDA for"}
{"input": " GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on AzureML.\n---\n\n# AzureML Orchestrator\n\n[AzureML](https://azure.microsoft.com/en-us/products/machine-learning) is a\ncloud-based orchestration service provided by Microsoft, that enables \ndata scientists, machine learning engineers, and developers to build, train, \ndeploy, and manage machine learning models. It offers a comprehensive and \nintegrated environment that supports the entire machine learning lifecycle, \nfrom data preparation and model development to deployment and monitoring.\n\n## When to use it\n\nYou should use the AzureML orchestrator if:\n\n* you're already using Azure.\n* you're looking for a proven production-grade orchestrator.\n* you're looking for a UI in which you can track your pipeline runs.\n* you're looking for a managed solution for running your pipelines.\n\n## How it works\n\nThe ZenML AzureML orchestrator implementation uses [the Python SDK v2 of \nAzureML](https://learn.microsoft.com/en-gb/python/api/overview/azure/ai-ml-readme?view=azure-python) \nto allow our users to build their Machine Learning pipelines. For each ZenML step,\nit creates an AzureML [CommandComponent](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.commandcomponent?view=azure-python)\nand brings them together in a pipeline.\n\n## How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding an AzureML orchestrator? Check out the [in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Azure Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nIn order to use an AzureML orchestrator, you need to first \ndeploy [ZenML to the cloud](../../getting-started/deploying-zenml/README.md). \nIt would be recommended to deploy ZenML in the same region as you plan on \nusing for AzureML, but it is not necessary to do so. You must ensure that \nyou are [connected to the remote ZenML server](../../how-to/connecting-to-zenml/connect-in-with-your-user-interactive.md)\nbefore using this stack component.\n\n## How to use it\n\nIn order to use the AzureML orchestrator, you need:\n\n* The ZenML `azure` integration installed. If you haven't done so, run:\n\n```shell\nzenml integration install azure\n```\n\n* [Docker](https://www.docker.com) installed and running or a remote image builder in your stack.\n* A [remote artifact store](../artifact-stores/art"}
{"input": "ifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* An [Azure resource group equipped with an AzureML workspace](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2) to run your pipeline on.\n\nThere are two ways of authenticating your orchestrator with AzureML:\n\n1. **Default Authentication** simplifies the authentication process while \ndeveloping your workflows that deploy to Azure by combining credentials used in \nAzure hosting environments and credentials used in local development.\n2. **Service Principal Authentication (recommended)** is using the concept \nof service principals on Azure to allow you to connect your cloud components \nwith proper authentication. For this method, you will need to [create a service \nprincipal on Azure](https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication-on-premises-apps?tabs=azure-portal), \nassign it the correct permissions and use it to [register a ZenML Azure Service \nConnector](https://docs.zenml.io/how-to/auth-management/azure-service-connector).\n    ```bash\n    zenml service-connector register <CONNECTOR_NAME> --type azure -i\n    zenml orchestrator connect <ORCHESTRATOR_NAME> -c <CONNECTOR_NAME>\n    ```\n\n## Docker\n\nFor each pipeline run, ZenML will build a Docker image called \n`<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code \nand use it to run your pipeline steps in AzureML. Check out \n[this page](../../how-to/customize-docker-builds/README.md) if you want to \nlearn more about how ZenML builds these images and how you can customize them.\n\n## AzureML UI\n\nEach AzureML workspace comes equipped with an Azure Machine Learning studio. \nHere you can inspect, manage, and debug your pipelines and steps.\n\n![AzureML pipeline example](../../.gitbook/assets/azureml-pipelines.png)\n\nDouble-clicking any of the steps on this view will open up the overview page \nfor that specific step. Here you can check the configuration of the component \nand its execution logs.\n\n## Settings\n\nThe ZenML AzureML orchestrator comes with a dedicated class called \n`AzureMLOrchestratorSettings` for configuring its settings, and it controls\nthe compute resources used for pipeline execution in AzureML.\n\nCurrently, it supports three different modes of operation.\n\n### 1. Serverless Compute (Default)\n- Set `mode` to `serverless`.\n- Other parameters are ignored.\n\n**Example:**\n\n```python\nfrom zenml import step, pipeline\nfrom zenml.integrations.azure.flavors import AzureMLOrchestratorSettings\n\nazureml_settings = AzureMLOrchestratorSettings(\n  mode=\"serverless\"  #"}
{"input": " It's the default behaviour\n)\n\n@step\ndef example_step() -> int:\n    return 3\n\n\n@pipeline(settings={\"orchestrator.azureml\": azureml_settings})\ndef pipeline():\n    example_step()\n\npipeline()\n```\n\n### 2. Compute Instance\n- Set `mode` to `compute-instance`.\n- Requires a `compute_name`.\n  - If a compute instance with the same name exists, it uses the existing \n  compute instance and ignores other parameters. (It will throw a warning if the \n  provided configuration does not match the existing instance.)\n  - If a compute instance with the same name doesn't exist, it creates a \n  new compute instance with the `compute_name`. For this process, you can \n  specify `size` and `idle_type_before_shutdown_minutes`.\n\n**Example:**\n\n```python\nfrom zenml import step, pipeline\nfrom zenml.integrations.azure.flavors import AzureMLOrchestratorSettings\n\nazureml_settings = AzureMLOrchestratorSettings(\n    mode=\"compute-instance\",\n    compute_name=\"my-gpu-instance\",  # Will fetch or create this instance\n    size=\"Standard_NC6s_v3\",  # Using a NVIDIA Tesla V100 GPU\n    idle_time_before_shutdown_minutes=20,\n)\n\n@step\ndef example_step() -> int:\n    return 3\n\n\n@pipeline(settings={\"orchestrator.azureml\": azureml_settings})\ndef pipeline():\n    example_step()\n\npipeline()\n```\n\n### 3. Compute Cluster\n- Set `mode` to `compute-cluster`.\n- Requires a `compute_name`.\n  - If a compute cluster with the same name exists, it uses existing cluster, \n  ignores other parameters. (It will throw a warning if the provided \n  - configuration does not match the existing cluster.)\n  - If a compute cluster with the same name doesn't exist, it creates a new \n  compute cluster. Additional parameters can be used for configuring this \n  process.\n\n**Example:**\n\n```python\nfrom zenml import step, pipeline\nfrom zenml.integrations.azure.flavors import AzureMLOrchestratorSettings\n\nazureml_settings = AzureMLOrchestratorSettings(\n    mode=\"compute-cluster\",\n    compute_name=\"my-gpu-cluster\",  # Will fetch or create this instance\n    size=\"Standard_NC6s_v3\",  # Using a NVIDIA Tesla V100 GPU\n    tier=\"Dedicated\",  # Can be set to either \"Dedicated\" or \"LowPriority\"\n    min_instances=2,\n    max_instances=10,\n    idle_time_before_scaledown_down=60,\n)\n\n@step\ndef example_step() -> int:\n    return 3\n\n\n@pipeline(settings={\"orchestrator.azureml\": azureml_settings})\ndef pipeline():\n    example_step()\n\npipeline()\n```\n\n{% hint style=\"info\" %}\nIn order to learn more about the supported sizes for compute instances and"}
{"input": " \nclusters, you can check [the AzureML documentation](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target?view=azureml-api-2#supported-vm-series-and-sizes).\n{% endhint %}\n\n### Run pipelines on a schedule\n\nThe AzureML orchestrator supports running pipelines on a schedule using \nits [JobSchedules](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-schedule-pipeline-job?view=azureml-api-2&tabs=python). \nBoth cron expression and intervals are supported.\n\n```python\nfrom zenml.config.schedule import Schedule\n\n# Run a pipeline every 5th minute\npipeline.run(schedule=Schedule(cron_expression=\"*/5 * * * *\"))\n```\n\nOnce you run the pipeline with a schedule, you can find the schedule and \nthe corresponding run under the `All Schedules` tab `Jobs` in the jobs page\non AzureML.\n\n{% hint style=\"warning\" %}\nNote that ZenML only gets involved to schedule a run, but maintaining the \nlifecycle of the schedule is the responsibility of the user. That means, if you \nwant to cancel a schedule that you created on AzureML, you will have to do \nit through the Azure UI.\n{% endhint %}\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Tekton.\n---\n\n# Tekton Orchestrator\n\n[Tekton](https://tekton.dev/) is a powerful and flexible open-source framework for creating CI/CD systems, allowing developers to build, test, and deploy across cloud providers and on-premise systems.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n### When to use it\n\nYou should use the Tekton orchestrator if:\n\n* you're looking for a proven production-grade orchestrator.\n* you're looking for a UI in which you can track your pipeline runs.\n* you're already using Kubernetes or are not afraid of setting up and maintaining a Kubernetes cluster.\n* you're willing to deploy and maintain Tekton Pipelines on your cluster.\n\n### How to deploy it\n\nYou'll first need to set up a Kubernetes cluster and deploy Tekton Pipelines:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n* A remote ZenML server. See the [deployment guide](../../getting-started/deploying-zenml/README.md) for more information.\n* Have an existing AWS [EKS cluster](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html) set up.\n* Make sure you have the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) set up.\n*   Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and [configure](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-connection/) it to talk to your EKS cluster using the following command:\n\n    ```powershell\n    aws eks --region REGION update-kubeconfig --name CLUSTER_NAME\n    ```\n* [Install](https://tekton.dev/docs/pipelines/install/) Tekton Pipelines onto your cluster.\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n* A remote ZenML server. See the [deployment guide](../../getting-started/deploying-zenml/README.md) for more information.\n* Have an existing GCP [GKE cluster](https://cloud.google.com/kubernetes-engine/docs/quickstart) set up.\n* Make sure you have the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk) set up first.\n*   Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and [configure](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl) it to talk to your GKE cluster using the following command:\n\n    ```powershell\n    gcloud container clusters get-credentials CLUSTER_NAME\n    ```\n* [Install](https://tekton.dev/docs"}
{"input": "/pipelines/install/) Tekton Pipelines onto your cluster.\n{% endtab %}\n\n{% tab title=\"Azure\" %}\n* A remote ZenML server. See the [deployment guide](../../getting-started/deploying-zenml/README.md) for more information.\n* Have an existing [AKS cluster](https://azure.microsoft.com/en-in/services/kubernetes-service/#documentation) set up.\n* Make sure you have the [`az` CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) set up first.\n*   Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and it to talk to your AKS cluster using the following command:\n\n    ```powershell\n    az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER_NAME\n    ```\n* [Install](https://tekton.dev/docs/pipelines/install/) Tekton Pipelines onto your cluster.\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nIf one or more of the deployments are not in the `Running` state, try increasing the number of nodes in your cluster.\n{% endhint %}\n\n{% hint style=\"warning\" %}\nZenML has only been tested with Tekton Pipelines >=0.38.3 and may not work with previous versions.\n{% endhint %}\n\n#### Infrastructure Deployment\n\nA Tekton orchestrator can be deployed directly from the ZenML CLI:\n\n```shell\nzenml orchestrator deploy tekton_orchestrator --flavor=tekton --provider=<YOUR_PROVIDER> ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n### How to use it\n\nTo use the Tekton orchestrator, we need:\n\n*   The ZenML `tekton` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install tekton -y\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* Tekton pipelines deployed on a remote cluster. See the [deployment section](tekton.md#how-to-deploy-it) for more information.\n* The name of your Kubernetes context which points to your remote cluster. Run `kubectl config get-contexts` to see a list of available contexts.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) installed and the name of the Kubernetes configuration context which points to the target cluster (i.e. run`kubectl config get-contexts` to see a list of available contexts)."}
{"input": " This is optional (see below).\n\n{% hint style=\"info\" %}\nIt is recommended that you set up [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) and use it to connect ZenML Stack Components to the remote Kubernetes cluster, especially If you are using a Kubernetes cluster managed by a cloud provider like AWS, GCP or Azure, This guarantees that your Stack is fully portable on other environments and your pipelines are fully reproducible.\n{% endhint %}\n\nWe can then register the orchestrator and use it in our active stack. This can be done in two ways:\n\n1.  If you have [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) configured to access the remote Kubernetes cluster, you no longer need to set the `kubernetes_context` attribute to a local `kubectl` context. In fact, you don't need the local Kubernetes CLI at all. You can [connect the stack component to the Service Connector](../../how-to/auth-management/service-connectors-guide.md#connect-stack-components-to-resources) instead:\n\n    ```\n    $ zenml orchestrator register <ORCHESTRATOR_NAME> --flavor tekton\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully registered orchestrator `<ORCHESTRATOR_NAME>`.\n\n    $ zenml service-connector list-resources --resource-type kubernetes-cluster -e\n    The following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES      \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 e33c9fac-5daa-48b2-87bb-0187d3782cde \u2502 aws-iam-multi-eu      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 kubeflowmultitenant \u2503\n    \u2503                                      \u2502                       \u2502                \u2502                       \u2502 zenbox              \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us      \u2502 \ud83d\udd36"}
{"input": " aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster    \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 1c54b32a-4889-4417-abbd-42d3ace3d03a \u2502 gcp-sa-multi          \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster  \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    $ zenml orchestrator connect <ORCHESTRATOR_NAME> --connector aws-iam-multi-us\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully connected orchestrator `<ORCHESTRATOR_NAME>` to the following resources:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME   \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES   \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    # Register and activate a stack with the new orchestrator\n    $ zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n    ```\n2.  if you don't have a Service Connector on hand and you don't want to [register one](../../how-to/auth-management/service-connectors-guide.md#register-service-connectors) , the local Kubernetes `kubectl` client needs to be configured with a configuration context pointing to the remote cluster"}
{"input": ". The `kubernetes_context` stack component must also be configured with the value of that context:\n\n    ```shell\n    zenml orchestrator register <ORCHESTRATOR_NAME> \\\n        --flavor=tekton \\\n        --kubernetes_context=<KUBERNETES_CONTEXT>\n\n    # Register and activate a stack with the new orchestrator\n    zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n    ```\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your pipeline steps in Tekton. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\nYou can now run any ZenML pipeline using the Tekton orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\n#### Tekton UI\n\nTekton comes with its own UI that you can use to find further details about your pipeline runs, such as the logs of your steps.\n\n![Tekton UI](../../.gitbook/assets/TektonUI.png)\n\nTo find the Tekton UI endpoint, we can use the following command:\n\n```bash\nkubectl get ingress -n tekton-pipelines  -o jsonpath='{.items[0].spec.rules[0].host}'\n```\n\n#### Additional configuration\n\nFor additional configuration of the Tekton orchestrator, you can pass `TektonOrchestratorSettings` which allows you to configure node selectors, affinity, and tolerations to apply to the Kubernetes Pods running your pipeline. These can be either specified using the Kubernetes model objects or as dictionaries.\n\n```python\nfrom zenml.integrations.tekton.flavors.tekton_orchestrator_flavor import TektonOrchestratorSettings\nfrom kubernetes.client.models import V1Toleration\n\ntekton_settings = TektonOrchestratorSettings(\n    pod_settings={\n        \"affinity\": {\n            \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                    \"nodeSelectorTerms\": [\n                        {\n                            \"matchExpressions\": [\n                                {\n                                    \"key\": \"node.kubernetes.io/name\",\n                                    \"operator\": \"In\",\n                                    \"values\": [\"my_powerful_node_group\"],\n                                }\n                            ]\n                        }\n                    ]\n                }\n            }\n        },\n        \"tolerations\": [\n            V1Toleration(\n                key=\"node.kubernetes.io/name\",\n                operator=\"Equal\",\n                value=\"\",\n                effect=\"NoSchedule\"\n            )\n        ]\n    }\n)\n```\n\nIf your pipelines steps have certain hardware requirements, you can specify them as `ResourceSettings`:\n\n```python\nresource_settings = ResourceSettings(cpu_count=8, memory"}
{"input": "=\"16GB\")\n```\n\nThese settings can then be specified on either pipeline-level or step-level:\n\n```python\n# Either specify on pipeline-level\n@pipeline(\n    settings={\n        \"orchestrator.tekton\": tekton_settings,\n        \"resources\": resource_settings,\n    }\n)\ndef my_pipeline():\n    ...\n\n# OR specify settings on step-level\n@step(\n    settings={\n        \"orchestrator.tekton\": tekton_settings,\n        \"resources\": resource_settings,\n    }\n)\ndef my_step():\n    ...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-tekton/#zenml.integrations.tekton.flavors.tekton\\_orchestrator\\_flavor.TektonOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the Tekton orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-tekton/#zenml.integrations.tekton.orchestrators.tekton\\_orchestrator.TektonOrchestrator) .\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on VMs using SkyPilot.\n---\n\n# Skypilot VM Orchestrator\n\nThe SkyPilot VM Orchestrator is an integration provided by ZenML that allows you to provision and manage virtual machines (VMs) on any cloud provider supported by the [SkyPilot framework](https://skypilot.readthedocs.io/en/latest/index.html). This integration is designed to simplify the process of running machine learning workloads on the cloud, offering cost savings, high GPU availability, and managed execution, We recommend using the SkyPilot VM Orchestrator if you need access to GPUs for your workloads, but don't want to deal with the complexities of managing cloud infrastructure or expensive managed solutions.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n{% hint style=\"warning\" %}\nSkyPilot VM Orchestrator is currently supported only for Python 3.8 and 3.9.\n{% endhint %}\n\n## When to use it\n\nYou should use the SkyPilot VM Orchestrator if:\n\n* you want to maximize cost savings by leveraging spot VMs and auto-picking the cheapest VM/zone/region/cloud.\n* you want to ensure high GPU availability by provisioning VMs in all zones/regions/clouds you have access to.\n* you don't need a built-in UI of the orchestrator. (You can still use ZenML's Dashboard to view and monitor your pipelines/artifacts.)\n* you're not willing to maintain Kubernetes-based solutions or pay for managed solutions like [Sagemaker](sagemaker.md).\n\n## How it works\n\nThe orchestrator leverages the SkyPilot framework to handle the provisioning and scaling of VMs. It automatically manages the process of launching VMs for your pipelines, with support for both on-demand and managed spot VMs. While you can select the VM type you want to use, the orchestrator also includes an optimizer that automatically selects the cheapest VM/zone/region/cloud for your workloads. Finally, the orchestrator includes an autostop feature that cleans up idle clusters, preventing unnecessary cloud costs.\n\n{% hint style=\"info\" %}\nYou can configure the SkyPilot VM Orchestrator to use a specific VM type, and resources for each step of your pipeline can be configured individually. Read more about how to configure step-specific resources [here](skypilot-vm.md#configuring-step-specific-resources).\n{% endhint %}\n\n{% hint style=\"warning\" %}\nThe SkyPilot VM Orchestrator does not currently support the ability to [schedule pipelines runs](../../how-to/build-pipelines/schedule-a-pipeline.md)\n{% endhint %}\n\n{% hint style=\"info\" %}\n"}
{"input": "All ZenML pipeline runs are executed using Docker containers within the VMs provisioned by the orchestrator. For that reason, you may need to configure your pipeline settings with `docker_run_args=[\"--gpus=all\"]` to enable GPU support in the Docker container.\n{% endhint %}\n\n## How to deploy it\n\nYou don't need to do anything special to deploy the SkyPilot VM Orchestrator. As the SkyPilot integration itself takes care of provisioning VMs, you can simply use the orchestrator as you would any other ZenML orchestrator. However, you will need to ensure that you have the appropriate permissions to provision VMs on your cloud provider of choice and to configure your SkyPilot orchestrator accordingly using the [service connectors](../../how-to/auth-management/service-connectors-guide.md) feature.\n\n{% hint style=\"info\" %}\nThe SkyPilot VM Orchestrator currently only supports the AWS, GCP, and Azure cloud platforms.\n{% endhint %}\n\n## How to use it\n\nTo use the SkyPilot VM Orchestrator, you need:\n\n*   One of the SkyPilot integrations installed. You can install the SkyPilot integration for your cloud provider of choice using the following command:\n\n    ```shell\n      # For AWS\n      pip install \"zenml[connectors-aws]\"\n      zenml integration install aws skypilot_aws \n\n      # for GCP\n      pip install \"zenml[connectors-gcp]\"\n      zenml integration install gcp skypilot_gcp # for GCP\n\n      # for Azure\n      pip install \"zenml[connectors-azure]\"\n      zenml integration install azure skypilot_azure # for Azure\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* A [remote ZenML deployment](../../getting-started/deploying-zenml/README.md).\n* The appropriate permissions to provision VMs on your cloud provider of choice.\n* A [service connector](../../how-to/auth-management/service-connectors-guide.md) configured to authenticate with your cloud provider of choice.\n\n{% tabs %}\n{% tab title=\"AWS\" %}\nWe need first to install the SkyPilot integration for AWS and the AWS connectors extra, using the following two commands:\n\n```shell\n  pip install \"zenml[connectors-aws]\"\n  zenml integration install aws skypilot_aws \n```\n\nTo provision VMs on AWS, your VM Orchestrator stack component needs to be configured to authenticate with [AWS Service Connector](../../how-to/auth-management/aws-service-connector.md). To configure the AWS Service Connector, you need to register a new service connector configured with AWS credentials that"}
{"input": " have at least the minimum permissions required by SkyPilot as documented [here](https://skypilot.readthedocs.io/en/latest/cloud-setup/cloud-permissions/aws.html).\n\nFirst, check that the AWS service connector type is available using the following command:\n\n```shell\nzenml service-connector list-types --type aws\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS     \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 AWS Service Connector \u2502 \ud83d\udd36 aws \u2502 \ud83d\udd36 aws-generic        \u2502 implicit         \u2502 \u2705    \u2502 \u2796     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 s3-bucket          \u2502 secret-key       \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 sts-token        \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 iam-role         \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 session-token    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 federation-token \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nNext, configure a service connector using the CLI or the dashboard with the AWS credentials. For example, the following command uses the local AWS CLI credentials to auto-configure the service connector:\n\n```shell\nzenml service-connector register aws-skypilot-vm --type aws --region=us-east-1 --auto-configure\n```\n\nThis will automatically configure the service connector with the appropriate credentials and permissions to provision VMs on AWS. You can then use the service connector to configure your registered VM Orchestrator stack component using the following command:\n\n```shell\n# Register the orchestrator\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_aws\n# Connect the orchestrator to the service connector\nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector aws-skypilot-vm\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"GCP"}
{"input": "\" %}\nWe need first to install the SkyPilot integration for GCP and the GCP extra for ZenML, using the following two commands:\n\n```shell\n  pip install \"zenml[connectors-gcp]\"\n  zenml integration install gcp skypilot_gcp\n```\n\nTo provision VMs on GCP, your VM Orchestrator stack component needs to be configured to authenticate with [GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md)\n\nTo configure the GCP Service Connector, you need to register a new service connector, but first let's check the available service connectors types using the following command:\n\n```shell\nzenml service-connector list-types --type gcp\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         NAME          \u2502 TYPE   \u2502 RESOURCE TYPES        \u2502 AUTH METHODS    \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 GCP Service Connector \u2502 \ud83d\udd35 gcp \u2502 \ud83d\udd35 gcp-generic        \u2502 implicit        \u2502 \u2705    \u2502 \u2796     \u2503\n\u2503                       \u2502        \u2502 \ud83d\udce6 gcs-bucket         \u2502 user-account    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83c\udf00 kubernetes-cluster \u2502 service-account \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502 \ud83d\udc33 docker-registry    \u2502 oauth2-token    \u2502       \u2502        \u2503\n\u2503                       \u2502        \u2502                       \u2502 impersonation   \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nFor this example we will configure a service connector using the `user-account` auth method. But before we can do that, we need to login to GCP using the following command:\n\n```shell\ngcloud auth application-default login \n```\n\nThis will open a browser window and ask you to login to your GCP account. Once you have logged in, you can register a new service connector using the following command:\n\n```shell\n# We want to use --auto-configure to automatically configure the service connector with the appropriate credentials and permissions to provision VMs on GCP.\nzenml service-connector register gcp-skypilot-vm -t gcp --auth-method user-account --auto-configure \n# using generic resource type"}
{"input": " requires disabling the generation of temporary tokens\nzenml service-connector update gcp-skypilot-vm --generate_temporary_tokens=False\n```\n\nThis will automatically configure the service connector with the appropriate credentials and permissions to provision VMs on GCP. You can then use the service connector to configure your registered VM Orchestrator stack component using the following commands:\n\n```shell\n# Register the orchestrator\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_gcp\n# Connect the orchestrator to the service connector\nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector gcp-skypilot-vm\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"Azure\" %}\nWe need first to install the SkyPilot integration for Azure and the Azure extra for ZenML, using the following two commands\n\n```shell\n  pip install \"zenml[connectors-azure]\"\n  zenml integration install azure skypilot_azure \n```\n\nTo provision VMs on Azure, your VM Orchestrator stack component needs to be configured to authenticate with [Azure Service Connector](../../how-to/auth-management/azure-service-connector.md)\n\nTo configure the Azure Service Connector, you need to register a new service connector, but first let's check the available service connectors types using the following command:\n\n```shell\nzenml service-connector list-types --type azure\n```\n```shell\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503          NAME           \u2502 TYPE      \u2502 RESOURCE TYPES        \u2502 AUTH METHODS      \u2502 LOCAL \u2502 REMOTE \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 Azure Service Connector \u2502 \ud83c\udde6  azure \u2502 \ud83c\udde6  azure-generic     \u2502 implicit          \u2502 \u2705    \u2502 \u2796     \u2503\n\u2503                         \u2502           \u2502 \ud83d\udce6 blob-container     \u2502 service-principal \u2502       \u2502        \u2503\n\u2503                         \u2502           \u2502 \ud83c\udf00 kubernetes-cluster \u2502 access-token      \u2502       \u2502        \u2503\n\u2503                         \u2502           \u2502 \ud83d\udc33 docker-registry    \u2502                   \u2502       \u2502        \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\nzenml service-connector register azure-skypilot-vm -t azure --auth-method access-token --auto-configure\n```\n\nThis will automatically configure the service connector with the appropriate credentials and permissions to provision VMs on Azure. You can then use the service connector to configure your registered VM Orchestrator stack component using the following commands:\n\n```shell\n# Register the orchestrator\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_azure\n# Connect the orchestrator to the service connector\nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector azure-skypilot-vm\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"Lambda Labs\" %}\n[Lambda Labs](https://lambdalabs.com/service/gpu-cloud) is a cloud provider that offers GPU instances for machine learning workloads. Unlike the major cloud providers, with Lambda Labs we don't need to configure a service connector to authenticate with the cloud provider. Instead, we can directly use API keys to authenticate with the Lambda Labs API.\n\n```shell\n  zenml integration install skypilot_lambda\n```\n\nOnce the integration is installed, we can register the orchestrator with the following command:\n\n```shell\n# For more secure and recommended way, we will register the API key as a secret\nzenml secret create lambda_api_key --scope user --api_key=<VALUE_1>\n# Register the orchestrator\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_lambda --api_key={{lambda_api_key.api_key}}\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\n{% hint style=\"info\" %}\nThe Lambda Labs orchestrator does not support some of the features like `job_recovery`, `disk_tier`, `image_id`, `zone`, `idle_minutes_to_autostop`, `disk_size`, `use_spot`. It is recommended not to use these features with the Lambda Labs orchestrator and not to use [step-specific settings](skypilot-vm.md#configuring-step-specific-resources).\n{% endhint %}\n\n{% hint style=\"warning\" %}\nWhile testing the orchestrator, we noticed that the Lambda Labs orchestrator does not support the `down` flag. This means the orchestrator will not automatically tear down the cluster after all jobs finish. We recommend manually tearing down the cluster after all jobs finish to avoid unnecessary costs.\n{% endhint %}\n{% endtab %}\n{% endtabs %}\n\n#### Additional Configuration\n\nFor additional configuration of the Skypilot orchestrator, you can pass `Settings` depending on which cloud you are using which allows you to configure (among"}
{"input": " others) the following attributes:\n\n* `instance_type`: The instance type to use.\n* `cpus`: The number of CPUs required for the task. If a string, must be a string of the form `'2'` or `'2+'`, where the `+` indicates that the task requires at least 2 CPUs.\n* `memory`: The amount of memory in GiB required. If a string, must be a string of the form `'16'` or `'16+'`, where the `+` indicates that the task requires at least 16 GB of memory.\n* `accelerators`: The accelerators required. If a string, must be a string of the form `'V100'` or `'V100:2'`, where the `:2` indicates that the task requires 2 V100 GPUs. If a dict, must be a dict of the form `{'V100': 2}` or `{'tpu-v2-8': 1}`.\n* `accelerator_args`: Accelerator-specific arguments. For example, `{'tpu_vm': True, 'runtime_version': 'tpu-vm-base'}` for TPUs.\n* `use_spot`: Whether to use spot instances. If None, defaults to False.\n* `job_recovery`: The spot recovery strategy to use for the managed spot to recover the cluster from preemption. Read more about the available strategies [here](https://skypilot.readthedocs.io/en/latest/reference/api.html?highlight=instance\\_type#resources)\n* `region`: The cloud region to use.\n* `zone`: The cloud zone to use within the region.\n* `image_id`: The image ID to use. If a string, must be a string of the image id from the cloud, such as AWS: `'ami-1234567890abcdef0'`, GCP: `'projects/my-project-id/global/images/my-image-name'`; Or, a image tag provided by SkyPilot, such as AWS: `'skypilot:gpu-ubuntu-2004'`. If a dict, must be a dict mapping from region to image ID.\n* `disk_size`: The size of the OS disk in GiB.\n* `disk_tier`: The disk performance tier to use. If None, defaults to `'medium'`.\n* `cluster_name`: Name of the cluster to create/reuse. If None, auto-generate a name. SkyPilot uses term `cluster` to refer to a group or a single VM that are provisioned to execute the task. The cluster name is used to identify the cluster and to determine whether to reuse an existing cluster or create a new one.\n* `retry_until_up`: Whether to retry launching the cluster until it is up.\n* `idle_minutes_to_autostop`: Automatically stop the cluster after this many minutes of idleness, i.e., no running or pending jobs in the cluster's job queue."}
{"input": " Idleness gets reset whenever setting-up/running/pending jobs are found in the job queue. Setting this flag is equivalent to running `sky.launch(..., detach_run=True, ...)` and then `sky.autostop(idle_minutes=<minutes>)`. If not set, the cluster will not be autostopped.\n* `down`: Tear down the cluster after all jobs finish (successfully or abnormally). If `idle_minutes_to_autostop` is also set, the cluster will be torn down after the specified idle time. Note that if errors occur during provisioning/data syncing/setting up, the cluster will not be torn down for debugging purposes.\n* `stream_logs`: If True, show the logs in the terminal as they are generated while the cluster is running.\n* `docker_run_args`: Additional arguments to pass to the `docker run` command. For example, `['--gpus=all']` to use all GPUs available on the VM.\n\nThe following code snippets show how to configure the orchestrator settings for each cloud provider:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n**Code Example:**\n\n```python\nfrom zenml.integrations.skypilot_aws.flavors.skypilot_orchestrator_aws_vm_flavor import SkypilotAWSOrchestratorSettings\n\nskypilot_settings = SkypilotAWSOrchestratorSettings(\n    cpus=\"2\",\n    memory=\"16\",\n    accelerators=\"V100:2\",\n    accelerator_args={\"tpu_vm\": True, \"runtime_version\": \"tpu-vm-base\"},\n    use_spot=True,\n    job_recovery=\"recovery_strategy\",\n    region=\"us-west-1\",\n    zone=\"us-west1-a\",\n    image_id=\"ami-1234567890abcdef0\",\n    disk_size=100,\n    disk_tier=\"high\",\n    cluster_name=\"my_cluster\",\n    retry_until_up=True,\n    idle_minutes_to_autostop=60,\n    down=True,\n    stream_logs=True\n    docker_run_args=[\"--gpus=all\"]\n)\n\n\n@pipeline(\n    settings={\n        \"orchestrator.vm_aws\": skypilot_settings\n    }\n)\n```\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n**Code Example:**\n\n```python\nfrom zenml.integrations.skypilot_gcp.flavors.skypilot_orchestrator_gcp_vm_flavor import SkypilotGCPOrchestratorSettings\n\n\nskypilot_settings = SkypilotGCPOrchestratorSettings(\n    cpus=\"2\",\n    memory=\"16\",\n    accelerators=\"V100:2\",\n    accelerator_args={\"tpu_vm\": True, \"runtime_version\": \"tpu-vm-base\"},\n    use_spot=True,\n    job_recovery=\"recovery_strategy\",\n    region=\"us-west1\",\n    zone=\"us-west1-a\",\n    image_id=\"ubuntu-pro-2004-focal-v20231101\",\n"}
{"input": "    disk_size=100,\n    disk_tier=\"high\",\n    cluster_name=\"my_cluster\",\n    retry_until_up=True,\n    idle_minutes_to_autostop=60,\n    down=True,\n    stream_logs=True\n)\n\n\n@pipeline(\n    settings={\n        \"orchestrator.vm_gcp\": skypilot_settings\n    }\n)\n```\n{% endtab %}\n\n{% tab title=\"Azure\" %}\n**Code Example:**\n\n```python\nfrom zenml.integrations.skypilot_azure.flavors.skypilot_orchestrator_azure_vm_flavor import SkypilotAzureOrchestratorSettings\n\n\nskypilot_settings = SkypilotAzureOrchestratorSettings(\n    cpus=\"2\",\n    memory=\"16\",\n    accelerators=\"V100:2\",\n    accelerator_args={\"tpu_vm\": True, \"runtime_version\": \"tpu-vm-base\"},\n    use_spot=True,\n    job_recovery=\"recovery_strategy\",\n    region=\"West Europe\",\n    image_id=\"Canonical:0001-com-ubuntu-server-jammy:22_04-lts-gen2:latest\",\n    disk_size=100,\n    disk_tier=\"high\",\n    cluster_name=\"my_cluster\",\n    retry_until_up=True,\n    idle_minutes_to_autostop=60,\n    down=True,\n    stream_logs=True\n)\n\n\n@pipeline(\n    settings={\n        \"orchestrator.vm_azure\": skypilot_settings\n    }\n)\n```\n{% endtab %}\n\n{% tab title=\"Lambda\" %}\n**Code Example:**\n\n```python\nfrom zenml.integrations.skypilot_lambda import SkypilotLambdaOrchestratorSettings\n\n\nskypilot_settings = SkypilotLambdaOrchestratorSettings(\n    instance_type=\"gpu_1x_h100_pcie\",\n    cluster_name=\"my_cluster\",\n    retry_until_up=True,\n    idle_minutes_to_autostop=60,\n    down=True,\n    stream_logs=True,\n    docker_run_args=[\"--gpus=all\"]\n)\n\n\n@pipeline(\n    settings={\n        \"orchestrator.vm_lambda\": skypilot_settings\n    }\n)\n```\n{% endtab %}\n{% endtabs %}\n\nOne of the key features of the SkyPilot VM Orchestrator is the ability to run each step of a pipeline on a separate VM with its own specific settings. This allows for fine-grained control over the resources allocated to each step, ensuring that each part of your pipeline has the necessary compute power while optimizing for cost and efficiency.\n\n## Configuring Step-Specific Resources\n\nThe SkyPilot VM Orchestrator allows you to configure resources for each step individually. This means you can specify different VM types, CPU and memory requirements, and even use spot instances for certain steps while using on-demand instances for others.\n\nIf no step-specific settings are specified, the orchestrator will use the resources specified in the orchestrator settings for each step and run the entire pipeline in one VM"}
{"input": ". If step-specific settings are specified, an orchestrator VM will be spun up first, which will subsequently spin out new VMs dependant on the step settings. You can disable this behavior by setting the `disable_step_based_settings` parameter to `True` in the orchestrator configuration, using the following command:\n\n```shell\nzenml orchestrator update <ORCHESTRATOR_NAME> --disable_step_based_settings=True\n```\n\nHere's an example of how to configure specific resources for a step for the AWS cloud:\n\n```python\nfrom zenml.integrations.skypilot_aws.flavors.skypilot_orchestrator_aws_vm_flavor import SkypilotAWSOrchestratorSettings\n\n# Settings for a specific step that requires more resources\nhigh_resource_settings = SkypilotAWSOrchestratorSettings(\n    instance_type='t2.2xlarge',\n    cpus=8,\n    memory=32,\n    use_spot=False,\n    region='us-east-1',\n    # ... other settings\n)\n\n@step(settings={\"orchestrator.vm_aws\": high_resource_settings})\ndef my_resource_intensive_step():\n    # Step implementation\n    pass\n```\n\n{% hint style=\"warning\" %}\nWhen configuring pipeline or step-specific resources, you can use the `settings` parameter to specifically target the orchestrator flavor you want to use `orchestrator.STACK_COMPONENT_FLAVOR` and not orchestrator component name `orchestrator.STACK_COMPONENT_NAME`. For example, if you want to configure resources for the `vm_gcp` flavor, you can use `settings={\"orchestrator.vm_gcp\": ...}`.\n{% endhint %}\n\nBy using the `settings` parameter, you can tailor the resources for each step according to its specific needs. This flexibility allows you to optimize your pipeline execution for both performance and cost.\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-skypilot/#zenml.integrations.skypilot.flavors.skypilot\\_orchestrator\\_base\\_vm\\_flavor.SkypilotBaseOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run in Docker.\n---\n\n# Local Docker Orchestrator\n\nThe local Docker orchestrator is an [orchestrator](./orchestrators.md) flavor that comes built-in with ZenML and runs your pipelines locally using Docker.\n\n### When to use it\n\nYou should use the local Docker orchestrator if:\n\n* you want the steps of your pipeline to run locally in isolated environments.\n* you want to debug issues that happen when running your pipeline in Docker containers without waiting and paying for remote infrastructure.\n\n### How to deploy it\n\nTo use the local Docker orchestrator, you only need to have [Docker](https://www.docker.com/) installed and running.\n\n### How to use it\n\nTo use the local Docker orchestrator, we can register it and use it in our active stack:\n\n```shell\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor=local_docker\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\nYou can now run any ZenML pipeline using the local Docker orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\n#### Additional configuration\n\nFor additional configuration of the Local Docker orchestrator, you can pass `LocalDockerOrchestratorSettings` when defining or running your pipeline. Check out the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-orchestrators/#zenml.orchestrators.local\\_docker.local\\_docker\\_orchestrator.LocalDockerOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings. A full list of what can be passed in via the `run_args` can be found [in the Docker Python SDK documentation](https://docker-py.readthedocs.io/en/stable/containers.html).\n\nFor more information and a full list of configurable attributes of the local Docker orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-orchestrators/#zenml.orchestrators.local\\_docker.local\\_docker\\_orchestrator.LocalDockerOrchestrator) .\n\nFor example, if you wanted to specify the CPU count available for the Docker image (note: only configurable for Windows), you could write a simple pipeline like the following:\n\n```python\nfrom zenml import step, pipeline\nfrom zenml.orchestrators.local_docker.local_docker_orchestrator import (\n    LocalDockerOrchestratorSettings,\n)\n\n\n@step\ndef return_one() -> int:\n    return 1\n\n\nsettings = {\n    \"orchestrator.local_docker\": LocalDockerOrchestratorSettings(\n       "}
{"input": " run_args={\"cpu_count\": 3}\n    )\n}\n\n\n@pipeline(settings=settings)\ndef simple_pipeline():\n    return_one()\n```\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Databricks.\n---\n\n# Databricks Orchestrator\n\n[Databricks](https://www.databricks.com/) is a unified data analytics platform that combines the best of data warehouses and data lakes to offer an integrated solution for big data processing and machine learning. It provides a collaborative environment for data scientists, data engineers, and business analysts to work together on data projects. Databricks offers optimized performance and scalability for big data workloads.\n\nThe Databricks orchestrator is an orchestrator flavor provided by the ZenML databricks integration that allows you to run your pipelines on Databricks. This integration enables you to leverage Databricks' powerful distributed computing capabilities and optimized environment for your ML pipelines within the ZenML framework.\n\n{% hint style=\"warning\" %}\nThe following features are currently in Alpha and may be subject to change. We recommend using them in a controlled environment and providing feedback to the ZenML team.\n{% endhint %}\n\n### When to use it\n\nYou should use the Databricks orchestrator if:\n\n* you're already using Databricks for your data and ML workloads.\n* you want to leverage Databricks' powerful distributed computing capabilities for your ML pipelines.\n* you're looking for a managed solution that integrates well with other Databricks services.\n* you want to take advantage of Databricks' optimization for big data processing and machine learning.\n\n### Prerequisites\n\nYou will need to do the following to start using the Databricks orchestrator:\n\n* An Active Databricks workspace, depends on the cloud provider you are using, you can find more information on how to create a workspace:\n    * [AWS](https://docs.databricks.com/en/getting-started/onboarding-account.html)\n    * [Azure](https://learn.microsoft.com/en-us/azure/databricks/getting-started/#--create-an-azure-databricks-workspace)\n    * [GCP](https://docs.gcp.databricks.com/en/getting-started/index.html)\n* Active Databricks account or service account with sufficient permission to create and run jobs\n\n## How it works\n\n\n![Databricks How It works Diagram](../../.gitbook/assets/Databricks_How_It_works.png)\n\nThe Databricks orchestrator in ZenML leverages the concept of Wheel Packages. When you run a pipeline with the Databricks orchestrator, ZenML creates a Python wheel package from your project. This wheel package contains all the necessary code and dependencies for your pipeline.\n\nOnce the wheel package is created, ZenML uploads it to Databricks. ZenML leverage Databricks SDK to create a job definition, This job definition includes information about the pipeline steps and ensures that each step is executed only after its upstream steps have successfully completed.\n\nThe Databricks job is also configured with the necessary cluster settings to run. This includes specifying the version of Spark to use, the"}
{"input": " number of workers, the node type, and other configuration options.\n\nWhen the Databricks job is executed, it retrieves the wheel package from Databricks and runs the pipeline using the specified cluster configuration. The job ensures that the steps are executed in the correct order based on their dependencies.\n\nOnce the job is completed, ZenML retrieves the logs and status of the job and updates the pipeline run accordingly. This allows you to monitor the progress of your pipeline and view the logs of each step.\n\n\n### How to use it\n\nTo use the Databricks orchestrator, you first need to register it and add it to your stack. Before registering the orchestrator, you need to install the Databricks integration by running the following command:\n\n```shell\nzenml integration install databricks\n```\n\nThis command will install the necessary dependencies, including the `databricks-sdk` package, which is required for authentication with Databricks. Once the integration is installed, you can proceed with registering the orchestrator and configuring the necessary authentication details.\n\n```shell\nzenml integration install databricks\n```\n\nThen, we can register the orchestrator and use it in our active stack:\n\n```shell\nzenml orchestrator register databricks_orchestrator --flavor=databricks --host=\"https://xxxxx.x.azuredatabricks.net\" --client_id={{databricks.client_id}} --client_secret={{databricks.client_secret}}\n```\n\n{% hint style=\"info\" %}\nWe recommend creating a Databricks service account with the necessary permissions to create and run jobs. You can find more information on how to create a service account [here](https://docs.databricks.com/dev-tools/api/latest/authentication.html). You can generate a client_id and client_secret for the service account and use them to authenticate with Databricks.\n\n![Databricks Service Account Permession](../../.gitbook/assets/DatabricksPermessions.png)\n{% endhint %}\n\n```shell\n# Add the orchestrator to your stack\nzenml stack register databricks_stack -o databricks_orchestrator ... --set\n```\n\nYou can now run any ZenML pipeline using the Databricks orchestrator:\n\n```shell\npython run.py\n```\n\n### Databricks UI\n\nDatabricks comes with its own UI that you can use to find further details about your pipeline runs, such as the logs of your steps.\n\n![Databricks UI](../../.gitbook/assets/DatabricksUI.png)\n\nFor any runs executed on Databricks, you can get the URL to the Databricks UI in Python using the following code snippet:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"<PIPELINE_RUN_NAME>\")\norchestrator_url = pipeline_run.run_metadata[\"orchestrator_url\"].value\n```\n\n![Databricks Run UI](../../.gitbook/assets/DatabricksRunUI.png)\n\n\n### Run pipelines"}
{"input": " on a schedule\n\nThe Databricks Pipelines orchestrator supports running pipelines on a schedule using its [native scheduling capability](https://docs.databricks.com/en/workflows/jobs/schedule-jobs.html).\n\n**How to schedule a pipeline**\n\n```python\nfrom zenml.config.schedule import Schedule\n\n# Run a pipeline every 5th minute\npipeline_instance.run(\n    schedule=Schedule(\n        cron_expression=\"*/5 * * * *\"\n    )\n)\n```\n\n{% hint style=\"warning\" %}\nThe Databricks orchestrator only supports the `cron_expression`, in the `Schedule` object, and will ignore all other parameters supplied to define the schedule.\n{% endhint %}\n\n{% hint style=\"warning\" %}\nThe Databricks orchestrator requires Java Timezone IDs to be used in the `cron_expression`. You can find a list of supported timezones [here](https://docs.oracle.com/middleware/1221/wcs/tag-ref/MISC/TimeZones.html), the timezone ID must be set in the settings of the orchestrator (see below for more imformation how to set settings for the orchestrator).\n{% endhint %}\n\n**How to delete a scheduled pipeline**\n\nNote that ZenML only gets involved to schedule a run, but maintaining the lifecycle of the schedule is the responsibility of the user.\n\nIn order to cancel a scheduled Databricks pipeline, you need to manually delete the schedule in Databricks (via the UI or the CLI).\n\n### Additional configuration\n\nFor additional configuration of the Databricks orchestrator, you can pass `DatabricksOrchestratorSettings` which allows you to change the Spark version, number of workers, node type, autoscale settings, Spark configuration, Spark environment variables, and schedule timezone.\n\n```python\nfrom zenml.integrations.databricks.flavors.databricks_orchestrator_flavor import DatabricksOrchestratorSettings\n\ndatabricks_settings = DatabricksOrchestratorSettings(\n    spark_version=\"15.3.x-scala2.12\",\n    num_workers=\"3\",\n    node_type_id=\"Standard_D4s_v5\",\n    policy_id=POLICY_ID,\n    autoscale=(2, 3),\n    spark_conf={},\n    spark_env_vars={},\n    schedule_timezone=\"America/Los_Angeles\" or \"PST\" # You can get the timezone ID from here: https://docs.oracle.com/middleware/1221/wcs/tag-ref/MISC/TimeZones.html\n)\n```\n\nThese settings can then be specified on either pipeline-level or step-level:\n\n```python\n# Either specify on pipeline-level\n@pipeline(\n    settings={\n        \"orchestrator.databricks\": databricks_settings,\n    }\n)\ndef my_pipeline():\n    ...\n```\n\nWe can also enable GPU support for the Databricks orchestrator changing the `spark_version` and `node_type_id` to a GPU-enabled version and node type:\n\n"}
{"input": "```python\nfrom zenml.integrations.databricks.flavors.databricks_orchestrator_flavor import DatabricksOrchestratorSettings\n\ndatabricks_settings = DatabricksOrchestratorSettings(\n    spark_version=\"15.3.x-gpu-ml-scala2.12\",\n    node_type_id=\"Standard_NC24ads_A100_v4\",\n    policy_id=POLICY_ID,\n    autoscale=(1, 2),\n)\n```\n\nWith these settings, the orchestrator will use a GPU-enabled Spark version and a GPU-enabled node type to run the pipeline on Databricks, next section will show how to enable CUDA for the GPU to give its full acceleration for your pipeline.\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-databricks/#zenml.integrations.databricks.flavors.databricks\\_orchestrator\\_flavor.DatabricksOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the Databricks orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-databricks/#zenml.integrations.databricks.orchestrators.databricks\\_orchestrator.DatabricksOrchestrator) ."}
{"input": "---\ndescription: Orchestrating the execution of ML pipelines.\n---\n\n# Orchestrators\n\nThe orchestrator is an essential component in any MLOps stack as it is responsible for running your machine learning\npipelines. To do so, the orchestrator provides an environment that is set up to execute the steps of your pipeline. It\nalso makes sure that the steps of your pipeline only get executed once all their inputs (which are outputs of previous\nsteps of your pipeline) are available.\n\n{% hint style=\"info\" %}\nMany of ZenML's remote orchestrators build [Docker](https://www.docker.com/) images in order to transport and execute\nyour pipeline code. If you want to learn more about how Docker images are built by ZenML, check\nout [this guide](../../how-to/customize-docker-builds/README.md).\n{% endhint %}\n\n### When to use it\n\nThe orchestrator is a mandatory component in the ZenML stack. It is used to store all artifacts produced by pipeline\nruns, and you are required to configure it in all of your stacks.\n\n### Orchestrator Flavors\n\nOut of the box, ZenML comes with a `local` orchestrator already part of the default stack that runs pipelines locally.\nAdditional orchestrators are provided by integrations:\n\n| Orchestrator                                | Flavor         | Integration       | Notes                                                                   |\n|---------------------------------------------|----------------|-------------------|-------------------------------------------------------------------------|\n| [LocalOrchestrator](local.md)               | `local`        | _built-in_        | Runs your pipelines locally.                                            |\n| [LocalDockerOrchestrator](local-docker.md)  | `local_docker` | _built-in_        | Runs your pipelines locally using Docker.                               |\n| [KubernetesOrchestrator](kubernetes.md)     | `kubernetes`   | `kubernetes`      | Runs your pipelines in Kubernetes clusters.                             |\n| [KubeflowOrchestrator](kubeflow.md)         | `kubeflow`     | `kubeflow`        | Runs your pipelines using Kubeflow.                                     |\n| [VertexOrchestrator](vertex.md)             | `vertex`       | `gcp`             | Runs your pipelines in Vertex AI.                                       |\n| [SagemakerOrchestrator](sagemaker.md)       | `sagemaker`    | `aws`             | Runs your pipelines in Sagemaker.                                       |\n| [AzureMLOrchestrator](azureml.md)           | `azureml`      | `azure`           | Runs your pipelines in AzureML.                                         |\n| [TektonOrchestrator](tekton.md)             | `tekton`       | `tekton`          | Runs your pipelines using Tekton.                                       |\n| [AirflowOrchestrator](airflow.md)           |"}
{"input": " `airflow`      | `airflow`         | Runs your pipelines using Airflow.                                      |\n| [SkypilotAWSOrchestrator](skypilot-vm.md)   | `vm_aws`       | `skypilot[aws]`   | Runs your pipelines in AWS VMs using SkyPilot                           |\n| [SkypilotGCPOrchestrator](skypilot-vm.md)   | `vm_gcp`       | `skypilot[gcp]`   | Runs your pipelines in GCP VMs using SkyPilot                           |\n| [SkypilotAzureOrchestrator](skypilot-vm.md) | `vm_azure`     | `skypilot[azure]` | Runs your pipelines in Azure VMs using SkyPilot                         |\n| [HyperAIOrchestrator](hyperai.md)           | `hyperai`      | `hyperai`         | Runs your pipeline in HyperAI.ai instances.                             |\n| [Custom Implementation](custom.md)          | _custom_       |                   | Extend the orchestrator abstraction and provide your own implementation |\n\nIf you would like to see the available flavors of orchestrators, you can use the command:\n\n```shell\nzenml orchestrator flavor list\n```\n\n### How to use it\n\nYou don't need to directly interact with any ZenML orchestrator in your code. As long as the orchestrator that you want\nto use is part of your active [ZenML stack](/docs/book/user-guide/production-guide/understand-stacks.md), using the\norchestrator is as simple as executing a Python file\nthat [runs a ZenML pipeline](/docs/book/user-guide/starter-guide/starter-guide.md):\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\n#### Inspecting Runs in the Orchestrator UI\n\nIf your orchestrator comes with a separate user interface (for example Kubeflow, Airflow, Vertex), you can get the URL\nto the orchestrator UI of a specific pipeline run using the following code snippet:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"<PIPELINE_RUN_NAME>\")\norchestrator_url = pipeline_run.run_metadata[\"orchestrator_url\"].value\n```\n\n#### Specifying per-step resources\n\nIf your steps require the orchestrator to execute them on specific hardware, you can specify them on your steps\nas described [here](../../how-to/use-configuration-files/runtime-configuration.md).\n\nIf your orchestrator of choice or the underlying hardware doesn't support this, you can also take a look\nat [step operators](../step-operators/step-operators.md).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?"}
{"input": "x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Airflow.\n---\n\n# Airflow Orchestrator\n\nZenML pipelines can be executed natively as [Airflow](https://airflow.apache.org/) \nDAGs. This brings together the power of the Airflow orchestration with the \nML-specific benefits of ZenML pipelines. Each ZenML step runs in a separate \nDocker container which is scheduled and started using Airflow.\n\n{% hint style=\"warning\" %}\nIf you're going to use a remote deployment of Airflow, you'll also need\na [remote ZenML deployment](../../getting-started/deploying-zenml/README.md).\n{% endhint %}\n\n### When to use it\n\nYou should use the Airflow orchestrator if\n\n* you're looking for a proven production-grade orchestrator.\n* you're already using Airflow.\n* you want to run your pipelines locally.\n* you're willing to deploy and maintain Airflow.\n\n### How to deploy it\n\nThe Airflow orchestrator can be used to run pipelines locally as well as remotely. In the local case, no additional\nsetup is necessary.\n\nThere are many options to use a deployed Airflow server:\n\n* Use one of [ZenML's Airflow stack recipes](https://github.com/zenml-io/mlstacks). This is the simplest solution to\n  get ZenML working with Airflow, as the recipe also takes care of additional steps such as installing required Python\n  dependencies in your Airflow server environment.\n* Use a managed deployment of Airflow such as [Google Cloud Composer](https://cloud.google.com/composer)\n  , [Amazon MWAA](https://aws.amazon.com/managed-workflows-for-apache-airflow/),\n  or [Astronomer](https://www.astronomer.io/).\n* Deploy Airflow manually. Check out the\n  official [Airflow docs](https://airflow.apache.org/docs/apache-airflow/stable/production-deployment.html) for more\n  information.\n\nIf you're not using `mlstacks` to deploy Airflow, there are some additional Python packages that you'll need to\ninstall in the Python environment of your Airflow server:\n\n* `pydantic~=2.7.1`: The Airflow DAG files that ZenML creates for you require Pydantic to parse and validate\n  configuration files.\n* `apache-airflow-providers-docker` or `apache-airflow-providers-cncf-kubernetes`, depending on which Airflow operator\n  you'll be using to run your pipeline steps. Check out [this section](airflow.md#using-different-airflow-operators) for\n  more information on supported operators.\n\n### How to use it\n\nTo use the Airflow orchestrator, we need:\n\n* The ZenML `airflow` integration installed. If you haven't done so, run\n\n  ```shell\n  zenml"}
{"input": " integration install airflow\n  ```\n* [Docker](https://docs.docker.com/get-docker/) installed and running.\n* The orchestrator registered and part of our active stack:\n\n```shell\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=airflow \\\n    --local=True  # set this to `False` if using a remote Airflow deployment\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\n{% tabs %}\n{% tab title=\"Local\" %}\nDue to dependency conflicts, we need to install the Python packages to start a local Airflow server\nin a separate Python environment.\n\n```bash\n# Create a fresh virtual environment in which we install the Airflow server dependencies\npython -m venv airflow_server_environment\nsource airflow_server_environment/bin/activate\n\n# Install the Airflow server dependencies\npip install \"apache-airflow==2.4.0\" \"apache-airflow-providers-docker<3.8.0\" \"pydantic~=2.7.1\"\n```\n\nBefore starting the local Airflow server, we can set a few environment variables to configure it:\n* `AIRFLOW_HOME`: This variable defines the location where the Airflow server stores its database and configuration\n   files. The default value is `~/airflow`.\n* `AIRFLOW__CORE__DAGS_FOLDER`: This variable defines the location where the Airflow server looks for DAG files. The\n   default value is `<AIRFLOW_HOME>/dags`.\n* `AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL`: This variable controls how often the Airflow scheduler checks for new or\n   updated DAGs. By default, the scheduler will check for new DAGs every 30 seconds. This variable can be used to\n   increase or decrease the frequency of the checks, depending on the specific needs of your pipeline.\n\n{% hint style=\"warning\" %}\nWhen running this on MacOS, you might need to set the `no_proxy` environment variable to prevent crashes due to a bug\nin Airflow (see [this page](https://github.com/apache/airflow/issues/28487) for more information):\n\n```bash\nexport no_proxy=*\n```\n{% endhint %}\n\nWe can now start the local Airflow server by running the following command:\n```bash\n# Switch to the Python environment that has Airflow installed before running this command\nairflow standalone\n```\n\nThis command will start up an Airflow server on your local machine. During the startup, it will print a username and\npassword which you can use to log in to the Airflow UI [here](http://0.0.0.0:8080).\n\nWe can now switch back the Python environment in which ZenML is installed and run a pipeline:\n```shell\n# Switch to the"}
{"input": " Python environment that has ZenML installed before running this command\npython file_that_runs_a_zenml_pipeline.py\n```\n\nThis call will produce a `.zip` file containing a representation of your ZenML pipeline for Airflow.\nThe location of this `.zip` file will be in the logs of the command above. We now need to copy this file\nto the Airflow DAGs directory, from where the local Airflow server will load it and run your pipeline\n(It might take a few seconds until the pipeline shows up in the Airflow UI).\nTo figure out the DAGs directory, we can run `airflow config get-value core DAGS_FOLDER` while having our\nPython environment with the Airflow installation active.\n\nTo make this process easier, we can configure our ZenML Airflow orchestrator to automatically copy the `.zip` file\nto this directory for us. To do so, run the following command:\n```bash\n# Switch to the Python environment that has ZenML installed before running this command\nzenml orchestrator update --dag_output_dir=<AIRFLOW_DAG_DIRECTORY>\n```\n\nNow that we've set this up, running a pipeline in Airflow is as simple as just running the Python file:\n```shell\n# Switch to the Python environment that has ZenML installed before running this command\npython file_that_runs_a_zenml_pipeline.py\n```\n{% endtab %}\n\n{% tab title=\"Remote\" %}\nWhen using the Airflow orchestrator with a remote deployment, you'll additionally need:\n\n* A remote ZenML server deployed to the cloud. See\n  the [deployment guide](../../getting-started/deploying-zenml/README.md) \n  for more information.\n* A deployed Airflow server. See the [deployment section](airflow.md#how-to-deploy-it) for more information.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n\nIn the remote case, the Airflow orchestrator works differently than other ZenML orchestrators. Executing a python file\nwhich runs a pipeline by calling `pipeline.run()` will not actually run the pipeline, but instead will create a `.zip`\nfile containing an Airflow representation of your ZenML pipeline. In one additional step, you need to make sure this zip\nfile ends up in\nthe [DAGs directory](https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html#architecture-overview)\nof your Airflow deployment.\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use\nit to run your pipeline steps in Airflow. Check\nout [this page](/docs"}
{"input": "/book/how-to/customize-docker-builds/README.md) if you want to learn\nmore about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\n#### Scheduling\n\nYou can [schedule pipeline runs](../../how-to/build-pipelines/schedule-a-pipeline.md)\non Airflow similarly to other orchestrators. However, note that \n**Airflow schedules always need to be set in the past**, e.g.,:\n\n```python\nfrom datetime import datetime, timedelta\n\nfrom zenml.pipelines import Schedule\n\nscheduled_pipeline = fashion_mnist_pipeline.with_options(\n    schedule=Schedule(\n        start_time=datetime.now() - timedelta(hours=1),  # start in the past\n        end_time=datetime.now() + timedelta(hours=1),\n        interval_second=timedelta(minutes=15),  # run every 15 minutes\n        catchup=False,\n    )\n)\nscheduled_pipeline()\n```\n\n#### Airflow UI\n\nAirflow comes with its own UI that you can use to find further details about your pipeline runs, such as the logs of\nyour steps. For local Airflow, you can find the Airflow UI at [http://localhost:8080](http://localhost:8080) by default.\n\n{% hint style=\"info\" %}\nIf you cannot see the Airflow UI credentials in the console, you can find the\npassword in `<AIRFLOW_HOME>/standalone_admin_password.txt`. `AIRFLOW_HOME` will usually be `~/airflow` unless you've\nmanually configured it with the `AIRFLOW_HOME` environment variable.\nYou can always run `airflow info` to figure out the directory for the active environment.\n\nThe username will always be `admin`.\n{% endhint %}\n\n#### Additional configuration\n\nFor additional configuration of the Airflow orchestrator, you can pass `AirflowOrchestratorSettings` when defining or\nrunning your pipeline. Check out\nthe [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-airflow/#zenml.integrations.airflow.flavors.airflow\\_orchestrator\\_flavor.AirflowOrchestratorSettings)\nfor a full list of available attributes and [this docs page](/docs/book/how-to/use-configuration-files/README.md) for\nmore information on how to specify settings.\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to\nfollow [the instructions on this page](/docs/book/how-to/training-with-gpus/training-with-gpus.md) to ensure that it\nworks. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full\nacceleration.\n\n#### Using different Airflow operators\n\nAirflow operators specify how a step in your pipeline gets executed. As ZenML relies on Docker images to run pipeline\nsteps"}
{"input": ", only operators that support executing a Docker image work in combination with ZenML. Airflow comes with two\noperators that support this:\n\n* the `DockerOperator` runs the Docker images for executing your pipeline steps on the same machine that your Airflow\n  server is running on. For this to work, the server environment needs to have the `apache-airflow-providers-docker`\n  package installed.\n* the `KubernetesPodOperator` runs the Docker image on a pod in the Kubernetes cluster that the Airflow server is\n  deployed to. For this to work, the server environment needs to have the `apache-airflow-providers-cncf-kubernetes`\n  package installed.\n\nYou can specify which operator to use and additional arguments to it as follows:\n\n```python\nfrom zenml import pipeline, step\nfrom zenml.integrations.airflow.flavors.airflow_orchestrator_flavor import AirflowOrchestratorSettings\n\nairflow_settings = AirflowOrchestratorSettings(\n    operator=\"docker\",  # or \"kubernetes_pod\"\n    # Dictionary of arguments to pass to the operator __init__ method\n    operator_args={}\n)\n\n# Using the operator for a single step\n@step(settings={\"orchestrator.airflow\": airflow_settings})\ndef my_step(...):\n\n\n# Using the operator for all steps in your pipeline\n@pipeline(settings={\"orchestrator.airflow\": airflow_settings})\ndef my_pipeline(...):\n```\n\n**Custom operators**\n\nIf you want to use any other operator to run your steps, you can specify the `operator` in your `AirflowSettings` as a\npath to the python operator class:\n\n```python\nfrom zenml.integrations.airflow.flavors.airflow_orchestrator_flavor import AirflowOrchestratorSettings\n\nairflow_settings = AirflowOrchestratorSettings(\n    # This could also be a reference to one of your custom classes.\n    # e.g. `my_module.MyCustomOperatorClass` as long as the class\n    # is importable in your Airflow server environment\n    operator=\"airflow.providers.docker.operators.docker.DockerOperator\",\n    # Dictionary of arguments to pass to the operator __init__ method\n    operator_args={}\n)\n```\n\n**Custom DAG generator file**\n\nTo run a pipeline in Airflow, ZenML creates a Zip archive that contains two files:\n\n* A JSON configuration file that the orchestrator creates. This file contains all the information required to create the\n  Airflow DAG to run the pipeline.\n* A Python file that reads this configuration file and actually creates the Airflow DAG. We call this file\n  the `DAG generator` and you can find the\n  implementation [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/airflow/orchestrators/dag\\_generator.py)\n  .\n\nIf you need more control over"}
{"input": " how the Airflow DAG is generated, you can provide a custom DAG generator file using the\nsetting `custom_dag_generator`. This setting will need to reference a Python module that can be imported into your\nactive Python environment. It will additionally need to contain the same classes (`DagConfiguration`\nand `TaskConfiguration`) and constants (`ENV_ZENML_AIRFLOW_RUN_ID`, `ENV_ZENML_LOCAL_STORES_PATH` and `CONFIG_FILENAME`)\nas\nthe [original module](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/airflow/orchestrators/dag\\_generator.py)\n. For this reason, we suggest starting by copying the original and modifying it according to your needs.\n\nCheck out our docs on how to apply settings to your\npipelines [here](/docs/book/how-to/use-configuration-files/README.md).\n\nFor more information and a full list of configurable attributes of the Airflow orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-airflow/#zenml.integrations.airflow.orchestrators.airflow_orchestrator.AirflowOrchestrator) .\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>"}
{"input": "---\ndescription: Learning how to develop a custom orchestrator.\n---\n\n# Develop a custom orchestrator\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n### Base Implementation\n\nZenML aims to enable orchestration with any orchestration tool. This is where the `BaseOrchestrator` comes into play. It abstracts away many of the ZenML-specific details from the actual implementation and exposes a simplified interface:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Type\n\nfrom zenml.models import PipelineDeploymentResponseModel\nfrom zenml.enums import StackComponentType\nfrom zenml.stack import StackComponent, StackComponentConfig, Stack, Flavor\n\n\nclass BaseOrchestratorConfig(StackComponentConfig):\n    \"\"\"Base class for all ZenML orchestrator configurations.\"\"\"\n\n\nclass BaseOrchestrator(StackComponent, ABC):\n    \"\"\"Base class for all ZenML orchestrators\"\"\"\n\n    @abstractmethod\n    def prepare_or_run_pipeline(\n        self,\n        deployment: PipelineDeploymentResponseModel,\n        stack: Stack,\n        environment: Dict[str, str],\n    ) -> Any:\n        \"\"\"Prepares and runs the pipeline outright or returns an intermediate\n        pipeline representation that gets deployed.\n        \"\"\"\n\n    @abstractmethod\n    def get_orchestrator_run_id(self) -> str:\n        \"\"\"Returns the run id of the active orchestrator run.\n\n        Important: This needs to be a unique ID and return the same value for\n        all steps of a pipeline run.\n\n        Returns:\n            The orchestrator run id.\n        \"\"\"\n\n\nclass BaseOrchestratorFlavor(Flavor):\n    \"\"\"Base orchestrator for all ZenML orchestrator flavors.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self):\n        \"\"\"Returns the name of the flavor.\"\"\"\n\n    @property\n    def type(self) -> StackComponentType:\n        \"\"\"Returns the flavor type.\"\"\"\n        return StackComponentType.ORCHESTRATOR\n\n    @property\n    def config_class(self) -> Type[BaseOrchestratorConfig]:\n        \"\"\"Config class for the base orchestrator flavor.\"\"\"\n        return BaseOrchestratorConfig\n\n    @property\n    @abstractmethod\n    def implementation_class(self) -> Type[\"BaseOrchestrator\"]:\n        \"\"\"Implementation class for this flavor.\"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the abstraction layer. In order to see the full implementation and get the complete docstrings, please check [the source code on GitHub](https://github.com/zenml-io/zenml/blob/main/src/zenml/orchestrators/base"}
{"input": "\\_orchestrator.py) .\n{% endhint %}\n\n### Build your own custom orchestrator\n\nIf you want to create your own custom flavor for an orchestrator, you can follow the following steps:\n\n1. Create a class that inherits from the `BaseOrchestrator` class and implement the abstract `prepare_or_run_pipeline(...)` and `get_orchestrator_run_id()` methods.\n2. If you need to provide any configuration, create a class that inherits from the `BaseOrchestratorConfig` class and add your configuration parameters.\n3. Bring both the implementation and the configuration together by inheriting from the `BaseOrchestratorFlavor` class. Make sure that you give a `name` to the flavor through its abstract property.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml orchestrator flavor register <path.to.MyOrchestratorFlavor>\n```\n\nFor example, if your flavor class `MyOrchestratorFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml orchestrator flavor register flavors.my_flavor.MyOrchestratorFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually, it's better to not have to rely on this mechanism and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml orchestrator flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomOrchestratorFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomOrchestratorConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` object are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomOrchestrator** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even"}
{"input": " when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomOrchestratorFlavor` and the `CustomOrchestratorConfig` are implemented in a different module/path than the actual `CustomOrchestrator`).\n{% endhint %}\n\n## Implementation guide\n\n1. **Create your orchestrator class:** This class should either inherit from `BaseOrchestrator`, or more commonly from `ContainerizedOrchestrator`. If your orchestrator uses container images to run code, you should inherit from `ContainerizedOrchestrator` which handles building all Docker images for the pipeline to be executed. If your orchestator does not use container images, you'll be responsible that the execution environment contains all the necessary requirements and code files to run the pipeline.\n2.  **Implement the `prepare_or_run_pipeline(...)` method:** This method is responsible for running or scheduling the pipeline. In most cases, this means converting the pipeline into a format that your orchestration tool understands and running it. To do so, you should:\n\n    * Loop over all steps of the pipeline and configure your orchestration tool to run the correct command and arguments in the correct Docker image\n    * Make sure the passed environment variables are set when the container is run\n    * Make sure the containers are running in the correct order\n\n    Check out the [code sample](custom.md#code-sample) below for more details on how to fetch the Docker image, command, arguments and step order.\n3. **Implement the `get_orchestrator_run_id()` method:** This must return a ID that is different for each pipeline run, but identical if called from within Docker containers running different steps of the same pipeline run. If your orchestrator is based on an external tool like Kubeflow or Airflow, it is usually best to use an unique ID provided by this tool.\n\n{% hint style=\"info\" %}\nTo see a full end-to-end worked example of a custom orchestrator, [see here](https://github.com/zenml-io/zenml-plugins/tree/main/how\\_to\\_custom\\_orchestrator).\n{% endhint %}\n\n### Optional features\n\nThere are some additional optional features that your orchestrator can implement:\n\n* **Running pipelines on a schedule**: if your orchestrator supports running pipelines on a schedule, make sure to handle `deployment.schedule` if it exists. If your orchestrator does not support schedules, you should either log a warning and or even raise an exception in case the user tries to schedule a pipeline.\n* **Specifying hardware resources**: If your orchestrator supports setting resources like CPUs, GPUs or memory for the pipeline or specific steps, make sure to handle the values defined in `step.config.resource_settings`. See the code sample below for additional helper methods to check whether any resources are required from your orchestrator.\n\n### Code sample\n\n```python\nfrom typing import Dict\n\nfrom zenml.entrypoints"}
{"input": " import StepEntrypointConfiguration\nfrom zenml.models import PipelineDeploymentResponseModel\nfrom zenml.orchestrators import ContainerizedOrchestrator\nfrom zenml.stack import Stack\n\n\nclass MyOrchestrator(ContainerizedOrchestrator):\n\n    def get_orchestrator_run_id(self) -> str:\n        # Return an ID that is different each time a pipeline is run, but the\n        # same for all steps being executed as part of the same pipeline run.\n        # If you're using some external orchestration tool like Kubeflow, you\n        # can usually use the run ID of that tool here.\n        ...\n\n    def prepare_or_run_pipeline(\n        self,\n        deployment: \"PipelineDeploymentResponseModel\",\n        stack: \"Stack\",\n        environment: Dict[str, str],\n    ) -> None:\n        # If your orchestrator supports scheduling, you should handle the schedule\n        # configured by the user. Otherwise you might raise an exception or log a warning\n        # that the orchestrator doesn't support scheduling\n        if deployment.schedule:\n            ...\n\n        for step_name, step in deployment.step_configurations.items():\n            image = self.get_image(deployment=deployment, step_name=step_name)\n            command = StepEntrypointConfiguration.get_entrypoint_command()\n            arguments = StepEntrypointConfiguration.get_entrypoint_arguments(\n                step_name=step_name, deployment_id=deployment.id\n            )\n            # Your orchestration tool should run this command and arguments\n            # in the Docker image fetched above. Additionally, the container which\n            # is running the command must contain the environment variables specified\n            # in the `environment` dictionary.\n            \n            # If your orchestrator supports parallel execution of steps, make sure\n            # each step only runs after all its upstream steps finished\n            upstream_steps = step.spec.upstream_steps\n\n            # You can get the settings your orchestrator like so.\n            # The settings are the \"dynamic\" part of your orchestrators config,\n            # optionally defined when you register your orchestrator but can be\n            # overridden at runtime.\n            # In contrast, the \"static\" part of your orchestrators config is\n            # always defined when you register the orchestrator and can be\n            # accessed via `self.config`.\n            step_settings = cast(\n                MyOrchestratorSettings, self.get_settings(step)\n            )\n\n            # If your orchestrator supports setting resources like CPUs, GPUs or\n            # memory for the pipeline or specific steps, you can find out whether\n            # specific resources were specified for this step:\n            if self.requires_resources_in_orchestration_environment(step):\n                resources = step.config.resource_settings\n```\n\n{% hint style=\"info\" %}\nTo see a full end-to-end worked example of a custom orchestrator, [see here](https://github.com/zenml-io/zenml-plugins/tree/main/how\\_to\\_custom\\_"}
{"input": "orchestrator).\n{% endhint %}\n\n### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use your custom orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Vertex AI.\n---\n\n# Google Cloud VertexAI Orchestrator\n\n[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) is a serverless ML workflow tool running on the Google Cloud Platform. It is an easy way to quickly run your code in a production-ready, repeatable cloud orchestrator that requires minimal setup without provisioning and paying for standby compute.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n## When to use it\n\nYou should use the Vertex orchestrator if:\n\n* you're already using GCP.\n* you're looking for a proven production-grade orchestrator.\n* you're looking for a UI in which you can track your pipeline runs.\n* you're looking for a managed solution for running your pipelines.\n* you're looking for a serverless solution for running your pipelines.\n\n## How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding a Vertex AI orchestrator? Check out the\n[in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML GCP Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\nIn order to use a Vertex AI orchestrator, you need to first deploy [ZenML to the cloud](../../getting-started/deploying-zenml/README.md). It would be recommended to deploy ZenML in the same Google Cloud project as where the Vertex infrastructure is deployed, but it is not necessary to do so. You must ensure that you are connected to the remote ZenML server before using this stack component.\n\nThe only other thing necessary to use the ZenML Vertex orchestrator is enabling Vertex-relevant APIs on the Google Cloud project.\n\nIn order to quickly enable APIs, and create other resources necessary for using this integration, you can also consider using [mlstacks](https://mlstacks.zenml.io/vertex), which helps you set up the infrastructure with one click.\n\n## How to use it\n\nTo use the Vertex orchestrator, we need:\n\n*   The ZenML `gcp` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install gcp\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of"}
{"input": " your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* [GCP credentials with proper permissions](vertex.md#gcp-credentials-and-permissions)\n* The GCP project ID and location in which you want to run your Vertex AI pipelines.\n\n### GCP credentials and permissions\n\nThis part is without doubt the most involved part of using the Vertex orchestrator. In order to run pipelines on Vertex AI, you need to have a GCP user account and/or one or more GCP service accounts set up with proper permissions, depending on whether you wish to practice [the principle of least privilege](https://en.wikipedia.org/wiki/Principle\\_of\\_least\\_privilege) and distribute permissions across multiple service accounts.\n\nYou also have three different options to provide credentials to the orchestrator:\n\n* use the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) to authenticate locally with GCP\n* configure the orchestrator to use a [service account key file](https://cloud.google.com/iam/docs/creating-managing-service-account-keys) to authenticate with GCP by setting the `service_account_path` parameter in the orchestrator configuration.\n* (recommended) configure [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) with GCP credentials and then link the Vertex AI Orchestrator stack component to the Service Connector.\n\nThis section [explains the different components and GCP resources](vertex.md#vertex-ai-pipeline-components) involved in running a Vertex AI pipeline and what permissions they need, then provides instructions for three different configuration use-cases:\n\n1. [use the local `gcloud` CLI configured with your GCP user account](vertex.md#configuration-use-case-local-gcloud-cli-with-user-account), including the ability to schedule pipelines\n2. [use a GCP Service Connector and a single service account](vertex.md#configuration-use-case-gcp-service-connector-with-single-service-account) with all permissions, including the ability to schedule pipelines\n3. [use a GCP Service Connector and multiple service accounts](vertex.md#configuration-use-case-gcp-service-connector-with-different-service-accounts) for different permissions, including the ability to schedule pipelines\n\n#### Vertex AI pipeline components\n\nTo understand what accounts you need to provision and why, let's look at the different components of the Vertex orchestrator:\n\n1. _the ZenML client environment_ is the environment where you run the ZenML code responsible for building the pipeline Docker image and submitting the pipeline to Vertex AI, among other things. This is usually your local machine or some other environment used to automate running pipelines, like a CI/CD job. This environment needs to be able to authenticate with GCP and needs to have the necessary permissions to create a job in Vertex Pipelines, (e.g. [the `Vertex AI User` role](https://cloud.google"}
{"input": ".com/vertex-ai/docs/general/access-control#aiplatform.user)). If you are planning to [run pipelines on a schedule](vertex.md#run-pipelines-on-a-schedule), _the ZenML client environment_ also needs additional permissions:\n   * the [`Storage Object Creator Role`](https://cloud.google.com/iam/docs/understanding-roles#storage.objectCreator) to be able to write the pipeline JSON file to the artifact store directly (NOTE: not needed if the Artifact Store is configured with credentials or is linked to Service Connector)\n2. _the Vertex AI pipeline environment_ is the GCP environment in which the pipeline steps themselves are running in GCP. The Vertex AI pipeline runs in the context of a GCP service account which we'll call here _the workload service account_. _The workload service account_ can be explicitly configured in the orchestrator configuration via the `workload_service_account` parameter. If it is omitted, the orchestrator will use [the Compute Engine default service account](https://cloud.google.com/compute/docs/access/service-accounts#default\\_service\\_account) for the GCP project in which the pipeline is running. This service account needs to have the following permissions:\n   * permissions to run a Vertex AI pipeline, (e.g. [the `Vertex AI Service Agent` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.serviceAgent)).\n\nAs you can see, there can be dedicated service accounts involved in running a Vertex AI pipeline. That's two service accounts if you also use a service account to authenticate to GCP in _the ZenML client environment_. However, you can keep it simple and use the same service account everywhere.\n\n#### Configuration use-case: local `gcloud` CLI with user account\n\nThis configuration use-case assumes you have configured the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) to authenticate locally with your GCP account (i.e. by running `gcloud auth login`). It also assumes the following:\n\n* your GCP account has permissions to create a job in Vertex Pipelines, (e.g. [the `Vertex AI User` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.user)).\n* [the Compute Engine default service account](https://cloud.google.com/compute/docs/access/service-accounts#default\\_service\\_account) for the GCP project in which the pipeline is running is updated with additional permissions required to run a Vertex AI pipeline, (e.g. [the `Vertex AI Service Agent` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.serviceAgent)).\n\nThis is the easiest way to configure the Vertex AI Orchestrator, but it has the following drawbacks:\n\n* the setup is not portable on other machines and reproducible by other users.\n* it uses the Compute Engine default service account, which is not recommended, given that"}
{"input": " it has a lot of permissions by default and is used by many other GCP services.\n\nWe can then register the orchestrator as follows:\n\n```shell\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=vertex \\\n    --project=<PROJECT_ID> \\\n    --location=<GCP_LOCATION> \\\n    --synchronous=true\n```\n\n#### Configuration use-case: GCP Service Connector with single service account\n\nThis use-case assumes you have already configured a GCP service account with the following permissions:\n\n* permissions to create a job in Vertex Pipelines, (e.g. [the `Vertex AI User` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.user)).\n* permissions to run a Vertex AI pipeline, (e.g. [the `Vertex AI Service Agent` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.serviceAgent)).\n* the [Storage Object Creator Role](https://cloud.google.com/iam/docs/understanding-roles#storage.objectCreator) to be able to write the pipeline JSON file to the artifact store directly.\n\nIt also assumes you have already created a service account key for this service account and downloaded it to your local machine (e.g. in a `connectors-vertex-ai-workload.json` file). This is not recommended if you are conscious about security. The principle of least privilege is not applied here and the environment in which the pipeline steps are running has many permissions that it doesn't need.\n\n```shell\nzenml service-connector register <CONNECTOR_NAME> --type gcp --auth-method=service-account --project_id=<PROJECT_ID> --service_account_json=@connectors-vertex-ai-workload.json --resource-type gcp-generic\n\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=vertex \\\n    --location=<GCP_LOCATION> \\\n    --synchronous=true \\\n    --workload_service_account=<SERVICE_ACCOUNT_NAME>@<PROJECT_NAME>.iam.gserviceaccount.com \n    \nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>\n```\n\n#### Configuration use-case: GCP Service Connector with different service accounts\n\nThis setup applies the principle of least privilege by using different service accounts with the minimum of permissions needed for [the different components involved in running a Vertex AI pipeline](vertex.md#vertex-ai-pipeline-components). It also uses a GCP Service Connector to make the setup portable and reproducible. This configuration is a best-in-class setup that you would normally use in production, but it requires a lot more work to prepare.\n\n{% hint style=\"info\" %}\nThis setup involves creating and configuring several GCP service accounts, which is a lot of work and can be error prone. If you don't really need the added security, you can use [the GCP Service Connector with a single service account](vertex.md#"}
{"input": "configuration-use-case-gcp-service-connector-with-single-service-account) instead.\n{% endhint %}\n\nThe following GCP service accounts are needed:\n\n1. a \"client\" service account that has the following permissions:\n   * permissions to create a job in Vertex Pipelines, (e.g. [the `Vertex AI User` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.user)).\n   * permissions to create a Google Cloud Function (e.g. with the [`Cloud Functions Developer Role`](https://cloud.google.com/functions/docs/reference/iam/roles#cloudfunctions.developer)).\n   * the [Storage Object Creator Role](https://cloud.google.com/iam/docs/understanding-roles#storage.objectCreator) to be able to write the pipeline JSON file to the artifact store directly (NOTE: not needed if the Artifact Store is configured with credentials or is linked to Service Connector).\n2. a \"workload\" service account that has permissions to run a Vertex AI pipeline, (e.g. [the `Vertex AI Service Agent` role](https://cloud.google.com/vertex-ai/docs/general/access-control#aiplatform.serviceAgent)).\n\nA key is also needed for the \"client\" service account. You can create a key for this service account and download it to your local machine (e.g. in a `connectors-vertex-ai-workload.json` file).\n\nWith all the service accounts and the key ready, we can register [the GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) and Vertex AI orchestrator as follows:\n\n```shell\nzenml service-connector register <CONNECTOR_NAME> --type gcp --auth-method=service-account --project_id=<PROJECT_ID> --service_account_json=@connectors-vertex-ai-workload.json --resource-type gcp-generic\n\nzenml orchestrator register <ORCHESTRATOR_NAME> \\\n    --flavor=vertex \\\n    --location=<GCP_LOCATION> \\\n    --synchronous=true \\\n    --workload_service_account=<WORKLOAD_SERVICE_ACCOUNT_NAME>@<PROJECT_NAME>.iam.gserviceaccount.com \n\nzenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>\n```\n\n### Configuring the stack\n\nWith the orchestrator registered, we can use it in our active stack:\n\n```shell\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your pipeline steps in Vertex AI. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n"}
{"input": "{% endhint %}\n\nYou can now run any ZenML pipeline using the Vertex orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\n### Vertex UI\n\nVertex comes with its own UI that you can use to find further details about your pipeline runs, such as the logs of your steps.\n\n![Vertex UI](../../.gitbook/assets/VertexUI.png)\n\nFor any runs executed on Vertex, you can get the URL to the Vertex UI in Python using the following code snippet:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"<PIPELINE_RUN_NAME>\")\norchestrator_url = pipeline_run.run_metadata[\"orchestrator_url\"].value\n```\n\n### Run pipelines on a schedule\n\nThe Vertex Pipelines orchestrator supports running pipelines on a schedule using its [native scheduling capability](https://cloud.google.com/vertex-ai/docs/pipelines/schedule-pipeline-run).\n\n**How to schedule a pipeline**\n\n```python\nfrom zenml.config.schedule import Schedule\n\n# Run a pipeline every 5th minute\npipeline_instance.run(\n    schedule=Schedule(\n        cron_expression=\"*/5 * * * *\"\n    )\n)\n\n# Run a pipeline every hour\n# starting in one day from now and ending in three days from now\npipeline_instance.run(\n    schedule=Schedule(\n        cron_expression=\"0 * * * *\"\n        start_time=datetime.datetime.now() + datetime.timedelta(days=1),\n        end_time=datetime.datetime.now() + datetime.timedelta(days=3),\n    )\n)\n```\n\n{% hint style=\"warning\" %}\nThe Vertex orchestrator only supports the `cron_expression`, `start_time` (optional) and `end_time` (optional) parameters in the `Schedule` object, and will ignore all other parameters supplied to define the schedule.\n{% endhint %}\n\nThe `start_time` and `end_time` timestamp parameters are both optional and are to be specified in local time. They define the time window in which the pipeline runs will be triggered. If they are not specified, the pipeline will run indefinitely.\n\nThe `cron_expression` parameter [supports timezones](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.schedules). For example, the expression `TZ=Europe/Paris 0 10 * * *` will trigger runs at 10:00 in the Europe/Paris timezone.\n\n**How to delete a scheduled pipeline**\n\nNote that ZenML only gets involved to schedule a run, but maintaining the lifecycle of the schedule is the responsibility of the user.\n\nIn order to cancel a scheduled Vertex pipeline, you need to manually delete the schedule in VertexAI (via the UI or the CLI).\n\n### Additional configuration\n\nFor additional configuration of the Vertex orchestrator, you can pass `VertexOrchestratorSettings` which allows you to configure labels for your Vertex Pipeline jobs or specify which GPU to use.\n\n```python\nfrom zenml.int"}
{"input": "egrations.gcp.flavors.vertex_orchestrator_flavor import VertexOrchestratorSettings\nfrom kubernetes.client.models import V1Toleration\n\nvertex_settings = VertexOrchestratorSettings(\n    labels={\"key\": \"value\"}\n)\n```\n\nIf your pipelines steps have certain hardware requirements, you can specify them as `ResourceSettings`:\n\n```python\nresource_settings = ResourceSettings(cpu_count=8, memory=\"16GB\")\n```\n\nTo run your pipeline (or some steps of it) on a GPU, you will need to set both a node selector\nand the gpu count as follows:\n```python\nvertex_settings = VertexOrchestratorSettings(\n    pod_settings={\n        \"node_selectors\": {\n            \"cloud.google.com/gke-accelerator\": \"NVIDIA_TESLA_A100\"\n        },\n    }\n)\nresource_settings = ResourceSettings(gpu_count=1)\n```\nYou can find available accelerator types [here](https://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus).\n\nThese settings can then be specified on either pipeline-level or step-level:\n\n```python\n# Either specify on pipeline-level\n@pipeline(\n    settings={\n        \"orchestrator.vertex\": vertex_settings,\n        \"resources\": resource_settings,\n    }\n)\ndef my_pipeline():\n    ...\n\n# OR specify settings on step-level\n@step(\n    settings={\n        \"orchestrator.vertex\": vertex_settings,\n        \"resources\": resource_settings,\n    }\n)\ndef my_step():\n    ...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-gcp/#zenml.integrations.gcp.flavors.vertex\\_orchestrator\\_flavor.VertexOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the Vertex orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-gcp/#zenml.integrations.gcp.orchestrators.vertex\\_orchestrator.VertexOrchestrator) .\n\n### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf"}
{"input": "\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run locally.\n---\n\n# Local Orchestrator\n\nThe local orchestrator is an [orchestrator](./orchestrators.md) flavor that comes built-in with ZenML and runs your pipelines locally.\n\n### When to use it\n\nThe local orchestrator is part of your default stack when you're first getting started with ZenML. Due to it running locally on your machine, it requires no additional setup and is easy to use and debug.\n\nYou should use the local orchestrator if:\n\n* you're just getting started with ZenML and want to run pipelines without setting up any cloud infrastructure.\n* you're writing a new pipeline and want to experiment and debug quickly\n\n### How to deploy it\n\nThe local orchestrator comes with ZenML and works without any additional setup.\n\n### How to use it\n\nTo use the local orchestrator, we can register it and use it in our active stack:\n\n```shell\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor=local\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\nYou can now run any ZenML pipeline using the local orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\nFor more information and a full list of configurable attributes of the local orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-orchestrators/#zenml.orchestrators.local.local\\_orchestrator.LocalOrchestrator) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on Kubeflow.\n---\n\n# Kubeflow Orchestrator\n\nThe Kubeflow orchestrator is an [orchestrator](./orchestrators.md) flavor provided by the ZenML `kubeflow` integration that uses [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/introduction/) to run your pipelines.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n### When to use it\n\nYou should use the Kubeflow orchestrator if:\n\n* you're looking for a proven production-grade orchestrator.\n* you're looking for a UI in which you can track your pipeline runs.\n* you're already using Kubernetes or are not afraid of setting up and maintaining a Kubernetes cluster.\n* you're willing to deploy and maintain Kubeflow Pipelines on your cluster.\n\n### How to deploy it\n\nThe Kubeflow orchestrator supports two different modes: `Local` and `remote`. In case you want to run the orchestrator on a local Kubernetes cluster running on your machine, there is no additional infrastructure setup necessary.\n\nIf you want to run your pipelines on a remote cluster instead, you'll need to set up a Kubernetes cluster and deploy Kubeflow Pipelines:\n\n{% tabs %}\n{% tab title=\"AWS\" %}\n* Have an existing AWS [EKS cluster](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html) set up.\n* Make sure you have the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) set up.\n*   Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and [configure](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-connection/) it to talk to your EKS cluster using the following command:\n\n    ```powershell\n    aws eks --region REGION update-kubeconfig --name CLUSTER_NAME\n    ```\n* [Install](https://www.kubeflow.org/docs/components/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines) Kubeflow Pipelines onto your cluster.\n* ( optional) [set up an AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) to grant ZenML Stack Components easy and secure access to the remote EKS cluster.\n{% endtab %}\n\n{% tab title=\"GCP\" %}\n* Have an existing GCP [GKE cluster](https://cloud.google.com/kubernetes-engine/docs/quickstart) set up.\n* Make sure you have the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk) set up first.\n*   Download"}
{"input": " and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and [configure](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl) it to talk to your GKE cluster using the following command:\n\n    ```powershell\n    gcloud container clusters get-credentials CLUSTER_NAME\n    ```\n* [Install](https://www.kubeflow.org/docs/distributions/gke/deploy/overview/) Kubeflow Pipelines onto your cluster.\n* ( optional) [set up a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) to grant ZenML Stack Components easy and secure access to the remote GKE cluster.\n{% endtab %}\n\n{% tab title=\"Azure\" %}\n* Have an existing [AKS cluster](https://azure.microsoft.com/en-in/services/kubernetes-service/#documentation) set up.\n* Make sure you have the [`az` CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) set up first.\n*   Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and ensure that it talks to your AKS cluster using the following command:\n\n    ```powershell\n    az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER_NAME\n    ```\n* [Install](https://www.kubeflow.org/docs/components/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines) Kubeflow Pipelines onto your cluster.\n\n> Since Kubernetes v1.19, AKS has shifted\n\nto [`containerd`](https://docs.microsoft.com/en-us/azure/aks/cluster-configuration#container-settings)\n\n> . However, the workflow controller installed with the Kubeflow installation has `Docker` set as the default runtime. In order to make your pipelines work, you have to change the value to one of the options\n\nlisted [here](https://argoproj.github.io/argo-workflows/workflow-executors/#workflow-executors)\n\n> , preferably `k8sapi`.\n>\n> This change has to be made by editing the `containerRuntimeExecutor` property of the `ConfigMap` corresponding to the workflow controller. Run the following commands to first know what config map to change and then to edit it to reflect your new value.\n>\n> ```\n> kubectl get configmap -n kubeflow\n> kubectl edit configmap CONFIGMAP_NAME -n kubeflow\n> # This opens up an editor that can be used to make the change.\n> ```\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nIf one or more of the deployments are not in the `Running` state, try increasing the number of nodes in your cluster.\n{% endhint %}\n\n{% hint style=\"warning\" %}\nIf you're installing Kubeflow Pipelines manually, make sure the Kubernetes service is called exactly"}
{"input": " `ml-pipeline`. This is a requirement for ZenML to connect to your Kubeflow Pipelines deployment.\n{% endhint %}\n\n#### Infrastructure Deployment\n\nA Kubeflow orchestrator can be deployed directly from the ZenML CLI:\n\n```shell\nzenml orchestrator deploy kubeflow_orchestrator --flavor=kubeflow --provider=<YOUR_PROVIDER> ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n### How to use it\n\nTo use the Kubeflow orchestrator, we need:\n\n*   The ZenML `kubeflow` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install kubeflow\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) installed (optional, see below)\n\n{% hint style=\"info\" %}\nIf you are using a single-tenant Kubeflow installed in a Kubernetes cluster managed by a cloud provider like AWS, GCP or Azure, it is recommended that you set up [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) and use it to connect ZenML Stack Components to the remote Kubernetes cluster. This guarantees that your Stack is fully portable on other environments and your pipelines are fully reproducible.\n{% endhint %}\n\n{% tabs %}\n{% tab title=\"Local\" %}\nWhen using the Kubeflow orchestrator locally, you'll additionally need:\n\n* [K3D](https://k3d.io/v5.2.1/#installation) installed to spin up a local Kubernetes cluster.\n* [Terraform](https://www.terraform.io/downloads.html) installed to set up the Kubernetes cluster with various deployments.\n* [MLStacks](https://mlstacks.zenml.io) installed to handle the deployment\n\nTo run the pipeline on a local Kubeflow Pipelines deployment, you can use the ZenML `mlstacks` package to spin up a local Kubernetes cluster and install Kubeflow Pipelines on it.\n\nTo deploy the stack, run the following commands:\n\n```shell\n# Deploy the stack using the ZenML CLI:\nzenml stack deploy k3d-modular -o kubeflow -a minio --provider k3d\nzenml stack set k3d-modular\n```\n\n```shell\n# Get the Kubeflow Pipelines UI endpoint\nkubectl get ingress -n kubeflow  -o jsonpath='{.items[0].spec.rules[0].host}'\n```\n\nYou can read more about `mlstacks` on [our dedicated documentation page here](https://mlstacks.zenml.io"}
{"input": ").\n\n{% hint style=\"warning\" %}\nThe local Kubeflow Pipelines deployment requires more than 4 GB of RAM, and 30 GB of disk space, so if you are using Docker Desktop make sure to update the resource limits in the preferences.\n{% endhint %}\n{% endtab %}\n\n{% tab title=\"Remote\" %}\nWhen using the Kubeflow orchestrator with a remote cluster, you'll additionally need:\n\n* A remote ZenML server deployed to the cloud. See the [deployment guide](../../getting-started/deploying-zenml/README.md) for more information.\n* Kubeflow pipelines deployed on a remote cluster. See the [deployment section](kubeflow.md#how-to-deploy-it) for more information.\n* The name of your Kubernetes context which points to your remote cluster. Run `kubectl config get-contexts` to see a list of available contexts. **NOTE**: this is no longer required if you are using [a Service Connector ](../../how-to/auth-management/service-connectors-guide.md)to connect your Kubeflow Orchestrator Stack Component to the remote Kubernetes cluster.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n\nWe can then register the orchestrator and use it in our active stack. This can be done in two ways:\n\n1.  If you have [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) configured to access the remote Kubernetes cluster, you no longer need to set the `kubernetes_context` attribute to a local `kubectl` context. In fact, you don't need the local Kubernetes CLI at all. You can [connect the stack component to the Service Connector](../../how-to/auth-management/service-connectors-guide.md#connect-stack-components-to-resources) instead:\n\n    ```\n    $ zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubeflow\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully registered orchestrator `<ORCHESTRATOR_NAME>`.\n\n    $ zenml service-connector list-resources --resource-type kubernetes-cluster -e\n    The following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES      \u2503\n    \ufffd"}
{"input": "\ufffd\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 e33c9fac-5daa-48b2-87bb-0187d3782cde \u2502 aws-iam-multi-eu      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 kubeflowmultitenant \u2503\n    \u2503                                      \u2502                       \u2502                \u2502                       \u2502 zenbox              \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster    \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 1c54b32a-4889-4417-abbd-42d3ace3d03a \u2502 gcp-sa-multi          \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster  \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    $ zenml orchestrator connect <ORCHESTRATOR_NAME> --connector aws-iam-multi-us\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully connected orchestrator `<ORCHESTRATOR_NAME>` to the following resources:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME   \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES   \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us \u2502 \ud83d\udd36 aws         \u2502 \ufffd"}
{"input": "\ufffd\ufffd kubernetes-cluster \u2502 zenhacks-cluster \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    # Add the orchestrator to the active stack\n    $ zenml stack update -o <ORCHESTRATOR_NAME>\n    ```\n2.  if you don't have a Service Connector on hand and you don't want to [register one](../../how-to/auth-management/service-connectors-guide.md#register-service-connectors) , the local Kubernetes `kubectl` client needs to be configured with a configuration context pointing to the remote cluster. The `kubernetes_context` stack component must also be configured with the value of that context:\n\n    ```shell\n    zenml orchestrator register <ORCHESTRATOR_NAME> \\\n        --flavor=kubeflow \\\n        --kubernetes_context=<KUBERNETES_CONTEXT>\n\n    # Add the orchestrator to the active stack\n    zenml stack update -o <ORCHESTRATOR_NAME>\n    ```\n{% endtab %}\n{% endtabs %}\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your pipeline steps in Kubeflow. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\nYou can now run any ZenML pipeline using the Kubeflow orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\n#### Kubeflow UI\n\nKubeflow comes with its own UI that you can use to find further details about your pipeline runs, such as the logs of your steps. For any runs executed on Kubeflow, you can get the URL to the Kubeflow UI in Python using the following code snippet:\n\n```python\nfrom zenml.client import Client\n\npipeline_run = Client().get_pipeline_run(\"<PIPELINE_RUN_NAME>\")\norchestrator_url = pipeline_run.run_metadata[\"orchestrator_url\"].value\n```\n\n#### Additional configuration\n\nFor additional configuration of the Kubeflow orchestrator, you can pass `KubeflowOrchestratorSettings` which allows you to configure (among others) the following attributes:\n\n* `client_args`: Arguments to pass when initializing the KFP client.\n* `user_namespace`: The user namespace to use when creating experiments and runs.\n* `pod_settings`: Node selectors, affinity, and tolerations to apply to the Kubernetes Pods running your pipeline. These can be either specified using the Kubernetes model"}
{"input": " objects or as dictionaries.\n\n```python\nfrom zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor import KubeflowOrchestratorSettings\nfrom kubernetes.client.models import V1Toleration\n\nkubeflow_settings = KubeflowOrchestratorSettings(\n    client_args={},\n    user_namespace=\"my_namespace\",\n    pod_settings={\n        \"affinity\": {\n            \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                    \"nodeSelectorTerms\": [\n                        {\n                            \"matchExpressions\": [\n                                {\n                                    \"key\": \"node.kubernetes.io/name\",\n                                    \"operator\": \"In\",\n                                    \"values\": [\"my_powerful_node_group\"],\n                                }\n                            ]\n                        }\n                    ]\n                }\n            }\n        },\n        \"tolerations\": [\n            V1Toleration(\n                key=\"node.kubernetes.io/name\",\n                operator=\"Equal\",\n                value=\"\",\n                effect=\"NoSchedule\"\n            )\n        ]\n    }\n)\n\n\n@pipeline(\n    settings={\n        \"orchestrator.kubeflow\": kubeflow_settings\n    }\n)\n\n\n...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kubeflow/#zenml.integrations.kubeflow.flavors.kubeflow\\_orchestrator\\_flavor.KubeflowOrchestratorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n### Important Note for Multi-Tenancy Deployments\n\nKubeflow has a notion of [multi-tenancy](https://www.kubeflow.org/docs/components/multi-tenancy/overview/) built into its deployment. Kubeflow's multi-user isolation simplifies user operations because each user only views and edited the Kubeflow components and model artifacts defined in their configuration.\n\nUsing the ZenML Kubeflow orchestrator on a multi-tenant deployment without any settings will result in the following error:\n\n```shell\nHTTP response body: {\"error\":\"Invalid input error: Invalid resource references for experiment. ListExperiment requires filtering by namespace.\",\"code\":3,\"message\":\"Invalid input error: Invalid resource references for experiment. ListExperiment requires filtering by \nnamespace.\",\"details\":[{\"@type\":\"type.googleapis.com/api.Error\",\"error_message\":\"Invalid resource references for experiment. ListExperiment requires filtering by namespace.\",\""}
{"input": "error_details\":\"Invalid input error: Invalid resource references for experiment. ListExperiment requires filtering by namespace.\"}]}\n```\n\nIn order to get it to work, we need to leverage the `KubeflowOrchestratorSettings` referenced above. By setting the namespace option, and by passing in the right authentication credentials to the Kubeflow Pipelines Client, we can make it work.\n\nFirst, when registering your Kubeflow orchestrator, please make sure to include the `kubeflow_hostname` parameter. The `kubeflow_hostname` **must end with the `/pipeline` post-fix**.\n\n```shell\nzenml orchestrator register <NAME> \\\n    --flavor=kubeflow \\\n    --kubeflow_hostname=<KUBEFLOW_HOSTNAME> # e.g. https://mykubeflow.example.com/pipeline\n```\n\nThen, ensure that you use the pass the right settings before triggering a pipeline run. The following snippet will prove useful:\n\n```python\nimport requests\n\nfrom zenml.client import Client\nfrom zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor import (\n    KubeflowOrchestratorSettings,\n)\n\nNAMESPACE = \"namespace_name\"  # This is the user namespace for the profile you want to use\nUSERNAME = \"admin\"  # This is the username for the profile you want to use\nPASSWORD = \"abc123\"  # This is the password for the profile you want to use\n\n# Use client_username and client_password and ZenML will automatically fetch a session cookie\nkubeflow_settings = KubeflowOrchestratorSettings(\n    client_username=USERNAME,\n    client_password=PASSWORD,\n    user_namespace=NAMESPACE\n)\n\n\n# You can also pass the cookie in `client_args` directly\n# kubeflow_settings = KubeflowOrchestratorSettings(\n#     client_args={\"cookies\": session_cookie}, user_namespace=NAMESPACE\n# )\n\n@pipeline(\n    settings={\n        \"orchestrator.kubeflow\": kubeflow_settings\n    }\n)\n\n:\n...\n\nif \"__name__\" == \"__main__\":\n# Run the pipeline\n```\n\nNote that the above is also currently not tested on all Kubeflow versions, so there might be further bugs with older Kubeflow versions. In this case, please reach out to us on [Slack](https://zenml.io/slack).\n\n#### Using secrets in settings\n\nThe above example encoded the username and password in plain text as settings. You can also set them as secrets.\n\n```shell\nzenml secret create kubeflow_secret \\\n    --username=admin \\\n    --password=abc123\n```\n\nAnd then you can use them in code:\n\n```python\n# Use client_username and client_password and ZenML will automatically fetch a session cookie\nkubeflow_settings = KubeflowOrchestr"}
{"input": "atorSettings(\n    client_username=\"{{kubeflow_secret.username}}\",  # secret reference\n    client_password=\"{{kubeflow_secret.password}}\",  # secret reference\n    user_namespace=\"namespace_name\"\n)\n```\n\nSee full documentation of using ZenML secrets [here](../../how-to/interact-with-secrets.md).\n\nFor more information and a full list of configurable attributes of the Kubeflow orchestrator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kubeflow/#zenml.integrations.kubeflow.orchestrators.kubeflow\\_orchestrator.KubeflowOrchestrator) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Orchestrating your pipelines to run on HyperAI.ai instances.\n---\n\n# HyperAI Orchestrator\n\n[HyperAI](https://www.hyperai.ai) is a cutting-edge cloud compute platform designed to make AI accessible for everyone. The HyperAI orchestrator is an [orchestrator](./orchestrators.md) flavor that allows you to easily deploy your pipelines on HyperAI instances.\n\n{% hint style=\"warning\" %}\nThis component is only meant to be used within the context of a [remote ZenML deployment scenario](../../getting-started/deploying-zenml/README.md). Usage with a local ZenML deployment may lead to unexpected behavior!\n{% endhint %}\n\n### When to use it\n\nYou should use the HyperAI orchestrator if:\n\n* you're looking for a managed solution for running your pipelines.\n* you're a HyperAI customer.\n\n### Prerequisites\n\nYou will need to do the following to start using the HyperAI orchestrator:\n\n* Have a running HyperAI instance. It must be accessible from the internet (or at least from the IP addresses of your ZenML users) and allow SSH key based access (passwords are not supported).\n* Ensure that a recent version of Docker is installed. This version must include Docker Compose, meaning that the command `docker compose` works.\n* Ensure that the appropriate [NVIDIA Driver](https://www.nvidia.com/en-us/drivers/unix/) is installed on the HyperAI instance (if not already installed by the HyperAI team).\n* Ensure that the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) is installed and configured on the HyperAI instance.\n\nNote that it is possible to omit installing the NVIDIA Driver and NVIDIA Container Toolkit. However, you will then be unable to use the GPU from within your ZenML pipeline. Additionally, you will then need to disable GPU access within the container when configuring the Orchestrator component, or the pipeline will not start correctly.\n\n## How it works\n\nThe HyperAI orchestrator works with Docker Compose, which can be used to construct machine learning pipelines. Under the hood, it creates a Docker Compose file which it then deploys and executes on the configured HyperAI instance. For each ZenML pipeline step, it creates a service in this file. It uses the `service_completed_successfully` condition to ensure that pipeline steps will only run if their connected upstream steps have successfully finished.\n\nIf configured for it, the HyperAI orchestrator will connect the HyperAI instance to the stack's container registry to ensure a smooth transfer of Docker images.\n\n### Scheduled pipelines\n\n[Scheduled pipelines](../../how-to/build-pipelines/schedule-a-pipeline.md) are supported by the HyperAI orchestrator. Currently, the HyperAI orchestrator supports the following inputs to `Schedule`:\n\n* Cron expressions via `cron_expression`. When pipeline runs are scheduled, they are added as a"}
{"input": " crontab entry on the HyperAI instance. Use this when you want pipelines to run in intervals. Using cron expressions assumes that `crontab` is available on your instance and that its daemon is running.\n* Scheduled runs via `run_once_start_time`. When pipeline runs are scheduled this way, they are added as an `at` entry on the HyperAI instance. Use this when you want pipelines to run just once and at a specified time. This assumes that `at` is available on your instance.\n\n### How to deploy it\n\nTo use the HyperAI orchestrator, you must configure a HyperAI Service Connector in ZenML and link it to the HyperAI orchestrator component. The service connector contains credentials with which ZenML connects to the HyperAI instance.\n\nAdditionally, the HyperAI orchestrator must be used in a stack that contains a container registry and an image builder.\n\n### How to use it\n\nTo use the HyperAI orchestrator, we must configure a HyperAI Service Connector first using one of its supported authentication methods. For example, for authentication with an RSA-based key, create the service connector as follows:\n\n```shell\nzenml service-connector register <SERVICE_CONNECTOR_NAME> --type=hyperai --auth-method=rsa-key --base64_ssh_key=<BASE64_SSH_KEY> --hostnames=<INSTANCE_1>,<INSTANCE_2>,..,<INSTANCE_N> --username=<INSTANCE_USERNAME>\n```\n\nHostnames are either DNS resolvable names or IP addresses.\n\nFor example, if you have two servers - one at `1.2.3.4` and another at `4.3.2.1`, you could provide them as `--hostnames=1.2.3.4,4.3.2.1`.\n\nOptionally, it is possible to provide a passphrase for the key (`--ssh_passphrase`).\n\nFollowing registering the service connector, we can register the orchestrator and use it in our active stack:\n\n```shell\nzenml orchestrator register <ORCHESTRATOR_NAME> --flavor=hyperai\n\n# Register and activate a stack with the new orchestrator\nzenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set\n```\n\nYou can now run any ZenML pipeline using the HyperAI orchestrator:\n\n```shell\npython file_that_runs_a_zenml_pipeline.py\n```\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a"}
{"input": "54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Tracking and managing ML models.\n---\n\n# Model Registries\n\nModel registries are centralized storage solutions for managing and tracking machine learning models across various stages of development and deployment. They help track the different versions and configurations of each model and enable reproducibility. By storing metadata such as version, configuration, and metrics, model registries help streamline the management of trained models. In ZenML, model registries are Stack Components that allow for the easy retrieval, loading, and deployment of trained models. They also provide information on the pipeline in which the model was trained and how to reproduce it.\n\n### Model Registry Concepts and Terminology\n\nZenML provides a unified abstraction for model registries through which it is possible to handle and manage the concepts of model groups, versions, and stages in a consistent manner regardless of the underlying registry tool or platform being used. The following concepts are useful to be aware of for this abstraction:\n\n* **RegisteredModel**: A logical grouping of models that can be used to track different versions of a model. It holds information about the model, such as its name, description, and tags, and can be created by the user or automatically created by the model registry when a new model is logged.\n* **RegistryModelVersion**: A specific version of a model identified by a unique version number or string. It holds information about the model, such as its name, description, tags, and metrics, and a reference to the model artifact logged to the model registry. In ZenML, it also holds a reference to the pipeline name, pipeline run ID, and step name. Each model version is associated with a model registration.\n* **ModelVersionStage**: A model version stage is a state in that a model version can be. It can be one of the following: `None`, `Staging`, `Production`, `Archived`. The model version stage is used to track the lifecycle of a model version. For example, a model version can be in the `Staging` stage while it is being tested and then moved to the `Production` stage once it is ready for deployment.\n\n### When to use it\n\nZenML provides a built-in mechanism for storing and versioning pipeline artifacts through its mandatory Artifact Store. While this is a powerful way to manage artifacts programmatically, it can be challenging to use without a visual interface.\n\nModel registries, on the other hand, offer a visual way to manage and track model metadata, particularly when using a remote orchestrator. They make it easy to retrieve and load models from storage, thanks to built-in integrations. A model registry is an excellent choice for interacting with all the models in your pipeline and managing their state in a centralized way.\n\nUsing a model registry in your stack is particularly useful if you want to interact with all the logged models in your pipeline, or if you need to manage the state of your models in a centralized way and make it easy to retrieve, load, and deploy these models.\n\n"}
{"input": "### How model registries fit into the ZenML stack\n\nHere is an architecture diagram that shows how a model registry fits into the overall story of a remote stack.\n\n![Model Registries](../../.gitbook/assets/Remote-with-model-registry.png)\n\n#### Model Registry Flavors\n\nModel Registries are optional stack components provided by integrations:\n\n| Model Registry                     | Flavor   | Integration | Notes                                      |\n| ---------------------------------- | -------- | ----------- | ------------------------------------------ |\n| [MLflow](mlflow.md)                | `mlflow` | `mlflow`    | Add MLflow as Model Registry to your stack |\n| [Custom Implementation](custom.md) | _custom_ |             | _custom_                                   |\n\nIf you would like to see the available flavors of Model Registry, you can use the command:\n\n```shell\nzenml model-registry flavor list\n```\n\n### How to use it\n\nModel registries are an optional component in the ZenML stack that is tied to the experiment tracker. This means that a model registry can only be used if you are also using an experiment tracker. If you're not using an experiment tracker, you can still store your models in ZenML, but you will need to manually retrieve model artifacts from the artifact store. More information on this can be found in the [documentation on the fetching runs](../../how-to/build-pipelines/fetching-pipelines.md).\n\nTo use model registries, you first need to register a model registry in your stack with the same flavor as your experiment tracker. Then, you can register your trained model in the model registry using one of three methods:\n\n* (1) using the built-in step in the pipeline.\n* (2) using the ZenML CLI to register the model from the command line.\n* (3) registering the model from the model registry UI. Finally, you can use the model registry to retrieve and load your models for deployment or further experimentation.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Managing MLFlow logged models and artifacts\n---\n\n# MLflow Model Registry\n\n[MLflow](https://www.mlflow.org/docs/latest/tracking.html) is a popular tool that helps you track experiments, manage models and even deploy them to different environments. ZenML already provides a [MLflow Experiment Tracker](../experiment-trackers/mlflow.md) that you can use to track your experiments, and an [MLflow Model Deployer](../model-deployers/mlflow.md) that you can use to deploy your models locally.\n\nThe MLflow model registry uses [the MLflow model registry service](https://mlflow.org/docs/latest/model-registry.html) to manage and track ML models and their artifacts and provides a user interface to browse them:\n\n![MLflow Model Registry UI](../../.gitbook/assets/mlflow-ui-models.png)\n\n## When would you want to use it?\n\nYou can use the MLflow model registry throughout your experimentation, QA, and production phases to manage and track machine learning model versions. It is designed to help teams collaborate on model development and deployment, and keep track of which models are being used in which environments. With the MLflow model registry, you can store and manage models, deploy them to different environments, and track their performance over time.\n\nThis is particularly useful in the following scenarios:\n\n* If you are working on a machine learning project and want to keep track of different model versions as they are developed and deployed.\n* If you need to deploy machine learning models to different environments and want to keep track of which version is being used in each environment.\n* If you want to monitor and compare the performance of different model versions over time and make data-driven decisions about which models to use in production.\n* If you want to simplify the process of deploying models either to a production environment or to a staging environment for testing.\n\n## How do you deploy it?\n\nThe MLflow Experiment Tracker flavor is provided by the MLflow ZenML integration, so you need to install it on your local machine to be able to register an MLflow model registry component. Note that the MLFlow model registry requires [MLFlow Experiment Tracker](../experiment-trackers/mlflow.md) to be present in the stack.\n\n```shell\nzenml integration install mlflow -y\n```\n\nOnce the MLflow integration is installed, you can register an MLflow model registry component in your stack:\n\n```shell\nzenml model-registry register mlflow_model_registry --flavor=mlflow\n\n# Register and set a stack with the new model registry as the active stack\nzenml stack register custom_stack -r mlflow_model_registry ... --set\n```\n\n{% hint style=\"info\" %}\nThe MLFlow model registry will automatically use the same configuration as the MLFlow Experiment Tracker. So if you have a remote MLFlow tracking server configured in your stack, the MLFlow model registry will also use the same configuration.\n{% endhint %}\n\n{% hint style=\""}
{"input": "warning\" %}\nDue to a [critical severity vulnerability](https://github.com/advisories/GHSA-xg73-94fp-g449) found in older versions of MLflow, we recommend using MLflow version 2.2.1 or higher.\n{% endhint %}\n\n## How do you use it?\n\nThere are different ways to use the MLflow model registry. You can use it in your ZenML pipelines with the built-in step, or you can use the ZenML CLI to register your model manually or call the model registry API within a custom step in your pipeline. The following sections show you how to use the MLflow model registry in your ZenML pipelines and with the ZenML CLI:\n\n### Register models inside a pipeline\n\nZenML provides a predefined `mlflow_model_deployer_step` that you can use to register a model in the MLflow model registry which you have previously logged to MLflow:\n\n```python\nfrom zenml import pipeline\nfrom zenml.integrations.mlflow.steps.mlflow_registry import (\n    mlflow_register_model_step,\n)\n\n@pipeline\ndef mlflow_registry_training_pipeline():\n    model = ...\n    mlflow_register_model_step(\n        model=model,\n        name=\"tensorflow-mnist-model\",\n    )\n```\n\n{% hint style=\"warning\" %}\nThe `mlflow_register_model_step` expects that the `model` it receives has already been logged to MLflow in a previous step. E.g., for a scikit-learn model, you would need to have used `mlflow.sklearn.autolog()` or `mlflow.sklearn.log_model(model)` in a previous step. See the [MLflow experiment tracker documentation](../experiment-trackers/mlflow.md) for more information on how to log models to MLflow from your ZenML steps.\n{% endhint %}\n\n#### List of available parameters\n\nWhen using the `mlflow_register_model_step`, you can set a variety of parameters for fine-grained control over which information is logged with your model:\n\n* `name`: The name of the model. This is a required parameter.\n* `version`: version: The version of the model.\n* `trained_model_name`: Name of the model artifact in MLflow.\n* `model_source_uri`: The path to the model. If not provided, the model will be fetched from the MLflow tracking server via the `trained_model_name`.\n* `description`: A description of the model version.\n* `metadata`: A list of metadata to associate with the model version.\n\n{% hint style=\"info\" %}\nThe `model_source_uri` parameter is the path to the model within the MLflow tracking server.\n\nIf you are using a local MLflow tracking server, the path will be something like `file:///.../mlruns/667102566783201219/3973eabc151c41e6ab98baeb20c5323b/artifacts/model`.\n\nIf you are using a remote MLflow tracking server"}
{"input": ", the path will be something like `s3://.../mlruns/667102566783201219/3973eabc151c41e6ab98baeb20c5323b/artifacts/model`.\n\nYou can find the path of the model in the MLflow UI. Go to the `Artifacts` tab of the run that produced the model and click on the model. The path will be displayed in the URL:\n\n<img src=\"../../../.gitbook/assets/mlflow-ui-model-uri.png\" alt=\"Model URI in MLflow UI\" data-size=\"original\">\n{% endhint %}\n\n### Register models via the CLI\n\nSometimes adding a `mlflow_registry_training_pipeline` step to your pipeline might not be the best option for you, as it will register a model in the MLflow model registry every time you run the pipeline.\n\nIf you want to register your models manually, you can use the `zenml model-registry models register-version` CLI command instead:\n\n```shell\nzenml model-registry models register-version Tensorflow-model \\\n    --description=\"A new version of the tensorflow model with accuracy 98.88%\" \\\n    -v 1 \\\n    --model-uri=\"file:///.../mlruns/667102566783201219/3973eabc151c41e6ab98baeb20c5323b/artifacts/model\" \\\n    -m key1 value1 -m key2 value2 \\\n    --zenml-pipeline-name=\"mlflow_training_pipeline\" \\\n    --zenml-step-name=\"trainer\"\n```\n\n### Deploy a registered model\n\nAfter you have registered a model in the MLflow model registry, you can also easily deploy it as a prediction service. Checkout the [MLflow model deployer documentation](../model-deployers/mlflow.md#deploy-from-model-registry) for more information on how to do that.\n\n### Interact with registered models\n\nYou can also use the ZenML CLI to interact with registered models and their versions.\n\nThe `zenml model-registry models list` command will list all registered models in the model registry:\n\n```shell\n$ zenml model-registry models list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503          NAME          \u2502 DESCRIPTION \u2502 METADATA \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 tensorflow-mnist-model \u2502             \u2502          \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nTo list all versions of a specific model, you can use the `zenml model-registry models list-versions REGISTERED_MODEL_NAME` command:\n\n```shell\n$ zenml model-registry models list-versions tensorflow-mnist-model\n\ufffd"}
{"input": "\ufffd\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503          NAME          \u2502 MODEL_VERSION \u2502 VERSION_DESCRIPTION                     \u2502 METADATA                                                                                                                                                                             \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 tensorflow-mnist-model \u2502 3             \u2502 Run #3 of the mlflow_training_pipeline. \u2502 {'zenml_version': '0.34.0', 'zenml_run_name': 'mlflow_training_pipeline-2023_03_01-08_09_23_672599', 'zenml_pipeline_name': 'mlflow_training_pipeline',                       \u2503\n\u2503                        \u2502               \u2502                                         \u2502 'zenml_pipeline_run_uuid': 'a5d4faae-ef70-48f2-9893-6e65d5e51e98', 'zenml_workspace': '10e060b3-2f7e-463d-9ec8-3a211ef4e1f6', 'epochs': '5', 'optimizer': 'Adam', 'lr': '0.005'}     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 tensorflow-mnist-model \u2502 2             \u2502 Run #2 of the mlflow_training_pipeline. \u2502 {'zenml_version': '0.34.0', 'zenml_run_name': 'mlflow_training_pipeline-2023_03_01-08_09_08_467212', 'zenml_pipeline_name': 'mlflow_training_pipeline',                       \u2503\n\u2503                        \u2502               \u2502                                         \u2502 'zenml_pipeline_run_uuid': '11858dcf-3e47-4b1a-82c5-6fa25ba4e037', 'zenml_workspace': '10e060b3-2f7e-463d-9ec8-3a211ef4e1f6', 'epochs': '5', 'optimizer':"}
{"input": " 'Adam', 'lr': '0.003'}     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 tensorflow-mnist-model \u2502 1             \u2502 Run #1 of the mlflow_training_pipeline. \u2502 {'zenml_version': '0.34.0', 'zenml_run_name': 'mlflow_training_pipeline-2023_03_01-08_08_52_398499', 'zenml_pipeline_name': 'mlflow_training_pipeline',                       \u2503\n\u2503                        \u2502               \u2502                                         \u2502 'zenml_pipeline_run_uuid': '29fb22c1-6e0b-4431-9e04-226226506d16', 'zenml_workspace': '10e060b3-2f7e-463d-9ec8-3a211ef4e1f6', 'epochs': '5', 'optimizer': 'Adam', 'lr': '0.001'}     \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nFor more details on a specific model version, you can use the `zenml model-registry models get-version REGISTERED_MODEL_NAME -v VERSION` command:\n\n```shell\n$ zenml model-registry models get-version tensorflow-mnist-model -v 1\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 MODEL VERSION PROPERTY \u2502 VALUE                                                                                                                                                                                                                                          \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"input": "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 REGISTERED_MODEL_NAME  \u2502 tensorflow-mnist-model                                                                                                                                                                                                                         \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 VERSION                \u2502 1                                                                                                                                                                                                                                              \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 VERSION_DESCRIPTION    \u2502 Run #1 of the mlflow_training_pipeline.                                                                                                                                                                                                        \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 CREATED_AT             \u2502 2023-03-01 09:09:06.899000                                                                                                                                                                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 UPDATED_AT             \u2502 2023-03-01 09:09:06.899000                                                                                                                                                                                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 METADATA               \u2502 {'zenml_version': '0.34.0', 'zenml_run_name': 'mlflow_training_pipeline-2023_03_01-08_08_52_398499', 'zenml_pipeline_name': 'mlflow_training_pipeline', 'zenml_pipeline_run_uuid': '29fb22c1-6e0b-4431-9e04-226226506d16',              \u2503\n\u2503                        \u2502 'zenml_workspace': '10e060b3-2f7e-463d-9ec8-3a211ef4e1f6', 'lr': '0.001', 'epochs': '5', 'optimizer': 'Adam'}                                                                                                                                  \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 MODEL_SOURCE_URI       \u2502 file:///Users/safoine-zenml/Library/Application Support/zenml/local_stores/0902a511-117d-4152-a098-b2f1124c4493/mlruns/489728212459131640/293a0d2e71e046"}
{"input": "999f77a79639f6eac2/artifacts/model                                                     \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503 STAGE                  \u2502 None                                                                                                                                                                                                                                           \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nFinally, to delete a registered model or a specific model version, you can use the `zenml model-registry models delete REGISTERED_MODEL_NAME` and `zenml model-registry models delete-version REGISTERED_MODEL_NAME -v VERSION` commands respectively.\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-mlflow/#zenml.integrations.mlflow.model\\_registry.MLFlowModelRegistry) to see more about the interface and implementation.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom model registry.\n---\n\n# Develop a Custom Model Registry\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n{% hint style=\"warning\" %}\n**Base abstraction in progress!**\n\nThe Model registry stack component is relatively new in ZenML. While it is fully functional, it can be challenging to cover all the ways ML systems deal with model versioning. This means that the API might change in the future. We will keep this page up-to-date with the latest changes.\n\nIf you are writing a custom model registry flavor, and you found that the base abstraction is lacking or not flexible enough, please let us know by messaging us on [Slack](https://zenml.io/slack), or by opening an issue on [GitHub](https://github.com/zenml-io/zenml/issues/new/choose)\n{% endhint %}\n\n### Base Abstraction\n\nThe `BaseModelRegistry` is the abstract base class that needs to be subclassed in order to create a custom component that can be used to register and retrieve models. As model registries can come in many shapes and forms, the base class exposes a deliberately basic and generic interface:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Type, cast\n\nfrom pydantic import BaseModel, Field, root_validator\n\nfrom zenml.enums import StackComponentType\nfrom zenml.stack import Flavor, StackComponent\nfrom zenml.stack.stack_component import StackComponentConfig\n\n\nclass BaseModelRegistryConfig(StackComponentConfig):\n    \"\"\"Base config for model registries.\"\"\"\n\n\nclass BaseModelRegistry(StackComponent, ABC):\n    \"\"\"Base class for all ZenML model registries.\"\"\"\n\n    @property\n    def config(self) -> BaseModelRegistryConfig:\n        \"\"\"Returns the config of the model registry.\"\"\"\n        return cast(BaseModelRegistryConfig, self._config)\n\n    # ---------\n    # Model Registration Methods\n    # ---------\n\n    @abstractmethod\n    def register_model(\n            self,\n            name: str,\n            description: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n    ) -> RegisteredModel:\n        \"\"\"Registers a model in the model registry.\"\"\"\n\n    @abstractmethod\n    def delete_model(\n            self,\n            name: str,\n    ) -> None:\n        \"\"\"Deletes a registered model from the model registry.\"\"\"\n\n    @abstractmethod\n    def update_model(\n            self,\n            name: str,\n            description: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n    ) -> RegisteredModel:\n        \"\"\"Updates a registered"}
{"input": " model in the model registry.\"\"\"\n\n    @abstractmethod\n    def get_model(self, name: str) -> RegisteredModel:\n        \"\"\"Gets a registered model from the model registry.\"\"\"\n\n    @abstractmethod\n    def list_models(\n            self,\n            name: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n    ) -> List[RegisteredModel]:\n        \"\"\"Lists all registered models in the model registry.\"\"\"\n\n    # ---------\n    # Model Version Methods\n    # ---------\n\n    @abstractmethod\n    def register_model_version(\n            self,\n            name: str,\n            description: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n            model_source_uri: Optional[str] = None,\n            version: Optional[str] = None,\n            description: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n            metadata: Optional[Dict[str, str]] = None,\n            zenml_version: Optional[str] = None,\n            zenml_run_name: Optional[str] = None,\n            zenml_pipeline_name: Optional[str] = None,\n            zenml_step_name: Optional[str] = None,\n            **kwargs: Any,\n    ) -> RegistryModelVersion:\n        \"\"\"Registers a model version in the model registry.\"\"\"\n\n    @abstractmethod\n    def delete_model_version(\n            self,\n            name: str,\n            version: str,\n    ) -> None:\n        \"\"\"Deletes a model version from the model registry.\"\"\"\n\n    @abstractmethod\n    def update_model_version(\n            self,\n            name: str,\n            version: str,\n            description: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n            stage: Optional[ModelVersionStage] = None,\n    ) -> RegistryModelVersion:\n        \"\"\"Updates a model version in the model registry.\"\"\"\n\n    @abstractmethod\n    def list_model_versions(\n            self,\n            name: Optional[str] = None,\n            model_source_uri: Optional[str] = None,\n            tags: Optional[Dict[str, str]] = None,\n            **kwargs: Any,\n    ) -> List[RegistryModelVersion]:\n        \"\"\"Lists all model versions for a registered model.\"\"\"\n\n    @abstractmethod\n    def get_model_version(self, name: str, version: str) -> RegistryModelVersion:\n        \"\"\"Gets a model version for a registered model.\"\"\"\n\n    @abstractmethod\n    def load_model_version(\n            self,\n            name: str,\n            version: str,\n            **kwargs: Any,\n    ) -> Any:\n        \"\"\"Loads a model version from the model registry.\"\"\"\n\n    @abstractmethod\n    def get_model_uri_artifact_store(\n            self,\n            model_version: RegistryModelVersion,\n    ) -> str:\n        \"\"\"Gets the URI artifact store for a model version.\"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the"}
{"input": " abstraction layer. To see the full implementation and get the complete docstrings, please check [the source code on GitHub](https://github.com/zenml-io/zenml/blob/main/src/zenml/model\\_registries/base\\_model\\_registry.py) .\n{% endhint %}\n\n### Build your own custom model registry\n\nIf you want to create your own custom flavor for a model registry, you can follow the following steps:\n\n1. Learn more about the core concepts for the model registry [here](./model-registries.md#model-registry-concepts-and-terminology). Your custom model registry will be built on top of these concepts so it helps to be aware of them.\n2. Create a class that inherits from `BaseModelRegistry` and implements the abstract methods.\n3. Create a `ModelRegistryConfig` class that inherits from `BaseModelRegistryConfig` and adds any additional configuration parameters that you need.\n4. Bring the implementation and the configuration together by inheriting from the `BaseModelRegistryFlavor` class. Make sure that you give a `name` to the flavor through its abstract property.\n\nOnce you are done with the implementation, you can register it through the CLI with the following command:\n\n```shell\nzenml model-registry flavor register <IMAGE-BUILDER-FLAVOR-SOURCE-PATH>\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to how and when these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomModelRegistryFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomModelRegistryConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Most of all, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are `pydantic` objects under the hood, you can also add your own custom validators here.\n* The **CustomModelRegistry** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomModelRegistryFlavor` and the `CustomModelRegistryConfig` are implemented in a different module/path than the actual `CustomModelRegistry`).\n{% endhint %}\n\nFor a full implementation example, please check out the [MLFlowModelRegistry](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-mlflow/#zenml.integrations.mlflow.model\\_registry.MLFlowModelRegistry)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815"}
{"input": "bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Sending automated alerts to chat services.\n---\n\n# Alerters\n\n**Alerters** allow you to send messages to chat services (like Slack, Discord, Mattermost, etc.) from within your\npipelines. This is useful to immediately get notified when failures happen, for general monitoring/reporting, and also\nfor building human-in-the-loop ML.\n\n## Alerter Flavors\n\nCurrently, the [SlackAlerter](slack.md) and [DiscordAlerter](discord.md) are the available alerter integrations. However, it is straightforward to\nextend ZenML and [build an alerter for other chat services](custom.md).\n\n| Alerter                            | Flavor    | Integration | Notes                                                              |\n|------------------------------------|-----------|-------------|--------------------------------------------------------------------|\n| [Slack](slack.md)                  | `slack`   | `slack`     | Interacts with a Slack channel                                     |\n| [Discord](discord.md)              | `discord` | `discord`   | Interacts with a Discord channel                                   |\n| [Custom Implementation](custom.md) | _custom_  |             | Extend the alerter abstraction and provide your own implementation |\n\n{% hint style=\"info\" %}\nIf you would like to see the available flavors of alerters in your terminal, you can use the following command:\n\n```shell\nzenml alerter flavor list\n```\n\n{% endhint %}\n\n## How to use Alerters with ZenML\n\nEach alerter integration comes with specific standard steps that you can use out of the box.\n\nHowever, you first need to register an alerter component in your terminal:\n\n```shell\nzenml alerter register <ALERTER_NAME> ...\n```\n\nThen you can add it to your stack using\n\n```shell\nzenml stack register ... -al <ALERTER_NAME>\n```\n\nAfterward, you can import the alerter standard steps provided by the respective integration and directly use them in\nyour pipelines.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Sending automated alerts to a Slack channel.\n---\n\n# Slack Alerter\n\nThe `SlackAlerter` enables you to send messages to a dedicated Slack channel directly from within your ZenML pipelines.\n\nThe `slack` integration contains the following two standard steps:\n\n* [slack\\_alerter\\_post\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-slack/#zenml.integrations.slack.steps.slack\\_alerter\\_post\\_step.slack\\_alerter\\_post\\_step) takes a string message or a custom [Slack block](https://api.slack.com/block-kit/building), posts it to a Slack channel, and returns whether the operation was successful.\n* [slack\\_alerter\\_ask\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-slack/#zenml.integrations.slack.steps.slack\\_alerter\\_ask\\_step.slack\\_alerter\\_ask\\_step) also posts a message or a custom [Slack block](https://api.slack.com/block-kit/building) to a Slack channel, but waits for user feedback, and only returns `True` if a user explicitly approved the operation from within Slack (e.g., by sending \"approve\" / \"reject\" to the bot in response).\n\nInteracting with Slack from within your pipelines can be very useful in practice:\n\n* The `slack_alerter_post_step` allows you to get notified immediately when failures happen (e.g., model performance degradation, data drift, ...),\n* The `slack_alerter_ask_step` allows you to integrate a human-in-the-loop into your pipelines before executing critical steps, such as deploying new models.\n\n## How to use it\n\n### Requirements\n\nBefore you can use the `SlackAlerter`, you first need to install ZenML's `slack` integration:\n\n```shell\nzenml integration install slack -y\n```\n\n{% hint style=\"info\" %}\nSee the [Integrations](../README.md) page for more details on ZenML integrations and how to install and use them.\n{% endhint %}\n\n### Setting Up a Slack Bot\n\nIn order to use the `SlackAlerter`, you first need to have a Slack workspace set up with a channel that you want your pipelines to post to.\n\nThen, you need to [create a Slack App](https://api.slack.com/apps?new\\_app=1) with a bot in your workspace.\n\n{% hint style=\"info\" %}\nMake sure to give your Slack bot `chat:write` and `chat:write.public` permissions in the `OAuth & Permissions` tab under `Scopes`.\n{% endhint %}\n\n### Registering a Slack Alerter in ZenML\n\nNext, you need to register a `slack` alerter in ZenML and link it to the bot"}
{"input": " you just created. You can do this with the following command:\n\n```shell\nzenml alerter register slack_alerter \\\n    --flavor=slack \\\n    --slack_token=<SLACK_TOKEN> \\\n    --default_slack_channel_id=<SLACK_CHANNEL_ID>\n```\n\nHere is where you can find the required parameters:\n\n* `<SLACK_CHANNEL_ID>`: Open your desired Slack channel in a browser, and copy out the last part of the URL starting with `C....`.\n* `<SLACK_TOKEN>`: This is the Slack token of your bot. You can find it in the Slack app settings under `OAuth & Permissions`. **IMPORTANT**: Please make sure that the token is the `Bot User OAuth Token` not the `User OAuth Token`.\n\n![Slack Token Image](../../.gitbook/assets/slack-alerter-token.jpg)\n\nAfter you have registered the `slack_alerter`, you can add it to your stack like this:\n\n```shell\nzenml stack register ... -al slack_alerter\n```\n\n### How to Use the Slack Alerter\n\nAfter you have a `SlackAlerter` configured in your stack, you can directly import the [slack\\_alerter\\_post\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-slack/#zenml.integrations.slack.steps.slack\\_alerter\\_post\\_step.slack\\_alerter\\_post\\_step) and [slack\\_alerter\\_ask\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-slack/#zenml.integrations.slack.steps.slack\\_alerter\\_ask\\_step.slack\\_alerter\\_ask\\_step) steps and use them in your pipelines.\n\nSince these steps expect a string message as input (which needs to be the output of another step), you typically also need to define a dedicated formatter step that takes whatever data you want to communicate and generates the string message that the alerter should post.\n\nAs an example, adding `slack_alerter_ask_step()` to your pipeline could look like this:\n\n```python\nfrom zenml.integrations.slack.steps.slack_alerter_ask_step import slack_alerter_ask_step\nfrom zenml import step, pipeline\n\n\n@step\ndef my_formatter_step(artifact_to_be_communicated) -> str:\n    return f\"Here is my artifact {artifact_to_be_communicated}!\"\n\n\n@pipeline\ndef my_pipeline(...):\n    ...\n    artifact_to_be_communicated = ...\n    message = my_formatter_step(artifact_to_be_communicated)\n    approved = slack_alerter_ask_step(message)\n    ... # Potentially have different behavior in subsequent steps if `approved`\n\nif __name__ == \"__main__\":\n    my_pipeline()\n```\n\nAn example of adding a custom Slack block as part of any alerter"}
{"input": " logic for your pipeline could look like this:\n\n```python\nfrom typing import List, Dict\nfrom zenml.integrations.slack.steps.slack_alerter_ask_step import slack_alerter_post_step\nfrom zenml.integrations.slack.alerters.slack_alerter import SlackAlerterParameters\nfrom zenml import step, pipeline\n\n\n@step\ndef my_custom_block_step(block_message) -> List[Dict]:\n    my_custom_block = [\n\t\t{\n\t\t\t\"type\": \"header\",\n\t\t\t\"text\": {\n\t\t\t\t\"type\": \"plain_text\",\n\t\t\t\t\"text\": f\":tada: {block_message}\",\n\t\t\t\t\"emoji\": true\n\t\t\t}\n\t\t}\n\t]\n    return SlackAlerterParameters(blocks = my_custom_block)\n\n\n@pipeline\ndef my_pipeline(...):\n    ...\n    message_blocks = my_custom_block_step(\"my custom block!\")\n    post_message = slack_alerter_post_step(params = message_blocks)\n    return post_message\n\nif __name__ == \"__main__\":\n    my_pipeline()\n\n```\n\nFor more information and a full list of configurable attributes of the Slack alerter, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-slack/#zenml.integrations.slack.alerters.slack\\_alerter.SlackAlerter) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom alerter.\n---\n\n# Develop a Custom Alerter\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n### Base Abstraction\n\nThe base abstraction for alerters is very basic, as it only defines two abstract methods that subclasses should implement:\n\n* `post()` takes a string, posts it to the desired chat service, and returns `True` if the operation succeeded, else `False`.\n* `ask()` does the same as `post()`, but after sending the message, it waits until someone approves or rejects the operation from within the chat service (e.g., by sending \"approve\" / \"reject\" to the bot as a response). `ask()` then only returns `True` if the operation succeeded and was approved, else `False`.\n\nThen base abstraction looks something like this:\n\n```python\nclass BaseAlerter(StackComponent, ABC):\n    \"\"\"Base class for all ZenML alerters.\"\"\"\n\n    def post(\n            self, message: str, params: Optional[BaseAlerterStepParameters]\n    ) -> bool:\n        \"\"\"Post a message to a chat service.\"\"\"\n        return True\n\n    def ask(\n            self, question: str, params: Optional[BaseAlerterStepParameters]\n    ) -> bool:\n        \"\"\"Post a message to a chat service and wait for approval.\"\"\"\n        return True\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation. To see the full docstrings and imports, please check [the source code on GitHub](https://github.com/zenml-io/zenml/blob/main/src/zenml/alerter/base\\_alerter.py).\n{% endhint %}\n\n### Building your own custom alerter\n\nCreating your own custom alerter can be done in three steps:\n\n1.  Create a class that inherits from the `BaseAlerter` and implement the `post()` and `ask()` methods.\n\n    ```python\n    from typing import Optional\n\n    from zenml.alerter import BaseAlerter, BaseAlerterStepParameters\n\n\n    class MyAlerter(BaseAlerter):\n        \"\"\"My alerter class.\"\"\"\n\n        def post(\n            self, message: str, config: Optional[BaseAlerterStepParameters]\n        ) -> bool:\n            \"\"\"Post a message to a chat service.\"\"\"\n            ...\n            return \"Hey, I implemented an alerter.\"\n\n        def ask(\n            self, question: str, config: Optional[BaseAlerterStepParameters]\n        ) -> bool:\n            \"\"\"Post a message to a chat service and"}
{"input": " wait for approval.\"\"\"\n            ...\n            return True\n    ```\n2.  If you need to configure your custom alerter, you can also implement a config object.\n\n    ```python\n    from zenml.alerter.base_alerter import BaseAlerterConfig\n\n\n    class MyAlerterConfig(BaseAlerterConfig):\n        my_param: str \n    ```\n3.  Finally, you can bring the implementation and the configuration together in a new flavor object.\n\n    ```python\n    from typing import Type, TYPE_CHECKING\n\n    from zenml.alerter import BaseAlerterFlavor\n\n    if TYPE_CHECKING:\n        from zenml.stack import StackComponent, StackComponentConfig\n\n\n    class MyAlerterFlavor(BaseAlerterFlavor):\n        @property\n        def name(self) -> str:\n            return \"my_alerter\"\n\n        @property\n        def config_class(self) -> Type[StackComponentConfig]:\n            from my_alerter_config import MyAlerterConfig\n\n            return MyAlerterConfig\n\n        @property\n        def implementation_class(self) -> Type[StackComponent]:\n            from my_alerter import MyAlerter\n\n            return MyAlerter\n\n    ```\n\nOnce you are done with the implementation, you can register your new flavor through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml alerter flavor register <path.to.MyAlerterFlavor>\n```\n\nFor example, if your flavor class `MyAlerterFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml alerter flavor register flavors.my_flavor.MyAlerterFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually, it's better to not have to rely on this mechanism and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new custom alerter flavor in the list of available alerter flavors:\n\n```shell\nzenml alerter flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these abstractions are coming into play in a ZenML workflow.\n\n* The **MyAlerterFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **MyAlerterConfig** class is imported when someone tries"}
{"input": " to register/update a stack component with the `my_alerter` flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **MyAlerter** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `MyAlerterFlavor` and the `MyAlerterConfig` are implemented in a different module/path than the actual `MyAlerter`).\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Sending automated alerts to a Discord channel.\n---\n\n# Discord Alerter\n\nThe `DiscordAlerter` enables you to send messages to a dedicated Discord channel \ndirectly from within your ZenML pipelines.\n\nThe `discord` integration contains the following two standard steps:\n\n* [discord\\_alerter\\_post\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-discord/#zenml.integrations.discord.steps.discord\\_alerter\\_post\\_step.discord\\_alerter\\_post\\_step)\n  takes a string message, posts it to a Discord channel, and returns whether the \n  operation was successful.\n* [discord\\_alerter\\_ask\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-discord/#zenml.integrations.discord.steps.discord\\_alerter\\_ask\\_step.discord\\_alerter\\_ask\\_step)\n  also posts a message to a Discord channel, but waits for user feedback, and \n  only returns `True` if a user explicitly approved the operation from within \n  Discord (e.g., by sending \"approve\" / \"reject\" to the bot in response).\n\nInteracting with Discord from within your pipelines can be very useful in practice:\n\n* The `discord_alerter_post_step` allows you to get notified immediately when failures happen (e.g., model performance\n  degradation, data drift, ...),\n* The `discord_alerter_ask_step` allows you to integrate a human-in-the-loop into your pipelines before executing critical\n  steps, such as deploying new models.\n\n## How to use it\n\n### Requirements\n\nBefore you can use the `DiscordAlerter`, you first need to install ZenML's `discord` integration:\n\n```shell\nzenml integration install discord -y\n```\n\n{% hint style=\"info\" %}\nSee the [Integrations](../README.md) page for more details on ZenML integrations and how to install and\nuse them.\n{% endhint %}\n\n### Setting Up a Discord Bot\n\nIn order to use the `DiscordAlerter`, you first need to have a Discord workspace set up with a channel that you want your\npipelines to post to. This is the `<DISCORD_CHANNEL_ID>` you will need when registering the discord alerter component.\n\nThen, you need to [create a Discord App with a bot in your server](https://discordpy.readthedocs.io/en/latest/discord.html)\n.\n\n{% hint style=\"info\" %}\nNote in the bot token copy step, if you don't find the copy button then click on reset token to reset the bot \nand you will get a new token which you can use. Also, make sure you give necessary permissions to the bot \nrequired for sending and receiving messages.\n{% endhint %}\n\n### Registering a Discord Alerter in"}
{"input": " ZenML\n\nNext, you need to register a `discord` alerter in ZenML and link it to the bot you just created. You can do this with the\nfollowing command:\n\n```shell\nzenml alerter register discord_alerter \\\n    --flavor=discord \\\n    --discord_token=<DISCORD_TOKEN> \\\n    --default_discord_channel_id=<DISCORD_CHANNEL_ID>\n```\n\nAfter you have registered the `discord_alerter`, you can add it to your stack like this:\n\n```shell\nzenml stack register ... -al discord_alerter\n```\n\nHere is where you can find the required parameters:\n\n#### DISCORD_CHANNEL_ID\n\nOpen the discord server, then right-click on the text channel and click on the \n'Copy Channel ID' option.\n\n{% hint style=\"info\" %}\nIf you don't see any 'Copy Channel ID' option for your channel, go to \"User Settings\" > \"Advanced\" and make sure \"Developer Mode\" is active.\n{% endhint %}\n\n#### DISCORD_TOKEN\n\nThis is the Discord token of your bot. You can find the instructions on how to set up a bot, invite it to your channel, and find its token\n[here](https://discordpy.readthedocs.io/en/latest/discord.html).\n\n{% hint style=\"warning\" %}\nWhen inviting the bot to your channel, make sure it has at least the following\npermissions: \n* Read Messages/View Channels\n* Send Messages\n* Send Messages in Threads\n{% endhint %}\n\n### How to Use the Discord Alerter\n\nAfter you have a `DiscordAlerter` configured in your stack, you can directly import\nthe [discord\\_alerter\\_post\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-discord/#zenml.integrations.discord.steps.discord\\_alerter\\_post\\_step.discord\\_alerter\\_post\\_step)\nand [discord\\_alerter\\_ask\\_step](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-discord/#zenml.integrations.discord.steps.discord\\_alerter\\_ask\\_step.discord\\_alerter\\_ask\\_step)\nsteps and use them in your pipelines.\n\nSince these steps expect a string message as input (which needs to be the output of another step), you typically also\nneed to define a dedicated formatter step that takes whatever data you want to communicate and generates the string\nmessage that the alerter should post.\n\nAs an example, adding `discord_alerter_ask_step()` to your pipeline could look like this:\n\n```python\nfrom zenml.integrations.discord.steps.discord_alerter_ask_step import discord_alerter_ask_step\nfrom zenml import step, pipeline\n\n\n@step\ndef my_formatter_step(artifact_to_be_communicated) -> str:\n   "}
{"input": " return f\"Here is my artifact {artifact_to_be_communicated}!\"\n\n\n@pipeline\ndef my_pipeline(...):\n    ...\n    artifact_to_be_communicated = ...\n    message = my_formatter_step(artifact_to_be_communicated)\n    approved = discord_alerter_ask_step(message)\n    ... # Potentially have different behavior in subsequent steps if `approved`\n\nif __name__ == \"__main__\":\n    my_pipeline()\n```\n\nFor more information and a full list of configurable attributes of the Discord alerter, check out\nthe [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-discord/#zenml.integrations.discord.alerters.discord\\_alerter.DiscordAlerter)\n.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: >-\n  How to collect and visualize statistics to track changes in your pipelines'\n  data with whylogs/WhyLabs profiling.\n---\n\n# Whylogs\n\nThe whylogs/WhyLabs [Data Validator](./data-validators.md) flavor provided with the ZenML integration uses [whylogs](https://whylabs.ai/whylogs) and [WhyLabs](https://whylabs.ai) to generate and track data profiles, highly accurate descriptive representations of your data. The profiles can be used to implement automated corrective actions in your pipelines, or to render interactive representations for further visual interpretation, evaluation and documentation.\n\n### When would you want to use it?\n\n[Whylogs](https://whylabs.ai/whylogs) is an open-source library that analyzes your data and creates statistical summaries called whylogs profiles. Whylogs profiles can be processed in your pipelines and visualized locally or uploaded to the [WhyLabs platform](https://whylabs.ai/), where more in depth analysis can be carried out. Even though [whylogs also supports other data types](https://github.com/whylabs/whylogs#data-types), the ZenML whylogs integration currently only works with tabular data in `pandas.DataFrame` format.\n\nYou should use the whylogs/WhyLabs Data Validator when you need the following data validation features that are possible with whylogs and WhyLabs:\n\n* Data Quality: validate data quality in model inputs or in a data pipeline\n* Data Drift: detect data drift in model input features\n* Model Drift: Detect training-serving skew, concept drift, and model performance degradation\n\nYou should consider one of the other [Data Validator flavors](./data-validators.md#data-validator-flavors) if you need a different set of data validation features.\n\n### How do you deploy it?\n\nThe whylogs Data Validator flavor is included in the whylogs ZenML integration, you need to install it on your local machine to be able to register a whylogs Data Validator and add it to your stack:\n\n```shell\nzenml integration install whylogs -y\n```\n\nIf you don't need to connect to the WhyLabs platform to upload and store the generated whylogs data profiles, the Data Validator stack component does not require any configuration parameters. Adding it to a stack is as simple as running e.g.:\n\n```shell\n# Register the whylogs data validator\nzenml data-validator register whylogs_data_validator --flavor=whylogs\n\n# Register and set a stack with the new data validator\nzenml stack register custom_stack -dv whylogs_data_validator ... --set\n```\n\nAdding WhyLabs logging capabilities to your whylogs Data Validator is just slightly more complicated, as you also need to create a [ZenML Secret](../../getting-started/deploying-zenml/secret-management.md) to store the sensitive WhyLabs authentication information in a secure location and"}
{"input": " then reference the secret in the Data Validator configuration. To generate a WhyLabs access token, you can follow [the official WhyLabs instructions documented here](https://docs.whylabs.ai/docs/whylabs-api/#creating-an-api-token) .\n\nThen, you can register the whylogs Data Validator with WhyLabs logging capabilities as follows:\n\n```shell\n# Create the secret referenced in the data validator\nzenml secret create whylabs_secret \\\n    --whylabs_default_org_id=<YOUR-WHYLOGS-ORGANIZATION-ID> \\\n    --whylabs_api_key=<YOUR-WHYLOGS-API-KEY>\n\n# Register the whylogs data validator\nzenml data-validator register whylogs_data_validator --flavor=whylogs \\\n    --authentication_secret=whylabs_secret\n```\n\nYou'll also need to enable whylabs logging for your custom pipeline steps if you want to upload the whylogs data profiles that they return as artifacts to the WhyLabs platform. This is enabled by default for the standard whylogs step. For custom steps, you can enable WhyLabs logging by setting the `upload_to_whylabs` parameter to `True` in the step configuration, e.g.:\n\n```python\nfrom typing_extensions import Annotated  # or `from typing import Annotated on Python 3.9+\nfrom typing import Tuple\nimport pandas as pd\nimport whylogs as why\nfrom sklearn import datasets\nfrom whylogs.core import DatasetProfileView\n\nfrom zenml.integrations.whylogs.flavors.whylogs_data_validator_flavor import (\n    WhylogsDataValidatorSettings,\n)\nfrom zenml import step\n\n\n@step(\n    settings={\n        \"data_validator.whylogs\": WhylogsDataValidatorSettings(\n            enable_whylabs=True, dataset_id=\"model-1\"\n        )\n    }\n)\ndef data_loader() -> Tuple[\n    Annotated[pd.DataFrame, \"data\"],\n    Annotated[DatasetProfileView, \"profile\"]\n]:\n    \"\"\"Load the diabetes dataset.\"\"\"\n    X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n    # merge X and y together\n    df = pd.merge(X, y, left_index=True, right_index=True)\n\n    profile = why.log(pandas=df).profile().view()\n    return df, profile\n```\n\n### How do you use it?\n\nWhylogs's profiling functions take in a `pandas.DataFrame` dataset generate a `DatasetProfileView` object containing all the relevant information extracted from the dataset.\n\nThere are three ways you can use whylogs in your ZenML pipelines that allow different levels of flexibility:\n\n* instantiate, configure and insert [the standard `WhylogsProfilerStep`](whylogs.md#the-whylogs-standard-step) shipped with ZenML into your pipelines. This is the easiest way and the recommended approach, but can only be customized through the supported step configuration parameters.\n* call the data validation methods provided"}
{"input": " by [the whylogs Data Validator](whylogs.md#the-whylogs-data-validator) in your custom step implementation. This method allows for more flexibility concerning what can happen in the pipeline step, but you are still limited to the functionality implemented in the Data Validator.\n* [use the whylogs library directly](whylogs.md#call-whylogs-directly) in your custom step implementation. This gives you complete freedom in how you are using whylogs's features.\n\nYou can [visualize whylogs profiles](whylogs.md#visualizing-whylogs-profiles) in Jupyter notebooks or view them directly in the ZenML dashboard.\n\n#### The whylogs standard step\n\nZenML wraps the whylogs/WhyLabs functionality in the form of a standard `WhylogsProfilerStep` step. The only field in the step config is a `dataset_timestamp` attribute which is only relevant when you upload the profiles to WhyLabs that uses this field to group and merge together profiles belonging to the same dataset. The helper function `get_whylogs_profiler_step` used to create an instance of this standard step takes in an optional `dataset_id` parameter that is also used only in the context of WhyLabs upload to identify the model in the context of which the profile is uploaded, e.g.:\n\n```python\nfrom zenml.integrations.whylogs.steps import get_whylogs_profiler_step\n\n\ntrain_data_profiler = get_whylogs_profiler_step(dataset_id=\"model-2\")\ntest_data_profiler = get_whylogs_profiler_step(dataset_id=\"model-3\")\n```\n\nThe step can then be inserted into your pipeline where it can take in a `pandas.DataFrame` dataset, e.g.:\n\n```python\nfrom zenml import pipeline\n\n@pipeline\ndef data_profiling_pipeline():\n    data, _ = data_loader()\n    train, test = data_splitter(data)\n    train_data_profiler(train)\n    test_data_profiler(test)\n    \n\ndata_profiling_pipeline()\n```\n\nAs can be seen from the [step definition](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-whylogs/#zenml.integrations.whylogs.steps.whylogs\\_profiler.whylogs\\_profiler\\_step) , the step takes in a dataset and returns a whylogs `DatasetProfileView` object:\n\n```python\n@step\ndef whylogs_profiler_step(\n    dataset: pd.DataFrame,\n    dataset_timestamp: Optional[datetime.datetime] = None,\n) -> DatasetProfileView:\n    ...\n```\n\nYou should consult [the official whylogs documentation](https://whylogs.readthedocs.io/en/latest/index.html) for more information on what you can do with the collected profiles.\n\nYou can view [the complete list of configuration parameters](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-whylogs/#zenml.int"}
{"input": "egrations.whylogs.steps.whylogs\\_profiler.WhylogsProfilerConfig) in the SDK docs.\n\n#### The whylogs Data Validator\n\nThe whylogs Data Validator implements the same interface as do all Data Validators, so this method forces you to maintain some level of compatibility with the overall Data Validator abstraction, which guarantees an easier migration in case you decide to switch to another Data Validator.\n\nAll you have to do is call the whylogs Data Validator methods when you need to interact with whylogs to generate data profiles. You may optionally enable whylabs logging to automatically upload the returned whylogs profile to WhyLabs, e.g.:\n\n```python\n\nimport pandas as pd\nfrom whylogs.core import DatasetProfileView\nfrom zenml.integrations.whylogs.data_validators.whylogs_data_validator import (\n    WhylogsDataValidator,\n)\nfrom zenml.integrations.whylogs.flavors.whylogs_data_validator_flavor import (\n    WhylogsDataValidatorSettings,\n)\nfrom zenml import step\n\nwhylogs_settings = WhylogsDataValidatorSettings(\n    enable_whylabs=True, dataset_id=\"<WHYLABS_DATASET_ID>\"\n)\n\n\n@step(\n    settings={\n        \"data_validator.whylogs\": whylogs_settings\n    }\n)\ndef data_profiler(\n        dataset: pd.DataFrame,\n) -> DatasetProfileView:\n    \"\"\"Custom data profiler step with whylogs\n\n    Args:\n        dataset: a Pandas DataFrame\n\n    Returns:\n        Whylogs profile generated for the data\n    \"\"\"\n\n    # validation pre-processing (e.g. dataset preparation) can take place here\n\n    data_validator = WhylogsDataValidator.get_active_data_validator()\n    profile = data_validator.data_profiling(\n        dataset,\n    )\n    # optionally upload the profile to WhyLabs, if WhyLabs credentials are configured\n    data_validator.upload_profile_view(profile)\n\n    # validation post-processing (e.g. interpret results, take actions) can happen here\n\n    return profile\n```\n\nHave a look at [the complete list of methods and parameters available in the `WhylogsDataValidator` API](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-whylogs/#zenml.integrations.whylogs.data\\_validators.whylogs\\_data\\_validator.WhylogsDataValidator) in the SDK docs.\n\n#### Call whylogs directly\n\nYou can use the whylogs library directly in your custom pipeline steps, and only leverage ZenML's capability of serializing, versioning and storing the `DatasetProfileView` objects in its Artifact Store. You may optionally enable whylabs logging to automatically upload the returned whylogs profile to WhyLabs, e.g.:\n\n```python\n\nimport pandas as pd\nfrom whylogs.core import DatasetProfileView\nimport whylogs as why\nfrom zenml import step\nfrom zenml.integrations.whylogs.flavors.whylogs_data_validator_flavor import (\n    Why"}
{"input": "logsDataValidatorSettings,\n)\n\nwhylogs_settings = WhylogsDataValidatorSettings(\n    enable_whylabs=True, dataset_id=\"<WHYLABS_DATASET_ID>\"\n)\n\n\n@step(\n    settings={\n        \"data_validator.whylogs\": whylogs_settings\n    }\n)\ndef data_profiler(\n        dataset: pd.DataFrame,\n) -> DatasetProfileView:\n    \"\"\"Custom data profiler step with whylogs\n\n    Args:\n        dataset: a Pandas DataFrame\n\n    Returns:\n        Whylogs Profile generated for the dataset\n    \"\"\"\n\n    # validation pre-processing (e.g. dataset preparation) can take place here\n\n    results = why.log(dataset)\n    profile = results.profile()\n\n    # validation post-processing (e.g. interpret results, take actions) can happen here\n\n    return profile.view()\n```\n\n### Visualizing whylogs Profiles\n\nYou can view visualizations of the whylogs profiles generated by your pipeline steps directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\nAlternatively, if you are running inside a Jupyter notebook, you can load and render the whylogs profiles using the [artifact.visualize() method](../../how-to/visualize-artifacts/README.md), e.g.:\n\n```python\nfrom zenml.client import Client\n\n\ndef visualize_statistics(\n    step_name: str, reference_step_name: Optional[str] = None\n) -> None:\n    \"\"\"Helper function to visualize whylogs statistics from step artifacts.\n\n    Args:\n        step_name: step that generated and returned a whylogs profile\n        reference_step_name: an optional second step that generated a whylogs\n            profile to use for data drift visualization where two whylogs\n            profiles are required.\n    \"\"\"\n    pipe = Client().get_pipeline(pipeline=\"data_profiling_pipeline\")\n    whylogs_step = pipe.last_run.steps[step_name]\n    whylogs_step.visualize()\n\n\nif __name__ == \"__main__\":\n    visualize_statistics(\"data_loader\")\n    visualize_statistics(\"train_data_profiler\", \"test_data_profiler\")\n```\n\n![Whylogs Visualization Example 1](<../../.gitbook/assets/whylogs-visualizer-01.png>)\n\n![Whylogs Visualization Example 2](../../.gitbook/assets/whylogs-visualizer-02.png)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  How to use Great Expectations to run data quality checks in your pipelines and\n  document the results\n---\n\n# Great Expectations\n\nThe Great Expectations [Data Validator](./data-validators.md) flavor provided with the ZenML integration uses [Great Expectations](https://greatexpectations.io/) to run data profiling and data quality tests on the data circulated through your pipelines. The test results can be used to implement automated corrective actions in your pipelines. They are also automatically rendered into documentation for further visual interpretation and evaluation.\n\n### When would you want to use it?\n\n[Great Expectations](https://greatexpectations.io/) is an open-source library that helps keep the quality of your data in check through data testing, documentation, and profiling, and to improve communication and observability. Great Expectations works with tabular data in a variety of formats and data sources, of which ZenML currently supports only `pandas.DataFrame` as part of its pipelines.\n\nYou should use the Great Expectations Data Validator when you need the following data validation features that are possible with Great Expectations:\n\n* [Data Profiling](https://docs.greatexpectations.io/docs/oss/guides/expectations/creating_custom_expectations/how_to_add_support_for_the_auto_initializing_framework_to_a_custom_expectation/#build-a-custom-profiler-for-your-expectation): generates a set of validation rules (Expectations) automatically by inferring them from the properties of an input dataset.\n* [Data Quality](https://docs.greatexpectations.io/docs/oss/guides/validation/checkpoints/how_to_pass_an_in_memory_dataframe_to_a_checkpoint/): runs a set of predefined or inferred validation rules (Expectations) against an in-memory dataset.\n* [Data Docs](https://docs.greatexpectations.io/docs/reference/learn/terms/data_docs_store/): generate and maintain human-readable documentation of all your data validation rules, data quality checks and their results.\n\nYou should consider one of the other [Data Validator flavors](./data-validators.md#data-validator-flavors) if you need a different set of data validation features.\n\n### How do you deploy it?\n\nThe Great Expectations Data Validator flavor is included in the Great Expectations ZenML integration, you need to install it on your local machine to be able to register a Great Expectations Data Validator and add it to your stack:\n\n```shell\nzenml integration install great_expectations -y\n```\n\nDepending on how you configure the Great Expectations Data Validator, it can reduce or even completely eliminate the complexity associated with setting up the store backends for Great Expectations. If you're only looking for a quick and easy way of adding Great Expectations to your stack and are not concerned with the configuration details, you can simply run:\n\n```shell\n# Register the Great Expectations data validator\nzenml data-validator register ge_data_validator --flavor=great_expectations\n\n# Register and set a"}
{"input": " stack with the new data validator\nzenml stack register custom_stack -dv ge_data_validator ... --set\n```\n\nIf you already have a Great Expectations deployment, you can configure the Great Expectations Data Validator to reuse or even replace your current configuration. You should consider the pros and cons of every deployment use-case and choose the one that best fits your needs:\n\n1. let ZenML initialize and manage the Great Expectations configuration. The Artifact Store will serve as a storage backend for all the information that Great Expectations needs to persist (e.g. Expectation Suites, Validation Results). However, you will not be able to setup new Data Sources, Metadata Stores or Data Docs sites. Any changes you try and make to the configuration through code will not be persisted and will be lost when your pipeline completes or your local process exits.\n2. use ZenML with your existing Great Expectations configuration. You can tell ZenML to replace your existing Metadata Stores with the active ZenML Artifact Store by setting the `configure_zenml_stores` attribute in the Data Validator. The downside is that you will only be able to run pipelines locally with this setup, given that the Great Expectations configuration is a file on your local machine.\n3. migrate your existing Great Expectations configuration to ZenML. This is a compromise between 1. and 2. that allows you to continue to use your existing Data Sources, Metadata Stores and Data Docs sites even when running pipelines remotely.\n\n{% hint style=\"warning\" %}\nSome Great Expectations CLI commands will not work well with the deployment methods that puts ZenML in charge of your Great Expectations configuration (i.e. 1. and 3.). You will be required to use Python code to manage your Expectations and you will have to edit the Jupyter notebooks generated by the Great Expectations CLI to connect them to your ZenML managed configuration. .\n{% endhint %}\n\n{% tabs %}\n{% tab title=\"Let ZenML Manage The Configuration\" %}\nThe default Data Validator setup plugs Great Expectations directly into the [Artifact Store](../artifact-stores/artifact-stores.md) component that is part of the same stack. As a result, the Expectation Suites, Validation Results and Data Docs are stored in the ZenML Artifact Store and you don't have to configure Great Expectations at all, ZenML takes care of that for you:\n\n```shell\n# Register the Great Expectations data validator\nzenml data-validator register ge_data_validator --flavor=great_expectations\n\n# Register and set a stack with the new data validator\nzenml stack register custom_stack -dv ge_data_validator ... --set\n```\n{% endtab %}\n\n{% tab title=\"Use Your Own Configuration\" %}\nIf you have an existing Great Expectations configuration that you would like to reuse with your ZenML pipelines, the Data Validator allows you to do so. All you need is to point it to the folder where your local `great_expectations"}
{"input": ".yaml` configuration file is located:\n\n```shell\n# Register the Great Expectations data validator\nzenml data-validator register ge_data_validator --flavor=great_expectations \\\n    --context_root_dir=/path/to/my/great_expectations\n\n# Register and set a stack with the new data validator\nzenml stack register custom_stack -dv ge_data_validator ... --set\n```\n\nYou can continue to edit your local Great Expectations configuration (e.g. add new Data Sources, update the Metadata Stores etc.) and these changes will be visible in your ZenML pipelines. You can also use the Great Expectations CLI as usual to manage your configuration and your Expectations.\n{% endtab %}\n\n{% tab title=\"Migrate Your Configuration to ZenML\" %}\nThis deployment method migrates your existing Great Expectations configuration to ZenML and allows you to use it with local as well as remote orchestrators. You have to load the Great Expectations configuration contents in one of the Data Validator configuration parameters using the `@` operator, e.g.:\n\n```shell\n# Register the Great Expectations data validator\nzenml data-validator register ge_data_validator --flavor=great_expectations \\\n    --context_config=@/path/to/my/great_expectations/great_expectations.yaml\n\n# Register and set a stack with the new data validator\nzenml stack register custom_stack -dv ge_data_validator ... --set\n```\n\nWhen you are migrating your existing Great Expectations configuration to ZenML, keep in mind that the Metadata Stores that you configured there will also need to be accessible from the location where pipelines are running. For example, you cannot use a non-local orchestrator with a Great Expectations Metadata Store that is located on your filesystem.\n{% endtab %}\n{% endtabs %}\n\n#### Advanced Configuration\n\nThe Great Expectations Data Validator has a few advanced configuration attributes that might be useful for your particular use-case:\n\n* `configure_zenml_stores`: if set, ZenML will automatically update the Great Expectation configuration to include Metadata Stores that use the Artifact Store as a backend. If neither `context_root_dir` nor `context_config` are set, this is the default behavior. You can set this flag to use the ZenML Artifact Store as a backend for Great Expectations with any of the deployment methods described above. Note that ZenML will not copy the information in your existing Great Expectations stores (e.g. Expectation Suites, Validation Results) in the ZenML Artifact Store. This is something that you will have to do yourself.\n* `configure_local_docs`: set this flag to configure a local Data Docs site where Great Expectations docs are generated and can be visualized locally. Use this in case you don't already have a local Data Docs site in your existing Great Expectations configuration.\n\nFor more, up-to-date information on the Great Expectations Data Validator configuration, you can have a look at [the SDK docs](https://sdkdocs.z"}
{"input": "enml.io/latest/integration\\_code\\_docs/integrations-great\\_expectations/#zenml.integrations.great\\_expectations.data\\_validators.ge\\_data\\_validator.GreatExpectationsDataValidator) .\n\n### How do you use it?\n\nThe core Great Expectations concepts that you should be aware of when using it within ZenML pipelines are Expectations / Expectation Suites, Validations and Data Docs.\n\nZenML wraps the Great Expectations' functionality in the form of two standard steps:\n\n* a Great Expectations data profiler that can be used to automatically generate Expectation Suites from an input `pandas.DataFrame` dataset\n* a Great Expectations data validator that uses an existing Expectation Suite to validate an input `pandas.DataFrame` dataset\n\nYou can visualize Great Expectations Suites and Results in Jupyter notebooks or view them directly in the ZenML dashboard.\n\n#### The Great Expectation's data profiler step\n\nThe standard Great Expectation's data profiler step builds an Expectation Suite automatically by running a [`UserConfigurableProfiler`](https://docs.greatexpectations.io/docs/guides/expectations/how\\_to\\_create\\_and\\_edit\\_expectations\\_with\\_a\\_profiler) on an input `pandas.DataFrame` dataset. The generated Expectation Suite is saved in the Great Expectations Expectation Store, but also returned as an `ExpectationSuite` artifact that is versioned and saved in the ZenML Artifact Store. The step automatically rebuilds the Data Docs.\n\nAt a minimum, the step configuration expects a name to be used for the Expectation Suite:\n\n```python\nfrom zenml.integrations.great_expectations.steps import (\n    great_expectations_profiler_step,\n)\n\nge_profiler_step = great_expectations_profiler_step.with_options(\n    parameters={\n        \"expectation_suite_name\": \"steel_plates_suite\",\n        \"data_asset_name\": \"steel_plates_train_df\",\n    }\n)\n```\n\nThe step can then be inserted into your pipeline where it can take in a pandas dataframe, e.g.:\n\n```python\nfrom zenml import pipeline\n\ndocker_settings = DockerSettings(required_integrations=[SKLEARN, GREAT_EXPECTATIONS])\n\n@pipeline(settings={\"docker\": docker_settings})\ndef profiling_pipeline():\n    \"\"\"Data profiling pipeline for Great Expectations.\n\n    The pipeline imports a reference dataset from a source then uses the builtin\n    Great Expectations profiler step to generate an expectation suite (i.e.\n    validation rules) inferred from the schema and statistical properties of the\n    reference dataset.\n\n    Args:\n        importer: reference data importer step\n        profiler: data profiler step\n    \"\"\"\n    dataset, _ = importer()\n    ge_profiler_step(dataset)\n\n\nprofiling_pipeline()\n```\n\nAs can be seen from the [step definition](https://apidocs.zenml.io/latest/integration\\_code\\_docs/integrations-great\\_expectations/#zenml.integrations.great\\_expect"}
{"input": "ations.steps.ge\\_profiler.great\\_expectations\\_profiler\\_step) , the step takes in a `pandas.DataFrame` dataset, and it returns a Great Expectations `ExpectationSuite` object:\n\n```python\n@step\ndef great_expectations_profiler_step(\n    dataset: pd.DataFrame,\n    expectation_suite_name: str,\n    data_asset_name: Optional[str] = None,\n    profiler_kwargs: Optional[Dict[str, Any]] = None,\n    overwrite_existing_suite: bool = True,\n) -> ExpectationSuite:\n    ...\n```\n\nYou can view [the complete list of configuration parameters](https://apidocs.zenml.io/latest/integration\\_code\\_docs/integrations-great\\_expectations/#zenml.integrations.great\\_expectations.steps.ge\\_profiler.great\\_expectations\\_profiler\\_step) in the SDK docs.\n\n#### The Great Expectations data validator step\n\nThe standard Great Expectations data validator step validates an input `pandas.DataFrame` dataset by running an existing Expectation Suite on it. The validation results are saved in the Great Expectations Validation Store, but also returned as an `CheckpointResult` artifact that is versioned and saved in the ZenML Artifact Store. The step automatically rebuilds the Data Docs.\n\nAt a minimum, the step configuration expects the name of the Expectation Suite to be used for the validation:\n\n```python\nfrom zenml.integrations.great_expectations.steps import (\n    great_expectations_validator_step,\n)\n\nge_validator_step = great_expectations_validator_step.with_options(\n    parameters={\n        \"expectation_suite_name\": \"steel_plates_suite\",\n        \"data_asset_name\": \"steel_plates_train_df\",\n    }\n)\n```\n\nThe step can then be inserted into your pipeline where it can take in a pandas dataframe and a bool flag used solely for order reinforcement purposes, e.g.:\n\n```python\ndocker_settings = DockerSettings(required_integrations=[SKLEARN, GREAT_EXPECTATIONS])\n\n@pipeline(settings={\"docker\": docker_settings})\ndef validation_pipeline():\n    \"\"\"Data validation pipeline for Great Expectations.\n\n    The pipeline imports a test data from a source, then uses the builtin\n    Great Expectations data validation step to validate the dataset against\n    the expectation suite generated in the profiling pipeline.\n\n    Args:\n        importer: test data importer step\n        validator: dataset validation step\n        checker: checks the validation results\n    \"\"\"\n    dataset, condition = importer()\n    results = ge_validator_step(dataset, condition)\n    message = checker(results)\n\n\nvalidation_pipeline()\n```\n\nAs can be seen from the [step definition](https://apidocs.zenml.io/latest/integration\\_code\\_docs/integrations-great\\_expectations/#zenml.integrations.great\\_expectations.steps.ge\\_validator.great\\_expectations\\_validator\\_step) , the step takes in a `pandas.DataFrame` dataset and a boolean `condition"}
{"input": "` and it returns a Great Expectations `CheckpointResult` object. The boolean `condition` is only used as a means of ordering steps in a pipeline (e.g. if you must force it to run only after the data profiling step generates an Expectation Suite):\n\n```python\n@step\ndef great_expectations_validator_step(\n    dataset: pd.DataFrame,\n    expectation_suite_name: str,\n    data_asset_name: Optional[str] = None,\n    action_list: Optional[List[Dict[str, Any]]] = None,\n    exit_on_error: bool = False,\n) -> CheckpointResult:\n```\n\nYou can view [the complete list of configuration parameters](https://apidocs.zenml.io/latest/integration\\_code\\_docs/integrations-great\\_expectations/#zenml.integrations.great\\_expectations.steps.ge\\_validator.great\\_expectations\\_validator\\_step) in the SDK docs.\n\n#### Call Great Expectations directly\n\nYou can use the Great Expectations library directly in your custom pipeline steps, while leveraging ZenML's capability of serializing, versioning and storing the `ExpectationSuite` and `CheckpointResult` objects in its Artifact Store. To use the Great Expectations configuration managed by ZenML while interacting with the Great Expectations library directly, you need to use the Data Context managed by ZenML instead of the default one provided by Great Expectations, e.g.:\n\n```python\nimport great_expectations as ge\nfrom zenml.integrations.great_expectations.data_validators import (\n    GreatExpectationsDataValidator\n)\n\nimport pandas as pd\nfrom great_expectations.core import ExpectationSuite\nfrom zenml import step\n\n\n@step\ndef create_custom_expectation_suite(\n) -> ExpectationSuite:\n    \"\"\"Custom step that creates an Expectation Suite\n\n    Returns:\n        An Expectation Suite\n    \"\"\"\n    context = GreatExpectationsDataValidator.get_data_context()\n    # instead of:\n    # context = ge.get_context()\n\n    expectation_suite_name = \"custom_suite\"\n    suite = context.create_expectation_suite(\n        expectation_suite_name=expectation_suite_name\n    )\n    expectation_configuration = ExpectationConfiguration(...)\n    suite.add_expectation(expectation_configuration=expectation_configuration)\n    ...\n    context.save_expectation_suite(\n        expectation_suite=suite,\n        expectation_suite_name=expectation_suite_name,\n    )\n    context.build_data_docs()\n    return suite\n```\n\nThe same approach must be used if you are using a Great Expectations configuration managed by ZenML and are using the Jupyter notebooks generated by the Great Expectations CLI.\n\n#### Visualizing Great Expectations Suites and Results\n\nYou can view visualizations of the suites and results generated by your pipeline steps directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\nAlternatively, if you are running inside a Jupyter notebook, you can load and render the suites and results using the [`artifact.visualize()`"}
{"input": " method](../../how-to/visualize-artifacts/README.md), e.g.:\n\n```python\nfrom zenml.client import Client\n\n\ndef visualize_results(pipeline_name: str, step_name: str) -> None:\n    pipeline = Client().get_pipeline(pipeline_name)\n    last_run = pipeline.last_run\n    validation_step = last_run.steps[step_name]\n    validation_step.visualize()\n\n\nif __name__ == \"__main__\":\n    visualize_results(\"validation_pipeline\", \"profiler\")\n    visualize_results(\"validation_pipeline\", \"train_validator\")\n    visualize_results(\"validation_pipeline\", \"test_validator\")\n```\n\n![Expectations Suite Visualization](<../../.gitbook/assets/expectation-suite.png>)\n\n![Validation Results Visualization](<../../.gitbook/assets/validation-result.png>)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  How to test the data and models used in your pipelines with Deepchecks test\n  suites\n---\n\n# Deepchecks\n\nThe Deepchecks [Data Validator](./data-validators.md) flavor provided with the ZenML integration uses [Deepchecks](https://deepchecks.com/) to run data integrity, data drift, model drift and model performance tests on the datasets and models circulated in your ZenML pipelines. The test results can be used to implement automated corrective actions in your pipelines or to render interactive representations for further visual interpretation, evaluation and documentation.\n\n### When would you want to use it?\n\n[Deepchecks](https://deepchecks.com/) is an open-source library that you can use to run a variety of data and model validation tests, from data integrity tests that work with a single dataset to model evaluation tests to data drift analyses and model performance comparison tests. All this can be done with minimal configuration input from the user, or customized with specialized conditions that the validation tests should perform.\n\nDeepchecks works with both tabular data and computer vision data (currently in beta). For tabular, the supported dataset format is `pandas.DataFrame` and the supported model format is `sklearn.base.ClassifierMixin`. For computer vision, the supported dataset format is `torch.utils.data.dataloader.DataLoader` and supported model format is `torch.nn.Module`.\n\nYou should use the Deepchecks Data Validator when you need the following data and/or model validation features that are possible with Deepchecks:\n\n* Data Integrity Checks [for tabular](https://docs.deepchecks.com/en/stable/checks\\_gallery/tabular.html#data-integrity) or [computer vision](https://docs.deepchecks.com/en/stable/checks\\_gallery/vision.html#data-integrity) data: detect data integrity problems within a single dataset (e.g. missing values, conflicting labels, mixed data types etc.).\n* Data Drift Checks [for tabular](https://docs.deepchecks.com/en/stable/checks\\_gallery/tabular.html#train-test-validation) or [computer vision](https://docs.deepchecks.com/en/stable/checks\\_gallery/vision.html#train-test-validation) data: detect data skew and data drift problems by comparing a target dataset against a reference dataset (e.g. feature drift, label drift, new labels etc.).\n* Model Performance Checks [for tabular](https://docs.deepchecks.com/en/stable/checks\\_gallery/tabular.html#model-evaluation) or [computer vision](https://docs.deepchecks.com/en/stable/checks\\_gallery/vision.html#model-evaluation) data: evaluate a model and detect problems with its performance (e.g. confusion matrix, boosting overfit, model error analysis)\n\nYou should consider one of the other [Data Validator flavors](./data-validators.md#data-validator-flavors) if you need a different set of data validation features.\n\n### How do you deploy it?\n\nThe Deepchecks"}
{"input": " Data Validator flavor is included in the Deepchecks ZenML integration, you need to install it on your local machine to be able to register a Deepchecks Data Validator and add it to your stack:\n\n```shell\nzenml integration install deepchecks -y\n```\n\nThe Data Validator stack component does not have any configuration parameters. Adding it to a stack is as simple as running e.g.:\n\n```shell\n# Register the Deepchecks data validator\nzenml data-validator register deepchecks_data_validator --flavor=deepchecks\n\n# Register and set a stack with the new data validator\nzenml stack register custom_stack -dv deepchecks_data_validator ... --set\n```\n\n### How do you use it?\n\nThe ZenML integration restructures the way Deepchecks validation checks are organized in four categories, based on the type and number of input parameters that they expect as input. This makes it easier to reason about them when you decide which tests to use in your pipeline steps:\n\n* **data integrity checks** expect a single dataset as input. These correspond one-to-one to the set of Deepchecks data integrity checks [for tabular](https://docs.deepchecks.com/en/stable/checks\\_gallery/tabular.html#data-integrity) and [computer vision](https://docs.deepchecks.com/en/stable/checks\\_gallery/vision.html#data-integrity) data\n* **data drift checks** require two datasets as input: target and reference. These correspond one-to-one to the set of Deepchecks train-test checks [for tabular data](https://docs.deepchecks.com/stable/checks\\_gallery/tabular.html#train-test-validation) and [for computer vision](https://docs.deepchecks.com/stable/checks\\_gallery/vision.html#train-test-validation).\n* **model validation checks** require a single dataset and a mandatory model as input. This list includes a subset of the model evaluation checks provided by Deepchecks [for tabular data](https://docs.deepchecks.com/en/stable/checks\\_gallery/tabular.html#model-evaluation) and [for computer vision](https://docs.deepchecks.com/stable/checks\\_gallery/vision.html#model-evaluation) that expect a single dataset as input.\n* **model drift checks** require two datasets and a mandatory model as input. This list includes a subset of the model evaluation checks provided by Deepchecks [for tabular data](https://docs.deepchecks.com/en/stable/checks\\_gallery/tabular.html#model-evaluation) and [for computer vision](https://docs.deepchecks.com/stable/checks\\_gallery/vision.html#model-evaluation) that expect two datasets as input: target and reference.\n\nThis structure is directly reflected in how Deepchecks can be used with ZenML: there are four different Deepchecks standard steps and four different [ZenML enums for Deepchecks checks](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-de"}
{"input": "epchecks/#zenml.integrations.deepchecks.validation\\_checks) . [The Deepchecks Data Validator API](deepchecks.md#the-deepchecks-data-validator) is also modeled to reflect this same structure.\n\nA notable characteristic of Deepchecks is that you don't need to customize the set of Deepchecks tests that are part of a test suite. Both ZenML and Deepchecks provide sane defaults that will run all available Deepchecks tests in a given category with their default conditions if a custom list of tests and conditions are not provided.\n\nThere are three ways you can use Deepchecks in your ZenML pipelines that allow different levels of flexibility:\n\n* instantiate, configure and insert one or more of [the standard Deepchecks steps](deepchecks.md#the-deepchecks-standard-steps) shipped with ZenML into your pipelines. This is the easiest way and the recommended approach, but can only be customized through the supported step configuration parameters.\n* call the data validation methods provided by [the Deepchecks Data Validator](deepchecks.md#the-deepchecks-data-validator) in your custom step implementation. This method allows for more flexibility concerning what can happen in the pipeline step, but you are still limited to the functionality implemented in the Data Validator.\n* [use the Deepchecks library directly](deepchecks.md#call-deepchecks-directly) in your custom step implementation. This gives you complete freedom in how you are using Deepchecks' features.\n\nYou can visualize Deepchecks results in Jupyter notebooks or view them directly in the ZenML dashboard.\n\n### Warning! Usage in remote orchestrators\n\nThe current ZenML version has a limitation in its base Docker image that requires a workaround for _all_ pipelines using Deepchecks with a remote orchestrator (e.g. [Kubeflow](../orchestrators/kubeflow.md) , [Vertex](../orchestrators/vertex.md)). The limitation being that the base Docker image needs to be extended to include binaries that are required by `opencv2`, which is a package that Deepchecks requires.\n\nWhile these binaries might be available on most operating systems out of the box (and therefore not a problem with the default local orchestrator), we need to tell ZenML to add them to the containerization step when running in remote settings. Here is how:\n\nFirst, create a file called `deepchecks-zenml.Dockerfile` and place it on the same level as your runner script (commonly called `run.py`). The contents of the Dockerfile are as follows:\n\n```shell\nARG ZENML_VERSION=0.20.0\nFROM zenmldocker/zenml:${ZENML_VERSION} AS base\n\nRUN apt-get update\nRUN apt-get install ffmpeg libsm6 libxext6  -y\n```\n\nThen, place the following snippet above your pipeline definition. Note that the path of the `dockerfile` are relative to where the pipeline definition file is. Read [the containerization"}
{"input": " guide](../../how-to/customize-docker-builds/README.md) for more details:\n\n```python\nimport zenml\nfrom zenml import pipeline\nfrom zenml.config import DockerSettings\nfrom pathlib import Path\nimport sys\n\ndocker_settings = DockerSettings(\n    dockerfile=\"deepchecks-zenml.Dockerfile\",\n    build_options={\n        \"buildargs\": {\n            \"ZENML_VERSION\": f\"{zenml.__version__}\"\n        },\n    },\n)\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline(...):\n    # same code as always\n    ...\n```\n\nFrom here on, you can continue to use the deepchecks integration as is explained below.\n\n#### The Deepchecks standard steps\n\nZenML wraps the Deepchecks functionality for tabular data in the form of four standard steps:\n\n* [`DeepchecksDataIntegrityCheckStep`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.steps.deepchecks\\_data\\_drift.DeepchecksDataDriftCheckStep): use it in your pipelines to run data integrity tests on a single dataset\n* [`DeepchecksDataDriftCheckStep`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.steps.deepchecks\\_data\\_integrity.DeepchecksDataIntegrityCheckStep): use it in your pipelines to run data drift tests on two datasets as input: target and reference.\n* [`DeepchecksModelValidationCheckStep`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.steps.deepchecks\\_model\\_validation.DeepchecksModelValidationCheckStep): class DeepchecksModelDriftCheckStep(BaseStep): use it in your pipelines to run model performance tests using a single dataset and a mandatory model artifact as input\n* [`DeepchecksModelDriftCheckStep`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.steps.deepchecks\\_model\\_drift.DeepchecksModelDriftCheckStep): use it in your pipelines to run model comparison/drift tests using a mandatory model artifact and two datasets as input: target and reference.\n\nThe integration doesn't yet include standard steps for computer vision, but you can still write your own custom steps that call [the Deepchecks Data Validator API](deepchecks.md#the-deepchecks-data-validator) or even [call the Deepchecks library directly](deepchecks.md#call-deepchecks-directly).\n\nAll four standard steps behave similarly regarding the configuration parameters and returned artifacts, with the following differences:\n\n* the type and number of input artifacts are different, as mentioned above\n* each step expects a different enum data type to be used when explicitly listing the checks to be performed via the `check_list"}
{"input": "` configuration attribute. See the [`zenml.integrations.deepchecks.validation_checks`](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.validation\\_checks) module for more details about these enums (e.g. the data integrity step expects a list of `DeepchecksDataIntegrityCheck` values).\n\nThis section will only cover how you can use the data integrity step, with a similar usage to be easily inferred for the other three steps.\n\nTo instantiate a data integrity step that will run all available Deepchecks data integrity tests with their default configuration, e.g.:\n\n```python\nfrom zenml.integrations.deepchecks.steps import (\n    deepchecks_data_integrity_check_step,\n)\n\ndata_validator = deepchecks_data_integrity_check_step.with_options(\n    parameters=dict(\n        dataset_kwargs=dict(label=\"target\", cat_features=[]),\n    ),\n)\n```\n\nThe step can then be inserted into your pipeline where it can take in a dataset, e.g.:\n\n```python\ndocker_settings = DockerSettings(required_integrations=[DEEPCHECKS, SKLEARN])\n\n@pipeline(settings={\"docker\": docker_settings})\ndef data_validation_pipeline():\n    df_train, df_test = data_loader()\n    data_validator(dataset=df_train)\n\n\ndata_validation_pipeline()\n```\n\nAs can be seen from the [step definition](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.steps.deepchecks\\_data\\_integrity.deepchecks\\_data\\_integrity\\_check\\_step) , the step takes in a dataset and it returns a Deepchecks `SuiteResult` object that contains the test results:\n\n```python\n@step\ndef deepchecks_data_integrity_check_step(\n    dataset: pd.DataFrame,\n    check_list: Optional[Sequence[DeepchecksDataIntegrityCheck]] = None,\n    dataset_kwargs: Optional[Dict[str, Any]] = None,\n    check_kwargs: Optional[Dict[str, Any]] = None,\n    run_kwargs: Optional[Dict[str, Any]] = None,\n) -> SuiteResult:\n    ...\n```\n\nIf needed, you can specify a custom list of data integrity Deepchecks tests to be executed by supplying a `check_list` argument:\n\n```python\nfrom zenml.integrations.deepchecks.validation_checks import DeepchecksDataIntegrityCheck\nfrom zenml.integrations.deepchecks.steps import deepchecks_data_integrity_check_step\n\n\n@pipeline\ndef validation_pipeline():\n    deepchecks_data_integrity_check_step(\n        check_list=[\n            DeepchecksDataIntegrityCheck.TABULAR_MIXED_DATA_TYPES,\n            DeepchecksDataIntegrityCheck.TABULAR_DATA_DUPLICATES,\n            DeepchecksDataIntegrityCheck.TABULAR_CONFLICTING_LABELS,\n        ],\n        dataset=...\n    )\n```\n\nYou should consult [the official Deepchecks documentation](https://docs.deep"}
{"input": "checks.com/en/stable/checks\\_gallery/tabular.html) for more information on what each test is useful for.\n\nFor more customization, the data integrity step also allows for additional keyword arguments to be supplied to be passed transparently to the Deepchecks library:\n\n*   `dataset_kwargs`: Additional keyword arguments to be passed to the Deepchecks `tabular.Dataset` or `vision.VisionData` constructor. This is used to pass additional information about how the data is structured, e.g.:\n\n    ```python\n    deepchecks_data_integrity_check_step(\n        dataset_kwargs=dict(label='class', cat_features=['country', 'state']),\n        ...\n    )\n    ```\n*   `check_kwargs`: Additional keyword arguments to be passed to the Deepchecks check object constructors. Arguments are grouped for each check and indexed using the full check class name or check enum value as dictionary keys, e.g.:\n\n    ```python\n    deepchecks_data_integrity_check_step(\n        check_list=[\n            DeepchecksDataIntegrityCheck.TABULAR_OUTLIER_SAMPLE_DETECTION,\n            DeepchecksDataIntegrityCheck.TABULAR_STRING_LENGTH_OUT_OF_BOUNDS,\n            DeepchecksDataIntegrityCheck.TABULAR_STRING_MISMATCH,\n        ],\n        check_kwargs={\n            DeepchecksDataIntegrityCheck.TABULAR_OUTLIER_SAMPLE_DETECTION: dict(\n                nearest_neighbors_percent=0.01,\n                extent_parameter=3,\n            ),\n            DeepchecksDataIntegrityCheck.TABULAR_STRING_LENGTH_OUT_OF_BOUNDS: dict(\n                num_percentiles=1000,\n                min_unique_values=3,\n            ),\n        },\n        ...\n    )\n    ```\n* `run_kwargs`: Additional keyword arguments to be passed to the Deepchecks Suite `run` method.\n\nThe `check_kwargs` attribute can also be used to customize [the conditions](https://docs.deepchecks.com/en/stable/user-guide/general/deepchecks\\_hierarchy.html#condition) configured for each Deepchecks test. ZenML attaches a special meaning to all check arguments that start with `condition_` and have a dictionary as value. This is required because there is no declarative way to specify conditions for Deepchecks checks. For example, the following step configuration:\n\n```python\ndeepchecks_data_integrity_check_step(\n    check_list=[\n        DeepchecksDataIntegrityCheck.TABULAR_OUTLIER_SAMPLE_DETECTION,\n        DeepchecksDataIntegrityCheck.TABULAR_STRING_LENGTH_OUT_OF_BOUNDS,\n    ],\n    dataset_kwargs=dict(label='class', cat_features=['country', 'state']),\n    check_kwargs={\n        DeepchecksDataIntegrityCheck.TABULAR_OUTLIER_SAMPLE_DETECTION: dict(\n            nearest_neighbors_percent=0.01,\n            extent_parameter=3,\n            condition_outlier_ratio_less_or_equal=dict(\n                max_outliers_ratio=0.007,\n                outlier_score_threshold=0.5,\n            ),\n            condition_no_outliers=dict(\n                outlier_score_threshold=0.6,\n            )\n       "}
{"input": " ),\n        DeepchecksDataIntegrityCheck.TABULAR_STRING_LENGTH_OUT_OF_BOUNDS: dict(\n            num_percentiles=1000,\n            min_unique_values=3,\n            condition_number_of_outliers_less_or_equal=dict(\n                max_outliers=3,\n            )\n        ),\n    },\n    ...\n)\n```\n\nis equivalent to running the following Deepchecks tests:\n\n```python\nimport deepchecks.tabular.checks as tabular_checks\nfrom deepchecks.tabular import Suite\nfrom deepchecks.tabular import Dataset\n\ntrain_dataset = Dataset(\n    reference_dataset,\n    label='class',\n    cat_features=['country', 'state']\n)\n\nsuite = Suite(name=\"custom\")\ncheck = tabular_checks.OutlierSampleDetection(\n    nearest_neighbors_percent=0.01,\n    extent_parameter=3,\n)\ncheck.add_condition_outlier_ratio_less_or_equal(\n    max_outliers_ratio=0.007,\n    outlier_score_threshold=0.5,\n)\ncheck.add_condition_no_outliers(\n    outlier_score_threshold=0.6,\n)\nsuite.add(check)\ncheck = tabular_checks.StringLengthOutOfBounds(\n    num_percentiles=1000,\n    min_unique_values=3,\n)\ncheck.add_condition_number_of_outliers_less_or_equal(\n    max_outliers=3,\n)\nsuite.run(train_dataset=train_dataset)\n```\n\nYou can view [the complete list of configuration parameters](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.steps.deepchecks\\_data\\_integrity.DeepchecksDataIntegrityCheckStepParameters) in the SDK docs.\n\n#### The Deepchecks Data Validator\n\nThe Deepchecks Data Validator implements the same interface as do all Data Validators, so this method forces you to maintain some level of compatibility with the overall Data Validator abstraction, which guarantees an easier migration in case you decide to switch to another Data Validator.\n\nAll you have to do is call the Deepchecks Data Validator methods when you need to interact with Deepchecks to run tests, e.g.:\n\n```python\n\nimport pandas as pd\nfrom deepchecks.core.suite import SuiteResult\nfrom zenml.integrations.deepchecks.data_validators import DeepchecksDataValidator\nfrom zenml.integrations.deepchecks.validation_checks import DeepchecksDataIntegrityCheck\nfrom zenml import step\n\n\n@step\ndef data_integrity_check(\n        dataset: pd.DataFrame,\n) -> SuiteResult:\n    \"\"\"Custom data integrity check step with Deepchecks\n\n    Args:\n        dataset: input Pandas DataFrame\n\n    Returns:\n        Deepchecks test suite execution result\n    \"\"\"\n\n    # validation pre-processing (e.g. dataset preparation) can take place here\n\n    data_validator = DeepchecksDataValidator.get_active_data_validator()\n    suite = data_validator.data_validation(\n        dataset=dataset,\n        check_list=[\n            DeepchecksDataIntegrityCheck.TABULAR_OUTLIER_SAMPLE_DETECTION,\n            DeepchecksDataIntegrityCheck.TABULAR"}
{"input": "_STRING_LENGTH_OUT_OF_BOUNDS,\n        ],\n    )\n\n    # validation post-processing (e.g. interpret results, take actions) can happen here\n\n    return suite\n```\n\nThe arguments that the Deepchecks Data Validator methods can take in are the same as those used for [the Deepchecks standard steps](deepchecks.md#the-deepchecks-standard-steps).\n\nHave a look at [the complete list of methods and parameters available in the `DeepchecksDataValidator` API](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-deepchecks/#zenml.integrations.deepchecks.data\\_validators.deepchecks\\_data\\_validator.DeepchecksDataValidator) in the SDK docs.\n\n#### Call Deepchecks directly\n\nYou can use the Deepchecks library directly in your custom pipeline steps, and only leverage ZenML's capability of serializing, versioning and storing the `SuiteResult` objects in its Artifact Store, e.g.:\n\n```python\nimport pandas as pd\nimport deepchecks.tabular.checks as tabular_checks\n\nfrom deepchecks.core.suite import SuiteResult\nfrom deepchecks.tabular import Suite\nfrom deepchecks.tabular import Dataset\nfrom zenml import step\n\n\n@step\ndef data_integrity_check(\n    dataset: pd.DataFrame,\n) -> SuiteResult:\n    \"\"\"Custom data integrity check step with Deepchecks\n\n    Args:\n        dataset: a Pandas DataFrame\n\n    Returns:\n        Deepchecks test suite execution result\n    \"\"\"\n\n    # validation pre-processing (e.g. dataset preparation) can take place here\n\n    train_dataset = Dataset(\n        dataset,\n        label='class',\n        cat_features=['country', 'state']\n    )\n\n    suite = Suite(name=\"custom\")\n    check = tabular_checks.OutlierSampleDetection(\n        nearest_neighbors_percent=0.01,\n        extent_parameter=3,\n    )\n    check.add_condition_outlier_ratio_less_or_equal(\n        max_outliers_ratio=0.007,\n        outlier_score_threshold=0.5,\n    )\n    suite.add(check)\n    check = tabular_checks.StringLengthOutOfBounds(\n        num_percentiles=1000,\n        min_unique_values=3,\n    )\n    check.add_condition_number_of_outliers_less_or_equal(\n        max_outliers=3,\n    )\n    results = suite.run(train_dataset=train_dataset)\n\n    # validation post-processing (e.g. interpret results, take actions) can happen here\n\n    return results\n```\n\n#### Visualizing Deepchecks Suite Results\n\nYou can view visualizations of the suites and results generated by your pipeline steps directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\nAlternatively, if you are running inside a Jupyter notebook, you can load and render the suites and results using the [artifact.visualize() method](../../how-to/visualize-artifacts/README.md), e.g.:\n\n```python\nfrom zenml.client import Client\n\n\ndef visualize_results(p"}
{"input": "ipeline_name: str, step_name: str) -> None:\n    pipeline = Client().get_pipeline(pipeline=pipeline_name)\n    last_run = pipeline.last_run\n    step = last_run.steps[step_name]\n    step.visualize()\n\n\nif __name__ == \"__main__\":\n    visualize_results(\"data_validation_pipeline\", \"data_integrity_check\")\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  How to keep your data quality in check and guard against data and model drift\n  with Evidently profiling\n---\n\n# Evidently\n\nThe Evidently [Data Validator](./data-validators.md) flavor provided with the ZenML integration uses [Evidently](https://evidentlyai.com/) to perform data quality, data drift, model drift and model performance analyses, to generate reports and run checks. The reports and check results can be used to implement automated corrective actions in your pipelines or to render interactive representations for further visual interpretation, evaluation and documentation.\n\n### When would you want to use it?\n\n[Evidently](https://evidentlyai.com/) is an open-source library that you can use to monitor and debug machine learning models by analyzing the data that they use through a powerful set of data profiling and visualization features, or to run a variety of data and model validation reports and tests, from data integrity tests that work with a single dataset to model evaluation tests to data drift analysis and model performance comparison tests. All this can be done with minimal configuration input from the user, or customized with specialized conditions that the validation tests should perform.\n\nEvidently currently works with tabular data in `pandas.DataFrame` or CSV file formats and can handle both regression and classification tasks.\n\nYou should use the Evidently Data Validator when you need the following data and/or model validation features that are possible with Evidently:\n\n* [Data Quality](https://docs.evidentlyai.com/presets/data-quality) reports and tests: provides detailed feature statistics and a feature behavior overview for a single dataset. It can also compare any two datasets. E.g. you can use it to compare train and test data, reference and current data, or two subgroups of one dataset.\n* [Data Drift](https://docs.evidentlyai.com/presets/data-drift) reports and tests: helps detects and explore feature distribution changes in the input data by comparing two datasets with identical schema.\n* [Target Drift](https://docs.evidentlyai.com/presets/target-drift) reports and tests: helps detect and explore changes in the target function and/or model predictions by comparing two datasets where the target and/or prediction columns are available.\n* [Regression Performance](https://docs.evidentlyai.com/presets/reg-performance) or [Classification Performance](https://docs.evidentlyai.com/presets/class-performance) reports and tests: evaluate the performance of a model by analyzing a single dataset where both the target and prediction columns are available. It can also compare it to the past performance of the same model, or the performance of an alternative model by providing a second dataset.\n\nYou should consider one of the other [Data Validator flavors](./data-validators.md#data-validator-flavors) if you need a different set of data validation features.\n\n### How do you deploy it?\n\nThe Evidently Data"}
{"input": " Validator flavor is included in the Evidently ZenML integration, you need to install it on your local machine to be able to register an Evidently Data Validator and add it to your stack:\n\n```shell\nzenml integration install evidently -y\n```\n\nThe Data Validator stack component does not have any configuration parameters. Adding it to a stack is as simple as running e.g.:\n\n```shell\n# Register the Evidently data validator\nzenml data-validator register evidently_data_validator --flavor=evidently\n\n# Register and set a stack with the new data validator\nzenml stack register custom_stack -dv evidently_data_validator ... --set\n```\n\n### How do you use it?\n\n#### Data Profiling\n\nEvidently's profiling functions take in a `pandas.DataFrame` dataset or a pair of datasets and generate results in the form of a `Report` object.\n\nOne of Evidently's notable characteristics is that it only requires datasets as input. Even when running model performance comparison analyses, no model needs to be present. However, that does mean that the input data needs to include additional `target` and `prediction` columns for some profiling reports and, you have to include additional information about the dataset columns in the form of [column mappings](https://docs.evidentlyai.com/user-guide/tests-and-reports/column-mapping). Depending on how your data is structured, you may also need to include additional steps in your pipeline before the data validation step to insert the additional `target` and `prediction` columns into your data. This may also require interacting with one or more models.\n\nThere are three ways you can use Evidently to generate data reports in your ZenML pipelines that allow different levels of flexibility:\n\n* instantiate, configure and insert the standard Evidently report step shipped with ZenML into your pipelines. This is the easiest way and the recommended approach.\n* call the data validation methods provided by [the Evidently Data Validator](evidently.md#the-evidently-data-validator) in your custom step implementation. This method allows for more flexibility concerning what can happen in the pipeline step.\n* [use the Evidently library directly](evidently.md#call-evidently-directly) in your custom step implementation. This gives you complete freedom in how you are using Evidently's features.\n\nYou can [visualize Evidently reports](evidently.md#visualizing-evidently-reports) in Jupyter notebooks or view them directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\n**The Evidently Report step**\n\nZenML wraps the Evidently data profiling functionality in the form of a standard Evidently report pipeline step that you can simply instantiate and insert in your pipeline. Here you can see how instantiating and configuring the standard Evidently report step can be done:\n\n```python\nfrom zenml.integrations.evidently.metrics import E"}
{"input": "videntlyMetricConfig\nfrom zenml.integrations.evidently.steps import (\n    EvidentlyColumnMapping,\n    evidently_report_step,\n)\n\ntext_data_report = evidently_report_step.with_options(\n    parameters=dict(\n        column_mapping=EvidentlyColumnMapping(\n            target=\"Rating\",\n            numerical_features=[\"Age\", \"Positive_Feedback_Count\"],\n            categorical_features=[\n                \"Division_Name\",\n                \"Department_Name\",\n                \"Class_Name\",\n            ],\n            text_features=[\"Review_Text\", \"Title\"],\n        ),\n        metrics=[\n            EvidentlyMetricConfig.metric(\"DataQualityPreset\"),\n            EvidentlyMetricConfig.metric(\n                \"TextOverviewPreset\", column_name=\"Review_Text\"\n            ),\n            EvidentlyMetricConfig.metric_generator(\n                \"ColumnRegExpMetric\",\n                columns=[\"Review_Text\", \"Title\"],\n                reg_exp=r\"[A-Z][A-Za-z0-9 ]*\",\n            ),\n        ],\n        # We need to download the NLTK data for the TextOverviewPreset\n        download_nltk_data=True,\n    ),\n)\n```\n\nThe configuration shown in the example is the equivalent of running the following Evidently code inside the step:\n\n```python\nfrom evidently.metrics import ColumnRegExpMetric\nfrom evidently.metric_preset import DataQualityPreset, TextOverviewPreset\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metrics.base_metric import generate_column_metrics\nimport nltk\n\nnltk.download(\"words\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\ncolumn_mapping = ColumnMapping(\n    target=\"Rating\",\n    numerical_features=[\"Age\", \"Positive_Feedback_Count\"],\n    categorical_features=[\n        \"Division_Name\",\n        \"Department_Name\",\n        \"Class_Name\",\n    ],\n    text_features=[\"Review_Text\", \"Title\"],\n)\n\nreport = Report(\n    metrics=[\n        DataQualityPreset(),\n        TextOverviewPreset(column_name=\"Review_Text\"),\n        generate_column_metrics(\n            ColumnRegExpMetric,\n            columns=[\"Review_Text\", \"Title\"],\n            parameters={\"reg_exp\": r\"[A-Z][A-Za-z0-9 ]*\"}\n        )\n    ]\n)\n\n# The datasets are those that are passed to the Evidently step\n# as input artifacts\nreport.run(\n    current_data=current_dataset,\n    reference_data=reference_dataset,\n    column_mapping=column_mapping,\n)\n```\n\nLet's break this down...\n\nWe configure the `evidently_report_step` using parameters that you would normally pass to the Evidently `Report` object to [configure and run an Evidently report](https://docs.evidentlyai.com/user-guide/tests-and-reports/custom-report). It consists of the following fields:\n\n* `column_mapping`: This is an `EvidentlyColumnMapping` object that is the exact equivalent of [the `ColumnMapping` object in Evidently](https://docs.evidentlyai.com/user-guide/input-data/column-mapping"}
{"input": "). It is used to describe the columns in the dataset and how they should be treated (e.g. as categorical, numerical, or text features).\n* `metrics`: This is a list of `EvidentlyMetricConfig` objects that are used to configure the metrics that should be used to generate the report in a declarative way. This is the same as configuring the `metrics` that go in the Evidently `Report`.\n* `download_nltk_data`: This is a boolean that is used to indicate whether the NLTK data should be downloaded. This is only needed if you are using Evidently reports that handle text data, which require the NLTK data to be downloaded ahead of time.\n\nThere are several ways you can reference the Evidently metrics when configuring `EvidentlyMetricConfig` items:\n\n* by class name: this is the easiest way to reference an Evidently metric. You can use the name of a metric or metric preset class as it appears in the Evidently documentation (e.g.`\"DataQualityPreset\"`, `\"DatasetDriftMetric\"`).\n* by full class path: you can also use the full Python class path of the metric or metric preset class ( e.g. `\"evidently.metric_preset.DataQualityPreset\"`, `\"evidently.metrics.DatasetDriftMetric\"`). This is useful if you want to use metrics or metric presets that are not included in Evidently library.\n*   by passing in the class itself: you can also import and pass in an Evidently metric or metric preset class itself, e.g.:\n\n    ```python\n    from evidently.metrics import DatasetDriftMetric\n\n    ...\n\n    evidently_report_step.with_options(\n        parameters=dict(\n            metrics=[EvidentlyMetricConfig.metric(DatasetDriftMetric)]\n        ),\n    )\n    ```\n\nAs can be seen in the example, there are two basic ways of adding metrics to your Evidently report step configuration:\n\n* to add a single metric or metric preset: call `EvidentlyMetricConfig.metric` with an Evidently metric or metric preset class name (or class path or class). The rest of the parameters are the same ones that you would usually pass to the Evidently metric or metric preset class constructor.\n* to generate multiple metrics, similar to calling [the Evidently column metric generator](https://docs.evidentlyai.com/user-guide/tests-and-reports/test-metric-generator#column-metric-generator): call `EvidentlyMetricConfig.metric_generator` with an Evidently metric or metric preset class name (or class path or class) and a list of column names. The rest of the parameters are the same ones that you would usually pass to the Evidently metric or metric preset class constructor.\n\nThe ZenML Evidently report step can then be inserted into your pipeline where it can take in two datasets and outputs the Evidently report generated in both JSON and"}
{"input": " HTML formats, e.g.:\n\n```python\n@pipeline(enable_cache=False, settings={\"docker\": docker_settings})\ndef text_data_report_test_pipeline():\n    \"\"\"Links all the steps together in a pipeline.\"\"\"\n    data = data_loader()\n    reference_dataset, comparison_dataset = data_splitter(data)\n    report, _ = text_data_report(\n        reference_dataset=reference_dataset,\n        comparison_dataset=comparison_dataset,\n    )\n    test_report, _ = text_data_test(\n        reference_dataset=reference_dataset,\n        comparison_dataset=comparison_dataset,\n    )\n    text_analyzer(report)\n\n\ntext_data_report_test_pipeline()\n```\n\nFor a version of the same step that works with a single dataset, simply don't pass any comparison dataset:\n\n```python\ntext_data_report(reference_dataset=reference_dataset)\n```\n\nYou should consult [the official Evidently documentation](https://docs.evidentlyai.com/reference/all-metrics) for more information on what each metric is useful for and what data columns it requires as input.\n\nThe `evidently_report_step` step also allows for additional Report [options](https://docs.evidentlyai.com/user-guide/customization) to be passed to the `Report` constructor e.g.:\n\n```python\nfrom zenml.integrations.evidently.steps import (\n    EvidentlyColumnMapping,\n)\n\ntext_data_report = evidently_report_step.with_options(\n    parameters=dict(\n        report_options = [\n            (\n                \"evidently.options.ColorOptions\", {\n                    \"primary_color\": \"#5a86ad\",\n                    \"fill_color\": \"#fff4f2\",\n                    \"zero_line_color\": \"#016795\",\n                    \"current_data_color\": \"#c292a1\",\n                    \"reference_data_color\": \"#017b92\",\n                }\n            ),\n        ],\n    )\n)\n```\n\nYou can view [the complete list of configuration parameters](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-evidently/#zenml.integrations.evidently.steps.evidently\\_report.evidently\\_report\\_step) in the SDK docs.\n\n#### Data Validation\n\nAside from data profiling, Evidently can also be used to configure and run automated data validation tests on your data.\n\nSimilar to using Evidently through ZenML to run data profiling, there are three ways you can use Evidently to run data validation tests in your ZenML pipelines that allow different levels of flexibility:\n\n* instantiate, configure and insert [the standard Evidently test step](evidently.md) shipped with ZenML into your pipelines. This is the easiest way and the recommended approach.\n* call the data validation methods provided by [the Evidently Data Validator](evidently.md#the-evidently-data-validator) in your custom step implementation. This method allows for more flexibility concerning what can happen in the pipeline step.\n* [use the Evidently library directly](evidently.md#call-evid"}
{"input": "ently-directly) in your custom step implementation. This gives you complete freedom in how you are using Evidently's features.\n\nYou can visualize Evidently reports in Jupyter notebooks or view them directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\nYou can [visualize Evidently reports](evidently.md#visualizing-evidently-reports) in Jupyter notebooks or view them directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\nZenML wraps the Evidently data validation functionality in the form of a standard Evidently test pipeline step that you can simply instantiate and insert in your pipeline. Here you can see how instantiating and configuring the standard Evidently test step can be done using our included `evidently_test_step` utility function:\n\n```python\nfrom zenml.integrations.evidently.steps import (\n    EvidentlyColumnMapping,\n    evidently_test_step,\n)\nfrom zenml.integrations.evidently.tests import EvidentlyTestConfig\n\n\ntext_data_test = evidently_test_step.with_options(\n    parameters=dict(\n        column_mapping=EvidentlyColumnMapping(\n            target=\"Rating\",\n            numerical_features=[\"Age\", \"Positive_Feedback_Count\"],\n            categorical_features=[\n                \"Division_Name\",\n                \"Department_Name\",\n                \"Class_Name\",\n            ],\n            text_features=[\"Review_Text\", \"Title\"],\n        ),\n        tests=[\n            EvidentlyTestConfig.test(\"DataQualityTestPreset\"),\n            EvidentlyTestConfig.test_generator(\n                \"TestColumnRegExp\",\n                columns=[\"Review_Text\", \"Title\"],\n                reg_exp=r\"[A-Z][A-Za-z0-9 ]*\",\n            ),\n        ],\n        # We need to download the NLTK data for the TestColumnRegExp test\n        download_nltk_data=True,\n    ),\n)\n```\n\nThe configuration shown in the example is the equivalent of running the following Evidently code inside the step:\n\n```python\nfrom evidently.tests import TestColumnRegExp\nfrom evidently.test_preset import DataQualityTestPreset\nfrom evidently import ColumnMapping\nfrom evidently.test_suite import TestSuite\nfrom evidently.tests.base_test import generate_column_tests\nimport nltk\n\nnltk.download(\"words\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\ncolumn_mapping = ColumnMapping(\n    target=\"Rating\",\n    numerical_features=[\"Age\", \"Positive_Feedback_Count\"],\n    categorical_features=[\n        \"Division_Name\",\n        \"Department_Name\",\n        \"Class_Name\",\n    ],\n    text_features=[\"Review_Text\", \"Title\"],\n)\n\ntest_suite = TestSuite(\n    tests=[\n        DataQualityTestPreset(),\n        generate_column_tests(\n            TestColumnRegExp,\n            columns=[\"Review_Text\", \"Title\"],\n            parameters={\"reg_exp\": r\"[A-Z][A-Za-z0-9 ]*\"}\n        )\n    ]\n)\n\n# The datasets are those that are passed"}
{"input": " to the Evidently step\n# as input artifacts\ntest_suite.run(\n    current_data=current_dataset,\n    reference_data=reference_dataset,\n    column_mapping=column_mapping,\n)\n```\n\nLet's break this down...\n\nWe configure the `evidently_test_step` using parameters that you would normally pass to the Evidently `TestSuite` object to [configure and run an Evidently test suite](https://docs.evidentlyai.com/user-guide/tests-and-reports/custom-test-suite) . It consists of the following fields:\n\n* `column_mapping`: This is an `EvidentlyColumnMapping` object that is the exact equivalent of [the `ColumnMapping` object in Evidently](https://docs.evidentlyai.com/user-guide/input-data/column-mapping). It is used to describe the columns in the dataset and how they should be treated (e.g. as categorical, numerical, or text features).\n* `tests`: This is a list of `EvidentlyTestConfig` objects that are used to configure the tests that will be run as part of your test suite in a declarative way. This is the same as configuring the `tests` that go in the Evidently `TestSuite`.\n* `download_nltk_data`: This is a boolean that is used to indicate whether the NLTK data should be downloaded. This is only needed if you are using Evidently tests or test presets that handle text data, which require the NLTK data to be downloaded ahead of time.\n\nThere are several ways you can reference the Evidently tests when configuring `EvidentlyTestConfig` items, similar to how you reference them in an `EvidentlyMetricConfig` object:\n\n* by class name: this is the easiest way to reference an Evidently test. You can use the name of a test or test preset class as it appears in the Evidently documentation (e.g.`\"DataQualityTestPreset\"`, `\"TestColumnRegExp\"`).\n* by full class path: you can also use the full Python class path of the test or test preset class ( e.g. `\"evidently.test_preset.DataQualityTestPreset\"`, `\"evidently.tests.TestColumnRegExp\"`). This is useful if you want to use tests or test presets that are not included in Evidently library.\n*   by passing in the class itself: you can also import and pass in an Evidently test or test preset class itself, e.g.:\n\n    ```python\n    from evidently.tests import TestColumnRegExp\n\n    ...\n\n    evidently_test_step.with_options(\n        parameters=dict(\n            tests=[EvidentlyTestConfig.test(TestColumnRegExp)]\n        ),\n    )\n    ```\n\nAs can be seen in the example, there are two basic ways of adding tests to your Evidently test step configuration:\n\n* to add a single test or test preset: call `EvidentlyTestConfig.test` with an"}
{"input": " Evidently test or test preset class name (or class path or class). The rest of the parameters are the same ones that you would usually pass to the Evidently test or test preset class constructor.\n* to generate multiple tests, similar to calling [the Evidently column test generator](https://docs.evidentlyai.com/user-guide/tests-and-reports/test-metric-generator#column-test-generator): call `EvidentlyTestConfig.test_generator` with an Evidently test or test preset class name (or class path or class) and a list of column names. The rest of the parameters are the same ones that you would usually pass to the Evidently test or test preset class constructor.\n\nThe ZenML Evidently test step can then be inserted into your pipeline where it can take in two datasets and outputs the Evidently test suite results generated in both JSON and HTML formats, e.g.:\n\n```python\n@pipeline(enable_cache=False, settings={\"docker\": docker_settings})\ndef text_data_test_pipeline():\n    \"\"\"Links all the steps together in a pipeline.\"\"\"\n    data = data_loader()\n    reference_dataset, comparison_dataset = data_splitter(data)\n    json_report, html_report = text_data_test(\n        reference_dataset=reference_dataset,\n        comparison_dataset=comparison_dataset,\n    )\n\n\ntext_data_test_pipeline()\n```\n\nFor a version of the same step that works with a single dataset, simply don't pass any comparison dataset:\n\n```python\ntext_data_test(reference_dataset=reference_dataset)\n```\n\nYou should consult [the official Evidently documentation](https://docs.evidentlyai.com/reference/all-tests) for more information on what each test is useful for and what data columns it requires as input.\n\nThe `evidently_test_step` step also allows for additional Test [options](https://docs.evidentlyai.com/user-guide/customization) to be passed to the `TestSuite` constructor e.g.:\n\n```python\nfrom zenml.integrations.evidently.steps import (\n    EvidentlyColumnMapping,\n)\n\ntext_data_test = evidently_test_step.with_options(\n    parameters=dict(\n        test_options = [\n            (\n                \"evidently.options.ColorOptions\", {\n                    \"primary_color\": \"#5a86ad\",\n                    \"fill_color\": \"#fff4f2\",\n                    \"zero_line_color\": \"#016795\",\n                    \"current_data_color\": \"#c292a1\",\n                    \"reference_data_color\": \"#017b92\",\n                }\n            ),\n        ],\n    ),\n)\n```\n\nYou can view [the complete list of configuration parameters](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-evidently/#zenml.integrations.evidently.steps.evidently\\_test.evidently\\_test\\_step) in the SDK docs.\n\n#### The Evidently Data Validator\n\nThe Evidently Data Validator implements the same interface as do all Data Validators, so this"}
{"input": " method forces you to maintain some level of compatibility with the overall Data Validator abstraction, which guarantees an easier migration in case you decide to switch to another Data Validator.\n\nAll you have to do is call the Evidently Data Validator methods when you need to interact with Evidently to generate data reports or to run test suites, e.g.:\n\n```python\nfrom typing_extensions import Annotated  # or `from typing import Annotated on Python 3.9+\nfrom typing import Tuple\nimport pandas as pd\nfrom evidently.pipeline.column_mapping import ColumnMapping\nfrom zenml.integrations.evidently.data_validators import EvidentlyDataValidator\nfrom zenml.integrations.evidently.metrics import EvidentlyMetricConfig\nfrom zenml.integrations.evidently.tests import EvidentlyTestConfig\nfrom zenml.types import HTMLString\nfrom zenml import step\n\n\n@step\ndef data_profiling(\n    reference_dataset: pd.DataFrame,\n    comparison_dataset: pd.DataFrame,\n) -> Tuple[\n    Annotated[str, \"report_json\"],\n    Annotated[HTMLString, \"report_html\"]\n]:\n    \"\"\"Custom data profiling step with Evidently.\n\n    Args:\n        reference_dataset: a Pandas DataFrame\n        comparison_dataset: a Pandas DataFrame of new data you wish to\n            compare against the reference data\n\n    Returns:\n        The Evidently report rendered in JSON and HTML formats.\n    \"\"\"\n    # pre-processing (e.g. dataset preparation) can take place here\n\n    data_validator = EvidentlyDataValidator.get_active_data_validator()\n    report = data_validator.data_profiling(\n        dataset=reference_dataset,\n        comparison_dataset=comparison_dataset,\n        profile_list=[\n            EvidentlyMetricConfig.metric(\"DataQualityPreset\"),\n            EvidentlyMetricConfig.metric(\n                \"TextOverviewPreset\", column_name=\"Review_Text\"\n            ),\n            EvidentlyMetricConfig.metric_generator(\n                \"ColumnRegExpMetric\",\n                columns=[\"Review_Text\", \"Title\"],\n                reg_exp=r\"[A-Z][A-Za-z0-9 ]*\",\n            ),\n        ],\n        column_mapping = ColumnMapping(\n            target=\"Rating\",\n            numerical_features=[\"Age\", \"Positive_Feedback_Count\"],\n            categorical_features=[\n                \"Division_Name\",\n                \"Department_Name\",\n                \"Class_Name\",\n            ],\n            text_features=[\"Review_Text\", \"Title\"],\n        ),\n        download_nltk_data = True,\n    )\n\n    # post-processing (e.g. interpret results, take actions) can happen here\n\n    return report.json(), HTMLString(report.show(mode=\"inline\").data)\n\n\n@step\ndef data_validation(\n    reference_dataset: pd.DataFrame,\n    comparison_dataset: pd.DataFrame,\n) -> Tuple[\n    Annotated[str, \"test_json\"],\n    Annotated[HTMLString, \"test_html\"]\n]:\n    \"\"\"Custom data validation step with Evidently.\n\n    Args:\n        reference_dataset: a Pandas DataFrame\n        comparison_dataset"}
{"input": ": a Pandas DataFrame of new data you wish to\n            compare against the reference data\n\n    Returns:\n        The Evidently test suite results rendered in JSON and HTML formats.\n    \"\"\"\n    # pre-processing (e.g. dataset preparation) can take place here\n\n    data_validator = EvidentlyDataValidator.get_active_data_validator()\n    test_suite = data_validator.data_validation(\n        dataset=reference_dataset,\n        comparison_dataset=comparison_dataset,\n        check_list=[\n            EvidentlyTestConfig.test(\"DataQualityTestPreset\"),\n            EvidentlyTestConfig.test_generator(\n                \"TestColumnRegExp\",\n                columns=[\"Review_Text\", \"Title\"],\n                reg_exp=r\"[A-Z][A-Za-z0-9 ]*\",\n            ),\n        ],\n        column_mapping = ColumnMapping(\n            target=\"Rating\",\n            numerical_features=[\"Age\", \"Positive_Feedback_Count\"],\n            categorical_features=[\n                \"Division_Name\",\n                \"Department_Name\",\n                \"Class_Name\",\n            ],\n            text_features=[\"Review_Text\", \"Title\"],\n        ),\n        download_nltk_data = True,\n    )\n\n    # post-processing (e.g. interpret results, take actions) can happen here\n\n    return test_suite.json(), HTMLString(test_suite.show(mode=\"inline\").data)\n```\n\nHave a look at [the complete list of methods and parameters available in the `EvidentlyDataValidator` API](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-evidently/#zenml.integrations.evidently.data\\_validators.evidently\\_data\\_validator.EvidentlyDataValidator) in the SDK docs.\n\n#### Call Evidently directly\n\nYou can use the Evidently library directly in your custom pipeline steps, e.g.:\n\n```python\nfrom typing_extensions import Annotated  # or `from typing import Annotated` on Python 3.9+\nfrom typing import Tuple\nimport pandas as pd\nfrom evidently.report import Report\nimport evidently.metric_preset as metric_preset\nfrom evidently.test_suite import TestSuite\nimport evidently.test_preset as test_preset\nfrom evidently.pipeline.column_mapping import ColumnMapping\nfrom zenml.types import HTMLString\nfrom zenml import step\n\n\n@step\ndef data_profiler(\n    dataset: pd.DataFrame,\n) -> Tuple[\n    Annotated[str, \"report_json\"],\n    Annotated[HTMLString, \"report_html\"]\n]:\n    \"\"\"Custom data profiler step with Evidently\n\n    Args:\n        dataset: a Pandas DataFrame\n\n    Returns:\n        Evidently report generated for the dataset in JSON and HTML format.\n    \"\"\"\n\n    # pre-processing (e.g. dataset preparation) can take place here\n\n    report = Report(metrics=[metric_preset.DataQualityPreset()])\n    report.run(\n        current_data=dataset,\n        reference_data=dataset,\n    )\n\n    # post-processing (e.g. interpret results, take actions) can happen here\n\n"}
{"input": "    return report.json(), HTMLString(report.show(mode=\"inline\").data)\n\n\n@step\ndef data_tester(\n    dataset: pd.DataFrame,\n) -> Tuple[\n    Annotated[str, \"test_json\"],\n    Annotated[HTMLString, \"test_html\"]\n]:\n    \"\"\"Custom data tester step with Evidently\n\n    Args:\n        dataset: a Pandas DataFrame\n\n    Returns:\n        Evidently test results generated for the dataset in JSON and HTML format.\n    \"\"\"\n\n    # pre-processing (e.g. dataset preparation) can take place here\n\n    test_suite = TestSuite(metrics=[test_preset.DataQualityTestPreset()])\n    report.run(\n        current_data=dataset,\n        reference_data=dataset,\n    )\n\n    # post-processing (e.g. interpret results, take actions) can happen here\n\n    return test_suite.json(), HTMLString(test_suite.show(mode=\"inline\").data)\n```\n\n### Visualizing Evidently Reports\n\nYou can view visualizations of the Evidently reports generated by your pipeline steps directly in the ZenML dashboard by clicking on the respective artifact in the pipeline run DAG.\n\nAlternatively, if you are running inside a Jupyter notebook, you can load and render the reports using the [artifact.visualize() method](../../how-to/visualize-artifacts/README.md), e.g.:\n\n```python\nfrom zenml.client import Client\n\n\ndef visualize_results(pipeline_name: str, step_name: str) -> None:\n    pipeline = Client().get_pipeline(pipeline=pipeline_name)\n    evidently_step = pipeline.last_run.steps[step_name]\n    evidently_step.visualize()\n\n\nif __name__ == \"__main__\":\n    visualize_results(\"text_data_report_pipeline\", \"text_report\")\n    visualize_results(\"text_data_test_pipeline\", \"text_test\")\n```\n\n![Evidently metrics report visualization](../../.gitbook/assets/evidently-metrics-report.png)\n\n![Evidently test results visualization](../../.gitbook/assets/evidently-test-results.png)\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to develop a custom data validator\n---\n\n# Develop a custom data validator\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n{% hint style=\"warning\" %}\n**Base abstraction in progress!**\n\nWe are actively working on the base abstraction for the Data Validators, which will be available soon. As a result, their extension is not recommended at the moment. When you are selecting a data validator for your stack, you can use one of [the existing flavors](./data-validators.md#data-validator-flavors).\n\nIf you need to implement your own Data Validator flavor, you can still do so, but keep in mind that you may have to refactor it when the base abstraction is updated.\n{% endhint %}\n\nZenML comes equipped with [Data Validator implementations](./data-validators.md#data-validator-flavors) that integrate a variety of data logging and validation libraries, frameworks and platforms. However, if you need to use a different library or service as a backend for your ZenML Data Validator, you can extend ZenML to provide your own custom Data Validator implementation.\n\n### Build your own custom data validator\n\nIf you want to implement your own custom Data Validator, you can follow the following steps:\n\n1. Create a class which inherits from [the `BaseDataValidator` class](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-data\\_validators/#zenml.data\\_validators.base\\_data\\_validator.BaseDataValidator) and override one or more of the abstract methods, depending on the capabilities of the underlying library/service that you want to integrate.\n2. If you need any configuration, you can create a class which inherits from the `BaseDataValidatorConfig` class.\n3. Bring both of these classes together by inheriting from the `BaseDataValidatorFlavor`.\n4. (Optional) You should also provide some standard steps that others can easily insert into their pipelines for instant access to data validation features.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml data-validator flavor register <path.to.MyDataValidatorFlavor>\n```\n\nFor example, if your flavor class `MyDataValidatorFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml data-validator flavor register flavors.my_flavor.MyDataValidatorFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow"}
{"input": " [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually it's better to not have to rely on this mechanism, and initialize zenml at the root.\n{% endhint %}\n\nAfterwards, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml data-validator flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions are coming into play in a ZenML workflow.\n\n* The **CustomDataValidatorFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomDataValidatorConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` object are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomDataValidator** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomDataValidatorFlavor` and the `CustomDataValidatorConfig` are implemented in a different module/path than the actual `CustomDataValidator`).\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: >-\n  How to enhance and maintain the quality of your data and the performance of\n  your models with data profiling and validation\n---\n\n# Data Validators\n\nWithout good data, even the best machine learning models will yield questionable results. A lot of effort goes into ensuring and maintaining data quality not only in the initial stages of model development, but throughout the entire machine learning project lifecycle. Data Validators are a category of ML libraries, tools and frameworks that grant a wide range of features and best practices that should be employed in the ML pipelines to keep data quality in check and to monitor model performance to keep it from degrading over time.\n\nData profiling, data integrity testing, data and model drift detection are all ways of employing data validation techniques at different points in your ML pipelines where data is concerned: data ingestion, model training and evaluation and online or batch inference. Data profiles and model performance evaluation results can be visualized and analyzed to detect problems and take preventive or correcting actions.\n\nRelated concepts:\n\n* the Data Validator is an optional type of Stack Component that needs to be registered as part of your ZenML [Stack](../../user-guide/production-guide/understand-stacks.md).\n* Data Validators used in ZenML pipelines usually generate data profiles and data quality check reports that are versioned and stored in the [Artifact Store](../artifact-stores/artifact-stores.md) and can be [retrieved and visualized](../../how-to/visualize-artifacts/README.md) later.\n\n### When to use it\n\n[Data-centric AI practices](https://blog.zenml.io/data-centric-mlops/) are quickly becoming mainstream and using Data Validators are an easy way to incorporate them into your workflow. These are some common cases where you may consider employing the use of Data Validators in your pipelines:\n\n* early on, even if it's just to keep a log of the quality state of your data and the performance of your models at different stages of development.\n* if you have pipelines that regularly ingest new data, you should use data validation to run regular data integrity checks to signal problems before they are propagated downstream.\n* in continuous training pipelines, you should use data validation techniques to compare new training data against a data reference and to compare the performance of newly trained models against previous ones.\n* when you have pipelines that automate batch inference or if you regularly collect data used as input in online inference, you should use data validation to run data drift analyses and detect training-serving skew, data drift and model drift.\n\n#### Data Validator Flavors\n\nData Validator are optional stack components provided by integrations. The following table lists the currently available Data Validators and summarizes their features and the data types and model types that they can be used with in ZenML pipelines:\n\n| Data Validator                              | Validation Features                                                   | Data Types                                                                                               | Model Types                                                                                   | Notes                                                                                               | Flavor/Integration   |\n| ------------------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | -------------------- |\n"}
{"input": "| [Deepchecks](deepchecks.md)                 | <p>data quality<br>data drift<br>model drift<br>model performance</p> | <p>tabular: <code>pandas.DataFrame</code><br>CV: <code>torch.utils.data.dataloader.DataLoader</code></p> | <p>tabular: <code>sklearn.base.ClassifierMixin</code><br>CV: <code>torch.nn.Module</code></p> | Add Deepchecks data and model validation tests to your pipelines                                    | `deepchecks`         |\n| [Evidently](evidently.md)                   | <p>data quality<br>data drift<br>model drift<br>model performance</p> | tabular: `pandas.DataFrame`                                                                              | N/A                                                                                           | Use Evidently to generate a variety of data quality and data/model drift reports and visualizations | `evidently`          |\n| [Great Expectations](great-expectations.md) | <p>data profiling<br>data quality</p>                                 | tabular: `pandas.DataFrame`                                                                              | N/A                                                                                           | Perform data testing, documentation and profiling with Great Expectations                           | `great_expectations` |\n| [Whylogs/WhyLabs](whylogs.md)               | data drift                                                            | tabular: `pandas.DataFrame`                                                                              | N/A                                                                                           | Generate data profiles with whylogs and upload them to WhyLabs                                      | `whylogs`            |\n\nIf you would like to see the available flavors of Data Validator, you can use the command:\n\n```shell\nzenml data-validator flavor list\n```\n\n### How to use it\n\nEvery Data Validator has different data profiling and testing capabilities and uses a slightly different way of analyzing your data and your models, but it generally works as follows:\n\n* first, you have to configure and add a Data Validator to your ZenML stack\n* every integration includes one or more builtin data validation steps that you can add to your pipelines. Of course, you can also use the libraries directly in your own custom pipeline steps and simply return the results (e.g. data profiles, test reports) as artifacts that are versioned and stored by ZenML in its Artifact Store.\n* you can access the data validation artifacts in subsequent pipeline steps, or [fetch them afterwards](../../how-to/handle-data-artifacts/load-artifacts-into-memory.md) to process them or visualize them as needed.\n\nConsult the documentation for the particular [Data Validator flavor](data-validators.md#data-validator-flavors) that you plan on using or are using in your stack for detailed information about how to use it in your ZenML pipelines.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"Zen"}
{"input": "ML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Managing data in feature stores.\n---\n\n# Feature Stores\n\nFeature stores allow data teams to serve data via an offline store and an online low-latency store where data is kept in\nsync between the two. It also offers a centralized registry where features (and feature schemas) are stored for use\nwithin a team or wider organization.\n\nAs a data scientist working on training your model, your requirements for how you access your batch / 'offline' data\nwill almost certainly be different from how you access that data as part of a real-time or online inference setting.\nFeast solves the problem of developing [train-serve skew](https://ploomber.io/blog/train-serve-skew/) where those two\nsources of data diverge from each other.\n\nFeature stores are a relatively recent addition to commonly-used machine learning stacks.\n\n### When to use it\n\nThe feature store is an optional stack component in the ZenML Stack. The feature store as a technology should be used to\nstore the features and inject them into the process on the server side. This includes\n\n* Productionalize new features\n* Reuse existing features across multiple pipelines and models\n* Achieve consistency between training and serving data (Training Serving Skew)\n* Provide a central registry of features and feature schemas\n\n### List of available feature stores\n\nFor production use cases, some more flavors can be found in specific `integrations` modules. In terms of features\nstores, ZenML features an integration of `feast`.\n\n| Feature Store                      | Flavor   | Integration | Notes                                                                    |\n|------------------------------------|----------|-------------|--------------------------------------------------------------------------|\n| [FeastFeatureStore](feast.md)      | `feast`  | `feast`     | Connect ZenML with already existing Feast                                |\n| [Custom Implementation](custom.md) | _custom_ |             | Extend the feature store abstraction and provide your own implementation |\n\nIf you would like to see the available flavors for feature stores, you can use the command:\n\n```shell\nzenml feature-store flavor list\n```\n\n### How to use it\n\nThe available implementation of the feature store is built on top of the feast integration, which means that using a\nfeature store is no different from what's described on the [feast page: How to use it?](feast.md#how-do-you-use-it).\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Managing data in Feast feature stores.\n---\n\n# Feast\n\nFeast (Feature Store) is an operational data system for managing and serving machine learning features to models in production. Feast is able to serve feature data to models from a low-latency online store (for real-time prediction) or from an offline store (for scale-out batch scoring or model training).\n\n### When would you want to use it?\n\nThere are two core functions that feature stores enable:\n\n* access to data from an offline / batch store for training.\n* access to online data at inference time.\n\nFeast integration currently supports your choice of offline data sources for\nyour online feature serving. We encourage users to check out [Feast's documentation](https://docs.feast.dev/)\nand [guides](https://docs.feast.dev/how-to-guides/) on how to set up your \noffline and online data sources via the configuration `yaml` file.\n\n{% hint style=\"info\" %}\nCOMING SOON: While the ZenML integration has an interface to access online feature store data, it currently is not usable in production settings with deployed models. We will update the docs when we enable this functionality.\n{% endhint %}\n\n### How to deploy it?\n\nZenML assumes that users already have a Feast feature store that they just need to connect with. If you don't have a feature store yet, follow the [Feast Documentation](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/deploy-a-feature-store) to deploy one first.\n\nTo use the feature store as a ZenML stack component, you also need to install the corresponding `feast` integration in ZenML:\n\n```shell\nzenml integration install feast\n```\n\nNow you can register your feature store as a ZenML stack component and add it into a corresponding stack:\n\n```shell\nzenml feature-store register feast_store --flavor=feast --feast_repo=\"<PATH/TO/FEAST/REPO>\"\nzenml stack register ... -f feast_store\n```\n\n### How do you use it?\n\n{% hint style=\"warning\" %}\nOnline data retrieval is possible in a local setting, but we don't currently support using the online data serving in the context of a deployed model or as part of model deployment. We will update this documentation as we develop this feature.\n{% endhint %}\n\nGetting features from a registered and active feature store is possible by creating your own step that interfaces into the feature store:\n\n```python\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Union\nimport pandas as pd\n\nfrom zenml import step\nfrom zenml.client import Client\n\n\n@step\ndef get_historical_features(\n    entity_dict: Union[Dict[str, Any], str],\n    features: List[str],\n    full_feature_names: bool = False\n) -> pd.DataFrame:\n    \"\"\"Feast Feature Store historical data step\n\n    Returns:\n       "}
{"input": " The historical features as a DataFrame.\n    \"\"\"\n    feature_store = Client().active_stack.feature_store\n    if not feature_store:\n        raise DoesNotExistException(\n            \"The Feast feature store component is not available. \"\n            \"Please make sure that the Feast stack component is registered as part of your current active stack.\"\n        )\n\n    params.entity_dict[\"event_timestamp\"] = [\n        datetime.fromisoformat(val)\n        for val in entity_dict[\"event_timestamp\"]\n    ]\n    entity_df = pd.DataFrame.from_dict(entity_dict)\n\n    return feature_store.get_historical_features(\n        entity_df=entity_df,\n        features=features,\n        full_feature_names=full_feature_names,\n    )\n\n\nentity_dict = {\n    \"driver_id\": [1001, 1002, 1003],\n    \"label_driver_reported_satisfaction\": [1, 5, 3],\n    \"event_timestamp\": [\n        datetime(2021, 4, 12, 10, 59, 42).isoformat(),\n        datetime(2021, 4, 12, 8, 12, 10).isoformat(),\n        datetime(2021, 4, 12, 16, 40, 26).isoformat(),\n    ],\n    \"val_to_add\": [1, 2, 3],\n    \"val_to_add_2\": [10, 20, 30],\n}\n\n\nfeatures = [\n    \"driver_hourly_stats:conv_rate\",\n    \"driver_hourly_stats:acc_rate\",\n    \"driver_hourly_stats:avg_daily_trips\",\n    \"transformed_conv_rate:conv_rate_plus_val1\",\n    \"transformed_conv_rate:conv_rate_plus_val2\",\n]\n\n@pipeline\ndef my_pipeline():\n    my_features = get_historical_features(entity_dict, features)\n    ...\n```\n\n{% hint style=\"warning\" %}\nNote that ZenML's use of Pydantic to serialize and deserialize inputs stored in the ZenML metadata means that we are limited to basic data types. Pydantic cannot handle Pandas `DataFrame`s, for example, or `datetime` values, so in the above code you can see that we have to convert them at various points.\n{% endhint %}\n\nFor more information and a full list of configurable attributes of the Feast feature store, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-feast/#zenml.integrations.feast.feature\\_stores.feast\\_feature\\_store.FeastFeatureStore) .\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom feature store.\n---\n\n# Develop a Custom Feature Store\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\nFeature stores allow data teams to serve data via an offline store, and an online low-latency store where data is kept in sync between the two. It also offers a centralized registry where features (and feature schemas) are stored for use within a team or wider organization.\n\n{% hint style=\"warning\" %}\n**Base abstraction in progress!**\n\nWe are actively working on the base abstraction for the feature stores, which will be available soon. As a result, their extension is not possible at the moment. If you would like to use a feature store in your stack, please check the list of already available feature stores down below.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Executing individual steps in SageMaker.\n---\n\n# Amazon SageMaker\n\n[SageMaker](https://aws.amazon.com/sagemaker/) offers specialized compute instances to run your training jobs and has a comprehensive UI to track and manage your models and logs. ZenML's SageMaker step operator allows you to submit individual steps to be run on Sagemaker compute instances.\n\n### When to use it\n\nYou should use the SageMaker step operator if:\n\n* one or more steps of your pipeline require computing resources (CPU, GPU, memory) that are not provided by your orchestrator.\n* you have access to SageMaker. If you're using a different cloud provider, take a look at the [Vertex](vertex.md) or [AzureML](azureml.md) step operators.\n\n### How to deploy it\n\n* Create a role in the IAM console that you want the jobs running in SageMaker to assume. This role should at least have the `AmazonS3FullAccess` and `AmazonSageMakerFullAccess` policies applied. Check [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-create-execution-role) for a guide on how to set up this role.\n\n#### Infrastructure Deployment\n\nA Sagemaker step operator can be deployed directly from the ZenML CLI:\n\n```shell\nzenml orchestrator deploy sagemaker_step_operator --flavor=sagemaker --provider=aws ...\n```\n\nYou can pass other configurations specific to the stack components as key-value arguments. If you don't provide a name, a random one is generated for you. For more information about how to work use the CLI for this, please refer to the dedicated documentation section.\n\n### How to use it\n\nTo use the SageMaker step operator, we need:\n\n*   The ZenML `aws` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install aws\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* An IAM role with the correct permissions. See the [deployment section](sagemaker.md#how-to-deploy-it) for detailed instructions.\n* An [AWS container registry](../container-registries/aws.md) as part of our stack. Take a look [here](../container-registries/aws.md#how-to-deploy-it) for a guide on how to set that up.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack. This is needed so that both your orchestration environment and SageMaker can read and write step artifacts. Check out the documentation page of the artifact store you want to use for more information on how to set that up and configure authentication for it.\n* An instance type that we want to execute our steps on. See [here](https://docs.aws.amazon.com/sagemaker/latest/dg/n"}
{"input": "otebooks-available-instance-types.html) for a list of available instance types.\n* (Optional) An experiment that is used to group SageMaker runs. Check [this guide](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html) to see how to create an experiment.\n\nThere are two ways you can authenticate your orchestrator to AWS to be able to run steps on SageMaker:\n\n{% tabs %}\n{% tab title=\"Authentication via Service Connector\" %}\nThe recommended way to authenticate your SageMaker step operator is by registering or using an existing [AWS Service Connector](../../how-to/auth-management/aws-service-connector.md) and connecting it to your SageMaker step operator. The credentials configured for the connector must have permissions to create and manage SageMaker runs (e.g. [the `AmazonSageMakerFullAccess` managed policy](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html) permissions). The SageMaker step operator uses these `aws-generic` resource type, so make sure to configure the connector accordingly:\n\n```shell\nzenml service-connector register <CONNECTOR_NAME> --type aws -i\nzenml step-operator register <STEP_OPERATOR_NAME> \\\n    --flavor=sagemaker \\\n    --role=<SAGEMAKER_ROLE> \\\n    --instance_type=<INSTANCE_TYPE> \\\n#   --experiment_name=<EXPERIMENT_NAME> # optionally specify an experiment to assign this run to\n\nzenml step-operator connect <STEP_OPERATOR_NAME> --connector <CONNECTOR_NAME>\nzenml stack register <STACK_NAME> -s <STEP_OPERATOR_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"Implicit Authentication\" %}\nIf you don't connect your step operator to a service connector:\n\n* If using a [local orchestrator](../orchestrators/local.md): ZenML will try to implicitly authenticate to AWS via the `default` profile in your local [AWS configuration file](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). Make sure this profile has permissions to create and manage SageMaker runs (e.g. [the `AmazonSageMakerFullAccess` managed policy](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html) permissions).\n* If using a remote orchestrator: the remote environment in which the orchestrator runs needs to be able to implicitly authenticate to AWS and assume the IAM role specified when registering the SageMaker step operator. This is only possible if the orchestrator is also running in AWS and uses a form of implicit workload authentication like the IAM role of an EC2 instance. If this is not the case, you will need to use a service connector.\n\n```shell\nzenml step-operator register <NAME> \\\n    --flavor=sagemaker \\\n    --role=<SAGEMAKER_ROLE> \\\n    --instance_type=<INSTANCE_TYPE> \\\n#   --"}
{"input": "experiment_name=<EXPERIMENT_NAME> # optionally specify an experiment to assign this run to\n\nzenml stack register <STACK_NAME> -s <STEP_OPERATOR_NAME> ... --set\npython run.py  # Authenticates with `default` profile in `~/.aws/config`\n```\n{% endtab %}\n{% endtabs %}\n\nOnce you added the step operator to your active stack, you can use it to execute individual steps of your pipeline by specifying it in the `@step` decorator as follows:\n\n```python\nfrom zenml import step\n\n\n@step(step_operator= <NAME>)\ndef trainer(...) -> ...:\n    \"\"\"Train a model.\"\"\"\n    # This step will be executed in SageMaker.\n```\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your steps in SageMaker. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\n#### Additional configuration\n\nFor additional configuration of the SageMaker step operator, you can pass `SagemakerStepOperatorSettings` when defining or running your pipeline. Check out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.flavors.sagemaker\\_step\\_operator\\_flavor.SagemakerStepOperatorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the SageMaker step operator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-aws/#zenml.integrations.aws.step\\_operators.sagemaker\\_step\\_operator.SagemakerStepOperator) .\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this step operator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Executing individual steps in Kubernetes Pods.\n---\n\n# Kubernetes Step Operator\n\nZenML's Kubernetes step operator allows you to submit individual steps to be run on Kubernetes pods.\n\n### When to use it\n\nYou should use the Kubernetes step operator if:\n\n* one or more steps of your pipeline require computing resources (CPU, GPU, memory) that are not provided by your orchestrator.\n* you have access to a Kubernetes cluster.\n\n### How to deploy it\n\nThe Kubernetes step operator requires a Kubernetes cluster in order to run. There are many ways to deploy a Kubernetes cluster using different cloud providers or on your custom infrastructure, and we can't possibly cover all of them, but you can check out our cloud guide.\n\n### How to use it\n\nTo use the Kubernetes step operator, we need:\n*   The ZenML `kubernetes` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install kubernetes\n    ```\n* A Kubernetes cluster [deployed](kubernetes.md#how-to-deploy-it)\n* Either [Docker](https://www.docker.com) installed and running or a remote [image builder](../image-builders/image-builders.md) in your stack.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack. This is needed so that both your orchestration environment and Kubernetes Pods can read and write step artifacts. Check out the documentation page of the artifact store you want to use for more information on how to set that up and configure authentication for it.\n\n\n{% hint style=\"info\" %}\nIt is recommended that you set up [a Service Connector](../../how-to/auth-management/service-connectors-guide.md) and use it to connect the Kubernetes step operator to the Kubernetes cluster, especially if you are using a Kubernetes cluster managed by a cloud provider like AWS, GCP or Azure.\n{% endhint %}\n\nWe can then register the step operator and use it in our stacks. This can be done in two ways:\n\n1.  Using a Service Connector configured to access the remote Kubernetes cluster. Depending on your cloud provider, this should be either an [AWS](../../how-to/auth-management/aws-service-connector.md), [Azure](../../how-to/auth-management/azure-service-connector.md) or [GCP](../../how-to/auth-management/gcp-service-connector.md) service connector. If you're using a Kubernetes cluster that is not provided by any of these, you can use the generic [Kubernetes](../../how-to/auth-management/kubernetes-service-connector.md) service connector. You can then [connect the stack component to the Service Connector](../../how-to/auth-management/service-connectors-guide.md#connect-stack-components-to-resources):\n\n    ```\n    $ zenml step-operator register <NAME> --flavor kubernetes\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully registered"}
{"input": " step operator `<NAME>`.\n\n    $ zenml service-connector list-resources --resource-type kubernetes-cluster -e\n    The following 'kubernetes-cluster' resources can be accessed by service connectors configured in your workspace:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME        \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES      \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 e33c9fac-5daa-48b2-87bb-0187d3782cde \u2502 aws-iam-multi-eu      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 kubeflowmultitenant \u2503\n    \u2503                                      \u2502                       \u2502                \u2502                       \u2502 zenbox              \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us      \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster    \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 1c54b32a-4889-4417-abbd-42d3ace3d03a \u2502 gcp-sa-multi          \u2502 \ud83d\udd35 gcp         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenml-test-cluster  \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n    $ zenml step-operator connect <NAME> --connector aws-iam-multi-us\n    Running with active workspace: 'default' (repository)\n    Running with active stack: 'default' (repository)\n    Successfully connected step_operator `<NAME>` to the following resources:\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503             CONNECTOR ID             \u2502 CONNECTOR NAME   \u2502 CONNECTOR TYPE \u2502 RESOURCE TYPE         \u2502 RESOURCE NAMES   \u2503\n    \u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n    \u2503 ed528d5a-d6cb-4fc4-bc52-c3d2d01643e5 \u2502 aws-iam-multi-us \u2502 \ud83d\udd36 aws         \u2502 \ud83c\udf00 kubernetes-cluster \u2502 zenhacks-cluster \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n    ```\n2.  Using the local Kubernetes `kubectl` client. This client needs to be configured with a configuration context pointing to the remote cluster. The `kubernetes_context` configuration attribute must also be configured with the value of that context:\n\n    ```shell\n    zenml step-operator register <NAME> \\\n        --flavor=kubernetes \\\n        --kubernetes_context=<KUBERNETES_CONTEXT>\n    ```\n\nWe can then use the registered step operator in our active stack:\n\n```shell\n# Add the step operator to the active stack\nzenml stack update -s <NAME>\n```\n\nOnce you added the step operator to your active stack, you can use it to execute individual steps of your pipeline by specifying it in the `@step` decorator as follows:\n\n```python\nfrom zenml import step\n\n\n@step(step_operator=<NAME>)\ndef trainer(...) -> ...:\n    \"\"\"Train a model.\"\"\"\n    # This step will be executed in Kubernetes.\n```\n\n{% hint style=\"info\" %}\nZenML will build a Docker images which includes your code and use it to run your steps in Kubernetes. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\n\n#### Interacting with pods via kubectl\n\nFor debugging, it can sometimes be handy to interact with the Kubernetes pods directly via kubectl. To make this easier, we have added the following labels to all pods:\n\n* `run`: the name of the ZenML run.\n* `pipeline`: the name of the ZenML pipeline associated with this run.\n\nE.g., you can use these labels to manually delete all pods related to a specific pipeline:\n\n```shell\nkubectl delete pod -n zenml -l pipeline=kubernetes"}
{"input": "_example_pipeline\n```\n\n#### Additional configuration\n\nFor additional configuration of the Kubernetes step operator, you can pass `KubernetesStepOperatorSettings` which allows you to configure (among others) the following attributes:\n\n* `pod_settings`: Node selectors, labels, affinity, and tolerations, and image pull secrets to apply to the Kubernetes Pods. These can be either specified using the Kubernetes model objects or as dictionaries.\n* `service_account_name`: The name of the service account to use for the Kubernetes Pods.\n\n```python\nfrom zenml.integrations.kubernetes.flavors import KubernetesStepOperatorSettings\nfrom kubernetes.client.models import V1Toleration\n\nkubernetes_settings = KubernetesStepOperatorSettings(\n    pod_settings={\n        \"node_selectors\": {\n            \"cloud.google.com/gke-nodepool\": \"ml-pool\",\n            \"kubernetes.io/arch\": \"amd64\"\n        },\n        \"affinity\": {\n            \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                    \"nodeSelectorTerms\": [\n                        {\n                            \"matchExpressions\": [\n                                {\n                                    \"key\": \"gpu-type\",\n                                    \"operator\": \"In\",\n                                    \"values\": [\"nvidia-tesla-v100\", \"nvidia-tesla-p100\"]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            }\n        },\n        \"tolerations\": [\n            V1Toleration(\n                key=\"gpu\",\n                operator=\"Equal\",\n                value=\"present\",\n                effect=\"NoSchedule\"\n            ),\n            V1Toleration(\n                key=\"high-priority\",\n                operator=\"Exists\",\n                effect=\"PreferNoSchedule\"\n            )\n        ],\n        \"resources\": {\n            \"requests\": {\n                \"cpu\": \"2\",\n                \"memory\": \"4Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            },\n            \"limits\": {\n                \"cpu\": \"4\",\n                \"memory\": \"8Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            }\n        },\n        \"annotations\": {\n            \"prometheus.io/scrape\": \"true\",\n            \"prometheus.io/port\": \"8080\"\n        },\n        \"volumes\": [\n            {\n                \"name\": \"data-volume\",\n                \"persistentVolumeClaim\": {\n                    \"claimName\": \"ml-data-pvc\"\n                }\n            },\n            {\n                \"name\": \"config-volume\",\n                \"configMap\": {\n                    \"name\": \"ml-config\"\n                }\n            }\n        ],\n        \"volume_mounts\": [\n            {\n                \"name\": \"data-volume\",\n                \"mountPath\": \"/mnt/data\"\n            },\n            {\n                \"name\": \"config-volume\",\n                \"mountPath\": \"/etc/ml-config\",\n                \"readOnly\": True\n            }\n        ],\n        \"host_ipc\": True,\n        \"image_pull_secrets\": [\"regcred\", \"gcr-secret\"],\n        \"labels\": {\n            \"app"}
{"input": "\": \"ml-pipeline\",\n            \"environment\": \"production\",\n            \"team\": \"data-science\"\n        }\n    },\n    kubernetes_namespace=\"ml-pipelines\",\n    service_account_name=\"zenml-pipeline-runner\"\n)\n\n@step(\n    settings={\n        \"step_operator.kubernetes\": kubernetes_settings\n    }\n)\ndef my_kubernetes_step():\n    ...\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kubernetes/#zenml.integrations.kubernetes.flavors.kubernetes\\_step\\_operator\\_flavor.KubernetesStepOperatorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the Kubernetes steop operator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-kubernetes/#zenml.integrations.kubernetes.step\\_operators.kubernetes\\step\\_operator.KubernetesStepOperator) .\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this step operator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Executing individual steps in AzureML.\n---\n\n# AzureML\n\n[AzureML](https://azure.microsoft.com/en-us/products/machine-learning/) offers specialized compute instances to run your training jobs and has a comprehensive UI to track and manage your models and logs. ZenML's AzureML step operator allows you to submit individual steps to be run on AzureML compute instances.\n\n### When to use it\n\nYou should use the AzureML step operator if:\n\n* one or more steps of your pipeline require computing resources (CPU, GPU, memory) that are not provided by your orchestrator.\n* you have access to AzureML. If you're using a different cloud provider, take a look at the [SageMaker](sagemaker.md) or [Vertex](vertex.md) step operators.\n\n### How to deploy it\n\n{% hint style=\"info\" %}\nWould you like to skip ahead and deploy a full ZenML cloud stack already,\nincluding an AzureML step operator? Check out the [in-browser stack deployment wizard](../../how-to/stack-deployment/deploy-a-cloud-stack.md),\nthe [stack registration wizard](../../how-to/stack-deployment/register-a-cloud-stack.md),\nor [the ZenML Azure Terraform module](../../how-to/stack-deployment/deploy-a-cloud-stack-with-terraform.md)\nfor a shortcut on how to deploy & register this stack component.\n{% endhint %}\n\n* Create a `Machine learning` [workspace on Azure](https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources). This should include an Azure container registry and an Azure storage account that will be used as part of your stack.\n* (Optional) Once your resource is created, you can head over to the `Azure Machine Learning Studio` and [create a compute instance or cluster](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-compute-instance?view=azureml-api-2&tabs=python) to run your pipelines. If omitted, the AzureML step operator will use the serverless compute target or will provision a new compute target on the fly, depending on the settings used to configure the step operator.\n* (Optional) Create a [Service Principal](https://docs.microsoft.com/en-us/azure/developer/java/sdk/identity-service-principal-auth) for authentication. This is required if you intend to use a service connector to authenticate your step operator.\n\n### How to use it\n\nTo use the AzureML step operator, we need:\n\n*   The ZenML `azure` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install azure\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* An [Azure container registry](../container-registries/azure.md) as part of your stack. Take a look [here](../container-registries/azure.md#how-to-deploy-it)"}
{"input": " for a guide on how to set that up.\n* An [Azure artifact store](../artifact-stores/azure.md) as part of your stack. This is needed so that both your orchestration environment and AzureML can read and write step artifacts. Take a look [here](../container-registries/azure.md#how-to-deploy-it) for a guide on how to set that up.\n* An AzureML workspace and an optional compute cluster. Note that the AzureML workspace can share the Azure container registry and Azure storage account that are required above. See the [deployment section](azureml.md#how-to-deploy-it) for detailed instructions.\n\nThere are two ways you can authenticate your step operator to be able to run steps on Azure:\n\n{% tabs %}\n{% tab title=\"Authentication via Service Connector\" %}\nThe recommended way to authenticate your AzureML step operator is by registering or using an existing [Azure Service Connector](../../how-to/auth-management/azure-service-connector.md) and connecting it to your AzureML step operator. The credentials configured for the connector must have permissions to create and manage AzureML jobs (e.g. [the `AzureML Data Scientist` and `AzureML Compute Operator` managed roles](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles?view=azureml-api-2&tabs=team-lead)). The AzureML step operator uses the `azure-generic` resource type, so make sure to configure the connector accordingly:\n\n```shell\nzenml service-connector register <CONNECTOR_NAME> --type azure -i\nzenml step-operator register <STEP_OPERATOR_NAME> \\\n    --flavor=azureml \\\n    --subscription_id=<AZURE_SUBSCRIPTION_ID> \\\n    --resource_group=<AZURE_RESOURCE_GROUP> \\\n    --workspace_name=<AZURE_WORKSPACE_NAME> \\\n#   --compute_target_name=<AZURE_COMPUTE_TARGET_NAME> # optionally specify an existing compute target\n\nzenml step-operator connect <STEP_OPERATOR_NAME> --connector <CONNECTOR_NAME>\nzenml stack register <STACK_NAME> -s <STEP_OPERATOR_NAME> ... --set\n```\n{% endtab %}\n\n{% tab title=\"Implicit Authentication\" %}\nIf you don't connect your step operator to a service connector:\n\n* If using a [local orchestrator](../orchestrators/local.md): ZenML will try to implicitly authenticate to Azure via the local [Azure CLI configuration](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively). Make sure the Azure CLI has permissions to create and manage AzureML jobs (e.g. [the `AzureML Data Scientist` and `AzureML Compute Operator` managed roles](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles?view=azureml-api-2&tabs=team-lead)).\n* If using a remote orchestrator: the remote environment in which the orchestrator"}
{"input": " runs needs to be able to implicitly authenticate to Azure and have permissions to create and manage AzureML jobs. This is only possible if the orchestrator is also running in Azure and uses a form of implicit workload authentication like a service role. If this is not the case, you will need to use a service connector.\n\n```shell\nzenml step-operator register <NAME> \\\n    --flavor=azureml \\\n    --subscription_id=<AZURE_SUBSCRIPTION_ID> \\\n    --resource_group=<AZURE_RESOURCE_GROUP> \\\n    --workspace_name=<AZURE_WORKSPACE_NAME> \\\n#   --compute_target_name=<AZURE_COMPUTE_TARGET_NAME> # optionally specify an existing compute target\n\nzenml stack register <STACK_NAME> -s <STEP_OPERATOR_NAME> ... --set\n```\n{% endtab %}\n{% endtabs %}\n\nOnce you added the step operator to your active stack, you can use it to execute individual steps of your pipeline by specifying it in the `@step` decorator as follows:\n\n```python\nfrom zenml import step\n\n\n@step(step_operator=<NAME>)\ndef trainer(...) -> ...:\n    \"\"\"Train a model.\"\"\"\n    # This step will be executed in AzureML.\n```\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your steps in AzureML. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize them.\n{% endhint %}\n\n#### Additional configuration\n\nThe ZenML AzureML step operator comes with a dedicated class called \n`AzureMLStepOperatorSettings` for configuring its settings and it controls\nthe compute resources used for step execution in AzureML.\n\nCurrently, it supports three different modes of operation.\n\n1. Serverless Compute (Default)\n- Set `mode` to `serverless`.\n- Other parameters are ignored.\n\n2. Compute Instance\n- Set `mode` to `compute-instance`.\n- Requires a `compute_name`.\n  - If a compute instance with the same name exists, it uses the existing \n  compute instance and ignores other parameters.\n  - If a compute instance with the same name doesn't exist, it creates a \n  new compute instance with the `compute_name`. For this process, you can \n  specify `compute_size` and `idle_type_before_shutdown_minutes`.\n\n3. Compute Cluster\n- Set `mode` to `compute-cluster`.\n- Requires a `compute_name`.\n  - If a compute cluster with the same name exists, it uses existing cluster, \n  ignores other parameters.\n  - If a compute cluster with the same name doesn't exist, it creates a new \n  compute cluster. Additional parameters can be used for configuring this \n  process.\n\nHere is an example how you can use the `AzureMLStep"}
{"input": "OperatorSettings` to define \na compute instance:\n\n```python\nfrom zenml.integrations.azure.flavors import AzureMLStepOperatorSettings\n\nazureml_settings = AzureMLStepOperatorSettings(\n    mode=\"compute-instance\",\n    compute_name=\"MyComputeInstance\",\n    compute_size=\"Standard_NC6s_v3\",\n)\n\n@step(\n   settings={\n       \"step_operator.azureml\": azureml_settings\n   }\n)\ndef my_azureml_step():\n    # YOUR STEP CODE\n    ...\n```\n\nYou can check out the [AzureMLStepOperatorSettings SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-azure/#zenml.integrations.azure.flavors.azureml\\_step\\_operator\\_flavor.AzureMLStepOperatorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this step operator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Executing individual steps on Spark\n---\n\n# Spark\n\nThe `spark` integration brings two different step operators:\n\n* **Step Operator**: The `SparkStepOperator` serves as the base class for all the Spark-related step operators.\n* **Step Operator**: The `KubernetesSparkStepOperator` is responsible for launching ZenML steps as Spark applications with Kubernetes as a cluster manager.\n\n## Step Operators: `SparkStepOperator`\n\nA summarized version of the implementation can be summarized in two parts. First, the configuration:\n\n```python\nfrom typing import Optional, Dict, Any\nfrom zenml.step_operators import BaseStepOperatorConfig\n\n\nclass SparkStepOperatorConfig(BaseStepOperatorConfig):\n    \"\"\"Spark step operator config.\n\n    Attributes:\n        master: is the master URL for the cluster. You might see different\n            schemes for different cluster managers which are supported by Spark\n            like Mesos, YARN, or Kubernetes. Within the context of this PR,\n            the implementation supports Kubernetes as a cluster manager.\n        deploy_mode: can either be 'cluster' (default) or 'client' and it\n            decides where the driver node of the application will run.\n        submit_kwargs: is the JSON string of a dict, which will be used\n            to define additional params if required (Spark has quite a\n            lot of different parameters, so including them, all in the step\n            operator was not implemented).\n    \"\"\"\n\n    master: str\n    deploy_mode: str = \"cluster\"\n    submit_kwargs: Optional[Dict[str, Any]] = None\n```\n\nand then the implementation:\n\n```python\nfrom typing import List\nfrom pyspark.conf import SparkConf\n\nfrom zenml.step_operators import BaseStepOperator\n\n\nclass SparkStepOperator(BaseStepOperator):\n    \"\"\"Base class for all Spark-related step operators.\"\"\"\n\n    def _resource_configuration(\n            self,\n            spark_config: SparkConf,\n            resource_configuration: \"ResourceSettings\",\n    ) -> None:\n        \"\"\"Configures Spark to handle the resource configuration.\"\"\"\n\n    def _backend_configuration(\n            self,\n            spark_config: SparkConf,\n            step_config: \"StepConfiguration\",\n    ) -> None:\n        \"\"\"Configures Spark to handle backends like YARN, Mesos or Kubernetes.\"\"\"\n\n    def _io_configuration(\n            self,\n            spark_config: SparkConf\n    ) -> None:\n        \"\"\"Configures Spark to handle different input/output sources.\"\"\"\n\n    def _additional_configuration(\n            self,\n            spark_config: SparkConf\n    ) -> None:\n        \"\"\"Appends the user-defined configuration parameters.\"\"\"\n\n    def _launch_spark_job(\n            self,\n            spark_config: SparkConf,\n            entrypoint_command: List[str]\n    ) -> None:\n        \"\"\"Generates and executes a spark-submit command.\"\"\"\n\n    def launch(\n            self,\n            info: \"StepRunInfo\",\n            entrypoint_command: List[str],\n    ) -> None:\n        \"\"\"Launches the step on"}
{"input": " Spark.\"\"\"\n```\n\nUnder the base configuration, you will see the main configuration parameters:\n\n* `master` is the master URL for the cluster where Spark will run. You might see different schemes for this URL with varying cluster managers such as Mesos, YARN, or Kubernetes.\n* `deploy_mode` can either be 'cluster' (default) or 'client' and it decides where the driver node of the application will run.\n* `submit_args` is the JSON string of a dictionary, which will be used to define additional parameters if required ( Spark has a wide variety of parameters, thus including them all in a single class was deemed unnecessary.).\n\nIn addition to this configuration, the `launch` method of the step operator gets additional configuration parameters from the `DockerSettings` and `ResourceSettings`. As a result, the overall configuration happens in 4 base methods:\n\n* `_resource_configuration` translates the ZenML `ResourceSettings` object to Spark's own resource configuration.\n* `_backend_configuration` is responsible for cluster-manager-specific configuration.\n* `_io_configuration` is a critical method. Even though we have materializers, Spark might require additional packages and configuration to work with a specific filesystem. This method is used as an interface to provide this configuration.\n* `_additional_configuration` takes the `submit_args`, converts, and appends them to the overall configuration.\n\nOnce the configuration is completed, `_launch_spark_job` comes into play. This takes the completed configuration and runs a Spark job on the given `master` URL with the specified `deploy_mode`. By default, this is achieved by creating and executing a `spark-submit` command.\n\n### Warning\n\nIn its first iteration, the pre-configuration with `_io_configuration` method is only effective when it is paired with an `S3ArtifactStore` (which has an authentication secret). When used with other artifact store flavors, you might be required to provide additional configuration through the `submit_args`.\n\n## Stack Component: `KubernetesSparkStepOperator`\n\nThe `KubernetesSparkStepOperator` is implemented by subclassing the base `SparkStepOperator` and uses the `PipelineDockerImageBuilder` class to build and push the required Docker images.\n\n```python\nfrom typing import Optional\n\nfrom zenml.integrations.spark.step_operators.spark_step_operator import (\n    SparkStepOperatorConfig\n)\n\n\nclass KubernetesSparkStepOperatorConfig(SparkStepOperatorConfig):\n    \"\"\"Config for the Kubernetes Spark step operator.\"\"\"\n\n    namespace: Optional[str] = None\n    service_account: Optional[str] = None\n```\n\n```python\nfrom pyspark.conf import SparkConf\n\nfrom zenml.utils.pipeline_docker_image_builder import PipelineDockerImageBuilder\nfrom zenml.integrations.spark.step_operators.spark_step_operator import (\n    SparkStepOperator\n)\n\n\nclass KubernetesSparkStepOperator(SparkStepOperator):\n    \"\"\"Step operator which runs Steps with Spark on Kubernetes.\"\"\"\n\n    def _backend_configuration(\n            self,\n            spark_config"}
{"input": ": SparkConf,\n            step_config: \"StepConfiguration\",\n    ) -> None:\n        \"\"\"Configures Spark to run on Kubernetes.\"\"\"\n        # Build and push the image\n        docker_image_builder = PipelineDockerImageBuilder()\n        image_name = docker_image_builder.build_and_push_docker_image(...)\n\n        # Adjust the spark configuration\n        spark_config.set(\"spark.kubernetes.container.image\", image_name)\n        ...\n```\n\nFor Kubernetes, there are also some additional important configuration parameters:\n\n* `namespace` is the namespace under which the driver and executor pods will run.\n* `service_account` is the service account that will be used by various Spark components (to create and watch the pods).\n\nAdditionally, the `_backend_configuration` method is adjusted to handle the Kubernetes-specific configuration.\n\n## When to use it\n\nYou should use the Spark step operator:\n\n* when you are dealing with large amounts of data.\n* when you are designing a step that can benefit from distributed computing paradigms in terms of time and resources.\n\n## How to deploy it\n\nTo use the `KubernetesSparkStepOperator` you will need to setup a few things first:\n\n* **Remote ZenML server:** See the [deployment guide](../../getting-started/deploying-zenml/README.md) for more information.\n* **Kubernetes cluster:** There are many ways to deploy a Kubernetes cluster using different cloud providers or on your custom infrastructure. For AWS, you can follow the [Spark EKS Setup Guide](spark-kubernetes.md#spark-eks-setup-guide) below.\n\n### Spark EKS Setup Guide\n\nThe following guide will walk you through how to spin up and configure a [Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/) with Spark on it:\n\n#### EKS Kubernetes Cluster\n\n* Follow [this guide](https://docs.aws.amazon.com/eks/latest/userguide/service\\_IAM\\_role.html#create-service-role) to create an Amazon EKS cluster role.\n* Follow [this guide](https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html#create-worker-node-role) to create an Amazon EC2 node role.\n* Go to the [IAM website](https://console.aws.amazon.com/iam), and select `Roles` to edit both roles.\n* Attach the `AmazonRDSFullAccess` and `AmazonS3FullAccess` policies to both roles.\n* Go to the [EKS website](https://console.aws.amazon.com/eks).\n* Make sure the correct region is selected on the top right.\n* Click on `Add cluster` and select `Create`.\n* Enter a name and select the **cluster role** for `Cluster service role`.\n* Keep the default values for the networking and logging steps and create the cluster.\n* Note down the cluster name and the API server endpoint:\n\n```bash\nEKS_CLUSTER_NAME=<EKS_CLUSTER_NAME>\nEKS_API_SERVER_ENDPOINT=<API_SERVER_ENDPOINT>\n```\n\n* After the cluster is created"}
{"input": ", select it and click on `Add node group` in the `Compute` tab.\n* Enter a name and select the **node role**.\n* For the instance type, we recommend `t3a.xlarge`, as it provides up to 4 vCPUs and 16 GB of memory.\n\n#### Docker image for the Spark drivers and executors\n\nWhen you want to run your steps on a Kubernetes cluster, Spark will require you to choose a base image for the driver and executor pods. Normally, for this purpose, you can either use one of the base images in [Spark\u2019s dockerhub](https://hub.docker.com/r/apache/spark-py/tags) or create an image using the [docker-image-tool](https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images) which will use your own Spark installation and build an image.\n\nWhen using Spark in EKS, you need to use the latter and utilize the `docker-image-tool`. However, before the build process, you also need to download the following packages\n\n* [`hadoop-aws` = 3.3.1](https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.3.1)\n* [`aws-java-sdk-bundle` = 1.12.150](https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-bundle/1.12.150)\n\nand put them in the `jars` folder within your Spark installation. Once that is set up, you can build the image as follows:\n\n```bash\ncd $SPARK_HOME # If this empty for you then you need to set the SPARK_HOME variable which points to your Spark installation\n\nSPARK_IMAGE_TAG=<SPARK_IMAGE_TAG>\n\n./bin/docker-image-tool.sh -t $SPARK_IMAGE_TAG -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile -u 0 build\n\nBASE_IMAGE_NAME=spark-py:$SPARK_IMAGE_TAG\n```\n\nIf you are working on an M1 Mac, you will need to build the image for the amd64 architecture, by using the prefix `-X` on the previous command. For example:\n\n```bash\n./bin/docker-image-tool.sh -X -t $SPARK_IMAGE_TAG -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile -u 0 build\n```\n\n#### Configuring RBAC\n\nAdditionally, you may need to create the several resources in Kubernetes in order to give Spark access to edit/manage your driver executor pods.\n\nTo do so, create a file called `rbac.yaml` with the following content:\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-namespace\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark-service-account\n  namespace: spark-namespace\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind:"}
{"input": " ClusterRoleBinding\nmetadata:\n  name: spark-role\n  namespace: spark-namespace\nsubjects:\n  - kind: ServiceAccount\n    name: spark-service-account\n    namespace: spark-namespace\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n---\n```\n\nAnd then execute the following command to create the resources:\n\n```bash\naws eks --region=$REGION update-kubeconfig --name=$EKS_CLUSTER_NAME\n\nkubectl create -f rbac.yaml\n```\n\nLastly, note down the **namespace** and the name of the **service account** since you will need them when registering the stack component in the next step.\n\n## How to use it\n\nTo use the `KubernetesSparkStepOperator`, you need:\n\n*   the ZenML `spark` integration. If you haven't installed it already, run\n\n    ```shell\n    zenml integration install spark\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack.\n* A [remote container registry](../container-registries/container-registries.md) as part of your stack.\n* A Kubernetes cluster [deployed](spark-kubernetes.md#how-to-deploy-it).\n\nWe can then register the step operator and use it in our active stack:\n\n```bash\nzenml step-operator register spark_step_operator \\\n\t--flavor=spark-kubernetes \\\n\t--master=k8s://$EKS_API_SERVER_ENDPOINT \\\n\t--namespace=<SPARK_KUBERNETES_NAMESPACE> \\\n\t--service_account=<SPARK_KUBERNETES_SERVICE_ACCOUNT>\n```\n\n```bash\n# Register the stack\nzenml stack register spark_stack \\\n    -o default \\\n    -s spark_step_operator \\\n    -a spark_artifact_store \\\n    -c spark_container_registry \\\n    -i local_builder \\\n    --set\n```\n\nOnce you added the step operator to your active stack, you can use it to execute individual steps of your pipeline by specifying it in the `@step` decorator as follows:\n\n```python\nfrom zenml import step\n\n\n@step(step_operator=<STEP_OPERATOR_NAME>)\ndef step_on_spark(...) -> ...:\n    \"\"\"Some step that should run with Spark on Kubernetes.\"\"\"\n    ...\n```\n\nAfter successfully running any step with a `KubernetesSparkStepOperator`, you should be able to see that a Spark driver pod was created in your cluster for each pipeline step when running `kubectl get pods -n $KUBERNETES_NAMESPACE`.\n\n{% hint style=\"info\" %}\nInstead of hardcoding a step operator name, you can also use the [Client](../../reference/python-client.md) to dynamically use the step operator of your active stack:\n\n```python\nfrom zenml.client import Client\n\nstep_operator = Client().active_stack.step"}
{"input": "_operator\n\n@step(step_operator=step_operator.name)\ndef step_on_spark(...) -> ...:\n    ...\n```\n{% endhint %}\n\n### Additional configuration\n\nFor additional configuration of the Spark step operator, you can pass `SparkStepOperatorSettings` when defining or running your pipeline. Check out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-spark/#zenml.integrations.spark.flavors.spark\\_step\\_operator\\_flavor.SparkStepOperatorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Executing individual steps in specialized environments.\n---\n\n# Step Operators\n\nThe step operator enables the execution of individual pipeline steps in specialized runtime environments that are\noptimized for certain workloads. These specialized environments can give your steps access to resources like GPUs or\ndistributed processing frameworks like [Spark](https://spark.apache.org/).\n\n{% hint style=\"info\" %}\n**Comparison to orchestrators:** The [orchestrator](../orchestrators/orchestrators.md) is a mandatory stack component\nthat is responsible for executing all steps of a pipeline in the correct order and providing additional features such as\nscheduling pipeline runs. The step operator on the other hand is used to only execute individual steps of the pipeline\nin a separate environment in case the environment provided by the orchestrator is not feasible.\n{% endhint %}\n\n### When to use it\n\nA step operator should be used if one or more steps of a pipeline require resources that are not available in the\nruntime environments provided by the [orchestrator](../orchestrators/orchestrators.md). An example would be a step that\ntrains a computer vision model and requires a GPU to run in a reasonable time, combined with\na [Kubeflow orchestrator](../orchestrators/kubeflow.md) running on a Kubernetes cluster that does not contain any GPU\nnodes. In that case, it makes sense to include a step operator like [SageMaker](sagemaker.md), [Vertex](vertex.md),\nor [AzureML](azureml.md) to execute the training step with a GPU.\n\n### Step Operator Flavors\n\nStep operators to execute steps on one of the big cloud providers are provided by the following ZenML integrations:\n\n| Step Operator                      | Flavor      | Integration | Notes                                                                    |\n|------------------------------------|-------------|-------------|--------------------------------------------------------------------------|\n| [SageMaker](sagemaker.md)          | `sagemaker` | `aws`       | Uses SageMaker to execute steps                                          |\n| [Vertex](vertex.md)                | `vertex`    | `gcp`       | Uses Vertex AI to execute steps                                          |\n| [AzureML](azureml.md)              | `azureml`   | `azure`     | Uses AzureML to execute steps                                            |\n| [Kubernetes](kubernetes.md)              | `kubernetes`   | `kubernetes`     | Uses Kubernetes Pods to execute steps                                            |\n| [Spark](spark-kubernetes.md)       | `spark`     | `spark`     | Uses Spark on Kubernetes to execute steps in a distributed manner        |\n| [Custom Implementation](custom.md) | _custom_    |             | Extend the step operator abstraction and provide your own implementation |\n\nIf you would like to see the available flavors of step operators, you can use the command:\n\n```shell\nzenml step-operator flavor list\n```\n\n### How to use it\n\nYou don't need"}
{"input": " to directly interact with any ZenML step operator in your code. As long as the step operator that you\nwant to use is part of your active [ZenML stack](/docs/book/user-guide/production-guide/understand-stacks.md), you can simply\nspecify it in the `@step` decorator of your step.\n\n```python\nfrom zenml import step\n\n\n@step(step_operator= <STEP_OPERATOR_NAME>)\ndef my_step(...) -> ...:\n    ...\n```\n\n#### Specifying per-step resources\n\nIf your steps require additional hardware resources, you can specify them on your steps as\ndescribed [here](../../how-to/training-with-gpus/training-with-gpus.md).\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use step operators to run steps on a GPU, you will need to\nfollow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure \nthat it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to \ngive its full acceleration.\n\n<!-- For scarf -->\n<figure><img alt=\"ZenML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: Learning how to develop a custom step operator.\n---\n\n# Develop a Custom Step Operator\n\n{% hint style=\"info\" %}\nBefore diving into the specifics of this component type, it is beneficial to familiarize yourself with our [general guide to writing custom component flavors in ZenML](../../how-to/stack-deployment/implement-a-custom-stack-component.md). This guide provides an essential understanding of ZenML's component flavor concepts.\n{% endhint %}\n\n### Base Abstraction\n\nThe `BaseStepOperator` is the abstract base class that needs to be subclassed in order to run specific steps of your pipeline in a separate environment. As step operators can come in many shapes and forms, the base class exposes a deliberately basic and generic interface:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Type\n\nfrom zenml.enums import StackComponentType\nfrom zenml.stack import StackComponent, StackComponentConfig, Flavor\nfrom zenml.config.step_run_info import StepRunInfo\n\n\nclass BaseStepOperatorConfig(StackComponentConfig):\n    \"\"\"Base config for step operators.\"\"\"\n\n\nclass BaseStepOperator(StackComponent, ABC):\n    \"\"\"Base class for all ZenML step operators.\"\"\"\n\n    @abstractmethod\n    def launch(\n            self,\n            info: StepRunInfo,\n            entrypoint_command: List[str],\n    ) -> None:\n        \"\"\"Abstract method to execute a step.\n\n        Subclasses must implement this method and launch a **synchronous**\n        job that executes the `entrypoint_command`.\n\n        Args:\n            info: Information about the step run.\n            entrypoint_command: Command that executes the step.\n        \"\"\"\n\n\nclass BaseStepOperatorFlavor(Flavor):\n    \"\"\"Base class for all ZenML step operator flavors.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Returns the name of the flavor.\"\"\"\n\n    @property\n    def type(self) -> StackComponentType:\n        \"\"\"Returns the flavor type.\"\"\"\n        return StackComponentType.STEP_OPERATOR\n\n    @property\n    def config_class(self) -> Type[BaseStepOperatorConfig]:\n        \"\"\"Returns the config class for this flavor.\"\"\"\n        return BaseStepOperatorConfig\n\n    @property\n    @abstractmethod\n    def implementation_class(self) -> Type[BaseStepOperator]:\n        \"\"\"Returns the implementation class for this flavor.\"\"\"\n```\n\n{% hint style=\"info\" %}\nThis is a slimmed-down version of the base implementation which aims to highlight the abstraction layer. In order to see the full implementation and get the complete docstrings, please check the [SDK docs](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-step\\_operators/#zenml.step\\_operators.base\\_step\\_operator.BaseStepOperator) .\n{% endhint %}\n\n### Build your own custom step operator\n\nIf you want to create your own custom flavor for a step operator, you can follow the following steps:\n\n1. Create a class that inherits from the `"}
{"input": "BaseStepOperator` class and implement the abstract `launch` method. This method has two main responsibilities:\n   * Preparing a suitable execution environment (e.g. a Docker image): The general environment is highly dependent on the concrete step operator implementation, but for ZenML to be able to run the step it requires you to install some `pip` dependencies. The list of requirements needed to successfully execute the step can be found via the Docker settings `info.pipeline.docker_settings` passed to the `launch()` method. Additionally, you'll have to make sure that all the source code of your ZenML step and pipeline are available within this execution environment.\n   * Running the entrypoint command: Actually running a single step of a pipeline requires knowledge of many ZenML internals and is implemented in the `zenml.step_operators.step_operator_entrypoint_configuration` module. As long as your environment was set up correctly (see the previous bullet point), you can run the step using the command provided via the `entrypoint_command` argument of the `launch()` method.\n2. If your step operator allows the specification of per-step resources, make sure to handle the resources defined on the step (`info.config.resource_settings`) that was passed to the `launch()` method.\n3. If you need to provide any configuration, create a class that inherits from the `BaseStepOperatorConfig` class adds your configuration parameters.\n4. Bring both the implementation and the configuration together by inheriting from the `BaseStepOperatorFlavor` class. Make sure that you give a `name` to the flavor through its abstract property.\n\nOnce you are done with the implementation, you can register it through the CLI. Please ensure you **point to the flavor class via dot notation**:\n\n```shell\nzenml step-operator flavor register <path.to.MyStepOperatorFlavor>\n```\n\nFor example, if your flavor class `MyStepOperatorFlavor` is defined in `flavors/my_flavor.py`, you'd register it by doing:\n\n```shell\nzenml step-operator flavor register flavors.my_flavor.MyStepOperatorFlavor\n```\n\n{% hint style=\"warning\" %}\nZenML resolves the flavor class by taking the path where you initialized zenml (via `zenml init`) as the starting point of resolution. Therefore, please ensure you follow [the best practice](../../how-to/setting-up-a-project-repository/best-practices.md) of initializing zenml at the root of your repository.\n\nIf ZenML does not find an initialized ZenML repository in any parent directory, it will default to the current working directory, but usually, it's better to not have to rely on this mechanism and initialize zenml at the root.\n{% endhint %}\n\nAfterward, you should see the new flavor in the list of available flavors:\n\n```shell\nzenml step-operator flavor list\n```\n\n{% hint style=\"warning\" %}\nIt is important to draw attention to when and how these base abstractions"}
{"input": " are coming into play in a ZenML workflow.\n\n* The **CustomStepOperatorFlavor** class is imported and utilized upon the creation of the custom flavor through the CLI.\n* The **CustomStepOperatorConfig** class is imported when someone tries to register/update a stack component with this custom flavor. Especially, during the registration process of the stack component, the config will be used to validate the values given by the user. As `Config` objects are inherently `pydantic` objects, you can also add your own custom validators here.\n* The **CustomStepOperator** only comes into play when the component is ultimately in use.\n\nThe design behind this interaction lets us separate the configuration of the flavor from its implementation. This way we can register flavors and components even when the major dependencies behind their implementation are not installed in our local setting (assuming the `CustomStepOperatorFlavor` and the `CustomStepOperatorConfig` are implemented in a different module/path than the actual `CustomStepOperator`).\n{% endhint %}\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use your custom step operator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Executing individual steps in Vertex AI.\n---\n\n# Google Cloud VertexAI\n\n[Vertex AI](https://cloud.google.com/vertex-ai) offers specialized compute instances to run your training jobs and has a comprehensive UI to track and manage your models and logs. ZenML's Vertex AI step operator allows you to submit individual steps to be run on Vertex AI compute instances.\n\n### When to use it\n\nYou should use the Vertex step operator if:\n\n* one or more steps of your pipeline require computing resources (CPU, GPU, memory) that are not provided by your orchestrator.\n* you have access to Vertex AI. If you're using a different cloud provider, take a look at the [SageMaker](sagemaker.md) or [AzureML](azureml.md) step operators.\n\n### How to deploy it\n\n* Enable Vertex AI [here](https://console.cloud.google.com/vertex-ai).\n* Create a [service account](https://cloud.google.com/iam/docs/service-accounts) with the right permissions to create Vertex AI jobs (`roles/aiplatform.admin`) and push to the container registry (`roles/storage.admin`).\n\n### How to use it\n\nTo use the Vertex step operator, we need:\n\n*   The ZenML `gcp` integration installed. If you haven't done so, run\n\n    ```shell\n    zenml integration install gcp\n    ```\n* [Docker](https://www.docker.com) installed and running.\n* Vertex AI enabled and a service account file. See the [deployment section](vertex.md#how-to-deploy-it) for detailed instructions.\n* A [GCR container registry](../container-registries/gcp.md) as part of our stack.\n* (Optional) A machine type that we want to execute our steps on (this defaults to `n1-standard-4`). See [here](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types) for a list of available machine types.\n* A [remote artifact store](../artifact-stores/artifact-stores.md) as part of your stack. This is needed so that both your orchestration environment and VertexAI can read and write step artifacts. Check out the documentation page of the artifact store you want to use for more information on how to set that up and configure authentication for it.\n\nYou have three different options to provide GCP credentials to the step operator:\n\n*   use the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) to authenticate locally with GCP. This only works in combination with the local orchestrator.\n\n    ```shell\n    gcloud auth login\n\n    zenml step-operator register <STEP_OPERATOR_NAME> \\\n        --flavor=vertex \\\n        --project=<GCP_PROJECT> \\\n        --region=<REGION> \\\n    #   --machine_type=<MACHINE_TYPE> # optionally specify the type of machine to run on\n"}
{"input": "    ```\n*   configure the orchestrator to use a [service account key file](https://cloud.google.com/iam/docs/creating-managing-service-account-keys) to authenticate with GCP by setting the `service_account_path` parameter in the orchestrator configuration to point to a service account key file. This also works only in combination with the local orchestrator.\n\n    ```shell\n    zenml step-operator register <STEP_OPERATOR_NAME> \\\n        --flavor=vertex \\\n        --project=<GCP_PROJECT> \\\n        --region=<REGION> \\\n        --service_account_path=<SERVICE_ACCOUNT_PATH> \\\n    #   --machine_type=<MACHINE_TYPE> # optionally specify the type of machine to run on\n    ```\n*   (recommended) configure [a GCP Service Connector](../../how-to/auth-management/gcp-service-connector.md) with GCP credentials coming from a [service account key file](https://cloud.google.com/iam/docs/creating-managing-service-account-keys) or the local `gcloud` CLI set up with user account credentials and then link the Vertex AI Step Operator stack component to the Service Connector. This option works with any orchestrator.\n\n    ```shell\n    zenml service-connector register <CONNECTOR_NAME> --type gcp --auth-method=service-account --project_id=<PROJECT_ID> --service_account_json=@<SERVICE_ACCOUNT_PATH> --resource-type gcp-generic\n\n    # Or, as an alternative, you could use the GCP user account locally set up with gcloud\n    # zenml service-connector register <CONNECTOR_NAME> --type gcp --resource-type gcp-generic --auto-configure\n\n    zenml step-operator register <STEP_OPERATOR_NAME> \\\n        --flavor=vertex \\\n        --region=<REGION> \\\n    #   --machine_type=<MACHINE_TYPE> # optionally specify the type of machine to run on\n\n    zenml step-operator connect <STEP_OPERATOR_NAME> --connector <CONNECTOR_NAME>\n    ```\n\nWe can then use the registered step operator in our active stack:\n\n```shell\n# Add the step operator to the active stack\nzenml stack update -s <NAME>\n```\n\nOnce you added the step operator to your active stack, you can use it to execute individual steps of your pipeline by specifying it in the `@step` decorator as follows:\n\n```python\nfrom zenml import step\n\n\n@step(step_operator=<NAME>)\ndef trainer(...) -> ...:\n    \"\"\"Train a model.\"\"\"\n    # This step will be executed in Vertex.\n```\n\n{% hint style=\"info\" %}\nZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your steps in Vertex AI. Check out [this page](../../how-to/customize-docker-builds/README.md) if you want to learn more about how ZenML builds these images and how you can customize"}
{"input": " them.\n{% endhint %}\n\n#### Additional configuration\n\nYou can specify the service account, network and reserved IP ranges to use for the VertexAI `CustomJob` by passing the `service_account`, `network` and `reserved_ip_ranges` parameters to the `step-operator register` command:\n\n```shell\n    zenml step-operator register <STEP_OPERATOR_NAME> \\\n        --flavor=vertex \\\n        --project=<GCP_PROJECT> \\\n        --region=<REGION> \\\n        --service_account=<SERVICE_ACCOUNT> # optionally specify the service account to use for the VertexAI CustomJob\n        --network=<NETWORK> # optionally specify the network to use for the VertexAI CustomJob\n        --reserved_ip_ranges=<RESERVED_IP_RANGES> # optionally specify the reserved IP range to use for the VertexAI CustomJob\n```\n\nFor additional configuration of the Vertex step operator, you can pass `VertexStepOperatorSettings` when defining or running your pipeline.\n\n```python\nfrom zenml import step\nfrom zenml.integrations.gcp.flavors.vertex_step_operator_flavor import VertexStepOperatorSettings\n\n@step(step_operator=<STEP_OPERATOR_NAME>, settings={\"step_operator.vertex\": VertexStepOperatorSettings(\n    accelerator_type= \"NVIDIA_TESLA_T4\",  # see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType\n    accelerator_count = 1,\n    machine_type = \"n1-standard-2\",       # see https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types\n    disk_type = \"pd-ssd\",                 # see https://cloud.google.com/vertex-ai/docs/training/configure-storage#disk-types\n    disk_size_gb = 100,                   # see https://cloud.google.com/vertex-ai/docs/training/configure-storage#disk-size\n)})\ndef trainer(...) -> ...:\n    \"\"\"Train a model.\"\"\"\n    # This step will be executed in Vertex.\n```\n\nCheck out the [SDK docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-gcp/#zenml.integrations.gcp.flavors.vertex\\_step\\_operator\\_flavor.VertexStepOperatorSettings) for a full list of available attributes and [this docs page](../../how-to/use-configuration-files/runtime-configuration.md) for more information on how to specify settings.\n\nFor more information and a full list of configurable attributes of the Vertex step operator, check out the [SDK Docs](https://sdkdocs.zenml.io/latest/integration\\_code\\_docs/integrations-gcp/#zenml.integrations.gcp.step\\_operators.vertex\\_step\\_operator.VertexStepOperator) .\n\n#### Enabling CUDA for GPU-backed hardware\n\nNote that if you wish to use this step operator to run steps on a GPU, you will need to follow [the instructions on this page](../../how-to"}
{"input": "/training-with-gpus/training-with-gpus.md) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to control ZenML behavior with environmental variables.\n---\n\n# \ud83c\udf0e Environment Variables\n\nThere are a few pre-defined environmental variables that can be used to control the behavior of ZenML. See the list below with default values and options:\n\n## Logging verbosity\n\n```bash\nexport ZENML_LOGGING_VERBOSITY=INFO\n```\n\nChoose from `INFO`, `WARN`, `ERROR`, `CRITICAL`, `DEBUG`.\n\n## Disable step logs\n\nUsually, ZenML [stores step logs in the artifact store](../how-to/control-logging/enable-or-disable-logs-storing.md), but this can sometimes cause performance bottlenecks, especially if the code utilizes progress bars.\n\nIf you want to configure whether logged output from steps is stored or not, set the `ZENML_DISABLE_STEP_LOGS_STORAGE` environment variable to `true`. Note that this will mean that logs from your steps will no longer be stored and thus won't be visible on the dashboard anymore.\n\n```bash\nexport ZENML_DISABLE_STEP_LOGS_STORAGE=false\n```\n\n## ZenML repository path\n\nTo configure where ZenML will install and look for its repository, set the environment variable `ZENML_REPOSITORY_PATH`.\n\n```bash\nexport ZENML_REPOSITORY_PATH=/path/to/somewhere\n```\n\n## Analytics\n\nPlease see [our full page](global-settings.md#usage-analytics) on what analytics are tracked and how you can opt out, but the quick summary is that you can set this to `false` if you want to opt out of analytics.\n\n```bash\nexport ZENML_ANALYTICS_OPT_IN=false\n```\n\n## Debug mode\n\nSetting to `true` switches to developer mode:\n\n```bash\nexport ZENML_DEBUG=true\n```\n\n## Active stack\n\nSetting the `ZENML_ACTIVE_STACK_ID` to a specific UUID will make the corresponding stack the active stack:\n\n```bash\nexport ZENML_ACTIVE_STACK_ID=<UUID-OF-YOUR-STACK>\n```\n\n## Prevent pipeline execution\n\nWhen `true`, this prevents a pipeline from executing:\n\n```bash\nexport ZENML_PREVENT_PIPELINE_EXECUTION=false\n```\n\n## Disable rich traceback\n\nSet to `false` to disable the [`rich` traceback](https://rich.readthedocs.io/en/stable/traceback.html):\n\n```bash\nexport ZENML_ENABLE_RICH_TRACEBACK=true\n```\n\n## Disable colourful logging\n\nIf you wish to disable colourful logging, set the following environment variable:\n\n```bash\nZENML_LOGGING_COLORS_DISABLED=true\n```\n\nNote that setting this on the [client environment](../how-to/configure-python-environments/README.md#client-environment-or-the-runner-environment) (e.g. your local machine which runs the pipeline) will automatically disable colorful logging on remote orchestrators. If you wish to disable it locally, but turn on for remote orchestrators, you can set the `ZEN"}
{"input": "ML_LOGGING_COLORS_DISABLED` environment variable in your orchestrator's environment as follows:\n\n```python\ndocker_settings = DockerSettings(environment={\"ZENML_LOGGING_COLORS_DISABLED\": \"false\"})\n\n# Either add it to the decorator\n@pipeline(settings={\"docker\": docker_settings})\ndef my_pipeline() -> None:\n    my_step()\n\n# Or configure the pipelines options\nmy_pipeline = my_pipeline.with_options(\n    settings={\"docker\": docker_settings}\n)\n```\n\n## ZenML global config path\n\nTo set the path to the global config file, used by ZenML to manage and store the state for a number of settings, set the environment variable as follows:\n\n```bash\nexport ZENML_CONFIG_PATH=/path/to/somewhere\n```\n\n## Server configuration\n\nFor more information on server configuration, see the [ZenML Server documentation](../getting-started/deploying-zenml/deploy-with-docker.md#zenml-server-configuration-options) for more, especially the section entitled \"ZenML server configuration options\".\n\n## Client configuration\n\nSetting the `ZENML_STORE_URL` and `ZENML_STORE_API_KEY` environment variables automatically connects your ZenML Client to the specified server. This method is particularly useful when you are using the ZenML client in an automated CI/CD workload environment like GitHub Actions or GitLab CI or in a containerized environment like Docker or Kubernetes:\n\n```bash\nexport ZENML_STORE_URL=https://...\nexport ZENML_STORE_API_KEY=<API_KEY>\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Understanding the global settings of your ZenML installation.\n---\n\n# \ud83d\udcfc Global settings\n\nThe information about the global settings of ZenML on a machine is kept in a folder commonly referred to as the **ZenML Global Config Directory** or the **ZenML Config Path**. The location of this folder depends on the operating system type and the current system user, but is usually located in the following locations:\n\n* Linux: `~/.config/zenml`\n* Mac: `~/Library/Application Support/zenml`\n* Windows: `C:\\Users\\%USERNAME%\\AppData\\Local\\zenml`\n\nThe default location may be overridden by setting the `ZENML_CONFIG_PATH` environment variable to a custom value. The current location of the global config directory used on a system can be retrieved by running the following commands:\n\n```shell\n# The output will tell you something like this:\n# Using configuration from: '/home/stefan/.config/zenml'\nzenml status\n\npython -c 'from zenml.utils.io_utils import get_global_config_directory; print(get_global_config_directory())'\n```\n\n{% hint style=\"warning\" %}\nManually altering or deleting the files and folders stored under the ZenML global config directory is not recommended, as this can break the internal consistency of the ZenML configuration. As an alternative, ZenML provides CLI commands that can be used to manage the information stored there:\n\n* `zenml analytics` - manage the analytics settings\n* `zenml clean` - to be used only in case of emergency, to bring the ZenML configuration back to its default factory state\n* `zenml downgrade` - downgrade the ZenML version in the global configuration to match the version of the ZenML package installed in the current environment. Read more about this in the [ZenML Version Mismatch](global-settings.md#version-mismatch-downgrading) section.\n{% endhint %}\n\nThe first time that ZenML is run on a machine, it creates the global config directory and initializes the default configuration in it, along with a default Stack:\n\n```\nInitializing the ZenML global configuration version to 0.13.2\nCreating default workspace 'default' ...\nCreating default user 'default' ...\nCreating default stack for user 'default' in workspace default...\nActive workspace not set. Setting it to the default.\nThe active stack is not set. Setting the active stack to the default workspace stack.\nUsing the default store for the global config.\nUnable to find ZenML repository in your current working directory (/tmp/folder) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run zenml init.\nRunning without an active repository root.\nUsing the default local database.\nRunning with active workspace: 'default' (global)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 STACK NAME \u2502 SHARED \u2502 OWNER   \u2502 ARTIFACT_STORE \u2502 ORCHESTRATOR \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default    \u2502 \u274c     \u2502 default \u2502 default        \u2502 default      \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nThe following is an example of the layout of the global config directory immediately after initialization:\n\n```\n/home/stefan/.config/zenml   <- Global Config Directory\n\u251c\u2500\u2500 config.yaml              <- Global Configuration Settings\n\u2514\u2500\u2500 local_stores             <- Every Stack component that stores information \n    |                           locally will have its own subdirectory here.              \n    \u251c\u2500\u2500 a1a0d3d0-d552-4a80-be09-67e5e29be8ee   <- e.g. Local Store path for the \n    |                                             `default` local Artifact Store                                           \n    \u2514\u2500\u2500 default_zen_store\n        |\n        \u2514\u2500\u2500 zenml.db         <- SQLite database where ZenML data (stacks, \n                                components, etc) are stored by default.\n```\n\nAs shown above, the global config directory stores the following information:\n\n1.  The `config.yaml` file stores the global configuration settings: the unique ZenML client ID, the active database configuration, the analytics-related options, the active Stack, and the active Workspace. This is an example of the `config.yaml` file contents immediately after initialization:\n\n    ```yaml\n    active_workspace_name: default\n    active_stack_id: ...\n    analytics_opt_in: true\n    store:\n      database: ...\n      url: ...\n      username: ...\n      ...\n    user_id: d980f13e-05d1-4765-92d2-1dc7eb7addb7\n    version: 0.13.2\n    ```\n2. The `local_stores` directory is where some \"local\" flavors of stack components, such as the local artifact store, or a local MLFlow experiment tracker, persist data locally. Every local stack component will have its own subdirectory here named after the stack component's unique UUID. One notable example is the local artifact store flavor that, when part of the active stack, stores all the artifacts generated by pipeline runs in the designated local directory.\n3. The `zenml.db` in the `default_zen_store` directory is the default SQLite database where ZenML stores"}
{"input": " all information about the stacks, stack components, custom stack component flavors, etc.\n\nIn addition to the above, you may also find the following files and folders under the global config directory, depending on what you do with ZenML:\n\n* `kubeflow` - this is where the Kubeflow orchestrators that are part of a stack store some of their configuration and logs.\n\n## Usage analytics\n\nIn order to help us better understand how the community uses ZenML, the pip package reports **anonymized** usage statistics. You can always opt out by using the CLI command:\n\n```bash\nzenml analytics opt-out\n```\n\n#### Why does ZenML collect analytics? <a href=\"#motivation\" id=\"motivation\"></a>\n\nIn addition to the community at large, **ZenML** is created and maintained by a startup based in Munich, Germany called [ZenML GmbH](https://zenml.io). We're a team of techies that love MLOps and want to build tools that fellow developers would love to use in their daily work. [This is us](https://zenml.io/company#CompanyTeam) if you want to put faces to the names!\n\nHowever, in order to improve **ZenML** and understand how it is being used, we need to use analytics to have an overview of how it is used 'in the wild'. This not only helps us find bugs but also helps us prioritize features and commands that might be useful in future releases. If we did not have this information, all we really get is pip download statistics and chatting with people directly, which while being valuable, is not enough to seriously better the tool as a whole.\n\n#### How does ZenML collect these statistics? <a href=\"#implementation\" id=\"implementation\"></a>\n\nWe use [Segment](https://segment.com) as the data aggregation library for all our analytics. However, before any events get sent to [Segment](https://segment.com), they first go through a central ZenML analytics server. This added layer allows us to put various countermeasures to incidents such as getting spammed with events and enables us to have a more optimized tracking process.\n\nThe client code is entirely visible and can be seen in the [`analytics`](https://github.com/zenml-io/zenml/tree/main/src/zenml/analytics) module of our main repository.\n\n#### If I share my email, will you spam me?\n\nNo, we won't. Our sole purpose of contacting you will be to ask for feedback (e.g. in the shape of a user interview). These interviews help the core team understand usage better and prioritize feature requests. If you have any concerns about data privacy and the usage of personal information, please [contact us](mailto:support@zenml.io), and we will try to alleviate any concerns as soon as possible.\n\n## Version mismatch (downgrading)\n\nIf you've recently downgraded your ZenML version to an earlier release or installed"}
{"input": " a newer version on a different environment on the same machine, you might encounter an error message when running ZenML that says:\n\n```shell\n`The ZenML global configuration version (%s) is higher than the version of ZenML \ncurrently being used (%s).`\n```\n\nWe generally recommend using the latest ZenML version. However, there might be cases where you need to match the global configuration version with the version of ZenML installed in the current environment. To do this, run the following command:\n\n```shell\nzenml downgrade\n```\n\n{% hint style=\"warning\" %}\nNote that downgrading the ZenML version may cause unexpected behavior, such as model schema validation failures or even data loss. In such cases, you may need to purge the local database and re-initialize the global configuration to bring it back to its default factory state. To do this, run the following command:\n\n```shell\nzenml clean\n```\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Find answers to the most frequently asked questions about ZenML.\n---\n\n# \u2753 FAQ\n\n#### Why did you build ZenML?\n\nWe built it because we scratched our own itch while deploying multiple machine-learning models in production over the past three years. Our team struggled to find a simple yet production-ready solution whilst developing large-scale ML pipelines. We built a solution for it that we are now proud to share with all of you! Read more about this backstory [on our blog here](https://blog.zenml.io/why-zenml/).\n\n#### Is ZenML just another orchestrator like Airflow, Kubeflow, Flyte, etc?\n\nNot really! An orchestrator in MLOps is the system component that is responsible for executing and managing the execution of an ML pipeline. ZenML is a framework that allows you to run your pipelines on whatever orchestrator you like, and we coordinate with all the other parts of an ML system in production. There are [standard orchestrators](../component-guide/orchestrators/orchestrators.md) that ZenML supports out-of-the-box, but you are encouraged to [write your own orchestrator](../component-guide/orchestrators/custom.md) in order to gain more control as to exactly how your pipelines are executed!\n\n#### Can I use the tool `X`? How does the tool `Y` integrate with ZenML?\n\nTake a look at our [documentation](https://docs.zenml.io) (in particular the [component guide](../component-guide/README.md)), which contains instructions and sample code to support each integration that ZenML supports out-of-the-box. You can also check out [our integration test code](https://github.com/zenml-io/zenml/tree/main/tests/integration/examples) to see active examples of many of our integrations in action.\n\nThe ZenML team and community are constantly working to include more tools and integrations to the above list (check out the [roadmap](https://zenml.io/roadmap) for more details). You can [upvote features](https://zenml.io/discussion) you'd like and add your ideas to the roadmap.\n\nMost importantly, ZenML is extensible, and we encourage you to use it with whatever other tools you require as part of your ML process and system(s). Check out [our documentation on how to get started](../introduction.md) with extending ZenML to learn more!\n\n#### Do you support Windows?\n\nZenML officially supports Windows if you're using WSL. Much of ZenML will also\nwork on Windows outside a WSL environment, but we don't officially support it\nand some features don't work (notably anything that requires spinning up a\nserver process).\n\n#### Do you support Macs running on Apple Silicon?\n\nYes, ZenML does support Macs running on Apple Silicon. You just need to make sure that you set the following environment variable:\n\n```bash"}
{"input": "\nexport OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n```\n\nThis is a known issue with how forking works on Macs running on Apple Silicon\nand it will enable you to use ZenML and the server. This environment variable is needed if you are working with a local server on your Mac, but if you're just using ZenML as a client / CLI and connecting to a deployed server then you don't need to set it.\n\n#### How can I make ZenML work with my custom tool? How can I extend or build on ZenML?\n\nThis depends on the tool and its respective MLOps category. We have a full guide on this over [here](../how-to/stack-deployment/implement-a-custom-stack-component.md)!\n\n#### How can I contribute?\n\nWe develop ZenML together with our community! To get involved, the best way to get started is to select any issue from the [`good-first-issue` label](https://github.com/zenml-io/zenml/labels/good%20first%20issue). If you would like to contribute, please review our [Contributing Guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md) for all relevant details.\n\n#### How can I speak with the community?\n\nThe first point of the call should be [our Slack group](https://zenml.io/slack/). Ask your questions about bugs or specific use cases and someone from the core team will respond.\n\n#### Which license does ZenML use?\n\nZenML is distributed under the terms of the Apache License Version 2.0. A complete version of the license is available in the [LICENSE.md](https://github.com/zenml-io/zenml/blob/main/LICENSE) in this repository. Any contribution made to this project will be licensed under the Apache License Version 2.0.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Links to common use cases, workflows and tasks using ZenML.\n---\n\n# \ud83d\udcda How do I...?\n\n**Last Updated**: December 13, 2023\n\nSome common questions that we get asked are:\n\n* **contribute** to ZenML's open-source codebase?\n\nPlease read [our Contribution guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md) for more information. For small features and bug fixes, please open a pull request as described in the guide. For anything bigger, it is worth [posting a message in Slack](https://zenml.io/slack/) or [creating an issue](https://github.com/zenml-io/zenml/issues/new/choose) so we can best discuss and support your plans.\n\n* **custom components**: adding them to ZenML?\n\nPlease start by [reading the general documentation page](../how-to/stack-deployment/implement-a-custom-stack-component.md) on implementing a custom stack component which offers some general advice on what you'll need to do.\n\nFrom there, each of the custom stack component types has a dedicated section about adding your own custom components. For example, for adding custom orchestrators you would [visit this page](../component-guide/orchestrators/custom.md).\n\n* **dependency clashes** mitigation with ZenML?\n\nCheck out [our dedicated documentation page](../how-to/configure-python-environments/handling-dependencies.md) on some ways you can try to solve these dependency and versioning issues.\n\n* **deploy cloud infrastructure** and/or MLOps stacks?\n\nZenML is designed to be stack-agnostic, so you can use it with any cloud infrastructure or MLOps stack. Each of the documentation pages for stack components explain how to deploy these components on the most popular cloud providers.\n\nWe also build and maintain [the `mlstacks` package](https://mlstacks.zenml.io/) and library which offers a dedicated way to spin up infrastructure for your ZenML pipelines. It's fully integrated into ZenML's CLI and is a great way to get started with deploying your infrastructure. ZenML also [publishes and maintains modules on the Terraform Registry](https://registry.terraform.io/namespaces/zenml-io) (which are used by `mlstacks` under the hood) which you can also use as a standalone solution if you are familiar with Terraform.\n\n* **deploy ZenML** on my internal company cluster?\n\nRead [the documentation on self-hosted ZenML deployments](../getting-started/deploying-zenml/README.md) in which several options are presented.\n\n* **hyperparameter tuning**?\n\n[Our dedicated documentation guide](../how-to/build-pipelines/hyper-parameter-tuning.md) on implementing this is the place to learn more.\n\n* **reset** things when something goes wrong?\n\nTo reset your ZenML client, you can run `zenml clean"}
{"input": "` which will wipe your local metadata database and reset your client. Note that this is a destructive action, so feel free to [reach out to us on Slack](https://zenml.io/slack/) before doing this if you are unsure.\n\n* **steps that create other steps AKA dynamic pipelines and steps**?\n\nPlease read our [general information on how to compose steps + pipelines together](../user-guide/starter-guide/create-an-ml-pipeline.md) to start with. You might also find the code examples in [our guide to implementing hyperparameter tuning](../how-to/build-pipelines/hyper-parameter-tuning.md) which is related to this topic.\n\n* **templates**: using starter code with ZenML?\n\n[Project templates](../how-to/setting-up-a-project-repository/using-project-templates.md) allow you to get going quickly with ZenML. We recommend the Starter template (`starter`) for most use cases which gives you a basic scaffold and structure around which you can write your own code. You can also build templates for others inside a Git repository and use them with ZenML's templates functionality.\n\n* **upgrade** my ZenML client and/or server?\n\nUpgrading your ZenML client package is as simple as running `pip install --upgrade zenml` in your terminal. For upgrading your ZenML server, please refer to [the dedicated documentation section](../getting-started/deploying-zenml/manage-the-deployed-services/upgrade-the-version-of-the-zenml-server.md) which covers most of the ways you might do this as well as common troubleshooting steps.\n\n* use a \\<YOUR\\_COMPONENT\\_GOES\\_HERE> stack component?\n\nFor information on how to use a specific stack component, please refer to [the component guide](../component-guide/README.md) which contains all our tips and advice on how to use each integration and component with ZenML.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: Interacting with your ZenML instance through the ZenML Client.\n---\n\n# \ud83d\udc0d Python Client\n\nPipelines, runs, stacks, and many other ZenML resources are stored and versioned in a database within your ZenML instance behind the scenes. The ZenML Python `Client` allows you to fetch, update, or even create any of these resources programmatically in Python.\n\n{% hint style=\"info\" %}\nIn all other programming languages and environments, you can interact with ZenML resources through the REST API endpoints of your ZenML server instead. Checkout the `/docs/` page of your server for an overview of all available endpoints.\n{% endhint %}\n\n### Usage Example\n\nThe following example shows how to use the ZenML Client to fetch the last 10 pipeline runs that you ran yourself on the stack that you have currently set:\n\n```python\nfrom zenml.client import Client\n\nclient = Client()\n\nmy_runs_on_current_stack = client.list_pipeline_runs(\n    stack_id=client.active_stack_model.id,  # on current stack\n    user_id=client.active_user.id,  # ran by you\n    sort_by=\"desc:start_time\",  # last 10\n    size=10,\n)\n\nfor pipeline_run in my_runs_on_current_stack:\n    print(pipeline_run.name)\n```\n\n### List of Resources\n\nThese are the main ZenML resources that you can interact with via the ZenML Client:\n\n#### Pipelines, Runs, Artifacts\n\n* **Pipelines**: The pipelines that were implicitly tracked when running ZenML pipelines.\n* **Pipeline Runs**: Information about all pipeline runs that were executed on your ZenML instance.\n* **Run Templates**: Templates to run pipelines from the server or dashboard.\n* **Step Runs**: The steps of all pipeline runs. Mainly useful for directly fetching a specific step of a run by its ID.\n* **Artifacts**: Information about all artifacts that were written to your artifact stores as part of pipeline runs.\n* **Schedules**: Metadata about the schedules that you have used to [schedule pipeline runs](../how-to/build-pipelines/schedule-a-pipeline.md).\n* **Builds**: The pipeline-specific Docker images that were created when [containerizing your pipeline](../how-to/customize-docker-builds/README.md).\n* **Code Repositories**: The git code repositories that you have connected with your ZenML instance. See [here](../user-guide/production-guide/connect-code-repository.md) for more information.\n\n{% hint style=\"info\" %}\nCheckout the [documentation on fetching runs](../how-to/build-pipelines/fetching-pipelines.md) for more information on the various ways how you can fetch and use the pipeline, pipeline run, step run, and artifact resources in code.\n{% endhint %}\n\n#### Stacks, Infrastructure, Authentication\n\n* **Stack**: The stacks registered in your ZenML instance.\n* **Stack Components**: The stack components registered in your Zen"}
{"input": "ML instance, e.g., all orchestrators, artifact stores, model deployers, ...\n* **Flavors**: The [stack component flavors](../getting-started/core-concepts.md#flavor) available to you, including:\n  * Built-in flavors like the [local orchestrator](../component-guide/orchestrators/local.md),\n  * Integration-enabled flavors like the [Kubeflow orchestrator](../component-guide/orchestrators/kubeflow.md),\n  * Custom flavors that you have [created yourself](../how-to/stack-deployment/implement-a-custom-stack-component.md).\n* **User**: The users registered in your ZenML instance. If you are running locally, there will only be a single `default` user.\n* **Secrets**: The infrastructure authentication secrets that you have registered in the [ZenML Secret Store](../how-to/interact-with-secrets.md).\n* **Service Connectors**: The service connectors that you have set up to [connect ZenML to your infrastructure](../how-to/auth-management/README.md).\n\n### Client Methods\n\n#### Reading and Writing Resources\n\n**List Methods**\n\nGet a list of resources, e.g.:\n\n```python\nclient.list_pipeline_runs(\n    stack_id=client.active_stack_model.id,  # filter by stack\n    user_id=client.active_user.id,  # filter by user\n    sort_by=\"desc:start_time\",  # sort by start time descending\n    size=10,  # limit page size to 10\n)\n```\n\nThese methods always return a [Page](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-models/#zenml.models.page\\_model) of resources, which behaves like a standard Python list and contains, by default, the first 50 results. You can modify the page size by passing the `size` argument or fetch a subsequent page by passing the `page` argument to the list method.\n\nYou can further restrict your search by passing additional arguments that will be used to filter the results. E.g., most resources have a `user_id` associated with them that can be set to only list resources created by that specific user. The available filter argument options are different for each list method; check out the method declaration in the [Client SDK documentation](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/) to find out which exact arguments are supported or have a look at the fields of the corresponding filter model class.\n\nExcept for pipeline runs, all other resources will by default be ordered by creation time ascending. E.g., `client.list_artifacts()` would return the first 50 artifacts ever created. You can change the ordering by specifying the `sort_by` argument when calling list methods.\n\n**Get Methods**\n\nFetch a specific instance of a resource by either resource ID, name, or name prefix, e.g.:\n\n```python\nclient.get_pipeline_run(\"413cfb"}
{"input": "42-a52c-4bf1-a2fd-78af2f7f0101\")  # ID\nclient.get_pipeline_run(\"first_pipeline-2023_06_20-16_20_13_274466\")  # Name\nclient.get_pipeline_run(\"first_pipeline-2023_06_20-16\")  # Name prefix\n```\n\n**Create, Update, and Delete Methods**\n\nMethods for creating / updating / deleting resources are only available for some of the resources and the required arguments are different for each resource. Checkout the [Client SDK Documentation](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-client/) to find out whether a specific resource supports write operations through the Client and which arguments are required.\n\n#### Active User and Active Stack\n\nFor some use cases you might need to know information about the user that you are authenticated as or the stack that you have currently set as active. You can fetch this information via the `client.active_user` and `client.active_stack_model` properties respectively, e.g.:\n\n```python\nmy_runs_on_current_stack = client.list_pipeline_runs(\n    stack_id=client.active_stack_model.id,  # on current stack\n    user_id=client.active_user.id,  # ran by you\n)\n```\n\n### Resource Models\n\nThe methods of the ZenML Client all return **Response Models**, which are [Pydantic Models](https://docs.pydantic.dev/latest/usage/models/) that allow ZenML to validate that the returned data always has the correct attributes and types. E.g., the `client.list_pipeline_runs` method always returns type `Page[PipelineRunResponseModel]`.\n\n{% hint style=\"info\" %}\nYou can think of these models as similar to types in strictly-typed languages, or as the requirements of a single endpoint in an API. In particular, they are **not related to machine learning models** like decision trees, neural networks, etc.\n{% endhint %}\n\nZenML also has similar models that define which information is required to create, update, or search resources, named **Request Models**, **Update Models**, and **Filter Models** respectively. However, these models are only used for the server API endpoints, and not for the Client methods.\n\n{% hint style=\"info\" %}\nTo find out which fields a specific resource model contains, checkout the [ZenML Models SDK Documentation](https://sdkdocs.zenml.io/latest/core\\_code\\_docs/core-models/#zenml.models) and expand the source code to see a list of all fields of the respective model. Note that all resources have **Base Models** that define fields that response, request, update, and filter models have in common, so you need to take a look at the base model source code as well.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f"}
{"input": "458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: All possible ways for our community to get in touch with ZenML.\n---\n\n# \ud83d\udc9c Community & content\n\nThe ZenML team and community have put together a list of references that can be used to get in touch with the\ndevelopment team of ZenML and develop a deeper understanding of the framework.\n\n### Slack Channel: Get help from the community\n\nThe ZenML [Slack channel](https://zenml.io/slack) is the main gathering point for the community. Not only is it\nthe best place to get in touch with the core team of ZenML, but it is also a great way to discuss new ideas and share\nyour ZenML projects with the community. If you have a question, there is a high chance someone else might have already\nanswered it on Slack!\n\n### Social Media: Bite-sized updates\n\nWe are active on [LinkedIn](https://www.linkedin.com/company/zenml) and [Twitter](https://twitter.com/zenml\\_io) where\nwe post bite-sized updates on releases, events, and MLOps in general. Follow us to interact and stay up to date! We\nwould appreciate it if you could comment on and share our posts so more people can benefit from our work at ZenML!\n\n### YouTube Channel: Video tutorials, workshops, and more\n\nOur [YouTube channel](https://www.youtube.com/c/ZenML) features a growing set of videos that take you through the entire\nframework. Go here if you are a visual learner, and follow along with some tutorials.\n\n### Public roadmap\n\nThe feedback from our community plays a significant role in the development of ZenML. That's why we have\na [public roadmap](https://zenml.io/roadmap) that serves as a bridge between our users and our development\nteam. If you have ideas regarding any new features or want to prioritize one over the other, feel free to share your\nthoughts here or vote on existing ideas.\n\n### Blog\n\nOn our [Blog](https://zenml.io/blog/) page, you can find various articles written by our team. We use it as a platform\nto share our thoughts and explain the implementation process of our tool, its new features, and the thought process\nbehind them.\n\n### Podcast\n\nWe also have a [Podcast](https://podcast.zenml.io/) series that brings you interviews and discussions with industry\nleaders, top technology professionals, and others. We discuss the latest developments in machine learning, deep\nlearning, and artificial intelligence, with a particular focus on MLOps, or how trained models are used in production.\n\n### Newsletter\n\nYou can also subscribe to our [Newsletter](https://zenml.io/newsletter-signup) where we share what we learn as we develop\nopen-source tooling for production machine learning. You will also get all the exciting news about ZenML in general.\n\n<!-- For scarf -->\n<figure><img alt=\"Zen"}
{"input": "ML Scarf\" referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" /></figure>\n"}
{"input": "---\ndescription: See the ZenML API reference.\n---\n\n# \ud83d\udc40 API reference\n\nThe ZenML server is a FastAPI application, therefore the OpenAPI-compliant docs are available at `/docs` or `/redoc`\nof your ZenML server:\n\n{% hint style=\"info\" %}\nIn the local case (i.e. using `zenml up`, the docs are available on `http://127.0.0.1:8237/docs`)\n{% endhint %}\n\n![ZenML API docs](../.gitbook/assets/zenml_api_docs.png)\n\n![ZenML API Redoc](../.gitbook/assets/zenml_api_redoc.png)\n\n## Using a bearer token to access the API programmatically\n\nIf you are using the ZenML server API using the above pages, it is enough to be logged in to your ZenML\naccount in the same browser session. However, in order to do this programmatically, the following steps\nneed to be followed:\n\n1. Create a [service account](../how-to/connecting-to-zenml/connect-with-a-service-account.md):\n\n```shell\nzenml service-account create myserviceaccount\n```\n\nThis will print out the `<ZENML_API_KEY>`, you can use in the next command.\n\n2. Get an access token by using the `/api/v1/login` endpoint:\n\n```shell\n\ncurl -X 'POST' \\\n  '<YOUR_ZENML_SERVER_URL>/api/v1/login' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/x-www-form-urlencoded' \\\n  -d 'grant_type=zenml-api-key&username=&password=<ZENML_API_KEY>'\\''&client_id=&device_code='\n```\n\nThis will return a response like this:\n\n```json\n{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI3MGJjZTg5NC1hN2VjLTRkOTYtYjE1Ny1kOTZkYWY5ZWM2M2IiLCJpc3MiOiJmMGQ5NjI1Ni04YmQyLTQxZDctOWVjZi0xMmYwM2JmYTVlMTYiLCJhdWQiOiJmMGQ5NjI1Ni04YmQyLTQxZDctOWVjZi0xMmYwM2JmYTVlMTYiLCJleHAiOjE3MTk0MDk0NjAsImFwaV9rZXlfaWQiOiIzNDkyM2U0NS0zMGFlLTRkMjctODZiZS0wZGR"}
{"input": "hNTdkMjA5MDcifQ.ByB1ngCPtBenGE6UugsWC6Blga3qPqkAiPJUSFDR-u4\",\n  \"token_type\": \"bearer\",\n  \"expires_in\": 3600,\n  \"refresh_token\": null,\n  \"scope\": null\n}\n```\n\n3. Use the access token to make subsequent curl commands:\n\n```shell\ncurl -X 'GET' \\\n  '<YOUR_ZENML_SERVER_URL>/api/v1/pipelines?hydrate=false&name=training' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer <ACCESS_TOKEN>'\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to migrate your ZenML pipelines and steps from version <=0.39.1 to 0.41.0.\n---\n\n# Migration guide 0.39.1 \u2192 0.41.0\n\nZenML versions 0.40.0 to 0.41.0 introduced a new and more flexible syntax to define ZenML steps and pipelines. This page contains code samples that show you how to upgrade your steps and pipelines to the new syntax.\n\n{% hint style=\"warning\" %}\nNewer versions of ZenML still work with pipelines and steps defined using the old syntax, but the old syntax is deprecated and will be removed in the future.\n{% endhint %}\n\n## Overview\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom typing import Optional\n\nfrom zenml.steps import BaseParameters, Output, StepContext, step\nfrom zenml.pipelines import pipeline\n\n# Define a Step\nclass MyStepParameters(BaseParameters):\n    param_1: int\n    param_2: Optional[float] = None\n\n@step\ndef my_step(\n    params: MyStepParameters, context: StepContext,\n) -> Output(int_output=int, str_output=str):\n    result = int(params.param_1 * (params.param_2 or 1))\n    result_uri = context.get_output_artifact_uri()\n    return result, result_uri\n\n# Run the Step separately\nmy_step.entrypoint()\n\n# Define a Pipeline\n@pipeline\ndef my_pipeline(my_step):\n    my_step()\n\nstep_instance = my_step(params=MyStepParameters(param_1=17))\npipeline_instance = my_pipeline(my_step=step_instance)\n\n# Configure and run the Pipeline\npipeline_instance.configure(enable_cache=False)\nschedule = Schedule(...)\npipeline_instance.run(schedule=schedule)\n\n# Fetch the Pipeline Run\nlast_run = pipeline_instance.get_runs()[0]\nint_output = last_run.get_step[\"my_step\"].outputs[\"int_output\"].read()\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom typing import Annotated, Optional, Tuple\n\nfrom zenml import get_step_context, pipeline, step\nfrom zenml.client import Client\n\n# Define a Step\n@step\ndef my_step(\n    param_1: int, param_2: Optional[float] = None\n) -> Tuple[Annotated[int, \"int_output\"], Annotated[str, \"str_output\"]]:\n    result = int(param_1 * (param_2 or 1))\n    result_uri = get_step_context().get_output_artifact_uri()\n    return result, result_uri\n\n# Run the Step separately\nmy_step()\n\n# Define a Pipeline\n@pipeline\ndef my_pipeline():\n    my_step(param_1=17)\n\n# Configure and run the Pipeline\nmy_pipeline = my_pipeline.with_options(enable_cache=False, schedule=schedule)\nmy_pipeline()\n\n# Fetch the Pipeline Run\nlast_run = my_pipeline.last_run\n"}
{"input": "int_output = last_run.steps[\"my_step\"].outputs[\"int_output\"].load()\n```\n{% endtab %}\n{% endtabs %}\n\n## Defining steps\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.steps import step, BaseParameters\nfrom zenml.pipelines import pipeline\n\n# Old: Subclass `BaseParameters` to define parameters for a step\nclass MyStepParameters(BaseParameters):\n    param_1: int\n    param_2: Optional[float] = None\n\n@step\ndef my_step(params: MyStepParameters) -> None:\n    ...\n\n@pipeline\ndef my_pipeline(my_step):\n    my_step()\n\nstep_instance = my_step(params=MyStepParameters(param_1=17))\npipeline_instance = my_pipeline(my_step=step_instance)\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\n# New: Directly define the parameters as arguments of your step function.\n# In case you still want to group your parameters in a separate class,\n# you can subclass `pydantic.BaseModel` and use that as an argument of your\n# step function\nfrom zenml import pipeline, step\n\n@step\ndef my_step(param_1: int, param_2: Optional[float] = None) -> None:\n    ...\n\n@pipeline\ndef my_pipeline():\n    my_step(param_1=17)\n```\n{% endtab %}\n{% endtabs %}\n\nCheck out [this page](../../how-to/build-pipelines/use-pipeline-step-parameters.md) for more information on how to parameterize your steps.\n\n## Calling a step outside of a pipeline\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.steps import step\n\n@step\ndef my_step() -> None:\n    ...\n\nmy_step.entrypoint()  # Old: Call `step.entrypoint(...)`\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml import step\n\n@step\ndef my_step() -> None:\n    ...\n\nmy_step()  # New: Call the step directly `step(...)`\n```\n{% endtab %}\n{% endtabs %}\n\n## Defining pipelines\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.pipelines import pipeline\n\n@pipeline\ndef my_pipeline(my_step):  # Old: steps are arguments of the pipeline function\n    my_step()\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml import pipeline, step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline():\n    my_step()  # New: The pipeline function calls the step directly\n```\n{% endtab %}\n{% endtabs %}\n\n## Configuring pipelines\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.pipelines import pipeline\nfrom"}
{"input": " zenml.steps import step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline(my_step):\n    my_step()\n\n# Old: Create an instance of the pipeline and then call `pipeline_instance.configure(...)`\npipeline_instance = my_pipeline(my_step=my_step())\npipeline_instance.configure(enable_cache=False)\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml import pipeline, step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline():\n    my_step()\n\n# New: Call the `with_options(...)` method on the pipeline\nmy_pipeline = my_pipeline.with_options(enable_cache=False)\n```\n{% endtab %}\n{% endtabs %}\n\n## Running pipelines\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.pipelines import pipeline\nfrom zenml.steps import step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline(my_step):\n    my_step()\n\n# Old: Create an instance of the pipeline and then call `pipeline_instance.run(...)`\npipeline_instance = my_pipeline(my_step=my_step())\npipeline_instance.run(...)\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml import pipeline, step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline():\n    my_step()\n\nmy_pipeline()  # New: Call the pipeline\n```\n{% endtab %}\n{% endtabs %}\n\n## Scheduling pipelines\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.pipelines import pipeline, Schedule\nfrom zenml.steps import step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline(my_step):\n    my_step()\n\n# Old: Create an instance of the pipeline and then call `pipeline_instance.run(schedule=...)`\nschedule = Schedule(...)\npipeline_instance = my_pipeline(my_step=my_step())\npipeline_instance.run(schedule=schedule)\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml.pipelines import Schedule\nfrom zenml import pipeline, step\n\n@step\ndef my_step() -> None:\n    ...\n\n@pipeline\ndef my_pipeline():\n    my_step()\n\n# New: Set the schedule using the `pipeline.with_options(...)` method and then run it\nschedule = Schedule(...)\nmy_pipeline = my_pipeline.with_options(schedule=schedule)\nmy_pipeline()\n```\n{% endtab %}\n{% endtabs %}\n\nCheck out [this page](../../how-to/build-pipelines/schedule-a-pipeline.md) for more information on how to schedule your pipelines.\n\n## Fetching pipelines after execution\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\npipeline: PipelineView = zenml.post_execution.get_pipeline(\"first_pipeline\")\n\nlast_run: PipelineRunView = pipeline.runs[0]\n#"}
{"input": " OR: last_run = my_pipeline.get_runs()[0]\n\nmodel_trainer_step: StepView = last_run.get_step(\"model_trainer\")\n\nmodel: ArtifactView = model_trainer_step.output\nloaded_model = model.read()\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\npipeline: PipelineResponseModel = zenml.client.Client().get_pipeline(\"first_pipeline\")\n# OR: pipeline = pipeline_instance.model\n\nlast_run: PipelineRunResponseModel = pipeline.last_run  \n# OR: last_run = pipeline.runs[0] \n# OR: last_run = pipeline.get_runs(custom_filters)[0] \n# OR: last_run = pipeline.last_successful_run\n\nmodel_trainer_step: StepRunResponseModel = last_run.steps[\"model_trainer\"]\n\nmodel: ArtifactResponseModel = model_trainer_step.output\nloaded_model = model.load()\n```\n{% endtab %}\n{% endtabs %}\n\nCheck out [this page](../../how-to/track-metrics-metadata/fetch-metadata-within-steps.md) for more information on how to programmatically fetch information about previous pipeline runs.\n\n## Controlling the step execution order\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.pipelines import pipeline\n\n@pipeline\ndef my_pipeline(step_1, step_2, step_3):\n    step_1()\n    step_2()\n    step_3()\n    step_3.after(step_1)  # Old: Use the `step.after(...)` method\n    step_3.after(step_2)\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml import pipeline\n\n@pipeline\ndef my_pipeline():\n    step_1()\n    step_2()\n    step_3(after=[\"step_1\", \"step_2\"])  # New: Pass the `after` argument when calling a step\n```\n{% endtab %}\n{% endtabs %}\n\nCheck out [this page](../../how-to/build-pipelines/control-execution-order-of-steps.md) for more information on how to control the step execution order.\n\n## Defining steps with multiple outputs\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\n# Old: Use the `Output` class\nfrom zenml.steps import step, Output\n\n@step\ndef my_step() -> Output(int_output=int, str_output=str):\n    ...\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\n# New: Use a `Tuple` annotation and optionally assign custom output names\nfrom typing_extensions import Annotated\nfrom typing import Tuple\nfrom zenml import step\n\n# Default output names `output_0`, `output_1`\n@step\ndef my_step() -> Tuple[int, str]:\n    ...\n\n# Custom output names\n@step\ndef my_step() -> Tuple[\n    Annotated[int, \"int_output\"],\n"}
{"input": "    Annotated[str, \"str_output\"],\n]:\n    ...\n```\n{% endtab %}\n{% endtabs %}\n\nCheck out [this page](../../how-to/build-pipelines/step-output-typing-and-annotation.md) for more information on how to annotate your step outputs.\n\n## Accessing run information inside steps\n\n{% tabs %}\n{% tab title=\"Old Syntax\" %}\n```python\nfrom zenml.steps import StepContext, step\nfrom zenml.environment import Environment\n\n@step\ndef my_step(context: StepContext) -> Any:  # Old: `StepContext` class defined as arg\n    env = Environment().step_environment\n    output_uri = context.get_output_artifact_uri()\n    step_name = env.step_name  # Old: Run info accessible via `StepEnvironment`\n    ...\n```\n{% endtab %}\n\n{% tab title=\"New Syntax\" %}\n```python\nfrom zenml import get_step_context, step\n\n@step\ndef my_step() -> Any:  # New: StepContext is no longer an argument of the step\n    context = get_step_context()\n    output_uri = context.get_output_artifact_uri()\n    step_name = context.step_name  # New: StepContext now has ALL run/step info\n    ...\n```\n{% endtab %}\n{% endtabs %}\n\nCheck out [this page](../../how-to/track-metrics-metadata/fetch-metadata-within-steps.md) for more information on how to fetch run information inside your steps using `get_step_context()`.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to migrate from ZenML <=0.13.2 to 0.20.0.\n---\n\n# Migration guide 0.13.2 \u2192 0.20.0\n\n*Last updated: 2023-07-24*\n\nThe ZenML 0.20.0 release brings a number of big changes to its architecture and its features, some of which are not backwards compatible with previous versions. This guide walks you through these changes and offers instructions on how to migrate your existing ZenML stacks and pipelines to the new version with minimal effort and disruption to your existing workloads.\n\n{% hint style=\"warning\" %}\nUpdating to ZenML 0.20.0 needs to be followed by a migration of your existing ZenML Stacks and you may also need to make changes to your current ZenML pipeline code. Please read this guide carefully and follow the migration instructions to ensure a smooth transition.\n\nIf you have updated to ZenML 0.20.0 by mistake or are experiencing issues with the new version, you can always go back to the previous version by using `pip install zenml==0.13.2` instead of `pip install zenml` when installing ZenML manually or in your scripts.\n{% endhint %}\n\nHigh-level overview of the changes:\n\n* [ZenML takes over the Metadata Store](migration-zero-twenty.md#zenml-takes-over-the-metadata-store-role) role. All information about your ZenML Stacks, pipelines, and artifacts is tracked by ZenML itself directly. If you are currently using remote Metadata Stores (e.g. deployed in cloud) in your stacks, you will probably need to replace them with a [ZenML server deployment](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md).\n* the [new ZenML Dashboard](migration-zero-twenty.md#the-zenml-dashboard-is-now-available) is now available with all ZenML deployments.\n* [ZenML Profiles have been removed](migration-zero-twenty.md#removal-of-profiles-and-the-local-yaml-database) in favor of ZenML Projects. You need to [manually migrate your existing ZenML Profiles](migration-zero-twenty.md#-how-to-migrate-your-profiles) after the update.\n* the [configuration of Stack Components is now decoupled from their implementation](migration-zero-twenty.md#decoupling-stack-component-configuration-from-implementation). If you extended ZenML with custom stack component implementations, you may need to update the way they are registered in ZenML.\n* the updated ZenML server provides a new and improved collaborative experience. When connected to a ZenML server, you can now [share your ZenML Stacks and Stack Components](migration-zero-twenty.md#shared-zenml-stacks-and-stack-components) with other users. If you were previously using the ZenML Profiles or the ZenML server to share your ZenML"}
{"input": " Stacks, you should switch to the new ZenML server and Dashboard and update your existing workflows to reflect the new features.\n\n## ZenML takes over the Metadata Store role\n\nZenML can now run [as a server](../../user-guide/getting-started/core-concepts.md#zenml-server-and-dashboard) that can be accessed via a REST API and also comes with a visual user interface (called the ZenML Dashboard). This server can be deployed in arbitrary environments (local, on-prem, via Docker, on AWS, GCP, Azure etc.) and supports user management, workspace scoping, and more.\n\nThe release introduces a series of commands to facilitate managing the lifecycle of the ZenML server and to access the pipeline and pipeline run information:\n\n* `zenml connect / disconnect / down / up / logs / status` can be used to configure your client to connect to a ZenML server, to start a local ZenML Dashboard or to deploy a ZenML server to a cloud environment. For more information on how to use these commands, see [the ZenML deployment documentation](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md).\n* `zenml pipeline list / runs / delete` can be used to display information and about and manage your pipelines and pipeline runs.\n\nIn ZenML 0.13.2 and earlier versions, information about pipelines and pipeline runs used to be stored in a separate stack component called the Metadata Store. Starting with 0.20.0, the role of the Metadata Store is now taken over by ZenML itself. This means that the Metadata Store is no longer a separate component in the ZenML architecture, but rather a part of the ZenML core, located wherever ZenML is deployed: locally on your machine or running remotely as a server.\n\nAll metadata is now stored, tracked, and managed by ZenML itself. The Metadata Store stack component type and all its implementations have been deprecated and removed. It is no longer possible to register them or include them in ZenML stacks. This is a key architectural change in ZenML 0.20.0 that further improves usability, reproducibility and makes it possible to visualize and manage all your pipelines and pipeline runs in the new ZenML Dashboard.\n\nThe architecture changes for the local case are shown in the diagram below:\n\n![ZenML local metadata before 0.20.0](../../user-guide/assets/migration/local-metadata-pre-0.20.png) ![ZenML local metadata after 0.20.0](../../user-guide/assets/migration/local-metadata-post-0.20.png)\n\nThe architecture changes for the remote case are shown in the diagram below:\n\n![ZenML remote metadata before 0.20.0](../../user-guide/assets/migration/remote-metadata-pre-0.20.png) ![ZenML remote metadata after 0.20.0](../../user-guide/assets/migration/remote-metadata-post-0"}
{"input": ".20.png)\n\nIf you're already using ZenML, aside from the above limitation, this change will impact you differently, depending on the flavor of Metadata Stores you have in your stacks:\n\n* if you're using the default `sqlite` Metadata Store flavor in your stacks, you don't need to do anything. ZenML will automatically switch to using its local database instead of your `sqlite` Metadata Stores when you update to 0.20.0 (also see how to [migrate your stacks](migration-zero-twenty.md#-how-to-migrate-your-profiles)).\n* if you're using the `kubeflow` Metadata Store flavor _only as a way to connect to the local Kubeflow Metadata Service_ (i.e. the one installed by the `kubeflow` Orchestrator in a local k3d Kubernetes cluster), you also don't need to do anything explicitly. When you [migrate your stacks](migration-zero-twenty.md#-how-to-migrate-your-profiles) to ZenML 0.20.0, ZenML will automatically switch to using its local database.\n* if you're using the `kubeflow` Metadata Store flavor to connect to a remote Kubeflow Metadata Service such as those provided by a Kubeflow installation running in AWS, Google or Azure, there is currently no equivalent in ZenML 0.20.0. You'll need to [deploy a ZenML Server](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md) instance close to where your Kubeflow service is running (e.g. in the same cloud region).\n* if you're using the `mysql` Metadata Store flavor to connect to a remote MySQL database service (e.g. a managed AWS, GCP or Azure MySQL service), you'll have to [deploy a ZenML Server](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md) instance connected to that same database.\n* if you deployed a `kubernetes` Metadata Store flavor (i.e. a MySQL database service deployed in Kubernetes), you can [deploy a ZenML Server](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md) in the same Kubernetes cluster and connect it to that same database. However, ZenML will no longer provide the `kubernetes` Metadata Store flavor and you'll have to manage the Kubernetes MySQL database service deployment yourself going forward.\n\n{% hint style=\"info\" %}\nThe ZenML Server inherits the same limitations that the Metadata Store had prior to ZenML 0.20.0:\n\n* it is not possible to use a local ZenML Server to track pipelines and pipeline runs that are running remotely in the cloud, unless the ZenML server is explicitly configured to be reachable from the cloud (e.g. by using a public IP address or a VPN connection).\n*"}
{"input": " using a remote ZenML Server to track pipelines and pipeline runs that are running locally is possible, but can have significant performance issues due to the network latency.\n\nIt is therefore recommended that you always use a ZenML deployment that is located as close as possible to and reachable from where your pipelines and step operators are running. This will ensure the best possible performance and usability.\n{% endhint %}\n\n### \ud83d\udc63 How to migrate pipeline runs from your old metadata stores\n\n{% hint style=\"info\" %}\nThe `zenml pipeline runs migrate` CLI command is only available under ZenML versions \\[0.21.0, 0.21.1, 0.22.0]. If you want to migrate your existing ZenML runs from `zenml<0.20.0` to `zenml>0.22.0`, please first upgrade to `zenml==0.22.0` and migrate your runs as shown below, then upgrade to the newer version.\n{% endhint %}\n\nTo migrate the pipeline run information already stored in an existing metadata store to the new ZenML paradigm, you can use the `zenml pipeline runs migrate` CLI command.\n\n1. Before upgrading ZenML, make a backup of all metadata stores you want to migrate, then upgrade ZenML.\n2. Decide the ZenML deployment model that you want to follow for your projects. See the [ZenML deployment documentation](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md) for available deployment scenarios. If you decide on using a local or remote ZenML server to manage your pipelines, make sure that you first connect your client to it by running `zenml connect`.\n3. Use the `zenml pipeline runs migrate` CLI command to migrate your old pipeline runs:\n\n* If you want to migrate from a local SQLite metadata store, you only need to pass the path to the metadata store to the command, e.g.:\n\n```bash\nzenml pipeline runs migrate PATH/TO/LOCAL/STORE/metadata.db\n```\n\n* If you would like to migrate any other store, you will need to set `--database_type=mysql` and provide the MySQL host, username, and password in addition to the database, e.g.:\n\n```bash\nzenml pipeline runs migrate DATABASE_NAME \\\n  --database_type=mysql \\\n  --mysql_host=URL/TO/MYSQL \\\n  --mysql_username=MYSQL_USERNAME \\\n  --mysql_password=MYSQL_PASSWORD\n```\n\n### \ud83d\udcbe The New Way (CLI Command Cheat Sheet)\n\n**Deploy the server**\n\n`zenml deploy --aws` (maybe don\u2019t do this :) since it spins up infrastructure on AWS\u2026)\n\n**Spin up a local ZenML Server**\n\n`zenml up`\n\n**Connect to a pre-existing server**\n\n`zenml connect` (pass in URL / etc, or zenml connect --config + yaml file)\n\n**List your deployed server details**\n\n`zen"}
{"input": "ml status`\n\n## The ZenML Dashboard is now available\n\nThe new ZenML Dashboard is now bundled into the ZenML Python package and can be launched directly from Python. The source code lives in the [ZenML Dashboard repository](https://github.com/zenml-io/zenml-dashboard).\n\nTo launch it locally, simply run `zenml up` on your machine and follow the instructions:\n\n```bash\n$ zenml up\nDeploying a local ZenML server with name 'local'.\nConnecting ZenML to the 'local' local ZenML server (http://127.0.0.1:8237).\nUpdated the global store configuration.\nConnected ZenML to the 'local' local ZenML server (http://127.0.0.1:8237).\nThe local ZenML dashboard is available at 'http://127.0.0.1:8237'. You can\nconnect to it using the 'default' username and an empty password.\n```\n\nThe Dashboard will be available at `http://localhost:8237` by default:\n\n![ZenML Dashboard Preview](../../user-guide/assets/migration/zenml-dashboard.png)\n\nFor more details on other possible deployment options, see the [ZenML deployment documentation](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md), and/or follow the [starter guide](../../user-guide/starter-guide/pipelines/pipelines.md) to learn more.\n\n## Removal of Profiles and the local YAML database\n\nPrior to 0.20.0, ZenML used used a set of local YAML files to store information about the Stacks and Stack Components that were registered on your machine. In addition to that, these Stacks could be grouped together and organized under individual Profiles.\n\nProfiles and the local YAML database have both been deprecated and removed in ZenML 0.20.0. Stack, Stack Components as well as all other information that ZenML tracks, such as Pipelines and Pipeline Runs, are now stored in a single SQL database. These entities are no longer organized into Profiles, but they can be scoped into different Projects instead.\n\n{% hint style=\"warning\" %}\nSince the local YAML database is no longer used by ZenML 0.20.0, you will lose all the Stacks and Stack Components that you currently have configured when you update to ZenML 0.20.0. If you still want to use these Stacks, you will need to [manually migrate](migration-zero-twenty.md#-how-to-migrate-your-profiles) them after the update.\n{% endhint %}\n\n### \ud83d\udc63 How to migrate your Profiles\n\nIf you're already using ZenML, you can migrate your existing Profiles to the new ZenML 0.20.0 paradigm by following these steps:\n\n1. first, update ZenML to 0.20.0. This will automatically invalidate all your existing Profiles.\n2. decide the ZenML"}
{"input": " deployment model that you want to follow for your projects. See the [ZenML deployment documentation](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md) for available deployment scenarios. If you decide on using a local or remote ZenML server to manage your pipelines, make sure that you first connect your client to it by running `zenml connect`.\n3. use the `zenml profile list` and `zenml profile migrate` CLI commands to import the Stacks and Stack Components from your Profiles into your new ZenML deployment. If you have multiple Profiles that you would like to migrate, you can either use a prefix for the names of your imported Stacks and Stack Components, or you can use a different ZenML Project for each Profile.\n\n{% hint style=\"warning\" %}\nThe ZenML Dashboard is currently limited to showing only information that is available in the `default` Project. If you wish to migrate your Profiles to a different Project, you will not be able to visualize the migrated Stacks and Stack Components in the Dashboard. This will be fixed in a future release.\n{% endhint %}\n\nOnce you've migrated all your Profiles, you can delete the old YAML files.\n\nExample of migrating a `default` profile into the `default` project:\n\n```bash\n$ zenml profile list\nZenML profiles have been deprecated and removed in this version of ZenML. All\nstacks, stack components, flavors etc. are now stored and managed globally,\neither in a local database or on a remote ZenML server (see the `zenml up` and\n`zenml connect` commands). As an alternative to profiles, you can use projects\nas a scoping mechanism for stacks, stack components and other ZenML objects.\n\nThe information stored in legacy profiles is not automatically migrated. You can\ndo so manually by using the `zenml profile list` and `zenml profile migrate` commands.\nFound profile with 1 stacks, 3 components and 0 flavors at: /home/stefan/.config/zenml/profiles/default\nFound profile with 3 stacks, 6 components and 0 flavors at: /home/stefan/.config/zenml/profiles/zenprojects\nFound profile with 3 stacks, 7 components and 0 flavors at: /home/stefan/.config/zenml/profiles/zenbytes\n\n$ zenml profile migrate /home/stefan/.config/zenml/profiles/default\nNo component flavors to migrate from /home/stefan/.config/zenml/profiles/default/stacks.yaml...\nMigrating stack components from /home/stefan/.config/zenml/profiles/default/stacks.yaml...\nCreated artifact_store 'cloud_artifact_store' with flavor 's3'.\nCreated container_registry 'cloud_registry' with flavor 'aws'.\nCreated container_registry 'local_registry' with flavor 'default'.\nCreated model_deployer '"}
{"input": "eks_seldon' with flavor 'seldon'.\nCreated orchestrator 'cloud_orchestrator' with flavor 'kubeflow'.\nCreated orchestrator 'kubeflow_orchestrator' with flavor 'kubeflow'.\nCreated secrets_manager 'aws_secret_manager' with flavor 'aws'.\nMigrating stacks from /home/stefan/.config/zenml/profiles/v/stacks.yaml...\nCreated stack 'cloud_kubeflow_stack'.\nCreated stack 'local_kubeflow_stack'.\n\n$ zenml stack list\nUsing the default local database.\nRunning with active project: 'default' (global)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 STACK NAME           \u2502 STACK ID                             \u2502 SHARED \u2502 OWNER   \u2502 CONTAINER_REGISTRY \u2502 ARTIFACT_STORE       \u2502 ORCHESTRATOR          \u2502 MODEL_DEPLOYER \u2502 SECRETS_MANAGER    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 local_kubeflow_stack \u2502 067cc6ee-b4da-410d-b7ed-06da4c983145 \u2502        \u2502 default \u2502 local_registry     \u2502 default              \u2502 kubeflow_orchestrator \u2502                \u2502                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 cloud_kubeflow_stack \u2502 054f5efb-9e80-48c0-852e-5114b1165d8b \u2502        \u2502 default \u2502 cloud_registry     \u2502 cloud_artifact_store \u2502 cloud_orchestrator    \u2502 eks_seldon     \u2502 aws_secret_manager \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default              \u2502 fe913bb5-e631-4"}
{"input": "d4e-8c1b-936518190ebb \u2502        \u2502 default \u2502                    \u2502 default              \u2502 default               \u2502                \u2502                    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nExample of migrating a profile into the `default` project using a name prefix:\n\n```bash\n$ zenml profile migrate /home/stefan/.config/zenml/profiles/zenbytes --prefix zenbytes_\nNo component flavors to migrate from /home/stefan/.config/zenml/profiles/zenbytes/stacks.yaml...\nMigrating stack components from /home/stefan/.config/zenml/profiles/zenbytes/stacks.yaml...\nCreated artifact_store 'zenbytes_s3_store' with flavor 's3'.\nCreated container_registry 'zenbytes_ecr_registry' with flavor 'default'.\nCreated experiment_tracker 'zenbytes_mlflow_tracker' with flavor 'mlflow'.\nCreated experiment_tracker 'zenbytes_mlflow_tracker_local' with flavor 'mlflow'.\nCreated model_deployer 'zenbytes_eks_seldon' with flavor 'seldon'.\nCreated model_deployer 'zenbytes_mlflow' with flavor 'mlflow'.\nCreated orchestrator 'zenbytes_eks_orchestrator' with flavor 'kubeflow'.\nCreated secrets_manager 'zenbytes_aws_secret_manager' with flavor 'aws'.\nMigrating stacks from /home/stefan/.config/zenml/profiles/zenbytes/stacks.yaml...\nCreated stack 'zenbytes_aws_kubeflow_stack'.\nCreated stack 'zenbytes_local_with_mlflow'.\n\n$ zenml stack list\nUsing the default local database.\nRunning with active project: 'default' (global)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 STACK NAME           \u2502 STACK ID             \u2502 SHARED \u2502 OWNER   \u2502 ORCHE"}
{"input": "STRATOR          \u2502 ARTIFACT_STORE    \u2502 CONTAINER_REGISTRY   \u2502 SECRETS_MANAGER       \u2502 MODEL_DEPLOYER      \u2502 EXPERIMENT_TRACKER   \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 zenbytes_aws_kubeflo \u2502 9fe90f0b-2a79-47d9-8 \u2502        \u2502 default \u2502 zenbytes_eks_orchestr \u2502 zenbytes_s3_store \u2502 zenbytes_ecr_registr \u2502 zenbytes_aws_secret_m \u2502 zenbytes_eks_seldon \u2502                      \u2503\n\u2503        \u2502 w_stack              \u2502 f80-04e45ff02cdb     \u2502        \u2502         \u2502 ator                  \u2502                   \u2502 y                    \u2502 anager                \u2502                     \u2502                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default              \u2502 7a587e0c-30fd-402f-a \u2502        \u2502 default \u2502 default               \u2502 default           \u2502                      \u2502                       \u2502                     \u2502                      \u2503\n\u2503        \u2502                      \u2502 3a8-03651fe1458f     \u2502        \u2502         \u2502                       \u2502                   \u2502                      \u2502                       \u2502                     \u2502                      \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 zenbytes_local_with_ \u2502 c2acd029-8eed-4b6e-a \u2502        \u2502 default \u2502 default               \u2502 default           \u2502                      \u2502                       \u2502 zenbytes_mlflow     \u2502 zenbytes_mlflow_trac \u2503\n\u2503        \u2502 mlflow               \u2502 d19-91c419ce91d4     \u2502        \u2502         \u2502                       \u2502                   \u2502                      \u2502                       \u2502                     \u2502 ker                  \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nExample of migrating a profile into a new project:\n\n```bash\n$ zenml profile migrate /home/stefan/.config/zenml/profiles/zenprojects --project zenprojects\nUnable to find ZenML repository in your current working directory (/home/stefan/aspyre/src/zenml) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run zenml init.\nRunning without an active repository root.\nCreating project zenprojects\nCreating default stack for user 'default' in project zenprojects...\nNo component flavors to migrate from /home/stefan/.config/zenml/profiles/zenprojects/stacks.yaml...\nMigrating stack components from /home/stefan/.config/zenml/profiles/zenprojects/stacks.yaml...\nCreated artifact_store 'cloud_artifact_store' with flavor 's3'.\nCreated container_registry 'cloud_registry' with flavor 'aws'.\nCreated container_registry 'local_registry' with flavor 'default'.\nCreated model_deployer 'eks_seldon' with flavor 'seldon'.\nCreated orchestrator 'cloud_orchestrator' with flavor 'kubeflow'.\nCreated orchestrator 'kubeflow_orchestrator' with flavor 'kubeflow'.\nCreated secrets_manager 'aws_secret_manager' with flavor 'aws'.\nMigrating stacks from /home/stefan/.config/zenml/profiles/zenprojects/stacks.yaml...\nCreated stack 'cloud_kubeflow_stack'.\nCreated stack 'local_kubeflow_stack'.\n\n$ zenml project set zenprojects\nCurrently the concept of `project` is not supported within the Dashboard. The Project functionality will be completed in the coming weeks. For the time being it is recommended to stay within the `default` \nproject.\nUsing the default local database.\nRunning with active project: 'default' (global)\nSet active project 'zenprojects'.\n\n$ zenml stack list\nUsing the default local database.\nRunning with active project: 'zenprojects' (global)\nThe current global active stack is not part of the active project. Resetting the active stack to default.\nYou are running with a non-default project 'zenprojects'. Any stacks, components, pipelines and pipeline runs produced in this project will currently not be accessible through the dashboard. However, this will be possible in the near future.\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501"}
{"input": "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ACTIVE \u2502 STACK NAME           \u2502 STACK ID                             \u2502 SHARED \u2502 OWNER   \u2502 ARTIFACT_STORE       \u2502 ORCHESTRATOR          \u2502 MODEL_DEPLOYER \u2502 CONTAINER_REGISTRY \u2502 SECRETS_MANAGER    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503   \ud83d\udc49   \u2502 default              \u2502 3ea77330-0c75-49c8-b046-4e971f45903a \u2502        \u2502 default \u2502 default              \u2502 default               \u2502                \u2502                    \u2502                    \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 cloud_kubeflow_stack \u2502 b94df4d2-5b65-4201-945a-61436c9c5384 \u2502        \u2502 default \u2502 cloud_artifact_store \u2502 cloud_orchestrator    \u2502 eks_seldon     \u2502 cloud_registry     \u2502 aws_secret_manager \u2503\n\u2520\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2528\n\u2503        \u2502 local_kubeflow_stack \u2502 8d9343ac-d405-43bd-ab9c-85637e479efe \u2502        \u2502 default \u2502 default              \u2502 kubeflow_orchestrator \u2502                \u2502 local_registry     \u2502                    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n```\n\nThe `zenml profile migrate` CLI command also provides command line flags for cases in which the user wants to overwrite existing components or stacks, or ignore errors.\n\n## Decou"}
{"input": "pling Stack Component configuration from implementation\n\nStack components can now be registered without having the required integrations installed. As part of this change, we split all existing stack component definitions into three classes: an implementation class that defines the logic of the stack component, a config class that defines the attributes and performs input validations, and a flavor class that links implementation and config classes together. See [**component flavor models #895**](https://github.com/zenml-io/zenml/pull/895) for more details.\n\nIf you are only using stack component flavors that are shipped with the zenml Python distribution, this change has no impact on the configuration of your existing stacks. However, if you are currently using custom stack component implementations, you will need to update them to the new format. See the [documentation on writing custom stack component flavors](../../how-to/stack-deployment/implement-a-custom-stack-component.md) for updated information on how to do this.\n\n## Shared ZenML Stacks and Stack Components\n\nWith collaboration being the key part of ZenML, the 0.20.0 release puts the concepts of Users in the front and center and introduces the possibility to share stacks and stack components with other users by means of the ZenML server.\n\nWhen your client is connected to a ZenML server, entities such as Stacks, Stack Components, Stack Component Flavors, Pipelines, Pipeline Runs, and artifacts are scoped to a Project and owned by the User that creates them. Only the objects that are owned by the current user used to authenticate to the ZenML server and that are part of the current project are available to the client.\n\nStacks and Stack Components can also be shared within the same project with other users. To share an object, either set it as shared during creation time (e.g. `zenml stack register mystack ... --share`) or afterwards (e.g. through `zenml stack share mystack`).\n\nTo differentiate between shared and private Stacks and Stack Components, these can now be addressed by name, id or the first few letters of the id in the cli. E.g. for a stack `default` with id `179ebd25-4c5b-480f-a47c-d4f04e0b6185` you can now run `zenml stack describe default` or `zenml stack describe 179` or `zenml stack describe 179ebd25-4c5b-480f-a47c-d4f04e0b6185`.\n\nWe also introduce the notion of `local` vs `non-local` stack components. Local stack components are stack components that are configured to run locally while non-local stack components are configured to run remotely or in a cloud environment. Consequently:\n\n* stacks made up of local stack components should not be shared on a central ZenML Server, even though this is not enforced by the system.\n* stacks made up of non-local stack components are"}
{"input": " only functional if they are shared through a remotely deployed ZenML Server.\n\nRead more about shared stacks in the new [starter guide](../../user-guide/starter-guide/stacks/managing-stacks.md#sharing-stacks-over-a-zenml-server).\n\n## Other changes\n\n### The `Repository` class is now called `Client`\n\nThe `Repository` object has been renamed to `Client` to better capture its functionality. You can continue to use the `Repository` object for backwards compatibility, but it will be removed in a future release.\n\n**How to migrate**: Rename all references to `Repository` in your code to `Client`.\n\n### The `BaseStepConfig` class is now called `BaseParameters`\n\nThe `BaseStepConfig` object has been renamed to `BaseParameters` to better capture its functionality. You can NOT continue to use the `BaseStepConfig`.\n\nThis is part of a broader configuration rehaul which is discussed next.\n\n**How to migrate**: Rename all references to `BaseStepConfig` in your code to `BaseParameters`.\n\n### Configuration Rework\n\nAlongside the architectural shift, Pipeline configuration has been completely rethought. This video gives an overview of how configuration has changed with ZenML in the post ZenML 0.20.0 world.\n\n{% embed url=\"https://www.youtube.com/embed/hI-UNV7uoNI\" %}\nConfiguring pipelines, steps, and stack components in ZenML\n{% endembed %}\n\n**What changed?**\n\nZenML pipelines and steps could previously be configured in many different ways:\n\n* On the `@pipeline` and `@step` decorators (e.g. the `requirements` variable)\n* In the `__init__` method of the pipeline and step class\n* Using `@enable_xxx` decorators, e.g. `@enable_mlflow`.\n* Using specialized methods like `pipeline.with_config(...)` or `step.with_return_materializer(...)`\n\nSome of the configuration options were quite hidden, difficult to access and not tracked in any way by the ZenML metadata store.\n\nWith ZenML 0.20.0, we introduce the `BaseSettings` class, a broad class that serves as a central object to represent all runtime configuration of a pipeline run (apart from the `BaseParameters`).\n\nPipelines and steps now allow all configurations on their decorators as well as the `.configure(...)` method. This includes configurations for stack components that are not infrastructure-related which was previously done using the `@enable_xxx` decorators). The same configurations can also be defined in a YAML file.\n\nRead more about this paradigm in the [new docs section about settings](../../how-to/use-configuration-files/what-can-be-configured.md).\n\nHere is a list of changes that are the most obvious in consequence of the above code. Please note that this list is not exhaustive, and if we have missed something let us know via [Slack](https://zenml.io/slack).\n\n"}
{"input": "**Deprecating the `enable_xxx` decorators**\n\nWith the above changes, we are deprecating the much-loved `enable_xxx` decorators, like `enable_mlflow` and `enable_wandb`.\n\n**How to migrate**: Simply remove the decorator and pass something like this instead to step directly:\n\n```python\n@step(\n    experiment_tracker=\"mlflow_stack_comp_name\",  # name of registered component\n    settings={  # settings of registered component\n        \"experiment_tracker.mlflow\": {  # this is `category`.`flavor`, so another example is `step_operator.spark`\n            \"experiment_name\": \"name\",\n            \"nested\": False\n        }\n    }\n)\n```\n\n**Deprecating `pipeline.with_config(...)`**\n\n**How to migrate**: Replaced with the new `pipeline.run(config_path=...)`.\n\n**Deprecating `step.with_return_materializer(...)`**\n\n**How to migrate**: Simply remove the `with_return_materializer` method and pass something like this instead to step directly:\n\n```python\n@step(\n  output_materializers=materializer_or_dict_of_materializers_mapped_to_outputs\n)\n```\n\n**`DockerConfiguration` is now renamed to `DockerSettings`**\n\n**How to migrate**: Rename `DockerConfiguration` to `DockerSettings` and instead of passing it in the decorator directly with `docker_configuration`, you can use:\n\n```python\nfrom zenml.config import DockerSettings\n\n@step(settings={\"docker\": DockerSettings(...)})\ndef my_step() -> None:\n  ...\n```\n\nWith this change, all stack components (e.g. Orchestrators and Step Operators) that accepted a `docker_parent_image` as part of its Stack Configuration should now pass it through the `DockerSettings` object.\n\nRead more [here](../../user-guide/starter-guide/production-fundamentals/containerization.md).\n\n**`ResourceConfiguration` is now renamed to `ResourceSettings`**\n\n**How to migrate**: Rename `ResourceConfiguration` to `ResourceSettings` and instead of passing it in the decorator directly with `resource_configuration`, you can use:\n\n```python\nfrom zenml.config import ResourceSettings\n\n@step(settings={\"resources\": ResourceSettings(...)})\ndef my_step() -> None:\n  ...\n```\n\n**Deprecating the `requirements` and `required_integrations` parameters**\n\nUsers used to be able to pass `requirements` and `required_integrations` directly in the `@pipeline` decorator, but now need to pass them through settings:\n\n**How to migrate**: Simply remove the parameters and use the `DockerSettings` instead\n\n```python\nfrom zenml.config import DockerSettings\n\n@step(settings={\"docker\": DockerSettings(requirements=[...], requirements_integrations=[...])})\ndef my_step() -> None:\n  ...\n```\n\nRead more [here](../../user-guide/starter-guide/production-fundamentals/containerization.md).\n\n**"}
{"input": "A new pipeline intermediate representation**\n\nAll the aforementioned configurations as well as additional information required to run a ZenML pipelines are now combined into an intermediate representation called `PipelineDeployment`. Instead of the user-facing `BaseStep` and `BasePipeline` classes, all the ZenML orchestrators and step operators now use this intermediate representation to run pipelines and steps.\n\n**How to migrate**: If you have written a [custom orchestrator](../../user-guide/component-gallery/orchestrators/custom.md) or [step operator](../../user-guide/component-gallery/step-operators/custom.md), then you should see the new base abstractions (seen in the links). You can adjust your stack component implementations accordingly.\n\n### `PipelineSpec` now uniquely defines pipelines\n\nOnce a pipeline has been executed, it is represented by a `PipelineSpec` that uniquely identifies it. Therefore, users are no longer able to edit a pipeline once it has been run once. There are now three options to get around this:\n\n* Pipeline runs can be created without being associated with a pipeline explicitly: We call these `unlisted` runs. Read more about unlisted runs [here](../../user-guide/starter-guide/pipelines/pipelines.md#unlisted-runs).\n* Pipelines can be deleted and created again.\n* Pipelines can be given unique names each time they are run to uniquely identify them.\n\n**How to migrate**: No code changes, but rather keep in mind the behavior (e.g. in a notebook setting) when quickly [iterating over pipelines as experiments](../../user-guide/starter-guide/pipelines/parameters-and-caching.md).\n\n### New post-execution workflow\n\nThe Post-execution workflow has changed as follows:\n\n* The `get_pipelines` and `get_pipeline` methods have been moved out of the `Repository` (i.e. the new `Client` ) class and lie directly in the post\\_execution module now. To use the user has to do:\n\n```python\nfrom zenml.post_execution import get_pipelines, get_pipeline\n```\n\n* New methods to directly get a run have been introduced: `get_run` and `get_unlisted_runs` method has been introduced to get unlisted runs.\n\nUsage remains largely similar. Please read the [new docs for post-execution](../../user-guide/starter-guide/pipelines/fetching-pipelines.md) to inform yourself of what further has changed.\n\n**How to migrate**: Replace all post-execution workflows from the paradigm of `Repository.get_pipelines` or `Repository.get_pipeline_run` to the corresponding post\\_execution methods.\n\n## \ud83d\udce1Future Changes\n\nWhile this rehaul is big and will break previous releases, we do have some more work left to do. However we also expect this to be the last big rehaul of ZenML before our 1.0.0 release, and no other release will be so hard breaking as this one. Currently planned future breaking changes are:\n\n* Following the metadata store,"}
{"input": " the secrets manager stack component might move out of the stack.\n* ZenML `StepContext` might be deprecated.\n\n## \ud83d\udc1e Reporting Bugs\n\nWhile we have tried our best to document everything that has changed, we realize that mistakes can be made and smaller changes overlooked. If this is the case, or you encounter a bug at any time, the ZenML core team and community are available around the clock on the growing [Slack community](https://zenml.io/slack).\n\nFor bug reports, please also consider submitting a [GitHub Issue](https://github.com/zenml-io/zenml/issues/new/choose).\n\nLastly, if the new changes have left you desiring a feature, then consider adding it to our [public feature voting board](https://zenml.io/discussion). Before doing so, do check what is already on there and consider upvoting the features you desire the most.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to migrate your ZenML code to the newest version.\n---\n\n# \u267b Migration guide\n\nMigrations are necessary for ZenML releases that include breaking changes, which are currently all releases that increment the minor version of the release, e.g., `0.X` -> `0.Y`. Furthermore, all releases that increment the first non-zero digit of the version contain major breaking changes or paradigm shifts that are explained in separate migration guides below.\n\n## Release Type Examples\n\n* `0.40.2` to `0.40.3` contains _no breaking changes_ and requires no migration whatsoever,\n* `0.40.3` to `0.41.0` contains _minor breaking changes_ that need to be taken into account when upgrading ZenML,\n* `0.39.1` to `0.40.0` contains _major breaking changes_ that introduce major shifts in how ZenML code is written or used.\n\n## Major Migration Guides\n\nThe following guides contain detailed instructions on how to migrate between ZenML versions that introduced major breaking changes or paradigm shifts. The migration guides are sequential, meaning if there is more than one migration guide between your current version and the latest release, follow each guide in order.\n\n* [Migration guide 0.13.2 \u2192 0.20.0](migration-zero-twenty.md)\n* [Migration guide 0.23.0 \u2192 0.30.0](migration-zero-thirty.md)\n* [Migration guide 0.39.1 \u2192 0.41.0](migration-zero-forty.md)\n* [Migration guide 0.58.2 \u2192 0.60.0](migration-zero-sixty.md)\n\n## Release Notes\n\nFor releases with minor breaking changes, e.g., `0.40.3` to `0.41.0`, check out the official [ZenML Release Notes](https://github.com/zenml-io/zenml/releases) to see which breaking changes were introduced.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to migrate your ZenML code to the newest version.\n---\n\n# \u267b Migration guide\n\nMigrations are necessary for ZenML releases that include breaking changes, which are currently all releases that increment the minor version of the release, e.g., `0.X` -> `0.Y`. Furthermore, all releases that increment the first non-zero digit of the version contain major breaking changes or paradigm shifts that are explained in separate migration guides below.\n\n## Release Type Examples\n\n* `0.40.2` to `0.40.3` contains _no breaking changes_ and requires no migration whatsoever,\n* `0.40.3` to `0.41.0` contains _minor breaking changes_ that need to be taken into account when upgrading ZenML,\n* `0.39.1` to `0.40.0` contains _major breaking changes_ that introduce major shifts in how ZenML code is written or used.\n\n## Major Migration Guides\n\nThe following guides contain detailed instructions on how to migrate between ZenML versions that introduced major breaking changes or paradigm shifts. The migration guides are sequential, meaning if there is more than one migration guide between your current version and the latest release, follow each guide in order.\n\n* [Migration guide 0.13.2 \u2192 0.20.0](migration-zero-twenty.md)\n* [Migration guide 0.23.0 \u2192 0.30.0](migration-zero-thirty.md)\n* [Migration guide 0.39.1 \u2192 0.41.0](migration-zero-forty.md)\n\n## Release Notes\n\nFor releases with minor breaking changes, e.g., `0.40.3` to `0.41.0`, check out the official [ZenML Release Notes](https://github.com/zenml-io/zenml/releases) to see which breaking changes were introduced.\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to migrate from ZenML 0.20.0-0.23.0 to 0.30.0-0.39.1.\n---\n\n{% hint style=\"warning\" %}\nMigrating to `0.30.0` performs non-reversible database changes so downgrading\nto `<=0.23.0` is not possible afterwards. If you are running on an older ZenML \nversion, please follow the \n[0.20.0 Migration Guide](migration-zero-twenty.md) first to prevent unexpected\ndatabase migration failures.\n{% endhint %}\n\nThe ZenML 0.30.0 release removed the `ml-pipelines-sdk` dependency in favor of\nnatively storing pipeline runs and artifacts in the ZenML database. The\ncorresponding database migration will happen automatically as soon as you run\nany `zenml ...` CLI command after installing the new ZenML version, e.g.:\n\n```bash\npip install zenml==0.30.0\nzenml version  # 0.30.0\n```\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
{"input": "---\ndescription: How to migrate from ZenML 0.58.2 to 0.60.0 (Pydantic 2 edition).\n---\n\n# Release Notes\n\nZenML now uses Pydantic v2. \ud83e\udd73\n\nThis upgrade comes with a set of critical updates. While your user experience\nmostly remains unaffected, you might see unexpected behavior due to the\nchanges in our dependencies. Moreover, since Pydantic v2 provides a slightly\nstricter validation process, you might end up bumping into some validation\nerrors which was not caught before, but it is all for the better \ud83d\ude42 If\nyou run into any other errors, please let us know either on\n[GitHub](https://github.com/zenml-io/zenml) or on\nour [Slack](https://zenml.io/slack-invite).\n\n## Changes in some of the critical dependencies\n\n- SQLModel is one of the core dependencies of ZenML and prior to this upgrade,\n  we were utilizing version `0.0.8`. However, this version is relatively\n  outdated and incompatible with Pydantic v2. Within the scope of this upgrade,\n  we upgraded it to `0.0.18`.\n- Due to the change in the SQLModel version, we also had to upgrade our\n  SQLAlchemy dependency from V1 to v2. While this does not affect the way\n  that you are using ZenML, if you are using SQLAlchemy in your environment,\n  you might have to migrate your code as well. For a detailed list of changes,\n  feel free to\n  check [their migration guide](https://docs.sqlalchemy.org/en/20/changelog/migration_20.html).\n\n## Changes in `pydantic`\n\nPydantic v2 brings a lot of new and exciting changes to the table. The core\nlogic now uses Rust and it is much faster and more efficient in terms of\nperformance. On top of it, the main concepts like model design, configuration,\nvalidation, or serialization now include a lot of new cool features. If you are\nusing `pydantic` in your workflow and are interested in the new changes, you can\ncheck [the brilliant migration guide](https://docs.pydantic.dev/2.7/migration/)\nprovided by the `pydantic` team to see the full list of changes.\n\n## Changes in our integrations changes\n\nMuch like ZenML, `pydantic` is an important dependency in many other Python\npackages. That\u2019s why conducting this upgrade helped us unlock a new version for\nseveral ZenML integration dependencies. Additionally, in some instances, we had\nto adapt the functionality of the integration to keep it compatible\nwith `pydantic`. So, if you are using any of these integrations, please go\nthrough the changes.\n\n### Airflow\n\nAs mentioned above upgrading our `pydantic` dependency meant we had to upgrade"}
{"input": "\nour `sqlmodel` dependency. Upgrading our `sqlmodel` dependency meant we had to\nupgrade our `sqlalchemy` dependency as well. Unfortunately, `apache-airflow`\nis still using `sqlalchemy` v1 and is incompatible with pydantic v2. As a\nsolution, we have removed the dependencies of the `airflow` integration. Now,\nyou can use ZenML to create your Airflow pipelines and use a separate\nenvironment to run them with Airflow. You can check the updated docs\n[right here](https://docs.zenml.io/stack-components/orchestrators/airflow).\n\n### AWS\n\nSome of our integrations now require `protobuf` 4. Since our\nprevious `sagemaker` version (`2.117.0`) did not support `protobof` 4, we could\nnot pair it with these new integrations. Thankfully `sagemaker` started\nsupporting `protobuf` 4 with version `2.172.0` and relaxing its dependency\nsolved the compatibility issue.\n\n### Evidently\n\nThe old version of our `evidently` integration was not compatible with Pydantic\nv2. They started supporting it starting from version `0.4.16`. As their latest\nversion is `0.4.22`, the new dependency of the integration is limited between\nthese two versions.\n\n### Feast\n\nOur previous implementation of the `feast` integration was not compatible with\nPydantic v2 due to the extra `redis` dependency we were using. This extra\ndependency is now removed and the `feast` integration is working as intended.\n\n### GCP\n\nThe previous version of the Kubeflow dependency (`kfp==1.8.22`) in our GCP\nintegration required Pydantic V1 to be installed. While we were upgrading our\nPydantic dependency, we saw this as an opportunity and wanted to use this chance\nto upgrade the `kfp` dependency to v2 (which has no dependencies on the Pydantic\nlibrary). This is why you may see some functional changes in the vertex step\noperator and orchestrator. If you would like to go through the changes in\nthe `kfp` library, you can\nfind [the migration guide here](https://www.kubeflow.org/docs/components/pipelines/v2/migration/).\n\n### Great Expectations\n\nGreat Expectations started supporting Pydantic v2 starting from\nversion `0.17.15` and they are closing in on their `1.0` release. Since this\nrelease might include a lot of big changes, we adjusted the dependency in our\nintegration to `great-expectations>=0.17.15,<1.0`. We will try to keep it\nupdated in the future once they release the `1.0` version\n\n### Kubeflow\n\nSimilar to"}
{"input": " the GCP integration, the previous version of the kubeflow\ndependency (`kfp==1.8.22`) in our `kubeflow` integration required Pydantic V1 to\nbe installed. While we were upgrading our Pydantic dependency, we saw this as an\nopportunity and wanted to use this chance to upgrade the `kfp` dependency to\nv2 (which has no dependencies on the Pydantic library). If you would like to go\nthrough the changes in the `kfp` library, you can\nfind [the migration guide here](https://www.kubeflow.org/docs/components/pipelines/v2/migration/). (\nWe also are considering adding an alternative version of this integration so our\nusers can keep using `kfp` V1 in their environment. Stay tuned for any updates.)\n\n### MLflow\n\n`mlflow` is compatible with both Pydantic V1 and v2. However, due to a known\nissue, if you install `zenml` first and then\ndo `zenml integration install mlflow -y`, it downgrades `pydantic` to V1. This\nis why we manually added the same duplicated `pydantic` requirement in the\nintegration definition as well. Keep in mind that the `mlflow` library is still\nusing some features of `pydantic` V1 which are deprecated. So, if the\nintegration is installed in your environment, you might run into some\ndeprecation warnings.\n\n### Label Studio\n\nWhile we were working on updating our `pydantic` dependency,\nthe `label-studio-sdk` has released its 1.0 version. In this new\nversion, `pydantic` v2 is also supported. The implementation and documentation\nof our Label Studio integration have been updated accordingly.\n\n### Skypilot\n\nWith the switch to `pydantic` v2, the implementation of our `skypilot`\nintegration mostly remained untouched. However, due to an incompatibility\nbetween the new version `pydantic` and the `azurecli`, the `skypilot[azure]`\nflavor can not be installed at the same time, thus our `skypilot_azure`\nintegration is currently deactivated. We are working on fixing this issue and if\nyou are using this integration in your workflows, we recommend staying on the\nprevious version of ZenML until we can solve this issue.\n\n### Tensorflow\n\nThe new version of `pydantic` creates a drift between `tensorflow`\nand  `typing_extensions`  packages and relaxing the dependencies here resolves\nthe issue. At the same time, the upgrade to `kfp` v2 (in integrations\nlike `kubeflow`, `tekton`, or `gcp`) bumps our `protobuf` dependency from `3.X`\nto `4.X`. To stay compatible with this requirement"}
{"input": ", the installed version\nof `tensorflow` needs to be `>=2.12.0`. While this change solves the dependency\nissues in most settings, we have bumped into some errors while\nusing `tensorflow` 2.12.0 on Python 3.8 on Ubuntu. If you would like to use this\nintegration, please consider using a higher Python version.\n\n### Tekton\n\nSimilar to the `gcp` and `kubeflow` integrations, the old version of\nour `tekton` integration was not compatible with `pydantic` V1 due to its `kfp`\ndependency. With the switch from `kfp` V1 to v2, we have adapted our\nimplementation to use the new version of `kfp` library and updated our\ndocumentation accordingly.\n\n{% hint style=\"warning\" %}\nDue to all aforementioned changes, when you upgrade ZenML to 0.60.0, you might \nrun into some dependency issues, especially if you were previously using an \nintegration which was not supporting Pydantic v2 before. In such cases, we \nhighly recommend setting up a fresh Python environment.\n{% endhint %}\n\n<figure><img src=\"https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc\" alt=\"ZenML Scarf\"><figcaption></figcaption></figure>\n"}
