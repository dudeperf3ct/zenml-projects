{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Personalized AI with Flux.1 and Stable Diffusion Image to Video: A Step-by-Step Guide\n",
    "\n",
    "Hey there, AI enthusiasts! 👋 Ready to dive into the wild world of personalized AI models? Buckle up, because we're about to embark on an epic journey to create a system that can handle thousands of personalized Flux.1 model finetunings like it's no big deal. We'll be using the awesome power of DreamBooth and some nifty open-source tools like ZenML to make this magic happen.\n",
    "\n",
    "By the time you're done with this notebook, you'll be slinging personalized AI models like a pro. Whether you're a seasoned ML wizard or a curious newbie, this guide will give you the superpowers you need to bring these ideas to life in your own mad scientist projects. Let's get this party started! 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Our Environment\n",
    "\n",
    "First things first, let's get our environment ready for some serious AI action. We'll import all the necessary libraries and set up our configuration classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ZenML server\n",
    "!zenml connect --url <your-zenml-server-url>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ZenML integrations\n",
    "!zenml stack set azure_temp_gpu\n",
    "!zenml integration install kubernetes azure -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do this to get the docker daemon running\n",
    "import os\n",
    "\n",
    "# Add Docker to the PATH\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/Applications/Docker.app/Contents/Resources/bin/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect our dataset\n",
    "\n",
    "Now that we've got our environment set up, let's create a function to load our training data. This bad boy will help us grab all those juicy image paths we'll use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "from zenml.utils import io_utils\n",
    "\n",
    "images_path = \"az://demo-zenmlartifactstore/hamza-faces\"\n",
    "images_dir_path = \"/tmp/hamza-faces/\"\n",
    "_ = Client().active_stack.artifact_store.path\n",
    "\n",
    "io_utils.copy_dir(\n",
    "    destination_dir=images_dir_path,\n",
    "    source_dir=images_path,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_image_gallery(\n",
    "    images_dir_path, sample_size=10, thumbnail_size=(200, 200)\n",
    "):\n",
    "    # Get all image files from the directory\n",
    "    image_files = [\n",
    "        f\n",
    "        for f in os.listdir(images_dir_path)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ]\n",
    "\n",
    "    # Sample the images\n",
    "    sampled_files = random.sample(\n",
    "        image_files, min(sample_size, len(image_files))\n",
    "    )\n",
    "\n",
    "    # Create thumbnail widgets\n",
    "    thumbnails = [\n",
    "        widgets.Image(\n",
    "            value=open(os.path.join(images_dir_path, img), \"rb\").read(),\n",
    "            format=img.split(\".\")[-1],\n",
    "            layout=widgets.Layout(\n",
    "                width=f\"{thumbnail_size[0]}px\",\n",
    "                height=f\"{thumbnail_size[1]}px\",\n",
    "                margin=\"5px\",\n",
    "            ),\n",
    "        )\n",
    "        for img in sampled_files\n",
    "    ]\n",
    "\n",
    "    # Create a grid of thumbnails\n",
    "    thumbnail_grid = widgets.GridBox(\n",
    "        thumbnails,\n",
    "        layout=widgets.Layout(\n",
    "            grid_template_columns=\"repeat(auto-fill, minmax(200px, 1fr))\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Display the gallery\n",
    "    display(\n",
    "        widgets.HTML(\n",
    "            f\"<h3>Displaying {len(sampled_files)} of {len(image_files)} images</h3>\"\n",
    "        )\n",
    "    )\n",
    "    display(thumbnail_grid)\n",
    "\n",
    "\n",
    "# Usage\n",
    "display_image_gallery(images_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Our Model Like a Boss\n",
    "\n",
    "Alright, now we're getting to the good stuff. Let's set up our model training step. This is where the magic happens, folks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from train_dreambooth_lora_flux import main as dreambooth_main\n",
    "from zenml import step\n",
    "from zenml.client import Client\n",
    "from zenml.integrations.huggingface.steps import run_with_accelerate\n",
    "from zenml.utils import io_utils\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "@run_with_accelerate(\n",
    "    num_processes=1, multi_gpu=False, mixed_precision=\"bf16\"\n",
    ")  # Adjust num_processes as needed\n",
    "@step\n",
    "def train_model(\n",
    "    images_path: str,\n",
    "    instance_name: str,\n",
    "    class_name: str,\n",
    "    model_name: str,\n",
    "    hf_repo_suffix: str,\n",
    "    prefix: str,\n",
    "    resolution: int,\n",
    "    train_batch_size: int,\n",
    "    rank: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "    learning_rate: float,\n",
    "    lr_scheduler: str,\n",
    "    lr_warmup_steps: int,\n",
    "    max_train_steps: int,\n",
    "    push_to_hub: bool,\n",
    "    checkpointing_steps: int,\n",
    "    seed: int,\n",
    ") -> None:\n",
    "\n",
    "    images_dir_path = \"/tmp/hamza-faces/\"\n",
    "    _ = Client().active_stack.artifact_store.path\n",
    "\n",
    "    io_utils.copy_dir(\n",
    "        destination_dir=images_dir_path,\n",
    "        source_dir=images_path,\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    instance_phrase = f\"{instance_name} the {class_name}\"\n",
    "    instance_prompt = f\"{prefix} {instance_phrase}\".strip()\n",
    "\n",
    "    # Create an ArgumentParser-like object to mimic the args in the original script\n",
    "    class Args:\n",
    "        def __init__(self, **kwargs):\n",
    "            self.mixed_precision = kwargs.get(\"mixed_precision\", \"bf16\")\n",
    "            self.pretrained_model_name_or_path = kwargs.get(\n",
    "                \"pretrained_model_name_or_path\"\n",
    "            )\n",
    "            self.revision = kwargs.get(\"revision\", None)\n",
    "            self.variant = kwargs.get(\"variant\", None)\n",
    "            self.dataset_name = kwargs.get(\"dataset_name\", None)\n",
    "            self.dataset_config_name = kwargs.get(\"dataset_config_name\", None)\n",
    "            self.instance_data_dir = kwargs.get(\"instance_data_dir\")\n",
    "            self.cache_dir = kwargs.get(\"cache_dir\", None)\n",
    "            self.image_column = kwargs.get(\"image_column\", \"image\")\n",
    "            self.caption_column = kwargs.get(\"caption_column\", None)\n",
    "            self.repeats = kwargs.get(\"repeats\", 1)\n",
    "            self.class_data_dir = kwargs.get(\"class_data_dir\", None)\n",
    "            self.output_dir = kwargs.get(\"output_dir\")\n",
    "            self.instance_prompt = kwargs.get(\"instance_prompt\")\n",
    "            self.class_prompt = kwargs.get(\"class_prompt\", None)\n",
    "            self.max_sequence_length = kwargs.get(\"max_sequence_length\", 512)\n",
    "            self.validation_prompt = kwargs.get(\"validation_prompt\", None)\n",
    "            self.num_validation_images = kwargs.get(\"num_validation_images\", 4)\n",
    "            self.validation_epochs = kwargs.get(\"validation_epochs\", 50)\n",
    "            self.rank = kwargs.get(\"rank\", 4)\n",
    "            self.with_prior_preservation = kwargs.get(\n",
    "                \"with_prior_preservation\", False\n",
    "            )\n",
    "            self.prior_loss_weight = kwargs.get(\"prior_loss_weight\", 1.0)\n",
    "            self.num_class_images = kwargs.get(\"num_class_images\", 100)\n",
    "            self.seed = kwargs.get(\"seed\", None)\n",
    "            self.resolution = kwargs.get(\"resolution\", 512)\n",
    "            self.center_crop = kwargs.get(\"center_crop\", False)\n",
    "            self.random_flip = kwargs.get(\"random_flip\", False)\n",
    "            self.train_text_encoder = kwargs.get(\"train_text_encoder\", False)\n",
    "            self.train_batch_size = kwargs.get(\"train_batch_size\", 4)\n",
    "            self.sample_batch_size = kwargs.get(\"sample_batch_size\", 4)\n",
    "            self.num_train_epochs = kwargs.get(\"num_train_epochs\", 1)\n",
    "            self.max_train_steps = kwargs.get(\"max_train_steps\", None)\n",
    "            self.checkpointing_steps = kwargs.get(\"checkpointing_steps\", 500)\n",
    "            self.checkpoints_total_limit = kwargs.get(\n",
    "                \"checkpoints_total_limit\", None\n",
    "            )\n",
    "            self.resume_from_checkpoint = kwargs.get(\n",
    "                \"resume_from_checkpoint\", None\n",
    "            )\n",
    "            self.gradient_accumulation_steps = kwargs.get(\n",
    "                \"gradient_accumulation_steps\", 1\n",
    "            )\n",
    "            self.gradient_checkpointing = kwargs.get(\n",
    "                \"gradient_checkpointing\", False\n",
    "            )\n",
    "            self.learning_rate = kwargs.get(\"learning_rate\", 1e-4)\n",
    "            self.guidance_scale = kwargs.get(\"guidance_scale\", 3.5)\n",
    "            self.text_encoder_lr = kwargs.get(\"text_encoder_lr\", 5e-6)\n",
    "            self.scale_lr = kwargs.get(\"scale_lr\", False)\n",
    "            self.lr_scheduler = kwargs.get(\"lr_scheduler\", \"constant\")\n",
    "            self.lr_warmup_steps = kwargs.get(\"lr_warmup_steps\", 500)\n",
    "            self.lr_num_cycles = kwargs.get(\"lr_num_cycles\", 1)\n",
    "            self.lr_power = kwargs.get(\"lr_power\", 1.0)\n",
    "            self.dataloader_num_workers = kwargs.get(\n",
    "                \"dataloader_num_workers\", 0\n",
    "            )\n",
    "            self.weighting_scheme = kwargs.get(\"weighting_scheme\", \"none\")\n",
    "            self.logit_mean = kwargs.get(\"logit_mean\", 0.0)\n",
    "            self.logit_std = kwargs.get(\"logit_std\", 1.0)\n",
    "            self.mode_scale = kwargs.get(\"mode_scale\", 1.29)\n",
    "            self.optimizer = kwargs.get(\"optimizer\", \"AdamW\")\n",
    "            self.use_8bit_adam = kwargs.get(\"use_8bit_adam\", False)\n",
    "            self.adam_beta1 = kwargs.get(\"adam_beta1\", 0.9)\n",
    "            self.adam_beta2 = kwargs.get(\"adam_beta2\", 0.999)\n",
    "            self.prodigy_beta3 = kwargs.get(\"prodigy_beta3\", None)\n",
    "            self.prodigy_decouple = kwargs.get(\"prodigy_decouple\", True)\n",
    "            self.adam_weight_decay = kwargs.get(\"adam_weight_decay\", 1e-04)\n",
    "            self.adam_weight_decay_text_encoder = kwargs.get(\n",
    "                \"adam_weight_decay_text_encoder\", 1e-03\n",
    "            )\n",
    "            self.adam_epsilon = kwargs.get(\"adam_epsilon\", 1e-08)\n",
    "            self.prodigy_use_bias_correction = kwargs.get(\n",
    "                \"prodigy_use_bias_correction\", True\n",
    "            )\n",
    "            self.prodigy_safeguard_warmup = kwargs.get(\n",
    "                \"prodigy_safeguard_warmup\", True\n",
    "            )\n",
    "            self.max_grad_norm = kwargs.get(\"max_grad_norm\", 1.0)\n",
    "            self.push_to_hub = kwargs.get(\"push_to_hub\", False)\n",
    "            self.hub_token = kwargs.get(\"hub_token\", None)\n",
    "            self.hub_model_id = kwargs.get(\"hub_model_id\", None)\n",
    "            self.logging_dir = kwargs.get(\"logging_dir\", \"logs\")\n",
    "            self.allow_tf32 = kwargs.get(\"allow_tf32\", False)\n",
    "            self.report_to = kwargs.get(\"report_to\", \"tensorboard\")\n",
    "            self.local_rank = kwargs.get(\"local_rank\", -1)\n",
    "            self.prior_generation_precision = kwargs.get(\n",
    "                \"prior_generation_precision\", None\n",
    "            )\n",
    "\n",
    "    # Usage example:\n",
    "    args = Args(\n",
    "        mixed_precision=\"bf16\",\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "        instance_data_dir=images_dir_path,\n",
    "        output_dir=hf_repo_suffix,\n",
    "        instance_prompt=instance_prompt,\n",
    "        resolution=resolution,\n",
    "        train_batch_size=train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        rank=rank,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lr_warmup_steps=lr_warmup_steps,\n",
    "        max_train_steps=max_train_steps,\n",
    "        checkpointing_steps=checkpointing_steps,\n",
    "        seed=seed,\n",
    "        push_to_hub=push_to_hub if push_to_hub else False,\n",
    "    )\n",
    "\n",
    "    # Run the main function with the created args\n",
    "    print(\"Launching dreambooth training script\")\n",
    "    dreambooth_main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Inference - Let's See What We've Created!\n",
    "\n",
    "Now that we've trained our model, it's time to put it to the test. Let's set up a batch inference step to generate some cool images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from PIL import Image as PILImage\n",
    "from zenml import step\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "@step\n",
    "def batch_inference(\n",
    "    hf_username: str,\n",
    "    hf_repo_suffix: str,\n",
    "    instance_name: str,\n",
    "    class_name: str,\n",
    ") -> PILImage.Image:\n",
    "    model_path = f\"{hf_username}/{hf_repo_suffix}\"\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    pipe.load_lora_weights(\n",
    "        model_path, weight_name=\"pytorch_lora_weights.safetensors\"\n",
    "    )\n",
    "\n",
    "    instance_phrase = f\"{instance_name} the {class_name}\"\n",
    "    prompts = [\n",
    "        f\"A portrait photo of {instance_phrase} in a Superman pose\",\n",
    "        f\"A portrait photo of {instance_phrase} flying like Superman\",\n",
    "        f\"A portrait photo of {instance_phrase} standing like Superman\",\n",
    "        f\"A portrait photo of {instance_phrase} as a football player in an action pose\",\n",
    "        f\"A portrait photo of {instance_phrase} as a firefighter in a heroic stance\",\n",
    "        f\"A portrait photo of {instance_phrase} in a spacesuit in space\",\n",
    "        f\"A portrait photo of {instance_phrase} on the Moon\",\n",
    "        f\"A portrait photo of {instance_phrase} as an astronaut working on a satellite\",\n",
    "        f\"A portrait photo of {instance_phrase} as an astronaut looking out a spacecraft window\",\n",
    "        f\"A portrait photo of {instance_phrase} as an astronaut on a spacewalk\",\n",
    "        f\"A portrait photo of {instance_phrase} in a heroic Superman pose\",\n",
    "        f\"A portrait photo of {instance_phrase} as an astronaut on Mars\",\n",
    "        f\"A portrait photo of {instance_phrase} flying like Superman\",\n",
    "        f\"A portrait photo of {instance_phrase} as an astronaut floating in zero gravity\",\n",
    "        f\"A portrait photo of {instance_phrase} as a superhero in a powerful stance\",\n",
    "    ]\n",
    "\n",
    "    images = pipe(\n",
    "        prompt=prompts,\n",
    "        num_inference_steps=35,\n",
    "        guidance_scale=8.5,\n",
    "        height=256,\n",
    "        width=256,\n",
    "    ).images\n",
    "\n",
    "    width, height = images[0].size\n",
    "    rows, cols = 3, 5\n",
    "    gallery_img = PILImage.new(\"RGB\", (width * cols, height * rows))\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        gallery_img.paste(image, ((i % cols) * width, (i // cols) * height))\n",
    "\n",
    "    return gallery_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: From Still to Motion - Let's Make Some Video Magic!\n",
    "\n",
    "Why stop at images when we can create videos? Let's add a step to turn our generated image into a short video clip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import Annotated, Tuple, List\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image, StableVideoDiffusionPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "from PIL import Image as PILImage\n",
    "from zenml import step\n",
    "from zenml.types import HTMLString\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def get_optimal_size(\n",
    "    image: PILImage.Image, max_size: int = 1024\n",
    ") -> Tuple[int, int]:\n",
    "    width, height = image.size\n",
    "    aspect_ratio = width / height\n",
    "    if width > height:\n",
    "        new_width = min(width, max_size)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = min(height, max_size)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    return (new_width, new_height)\n",
    "\n",
    "\n",
    "@step\n",
    "def image_to_video(\n",
    "    hf_username: str,\n",
    "    hf_repo_suffix: str,\n",
    "    instance_name: str,\n",
    ") -> Tuple[\n",
    "    Annotated[List[PILImage.Image], \"generated_images\"],\n",
    "    Annotated[List[bytes], \"video_data_list\"],\n",
    "    Annotated[HTMLString, \"video_html\"],\n",
    "]:\n",
    "\n",
    "    model_path = f\"{hf_username}/{hf_repo_suffix}\"\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    pipe.load_lora_weights(\n",
    "        model_path, weight_name=\"pytorch_lora_weights.safetensors\"\n",
    "    )\n",
    "\n",
    "    instance_phrase = f\"{instance_name} the man\"\n",
    "    prompts = [\n",
    "        f\"A portrait photo of {instance_phrase} in a Superman pose\",\n",
    "        f\"A portrait photo of {instance_phrase} flying like Superman\",\n",
    "        f\"A portrait photo of {instance_phrase} standing like Superman\",\n",
    "        f\"A portrait photo of {instance_phrase} as a football player in an action pose\",\n",
    "        f\"A portrait photo of {instance_phrase} as a firefighter in a heroic stance\",\n",
    "        f\"A portrait photo of {instance_phrase} in a spacesuit in space\",\n",
    "        f\"A portrait photo of {instance_phrase} on the Moon\",\n",
    "    ]\n",
    "\n",
    "    images = pipe(\n",
    "        prompt=prompts,\n",
    "        num_inference_steps=40,\n",
    "        guidance_scale=8.5,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    ).images\n",
    "\n",
    "    video_pipeline = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    video_pipeline.enable_model_cpu_offload()\n",
    "\n",
    "    video_data_list = []\n",
    "    for i, image in enumerate(images):\n",
    "        frames = video_pipeline(\n",
    "            image,\n",
    "            num_inference_steps=80,\n",
    "            generator=torch.manual_seed(77),\n",
    "            height=512,\n",
    "            width=512,\n",
    "        ).frames[0]\n",
    "\n",
    "        output_file = f\"generated_video_{i}.mp4\"\n",
    "        export_to_video(frames, output_file, fps=5)\n",
    "\n",
    "        with open(output_file, \"rb\") as file:\n",
    "            video_data = file.read()\n",
    "            video_data_list.append(video_data)\n",
    "\n",
    "    html_visualization_str = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div style=\"display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; padding: 0;\">\n",
    "    \"\"\"\n",
    "    for i, video_data in enumerate(video_data_list):\n",
    "        html_visualization_str += f\"\"\"\n",
    "            <div style=\"margin-bottom: 20px;\">\n",
    "                <video width=\"512\" height=\"512\" controls autoplay loop>\n",
    "                    <source src=\"data:video/mp4;base64,{base64.b64encode(video_data).decode()}\" type=\"video/mp4\">\n",
    "                    Your browser does not support the video tag.\n",
    "                </video>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    html_visualization_str += \"\"\"\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return (images, video_data_list, HTMLString(html_visualization_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Putting It All Together - Our Dreambooth Pipeline\n",
    "\n",
    "Now for the grand finale - let's string all these awesome steps together into one epic pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from zenml import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def dreambooth_pipeline(\n",
    "    instance_example_dir: str = \"az://demo-zenmlartifactstore/hamza-faces\",\n",
    "    instance_name: str = \"sks htahir1\",\n",
    "    class_name: str = \"man\",\n",
    "    model_name: str = \"black-forest-labs/FLUX.1-dev\",\n",
    "    hf_username: str = \"strickvl\",\n",
    "    hf_repo_suffix: str = \"flux-dreambooth-hamza\",\n",
    "    prefix: str = \"A portrait photo of\",\n",
    "    resolution: int = 512,\n",
    "    train_batch_size: int = 1,\n",
    "    rank: int = 32,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    learning_rate: float = 0.0002,\n",
    "    lr_scheduler: str = \"constant\",\n",
    "    lr_warmup_steps: int = 0,\n",
    "    max_train_steps: int = 1300,\n",
    "    push_to_hub: bool = True,\n",
    "    checkpointing_steps: int = 1000,\n",
    "    seed: int = 117,\n",
    "):\n",
    "    train_model(\n",
    "        instance_example_dir,\n",
    "        instance_name=instance_name,\n",
    "        class_name=class_name,\n",
    "        model_name=model_name,\n",
    "        hf_repo_suffix=hf_repo_suffix,\n",
    "        prefix=prefix,\n",
    "        resolution=resolution,\n",
    "        train_batch_size=train_batch_size,\n",
    "        rank=rank,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lr_warmup_steps=lr_warmup_steps,\n",
    "        max_train_steps=max_train_steps,\n",
    "        push_to_hub=push_to_hub,\n",
    "        checkpointing_steps=checkpointing_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "    batch_inference(\n",
    "        hf_username,\n",
    "        hf_repo_suffix,\n",
    "        instance_name,\n",
    "        class_name,\n",
    "        after=\"train_model\",\n",
    "    )\n",
    "    image_to_video(\n",
    "        hf_username, hf_repo_suffix, instance_name, after=\"batch_inference\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Dreambooth pipeline assembled and ready for action! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Launch the Pipeline and Watch the Magic Happen!\n",
    "\n",
    "Alright, folks, this is it - the moment of truth! Let's fire up our pipeline and see this baby in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreambooth_pipeline.with_options(config_path=\"configs/k8s_run_refactored_multi_video.yaml\")()\n",
    "\n",
    "print(\"Pipeline launched! Sit back, relax, and prepare to be amazed! 🍿\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it, folks! You've just built and launched a kickass pipeline for personalized AI model generation. From loading data to training models, from generating images to creating videos - you've done it all!\n",
    "\n",
    "Remember, this is just the beginning. Feel free to tweak, adjust, and experiment with the parameters to see what kind of magic you can create. The AI world is your oyster, and you've got the tools to make some serious pearls!\n",
    "\n",
    "Happy coding, and may your models be ever accurate and your latency low! 🚀🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
